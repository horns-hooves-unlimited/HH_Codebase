{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CI FACTOR DAILY GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULES IMPORT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import os ### To work with csv files - should be ignored in product code\n",
    "\n",
    "### To test parallel calculations:\n",
    "from joblib import Parallel, delayed\n",
    "### To profile only:\n",
    "%load_ext line_profiler\n",
    "import gc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  0.25.3\n",
      "python version:  3.7.4\n"
     ]
    }
   ],
   "source": [
    "## VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INTERNAL PARAMETERS INITIALIZATION (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "### Common constants:\n",
    "All = slice(None)\n",
    "\n",
    "### Results saving:\n",
    "str_ci_var1_raw_csv = 'Data_Files/Test_Files/ci_var1_raw.csv'\n",
    "str_ci_rbq_raw_csv = 'Data_Files/Test_Files/ci_rbq_raw.csv'\n",
    "str_ci_beta_nn_raw_csv = 'Data_Files/Test_Files/ci_beta_nn_raw.csv'\n",
    "### General daily-mode ranges parameters:\n",
    "str_date_source_start = '2002-01-01' ### Start date for ISON Universe\n",
    "str_date_factor_start = '2004-12-31' ### End date for efficacy measures\n",
    "str_date_factor_end = '2021-11-30' ### End date for efficacy measures\n",
    "idx_test_monthly_range = pd.date_range(str_date_factor_start, str_date_factor_end, freq = 'BM') ### Range for source data filtering\n",
    "idx_test_daily_range = pd.date_range(str_date_factor_start, str_date_factor_end, freq = 'B') ### Range for source data filtering\n",
    "### Sample data source:\n",
    "str_path_returns = 'Data_Files/Source_Files/msci_sample.xlsx'\n",
    "str_sheet_returns = 'Test Returns'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES\n",
    "\n",
    "def multistep_standartize(ser_data_source, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):  \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_data_source.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_data_source.dropna()\n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        if reuse_outliers:\n",
    "            ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        else:\n",
    "            ### Saving to result and excluding from further calculations truncated values:             \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)      \n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):\n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_data_source.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MATLAB STYLE PRCTILE\n",
    "\n",
    "def prctile_matlab(ser_to_perc, p):\n",
    "    ### Sorted list preparing:\n",
    "    list_sorted = sorted(ser_to_perc.dropna().values)\n",
    "    ### Length calculating:\n",
    "    num_len = len(list_sorted)    \n",
    "    ### Prctile calculating:\n",
    "    num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING LOG BASED MOVING AVERAGE FUNCTION\n",
    "\n",
    "def log_based_ma(ser_source_raw, int_roll_max, int_roll_min):\n",
    "    ### Drop level to avoid stack/unstack manipulations:\n",
    "    if (len(ser_source_raw.index.names) == 2):\n",
    "        ser_source_raw = ser_source_raw.droplevel(-1)\n",
    "    ### Log bsed moving average calculating:\n",
    "#    ser_source_res = ser_source_raw.rolling(window = int_roll_max, min_periods = int_roll_min)\\\n",
    "#                                   .apply(lambda ser_roll_win: np.log(1 + ser_roll_win).mean(), raw = False) ### REPLACED !!!!!!!!!!!!!!!!   \n",
    "    ser_source_res = ser_source_raw.rolling(window = int_roll_max, min_periods = int_roll_min)\\\n",
    "                                   .apply(lambda arr_roll_win: np.nanmean(np.log(1 + arr_roll_win)), raw = True) ### ADDED !!!!!!!!!!!!!!!!\n",
    "    return ser_source_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING STANDARTIZE -> RECOVER -> NEGATIVE RETURNS CLIP TRANSFORMATION\n",
    "\n",
    "def get_clean_returns(ser_source_raw, idx_source_range):\n",
    "#def get_country_vector(ser_source_raw):    \n",
    "    ### Drop level to avoid stack/unstack manipulations:\n",
    "    if (len(ser_source_raw.index.names) == 2):       \n",
    "        ser_source_raw = ser_source_raw.droplevel(-1)        \n",
    "    ### Source vector cleaning:\n",
    "    ser_source_raw = ser_source_raw.replace([np.inf, -np.inf], np.NaN).replace(0.0, np.NaN)\n",
    "    ### Source vector standartizing:\n",
    "    ser_source_std, list_mean, list_std = multistep_standartize(ser_source_raw, [7.5, 6.0], full_result = True)\n",
    "    ### Source vector recovering:\n",
    "    ser_source_rec = (ser_source_std * list_std[1] + list_mean[1]) * list_std[0] + list_mean[0]\n",
    "\n",
    "#    ### Source vector standartizing:\n",
    "#    ser_source_std, list_mean, list_std = multistep_standartize(ser_source_raw, [7.5], full_result = True)\n",
    "#    ### Source vector recovering:\n",
    "#    ser_source_rec = ser_source_std * list_std[0] + list_mean[0]\n",
    "\n",
    "#    ser_source_rec = ser_source_raw + 0\n",
    "\n",
    "    ser_source_rec.loc[ser_source_rec < -1.0] = -0.9\n",
    "    ### Source vector to rates:\n",
    "    ser_source_rate = (1 + ser_source_rec.fillna(0.0)).cumprod()\n",
    "    ### Rates vector reindexing:\n",
    "#    idx_source_range = pd.date_range(start = ser_source_rec.index[0], end = ser_source_rec.index[-1], freq = 'B') ### DROPPED !!!!!!!!!!!!!!!!!!\n",
    "    ser_source_rate = ser_source_rate.reindex(idx_source_range)\n",
    "    ### Rates vector forward filling:\n",
    "    ser_source_rate = ser_source_rate.ffill()\n",
    "    ### Rates vector to returns:\n",
    "    ser_source_res = ser_source_rate.diff() / ser_source_rate.shift()\n",
    "    ser_source_res = ser_source_res.replace(0.0, np.NaN)\n",
    "    ### Result output:    \n",
    "    ser_source_res.index.names = ['Date']\n",
    "    ser_source_res.name = 'Returns'\n",
    "    return ser_source_res\n",
    "\n",
    "def get_clean_returns_to_parallel(ser_source_raw, idx_source_range):\n",
    "#def get_country_vector(ser_source_raw):    \n",
    "    ### Drop level to avoid stack/unstack manipulations:\n",
    "    bool_country = False\n",
    "    if (len(ser_source_raw.index.names) == 2):\n",
    "        bool_country = True\n",
    "        str_country = ser_source_raw.index[0][-1]\n",
    "        ser_source_raw = ser_source_raw.droplevel(-1)\n",
    "        \n",
    "    ### Source vector cleaning:\n",
    "    ser_source_raw = ser_source_raw.replace([np.inf, -np.inf], np.NaN).replace(0.0, np.NaN)\n",
    "    ### Source vector standartizing:\n",
    "    ser_source_std, list_mean, list_std = multistep_standartize(ser_source_raw, [7.5, 6.0], full_result = True)\n",
    "    ### Source vector recovering:\n",
    "    ser_source_rec = (ser_source_std * list_std[1] + list_mean[1]) * list_std[0] + list_mean[0]\n",
    "\n",
    "#    ### Source vector standartizing:\n",
    "#    ser_source_std, list_mean, list_std = multistep_standartize(ser_source_raw, [7.5], full_result = True)\n",
    "#    ### Source vector recovering:\n",
    "#    ser_source_rec = ser_source_std * list_std[0] + list_mean[0]\n",
    "\n",
    "#    ser_source_rec = ser_source_raw + 0\n",
    "\n",
    "    ser_source_rec.loc[ser_source_rec < -1.0] = -0.9\n",
    "    ### Source vector to rates:\n",
    "    ser_source_rate = (1 + ser_source_rec.fillna(0.0)).cumprod()\n",
    "    ### Rates vector reindexing:\n",
    "#    idx_source_range = pd.date_range(start = ser_source_rec.index[0], end = ser_source_rec.index[-1], freq = 'B') ### DROPPED !!!!!!!!!!!!!!!!!!\n",
    "    ser_source_rate = ser_source_rate.reindex(idx_source_range)\n",
    "    ### Rates vector forward filling:\n",
    "    ser_source_rate = ser_source_rate.ffill()\n",
    "    ### Rates vector to returns:\n",
    "    ser_source_res = ser_source_rate.diff() / ser_source_rate.shift()\n",
    "    ser_source_res = ser_source_res.replace(0.0, np.NaN)\n",
    "    ### Result output:    \n",
    "    ser_source_res.index.names = ['Date']\n",
    "    ser_source_res.name = 'Returns'\n",
    "    if bool_country:\n",
    "        ser_source_res = pd.concat({str_country: ser_source_res}, names = ['Country'])  \n",
    "    return ser_source_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING PARALLEL PRCESSING DECORATORS\n",
    "\n",
    "def transformParallel(serGrouped, func):\n",
    "    retLst = Parallel(n_jobs = 4)(delayed(func)(group) for name, group in serGrouped)    \n",
    "    return pd.concat(retLst)\n",
    "\n",
    "def transformParallel_with_params(serGrouped, func, *args):\n",
    "    retLst = Parallel(n_jobs = 4)(delayed(func)(group, *args) for name, group in serGrouped)    \n",
    "    return pd.concat(retLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING VALUE-AT-RISK FACTOR CREATING FUNCTION\n",
    "\n",
    "def get_var1_factor(iter_date):\n",
    "    ### Factor parameters:\n",
    "    int_stand_win = 260 * 5\n",
    "    int_factor_win = 260 * 1\n",
    "    flo_pctile = 0.01\n",
    "    ### Source load parameters:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1)\n",
    "    date_factor_start = iter_date - pd.tseries.offsets.BDay(int_factor_win - 1)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    ser_iter_asset_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, All]\n",
    "    idx_iter_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Data source transforming:\n",
    "#    ser_iter_asset_trans = ser_iter_asset_raw.groupby('Country').apply(get_clean_returns, idx_iter_range).swaplevel().sort_index()\n",
    "    ser_iter_asset_trans = transformParallel_with_params(ser_iter_asset_raw.groupby('Country'), get_clean_returns_to_parallel, idx_iter_range).swaplevel().sort_index()  \n",
    "    ### Last year extracting:\n",
    "    ser_iter_asset_cut = ser_iter_asset_trans.loc[date_factor_start : iter_date, All]\n",
    "    ### Value-at-risk factor calculation (with minimum observations number check):\n",
    "    ser_iter_factor = ser_iter_asset_cut.groupby('Country').apply(lambda ser_country: prctile_matlab(ser_country, flo_pctile) \\\n",
    "                                                                  if (ser_country.count() > (int_factor_win // 2)) else np.NaN)\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_ci_var1_raw_csv, mode = 'a', header = not os.path.exists(str_ci_var1_raw_csv), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING RETURN BASED QUALITY FACTOR CREATING FUNCTION\n",
    "\n",
    "def get_rbq_factor(iter_date):\n",
    "    ### Factor parameters:\n",
    "    int_stand_win = 260 * 5\n",
    "    int_factor_win = 260 * 1\n",
    "    int_roll_max = 22\n",
    "    int_roll_min = 10    \n",
    "    ### Source load parameters:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1)\n",
    "    date_factor_start = iter_date - pd.tseries.offsets.BDay(int_factor_win - 1)\n",
    "    date_ma_start = iter_date - pd.tseries.offsets.BDay(int_factor_win + int_roll_max - 1)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    ser_iter_asset_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, All]\n",
    "    ser_iter_bench_raw = ser_bench_ret_raw.loc[date_stand_start : iter_date]\n",
    "    idx_iter_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Data source transforming:\n",
    "#    ser_iter_asset_trans = ser_iter_asset_raw.groupby('Country').apply(get_clean_returns, idx_iter_range).swaplevel().sort_index()\n",
    "    ser_iter_asset_trans = transformParallel_with_params(ser_iter_asset_raw.groupby('Country'), get_clean_returns_to_parallel, idx_iter_range).swaplevel().sort_index()    \n",
    "    ser_iter_bench_trans = get_clean_returns(ser_iter_bench_raw, idx_iter_range)\n",
    "    ### Moving average calculating:\n",
    "    ser_iter_asset_ma = ser_iter_asset_trans.loc[date_ma_start : iter_date, All].groupby('Country').apply(log_based_ma, int_roll_max, int_roll_min).swaplevel().sort_index()\n",
    "    ser_iter_bench_ma = log_based_ma(ser_iter_bench_trans[date_ma_start : iter_date], int_roll_max, int_roll_min)\n",
    "    ### Last year extracting:\n",
    "    ser_iter_asset_ma_cut = ser_iter_asset_ma.loc[date_factor_start : iter_date, All]\n",
    "    ser_iter_bench_ma_cut = ser_iter_bench_ma.loc[date_factor_start : iter_date]    \n",
    "    ### Benchmark quality factor calculation (with minimum observations number check):\n",
    "    date_idxmin = ser_iter_bench_ma_cut[::-1].idxmin()\n",
    "    ser_iter_factor = ser_iter_asset_ma_cut.groupby('Country')\\\n",
    "                                    .apply(lambda ser_country: ser_country.droplevel('Country')[date_idxmin] if (ser_country.count() > (int_factor_win // 2)) else np.NaN)\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_ci_rbq_raw_csv, mode = 'a', header = not os.path.exists(str_ci_rbq_raw_csv), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING NEGATIVE/NEGATIVE BETA FACTOR CREATING FUNCTION\n",
    "\n",
    "def get_beta_nn_factor(iter_date):\n",
    "    ### Factor parameters:\n",
    "    int_stand_win = 260 * 5\n",
    "    int_factor_win = 260 * 1\n",
    "    ### Source load parameters:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1)\n",
    "    date_factor_start = iter_date - pd.tseries.offsets.BDay(int_factor_win - 1)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    ser_iter_asset_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, All]\n",
    "    ser_iter_bench_raw = ser_bench_ret_raw.loc[date_stand_start : iter_date]\n",
    "    idx_iter_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Data source transforming:\n",
    "#    ser_iter_asset_trans = ser_iter_asset_raw.groupby('Country').apply(get_clean_returns, idx_iter_range).swaplevel().sort_index()\n",
    "    ser_iter_asset_trans = transformParallel_with_params(ser_iter_asset_raw.groupby('Country'), get_clean_returns_to_parallel, idx_iter_range).swaplevel().sort_index()    \n",
    "    ser_iter_bench_trans = get_clean_returns(ser_iter_bench_raw, idx_iter_range)\n",
    "    ### Last year extracting:\n",
    "    ser_iter_asset_cut = ser_iter_asset_trans.loc[date_factor_start : iter_date, All]\n",
    "    ser_iter_bench_cut = ser_iter_bench_trans.loc[date_factor_start : iter_date]    \n",
    "    ### Positive returns to zero:\n",
    "    ser_iter_asset_cut_nn = ser_iter_asset_cut.copy()\n",
    "    ser_iter_asset_cut_nn.loc[ser_iter_asset_cut_nn > 0.0] = 0.0\n",
    "    ser_iter_bench_cut_nn = ser_iter_bench_cut.copy()\n",
    "    ser_iter_bench_cut_nn.loc[ser_iter_bench_cut_nn > 0.0] = 0.0\n",
    "    ### Negative/Negative beta factor calculation (with minimum observations number check):    \n",
    "    ser_iter_factor = -ser_iter_asset_cut_nn.groupby('Country')\\\n",
    "                                            .apply(lambda ser_country: (ser_country.droplevel('Country') * ser_iter_bench_cut_nn).sum() / (ser_iter_bench_cut ** 2).sum() \\\n",
    "                                                   if (ser_country.count() > (int_factor_win // 2)) else np.NaN)\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_ci_beta_nn_raw_csv, mode = 'a', header = not os.path.exists(str_ci_beta_nn_raw_csv), sep = ';')    \n",
    "    ### Results output:\n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA LOADING\n",
    "\n",
    "### Sample returns data loading:\n",
    "df_sample_ret = pd.read_excel(io = str_path_returns, sheet_name = str_sheet_returns, header = 0, parse_dates = True, index_col = [0],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                              '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "ser_bench_ret_raw = df_sample_ret['BENCH']\n",
    "ser_country_ret_raw = df_sample_ret.iloc[:, :-1].stack().sort_index()\n",
    "ser_country_ret_raw.index.set_names('Country', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries number:  1000\n"
     ]
    }
   ],
   "source": [
    "### PROFILING\n",
    "\n",
    "df_ret_raw = df_sample_ret.iloc[:, :-1]\n",
    "for _ in range(199):\n",
    "    df_ret_raw = pd.concat([df_ret_raw, df_sample_ret.iloc[:, :-1]], axis = 1)\n",
    "df_ret_raw.columns = list(range(len(df_ret_raw.columns)))\n",
    "ser_country_ret_raw = df_ret_raw.stack().sort_index()\n",
    "ser_country_ret_raw.index.set_names('Country', 1, inplace = True)\n",
    "print('Countries number: ', len(df_ret_raw.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: PERFORMING FACTOR FOR DATE RANGE\n",
    "\n",
    "### Removing csv files before loop running:\n",
    "if (os.path.exists(str_ci_var1_raw_csv)):\n",
    "    os.remove(str_ci_var1_raw_csv)\n",
    "if (os.path.exists(str_ci_rbq_raw_csv)):\n",
    "    os.remove(str_ci_rbq_raw_csv)  \n",
    "if (os.path.exists(str_ci_beta_nn_raw_csv)):\n",
    "    os.remove(str_ci_beta_nn_raw_csv)    \n",
    "### Local testing parameters:\n",
    "int_interval = 10 ### Interval of progress displaying\n",
    "date_start = datetime.utcnow() ### Start time of calculations\n",
    "date_control = datetime.utcnow() ### Control time to display\n",
    "idx_test_date_range = idx_test_daily_range[ : 30] # idx_test_monthly_range # idx_test_daily_range # \n",
    "### Test performing:\n",
    "print('Start time:', date_start)\n",
    "for iter_num, iter_date in enumerate(idx_test_date_range):\n",
    "    ### Progress printing:\n",
    "    if not (divmod(iter_num, int_interval)[1]):\n",
    "        if iter_num:\n",
    "            print('Counter marker:', iter_num, '/', len(idx_test_date_range))\n",
    "            timedelta_interval = datetime.utcnow() - date_control\n",
    "            print('Time interval since last marker:', datetime.utcnow() - date_control)            \n",
    "            print('Average interval for single date:', str(timedelta_interval / int_interval))\n",
    "        date_control = datetime.utcnow()\n",
    "        \n",
    "    ### Value-at-risk factor calculating:\n",
    "    ser_iter_var1_factor = get_var1_factor(iter_date)  \n",
    "\n",
    "    ### Return based quality factor calculating:\n",
    "    ser_iter_rbq_factor = get_rbq_factor(iter_date)  \n",
    "\n",
    "    ### Negative/negative beta factor calculating:\n",
    "    ser_iter_beta_nn_factor = get_beta_nn_factor(iter_date)  \n",
    "\n",
    "date_finish = datetime.utcnow()\n",
    "### Overall statistics printing:\n",
    "print('Finish time:', date_finish)\n",
    "print('Full interval:', date_finish - date_start)\n",
    "print('Average interval for single date:', str((date_finish - date_start) / len(idx_test_date_range)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PROFILING\n",
    "\n",
    "def profile_it():\n",
    "    ### Removing csv files before loop running:\n",
    "    if (os.path.exists(str_ci_var1_raw_csv)):\n",
    "        os.remove(str_ci_var1_raw_csv)\n",
    "    if (os.path.exists(str_ci_rbq_raw_csv)):\n",
    "        os.remove(str_ci_rbq_raw_csv)  \n",
    "    if (os.path.exists(str_ci_beta_nn_raw_csv)):\n",
    "        os.remove(str_ci_beta_nn_raw_csv)    \n",
    "    ### Local testing parameters:\n",
    "    int_interval = 10 ### Interval of progress displaying\n",
    "    date_start = datetime.utcnow() ### Start time of calculations\n",
    "    date_control = datetime.utcnow() ### Control time to display\n",
    "    idx_test_date_range = idx_test_daily_range[-2 : ] # idx_test_monthly_range # idx_test_daily_range # \n",
    "    ### Test performing:\n",
    "    print('Start time:', date_start)\n",
    "    for iter_num, iter_date in enumerate(idx_test_date_range):\n",
    "        ### Progress printing:\n",
    "        if not (divmod(iter_num, int_interval)[1]):\n",
    "            if iter_num:\n",
    "                print('Counter marker:', iter_num, '/', len(idx_test_date_range))\n",
    "                timedelta_interval = datetime.utcnow() - date_control\n",
    "                print('Time interval since last marker:', datetime.utcnow() - date_control)            \n",
    "                print('Average interval for single date:', str(timedelta_interval / int_interval))\n",
    "            date_control = datetime.utcnow()\n",
    "\n",
    "        ### Value-at-risk factor calculating:\n",
    "        ser_iter_var1_factor = get_var1_factor(iter_date)  \n",
    "\n",
    "        ### Return based quality factor calculating:\n",
    "        ser_iter_rbq_factor = get_rbq_factor(iter_date)  \n",
    "\n",
    "        ### Negative/negative beta factor calculating:\n",
    "        ser_iter_beta_nn_factor = get_beta_nn_factor(iter_date)  \n",
    "\n",
    "    date_finish = datetime.utcnow()\n",
    "    ### Overall statistics printing:\n",
    "    print('Finish time:', date_finish)\n",
    "    print('Full interval:', date_finish - date_start)\n",
    "    print('Average interval for single date:', str((date_finish - date_start) / len(idx_test_date_range)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2022-01-25 09:41:33.358270\n",
      "Finish time: 2022-01-25 09:43:11.543175\n",
      "Full interval: 0:01:38.184905\n",
      "Average interval for single date: 0:00:49.092452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-07 s\n",
       "\n",
       "Total time: 98.1886 s\n",
       "File: <ipython-input-16-7c12830ba866>\n",
       "Function: profile_it at line 3\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     3                                           def profile_it():\n",
       "     4                                               ### Removing csv files before loop running:\n",
       "     5         1       1699.0   1699.0      0.0      if (os.path.exists(str_ci_var1_raw_csv)):\n",
       "     6         1       1767.0   1767.0      0.0          os.remove(str_ci_var1_raw_csv)\n",
       "     7         1        885.0    885.0      0.0      if (os.path.exists(str_ci_rbq_raw_csv)):\n",
       "     8         1       1121.0   1121.0      0.0          os.remove(str_ci_rbq_raw_csv)  \n",
       "     9         1       1093.0   1093.0      0.0      if (os.path.exists(str_ci_beta_nn_raw_csv)):\n",
       "    10         1       1206.0   1206.0      0.0          os.remove(str_ci_beta_nn_raw_csv)    \n",
       "    11                                               ### Local testing parameters:\n",
       "    12         1          8.0      8.0      0.0      int_interval = 10 ### Interval of progress displaying\n",
       "    13         1         84.0     84.0      0.0      date_start = datetime.utcnow() ### Start time of calculations\n",
       "    14         1          7.0      7.0      0.0      date_control = datetime.utcnow() ### Control time to display\n",
       "    15         1       4324.0   4324.0      0.0      idx_test_date_range = idx_test_daily_range[-2 : ] # idx_test_monthly_range # idx_test_daily_range # \n",
       "    16                                               ### Test performing:\n",
       "    17         1       1977.0   1977.0      0.0      print('Start time:', date_start)\n",
       "    18         3        973.0    324.3      0.0      for iter_num, iter_date in enumerate(idx_test_date_range):\n",
       "    19                                                   ### Progress printing:\n",
       "    20         2         45.0     22.5      0.0          if not (divmod(iter_num, int_interval)[1]):\n",
       "    21         1          6.0      6.0      0.0              if iter_num:\n",
       "    22                                                           print('Counter marker:', iter_num, '/', len(idx_test_date_range))\n",
       "    23                                                           timedelta_interval = datetime.utcnow() - date_control\n",
       "    24                                                           print('Time interval since last marker:', datetime.utcnow() - date_control)            \n",
       "    25                                                           print('Average interval for single date:', str(timedelta_interval / int_interval))\n",
       "    26         1         23.0     23.0      0.0              date_control = datetime.utcnow()\n",
       "    27                                           \n",
       "    28                                                   ### Value-at-risk factor calculating:\n",
       "    29         2  220959317.0 110479658.5     22.5          ser_iter_var1_factor = get_var1_factor(iter_date)  \n",
       "    30                                           \n",
       "    31                                                   ### Return based quality factor calculating:\n",
       "    32         2  536799931.0 268399965.5     54.7          ser_iter_rbq_factor = get_rbq_factor(iter_date)  \n",
       "    33                                           \n",
       "    34                                                   ### Negative/negative beta factor calculating:\n",
       "    35         2  224108479.0 112054239.5     22.8          ser_iter_beta_nn_factor = get_beta_nn_factor(iter_date)  \n",
       "    36                                           \n",
       "    37         1         37.0     37.0      0.0      date_finish = datetime.utcnow()\n",
       "    38                                               ### Overall statistics printing:\n",
       "    39         1       1931.0   1931.0      0.0      print('Finish time:', date_finish)\n",
       "    40         1        498.0    498.0      0.0      print('Full interval:', date_finish - date_start)\n",
       "    41         1        544.0    544.0      0.0      print('Average interval for single date:', str((date_finish - date_start) / len(idx_test_date_range)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### PROFILING\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "%lprun -f profile_it profile_it()\n",
    "#%lprun -f get_var1_factor profile_it()\n",
    "#%lprun -f get_rbq_factor profile_it()\n",
    "#%lprun -f get_beta_nn_factor profile_it()\n",
    "#%lprun -f get_country_vector profile_it()\n",
    "#%lprun -f log_based_ma profile_it()\n",
    "#%lprun -f multistep_standartize profile_it()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WAYS COMPARING\n",
    "\n",
    "### Parameter initializing:\n",
    "int_factor_win = 260\n",
    "### Dummy data loading:\n",
    "df_sample_ret = pd.read_excel(io = str_path_returns, sheet_name = str_sheet_returns, header = 0, parse_dates = True, index_col = [0],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                              '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "ser_iter_asset_ma_cut = df_sample_ret.iloc[:, :-1].stack().sort_index()\n",
    "ser_iter_asset_ma_cut.index.set_names('Country', 1, inplace = True)\n",
    "ser_iter_asset_ma_cut.name = 'Returns'\n",
    "### Dummy bench vector creating:\n",
    "ser_iter_bench_idxmin = pd.Series(pd.date_range(start = '2010-12-31', periods = 5 ,freq = 'BY'), index = ser_iter_asset_ma_cut.index.levels[-1])\n",
    "ser_iter_bench_idxmin.index.names = ['Country']\n",
    "ser_iter_bench_idxmin.name = 'Date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Way 01 results:\n",
      " Country\n",
      "AU    0.002081\n",
      "DE    0.013574\n",
      "GB    0.001541\n",
      "JP   -0.000666\n",
      "US   -0.010168\n",
      "Name: Returns, dtype: float64\n",
      "4.07 ms ± 218 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Way 02 results:\n",
      " Country\n",
      "AU    0.002081\n",
      "DE    0.013574\n",
      "GB    0.001541\n",
      "JP   -0.000666\n",
      "US   -0.010168\n",
      "Name: Returns, dtype: float64\n",
      "2.49 ms ± 165 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Way 03 results:\n",
      " Country\n",
      "AU    0.002081\n",
      "DE    0.013574\n",
      "GB    0.001541\n",
      "JP   -0.000666\n",
      "US   -0.010168\n",
      "Name: Returns, dtype: float64\n",
      "7.26 ms ± 216 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "### WAYS COMPARING\n",
    "\n",
    "### YOUR WAY:\n",
    "def way_01(ser_iter_asset_ma_cut, ser_iter_bench_idxmin):\n",
    "    ser_result = ser_iter_asset_ma_cut.groupby('Country')\\\n",
    "                                    .apply(lambda x: x.droplevel('Country').loc[ser_iter_bench_idxmin[x.index[0][1]]] if (x.count() > (int_factor_win // 2)) else np.NaN)\n",
    "    return ser_result\n",
    "\n",
    "### FACTOR CALCULATION BY REINDEXATION WITHOUT GROUPING, BUT NO COUNT CHECK:\n",
    "def way_02(ser_iter_asset_ma_cut, ser_iter_bench_idxmin):\n",
    "    df_iter_bench_idxmin = ser_iter_bench_idxmin.to_frame()\n",
    "    df_iter_bench_idxmin['Dummy'] = 1\n",
    "    df_iter_bench_idxmin = df_iter_bench_idxmin.set_index('Date', append = True).swaplevel()\n",
    "    ser_result = ser_iter_asset_ma_cut.reindex(df_iter_bench_idxmin.index).droplevel('Date')\n",
    "    return ser_result\n",
    "\n",
    "### FACTOR CALCULATION BY MERGE WITHOUT GROUPING, BUT NO COUNT CHECK:\n",
    "def way_03(ser_iter_asset_ma_cut, ser_iter_bench_idxmin):\n",
    "    df_result = ser_iter_bench_idxmin.to_frame().reset_index().merge(ser_iter_asset_ma_cut.to_frame().reset_index('Date'), how = 'left', on = ['Country', 'Date'])\n",
    "    ser_result = df_result.set_index('Country')['Returns']\n",
    "    return ser_result\n",
    "\n",
    "print('Way 01 results:\\n', way_01(ser_iter_asset_ma_cut, ser_iter_bench_idxmin))\n",
    "%timeit way_01(ser_iter_asset_ma_cut, ser_iter_bench_idxmin)\n",
    "print('Way 02 results:\\n', way_02(ser_iter_asset_ma_cut, ser_iter_bench_idxmin))\n",
    "%timeit way_02(ser_iter_asset_ma_cut, ser_iter_bench_idxmin)\n",
    "print('Way 03 results:\\n', way_03(ser_iter_asset_ma_cut, ser_iter_bench_idxmin))\n",
    "%timeit way_03(ser_iter_asset_ma_cut, ser_iter_bench_idxmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
