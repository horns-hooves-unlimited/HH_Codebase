{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: COMTRADE DATASETS EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:  3.7.4\n",
      "numpy version:  1.17.2\n",
      "pandas version:  0.25.3\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('python version: ', python_version())\n",
    "print('numpy version: ', np.__version__)\n",
    "print('pandas version: ', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS INITIALIZATION\n",
    "\n",
    "### Common constants:\n",
    "All = slice(None)\n",
    "### General daily-mode and ranges initialization:\n",
    "str_date_factor_start = '1994-12-31' ### Start date for efficacy measures\n",
    "str_date_factor_end = '2023-06-30' ### End date for efficacy measures\n",
    "idx_test_monthly_range = pd.date_range(str_date_factor_start, str_date_factor_end, freq = 'BM') ### Range for source data filtering\n",
    "idx_test_daily_range = pd.date_range(str_date_factor_start, str_date_factor_end, freq = 'B') ### Range for source data filtering\n",
    "### Results saving paths:\n",
    "str_pagerank_exp_path = 'Data_Files/Test_Files/pagerank_exp.csv'\n",
    "str_pagerank_imp_path = 'Data_Files/Test_Files/pagerank_imp.csv'\n",
    "str_herfindahl_comm_path = 'Data_Files/Test_Files/herfindahl_comm.csv'\n",
    "str_exclusivity_exp_path = 'Data_Files/Test_Files/exclusivity_exp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE CALCULATOR\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = None, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if ser_weight is None:\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO REPLACE WITH SQL REQUEST: EXTRACTION AF ACTUAL ISON MEMBERS LIST (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(engine = 'openpyxl', io = str_path_universe, sheet_name = 'Switchers', header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = list_na_excel_values, keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe\n",
    "\n",
    "### NA for MS Excel files:\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable', '---']\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### ISON membership history:\n",
    "ser_ison_membership = ison_membership_converting(str_path_universe, pd.to_datetime('2022-12-31'))\n",
    "### ISON Members:\n",
    "list_ison_countries = sorted(ser_ison_membership.index.get_level_values('Country').unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO REPLACE WITH SQL REQUEST: IMITATION OF SQL TABLE OF BILATERAL EXPORT FLOWS\n",
    "\n",
    "gc.collect()\n",
    "### Augmented bilateral export:\n",
    "str_path_export_bilateral = 'Data_Files/Source_Files/comtrade_export_bilateral.h5'\n",
    "### Export key:\n",
    "str_key_unc_export = 'export_augmented'\n",
    "### Checked EBOPS service IDs list (df_serv_to_gics['GICS Group Code']):\n",
    "list_services = ['206', '210', '214', '218', '219', '223', '227', '231', '232', '237', '240', '246', '247', '250', '251', '254', '255', '256', '257', '258', '263',\n",
    "                 '264', '269', '272', '273', '288', '289', '292', '293', '294', '310', '391', '431', '500', '888', '891', '892', '894', '950']\n",
    "\n",
    "list_export_chunks = []\n",
    "for num_iter_number, ser_iter_chunk in enumerate(pd.read_hdf(str_path_export_bilateral, key = str_key_unc_export, chunksize = 1000000)):\n",
    "    gc.collect()\n",
    "    print(num_iter_number, ': Extraction started')\n",
    "    ser_iter_chunk = ser_iter_chunk[ser_iter_chunk > 0.0].astype('int32')\n",
    "    df_iter_chunk = ser_iter_chunk.reset_index()\n",
    "    df_iter_chunk = df_iter_chunk[(df_iter_chunk['Reporter'] != df_iter_chunk['Partner']) & \\\n",
    "                                  ((df_iter_chunk['Type'] == 'Goods') | df_iter_chunk['Commodity_ID'].isin(list_services)) & (df_iter_chunk['Reporter'] != 'World') & \\\n",
    "                                  (df_iter_chunk['Partner'] != 'World')]\\\n",
    "                               .drop('Type', axis = 1)    \n",
    "    print(num_iter_number, ': Filtering performed')    \n",
    "    ser_iter_chunk = df_iter_chunk.set_index(['Date', 'Reporter', 'Partner', 'Commodity_ID']).squeeze().sort_index()\n",
    "    del df_iter_chunk\n",
    "    gc.collect()\n",
    "    list_export_chunks.append(ser_iter_chunk)\n",
    "    print(num_iter_number, ': Chunk added to container')    \n",
    "ser_bilateral_export = pd.concat(list_export_chunks, axis = 0, sort = False).sort_index()\n",
    "ser_bilateral_export.name = 'Export'\n",
    "del list_export_chunks\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO REPLACE WITH SQL REQUEST: IMITATION OF SQL TABLE OF BILATERAL IMPORT FLOWS\n",
    "\n",
    "gc.collect()\n",
    "### Augmented bilateral import:\n",
    "str_path_import_bilateral = 'Data_Files/Source_Files/comtrade_import_bilateral.h5'\n",
    "### Import key:\n",
    "str_key_unc_import = 'import_augmented'\n",
    "### Checked EBOPS service IDs list (df_serv_to_gics['GICS Group Code']):\n",
    "list_services = ['206', '210', '214', '218', '219', '223', '227', '231', '232', '237', '240', '246', '247', '250', '251', '254', '255', '256', '257', '258', '263',\n",
    "                 '264', '269', '272', '273', '288', '289', '292', '293', '294', '310', '391', '431', '500', '888', '891', '892', '894', '950']\n",
    "\n",
    "list_import_chunks = []\n",
    "for num_iter_number, ser_iter_chunk in enumerate(pd.read_hdf(str_path_import_bilateral, key = str_key_unc_import, chunksize = 1000000)):\n",
    "    gc.collect()\n",
    "    print(num_iter_number, ': Extraction started')\n",
    "    ser_iter_chunk = ser_iter_chunk[ser_iter_chunk > 0.0].astype('int32')\n",
    "    df_iter_chunk = ser_iter_chunk.reset_index()\n",
    "    df_iter_chunk = df_iter_chunk[(df_iter_chunk['Reporter'] != df_iter_chunk['Partner']) & \\\n",
    "                                  ((df_iter_chunk['Type'] == 'Goods') | df_iter_chunk['Commodity_ID'].isin(list_services)) & (df_iter_chunk['Reporter'] != 'World') & \\\n",
    "                                  (df_iter_chunk['Partner'] != 'World')]\\\n",
    "                               .drop('Type', axis = 1)     \n",
    "    print(num_iter_number, ': Filtering performed')    \n",
    "    ser_iter_chunk = df_iter_chunk.set_index(['Date', 'Reporter', 'Partner', 'Commodity_ID']).squeeze().sort_index()\n",
    "    del df_iter_chunk\n",
    "    gc.collect()\n",
    "    list_import_chunks.append(ser_iter_chunk)\n",
    "    print(num_iter_number, ': Chunk added to container')    \n",
    "ser_bilateral_import = pd.concat(list_import_chunks, axis = 0, sort = False).sort_index()\n",
    "ser_bilateral_import.name = 'Import'\n",
    "del list_import_chunks\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPORT PAGERANK FACTOR CALCULATION FUNCTION\n",
    "\n",
    "#gc.collect()\n",
    "def get_pagerank_exp_factor(iter_date):\n",
    "#iter_date = pd.to_datetime('2023-06-30')\n",
    "#if True:\n",
    "    ### Lag in months:\n",
    "    int_lag_mo = 9\n",
    "    ### Caclulating of the date to request Flows:\n",
    "    date_to_request = iter_date - pd.tseries.offsets.BMonthEnd(int_lag_mo - 1) - pd.tseries.offsets.BYearEnd(1)\n",
    "    ### Defining of pagerank calculation function:\n",
    "    def get_pagerank(df_group):\n",
    "        nx_graph = nx.from_pandas_edgelist(df_group, 'Reporter', 'Partner', edge_attr = 'Export', create_using = nx.DiGraph)\n",
    "        dict_pagerank = nx.pagerank(nx_graph)\n",
    "        ser_pagerank = pd.Series(dict_pagerank)\n",
    "        return ser_pagerank\n",
    "    ### Imitation of SQL Requests to load needed cross-sections:\n",
    "    ser_iter_export = ser_bilateral_export[date_to_request]\n",
    "    ser_iter_import = ser_bilateral_import[date_to_request]    \n",
    "    ### Performing of the export pagerank calculation for each commodity:    \n",
    "    ser_pagerank = ser_iter_export.reset_index().groupby('Commodity_ID').apply(get_pagerank)\n",
    "    ser_pagerank = ser_pagerank.swaplevel().sort_index()[list_ison_countries]\n",
    "    ser_pagerank.index.names = ['Reporter', 'Commodity_ID']\n",
    "    ser_pagerank.name = 'Pagerank'\n",
    "    ### Calculating of country trade by commodity:\n",
    "    ser_comm_export = ser_iter_export[list_ison_countries].groupby(['Reporter', 'Commodity_ID']).sum()\n",
    "    ser_comm_import = ser_iter_import[list_ison_countries].groupby(['Reporter', 'Commodity_ID']).sum()\n",
    "    ser_comm_trade = pd.concat([ser_comm_export, ser_comm_import], axis = 1).fillna(0.0).sum(axis = 1)\n",
    "    ser_comm_trade.name = 'Trade'\n",
    "    ### Aggregated pagerank calculation:\n",
    "    df_pagerank = ser_pagerank.to_frame().join(ser_comm_trade / 1000, how = 'left').fillna(0.0)\n",
    "    ser_iter_factor = df_pagerank.groupby('Reporter').apply(lambda df_group: weighted_average(df_group['Pagerank'], df_group['Trade']))\n",
    "    ser_iter_factor.name = 'Pagerank_Exp'\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_pagerank_exp_path, mode = 'a', header = not os.path.exists(str_pagerank_exp_path), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING IMPORT PAGERANK FACTOR CALCULATION FUNCTION\n",
    "\n",
    "#gc.collect()\n",
    "def get_pagerank_imp_factor(iter_date):\n",
    "#iter_date = pd.to_datetime('2023-06-30')\n",
    "#if True:\n",
    "    ### Lag in months:\n",
    "    int_lag_mo = 9\n",
    "    ### Caclulating of the date to request Flows:\n",
    "    date_to_request = iter_date - pd.tseries.offsets.BMonthEnd(int_lag_mo - 1) - pd.tseries.offsets.BYearEnd(1)\n",
    "    ### Defining of pagerank calculation function:\n",
    "    def get_pagerank(df_group):\n",
    "        nx_graph = nx.from_pandas_edgelist(df_group, 'Reporter', 'Partner', edge_attr = 'Import', create_using = nx.DiGraph)\n",
    "        dict_pagerank = nx.pagerank(nx_graph)\n",
    "        ser_pagerank = pd.Series(dict_pagerank)\n",
    "        return ser_pagerank\n",
    "    ### Imitation of SQL Requests to load needed cross-sections:\n",
    "    ser_iter_export = ser_bilateral_export[date_to_request]\n",
    "    ser_iter_import = ser_bilateral_import[date_to_request]    \n",
    "    ### Performing of the export pagerank calculation for each commodity:    \n",
    "    ser_pagerank = ser_iter_import.reset_index().groupby('Commodity_ID').apply(get_pagerank)\n",
    "    ser_pagerank = ser_pagerank.swaplevel().sort_index()[list_ison_countries]\n",
    "    ser_pagerank.index.names = ['Reporter', 'Commodity_ID']\n",
    "    ser_pagerank.name = 'Pagerank'\n",
    "    ### Calculating of country trade by commodity:\n",
    "    ser_comm_export = ser_iter_export[list_ison_countries].groupby(['Reporter', 'Commodity_ID']).sum()\n",
    "    ser_comm_import = ser_iter_import[list_ison_countries].groupby(['Reporter', 'Commodity_ID']).sum()\n",
    "    ser_comm_trade = pd.concat([ser_comm_export, ser_comm_import], axis = 1).fillna(0.0).sum(axis = 1)\n",
    "    ser_comm_trade.name = 'Trade'\n",
    "    ### Aggregated pagerank calculation:\n",
    "    df_pagerank = ser_pagerank.to_frame().join(ser_comm_trade / 1000, how = 'left').fillna(0.0)\n",
    "    ser_iter_factor = df_pagerank.groupby('Reporter').apply(lambda df_group: weighted_average(df_group['Pagerank'], df_group['Trade']))\n",
    "    ser_iter_factor.name = 'Pagerank_Imp'\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_pagerank_imp_path, mode = 'a', header = not os.path.exists(str_pagerank_imp_path), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING HERFINDAHL INDEX BY EXPORT WEIGHTS OF COMMODITIES FACTOR CALCULATION FUNCTION\n",
    "\n",
    "#gc.collect()\n",
    "def get_herfindahl_comm_factor(iter_date):\n",
    "#iter_date = pd.to_datetime('2023-06-30')\n",
    "#if True:\n",
    "    ### Lag in months:\n",
    "    int_lag_mo = 9\n",
    "    ### Caclulating of the date to request Flows:\n",
    "    date_to_request = iter_date - pd.tseries.offsets.BMonthEnd(int_lag_mo - 1) - pd.tseries.offsets.BYearEnd(1)\n",
    "    ### Defining Herfindahl index calculation:\n",
    "    def get_herfindahl(ser_group):\n",
    "        if (ser_group.count() > 0):\n",
    "            ser_norm = ser_group / ser_group.sum()\n",
    "            flo_herfindahl = 1 / ((ser_norm ** 2).sum() ** (1 / 2))\n",
    "        else:\n",
    "            flo_herfindahl = np.NaN\n",
    "        return flo_herfindahl\n",
    "    ### Imitation of SQL Requests to load needed cross-sections:\n",
    "    ser_iter_export = ser_bilateral_export[date_to_request]    \n",
    "    ### Calculating of country export by commodity:\n",
    "    ser_comm_export = ser_iter_export[list_ison_countries].groupby(['Reporter', 'Commodity_ID']).sum()\n",
    "    ### Herfindahl index for eaxh reporter (commodities export weights):\n",
    "    ser_iter_factor = ser_comm_export.groupby('Reporter').apply(get_herfindahl)\n",
    "    ser_iter_factor.name = 'Herfindahl_Comm'\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_herfindahl_comm_path, mode = 'a', header = not os.path.exists(str_herfindahl_comm_path), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPORT EXCLUSIVITY FACTOR CALCULATION FUNCTION\n",
    "\n",
    "#gc.collect()\n",
    "def get_exclusivity_exp_factor(iter_date):\n",
    "#iter_date = pd.to_datetime('2023-06-30')\n",
    "#if True:\n",
    "    ### Lag in months:\n",
    "    int_lag_mo = 9\n",
    "    ### Caclulating of the date to request Flows:\n",
    "    date_to_request = iter_date - pd.tseries.offsets.BMonthEnd(int_lag_mo - 1) - pd.tseries.offsets.BYearEnd(1)\n",
    "    def calculate_importance(ser_group):\n",
    "        ser_group = ser_group.droplevel('Commodity_ID')\n",
    "        ser_group.name = 'Export'\n",
    "        ser_sum_flow = ser_group.groupby('Partner').sum()\n",
    "        ser_sum_flow.name = 'Partner_Sum'\n",
    "        df_group = ser_group.to_frame().join(ser_sum_flow)\n",
    "        if (len(df_group.index.get_level_values('Reporter').unique()) > 1):\n",
    "            df_group['Importance'] = df_group.groupby('Reporter', group_keys = False).apply(lambda df_country: (df_country['Export'] / df_country['Partner_Sum']))\n",
    "        else:\n",
    "            df_group['Importance'] = 1.0\n",
    "        ser_result = df_group.groupby('Reporter', group_keys = False).apply(lambda df_country: weighted_average(df_country['Importance'], df_country['Export'] / 1000))\n",
    "        return ser_result\n",
    "    ### Imitation of SQL Requests to load needed cross-sections:\n",
    "    ser_iter_export = ser_bilateral_export[date_to_request]    \n",
    "    ### Calculating of country export by commodity:\n",
    "    ser_comm_export = ser_iter_export[list_ison_countries].groupby(['Reporter', 'Commodity_ID']).sum()\n",
    "    ser_comm_export.name = 'Total_Export'\n",
    "    ### Average Importance of Exporter by Commodity calculation:\n",
    "    ser_importance = ser_iter_export[list_ison_countries].groupby('Commodity_ID').apply(calculate_importance)\n",
    "    ser_importance = ser_importance.reorder_levels(['Reporter', 'Commodity_ID']).sort_index()\n",
    "    ser_importance.name = 'Importance'\n",
    "    ### Exclusivity of Exporter calculation:\n",
    "    df_exclusivity = ser_importance.to_frame().join(ser_comm_export / 1000, how = 'left').fillna(0.0)\n",
    "    ser_iter_factor = df_exclusivity.groupby('Reporter').apply(lambda df_group: weighted_average(df_group['Importance'], df_group['Total_Export']))\n",
    "    ser_iter_factor.name = 'Exclusivity_Exp'\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_exclusivity_exp_path, mode = 'a', header = not os.path.exists(str_exclusivity_exp_path), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2023-07-17 14:54:59.519242\n",
      "Counter marker: 10 / 342\n",
      "Time interval since last marker: 0:03:31.075790\n",
      "Average interval for single date: 0:00:21.107579\n",
      "Counter marker: 20 / 342\n",
      "Time interval since last marker: 0:03:48.766760\n",
      "Average interval for single date: 0:00:22.876676\n",
      "Counter marker: 30 / 342\n",
      "Time interval since last marker: 0:03:53.381596\n",
      "Average interval for single date: 0:00:23.338160\n",
      "Counter marker: 40 / 342\n",
      "Time interval since last marker: 0:03:56.475820\n",
      "Average interval for single date: 0:00:23.647582\n",
      "Counter marker: 50 / 342\n",
      "Time interval since last marker: 0:03:59.823285\n",
      "Average interval for single date: 0:00:23.982328\n",
      "Counter marker: 60 / 342\n",
      "Time interval since last marker: 0:03:57.236553\n",
      "Average interval for single date: 0:00:23.723655\n",
      "Counter marker: 70 / 342\n",
      "Time interval since last marker: 0:03:56.856960\n",
      "Average interval for single date: 0:00:23.685696\n",
      "Counter marker: 80 / 342\n",
      "Time interval since last marker: 0:04:04.597473\n",
      "Average interval for single date: 0:00:24.459747\n",
      "Counter marker: 90 / 342\n",
      "Time interval since last marker: 0:04:55.087417\n",
      "Average interval for single date: 0:00:29.508742\n",
      "Counter marker: 100 / 342\n",
      "Time interval since last marker: 0:05:01.280110\n",
      "Average interval for single date: 0:00:30.128011\n",
      "Counter marker: 110 / 342\n",
      "Time interval since last marker: 0:04:58.903442\n",
      "Average interval for single date: 0:00:29.890344\n",
      "Counter marker: 120 / 342\n",
      "Time interval since last marker: 0:05:04.157789\n",
      "Average interval for single date: 0:00:30.415779\n",
      "Counter marker: 130 / 342\n",
      "Time interval since last marker: 0:05:07.948071\n",
      "Average interval for single date: 0:00:30.794807\n",
      "Counter marker: 140 / 342\n",
      "Time interval since last marker: 0:05:08.180073\n",
      "Average interval for single date: 0:00:30.818007\n",
      "Counter marker: 150 / 342\n",
      "Time interval since last marker: 0:05:11.698634\n",
      "Average interval for single date: 0:00:31.169863\n",
      "Counter marker: 160 / 342\n",
      "Time interval since last marker: 0:05:18.726040\n",
      "Average interval for single date: 0:00:31.872604\n",
      "Counter marker: 170 / 342\n",
      "Time interval since last marker: 0:05:22.301811\n",
      "Average interval for single date: 0:00:32.230181\n",
      "Counter marker: 180 / 342\n",
      "Time interval since last marker: 0:05:25.224359\n",
      "Average interval for single date: 0:00:32.522436\n",
      "Counter marker: 190 / 342\n",
      "Time interval since last marker: 0:05:32.591713\n",
      "Average interval for single date: 0:00:33.259171\n",
      "Counter marker: 200 / 342\n",
      "Time interval since last marker: 0:05:25.784876\n",
      "Average interval for single date: 0:00:32.578488\n",
      "Counter marker: 210 / 342\n",
      "Time interval since last marker: 0:05:18.696346\n",
      "Average interval for single date: 0:00:31.869635\n",
      "Counter marker: 220 / 342\n",
      "Time interval since last marker: 0:05:21.876643\n",
      "Average interval for single date: 0:00:32.187664\n",
      "Counter marker: 230 / 342\n",
      "Time interval since last marker: 0:05:24.356796\n",
      "Average interval for single date: 0:00:32.435680\n",
      "Counter marker: 240 / 342\n",
      "Time interval since last marker: 0:05:23.914411\n",
      "Average interval for single date: 0:00:32.391441\n",
      "Counter marker: 250 / 342\n",
      "Time interval since last marker: 0:05:22.843380\n",
      "Average interval for single date: 0:00:32.284338\n",
      "Counter marker: 260 / 342\n",
      "Time interval since last marker: 0:05:23.364393\n",
      "Average interval for single date: 0:00:32.336439\n",
      "Counter marker: 270 / 342\n",
      "Time interval since last marker: 0:05:24.093659\n",
      "Average interval for single date: 0:00:32.409366\n",
      "Counter marker: 280 / 342\n",
      "Time interval since last marker: 0:05:23.903226\n",
      "Average interval for single date: 0:00:32.390323\n",
      "Counter marker: 290 / 342\n",
      "Time interval since last marker: 0:05:25.892549\n",
      "Average interval for single date: 0:00:32.589255\n",
      "Counter marker: 300 / 342\n",
      "Time interval since last marker: 0:05:27.255349\n",
      "Average interval for single date: 0:00:32.725535\n",
      "Counter marker: 310 / 342\n",
      "Time interval since last marker: 0:05:29.019011\n",
      "Average interval for single date: 0:00:32.901901\n",
      "Counter marker: 320 / 342\n",
      "Time interval since last marker: 0:05:30.451515\n",
      "Average interval for single date: 0:00:33.045152\n",
      "Counter marker: 330 / 342\n",
      "Time interval since last marker: 0:05:25.277038\n",
      "Average interval for single date: 0:00:32.527704\n",
      "Counter marker: 340 / 342\n",
      "Time interval since last marker: 0:05:25.356943\n",
      "Average interval for single date: 0:00:32.535694\n",
      "Finish time: 2023-07-17 17:45:31.051426\n",
      "Full interval: 2:50:31.532184\n",
      "Average interval for single date: 0:00:29.916761\n"
     ]
    }
   ],
   "source": [
    "### TESTING: PERFORMING FACTOR FOR DATE RANGE\n",
    "\n",
    "### Removing csv files before loop running:\n",
    "if (os.path.exists(str_pagerank_exp_path)):\n",
    "    os.remove(str_pagerank_exp_path)\n",
    "if (os.path.exists(str_pagerank_imp_path)):\n",
    "    os.remove(str_pagerank_imp_path)    \n",
    "if (os.path.exists(str_herfindahl_comm_path)):\n",
    "    os.remove(str_herfindahl_comm_path)    \n",
    "if (os.path.exists(str_exclusivity_exp_path)):\n",
    "    os.remove(str_exclusivity_exp_path)           \n",
    "### Local testing parameters:\n",
    "int_interval = 10 ### Interval of progress displaying\n",
    "date_start = datetime.utcnow() ### Start time of calculations\n",
    "date_control = datetime.utcnow() ### Control time to display\n",
    "idx_test_date_range = idx_test_monthly_range # idx_test_monthly_range[-50 :] # idx_test_monthly_range[-11 : -9] # \n",
    "### Test performing:\n",
    "print('Start time:', date_start)\n",
    "for iter_num, iter_date in enumerate(idx_test_date_range):\n",
    "    ### Progress printing:\n",
    "    if not (divmod(iter_num, int_interval)[1]):\n",
    "        if iter_num:\n",
    "            print('Counter marker:', iter_num, '/', len(idx_test_date_range))\n",
    "            timedelta_interval = datetime.utcnow() - date_control\n",
    "            print('Time interval since last marker:', datetime.utcnow() - date_control)            \n",
    "            print('Average interval for single date:', str(timedelta_interval / int_interval))\n",
    "        date_control = datetime.utcnow()\n",
    "        \n",
    "    ### Pagerank by Export Flows factor calculating:\n",
    "    ser_pagerank_exp_factor = get_pagerank_exp_factor(iter_date) \n",
    "    ### Pagerank by Import Flows factor calculating:\n",
    "    ser_pagerank_imp_factor = get_pagerank_imp_factor(iter_date)     \n",
    "    ### Herfindahl Index by Export Weights of Commodities factor calculating:\n",
    "    ser_herfindahl_comm_factor = get_herfindahl_comm_factor(iter_date) \n",
    "    ### Herfindahl Index by Export Weights of Commodities factor calculating:\n",
    "    ser_exclusivity_exp_factor = get_exclusivity_exp_factor(iter_date)      \n",
    "\n",
    "date_finish = datetime.utcnow()\n",
    "### Overall statistics printing:\n",
    "print('Finish time:', date_finish)\n",
    "print('Full interval:', date_finish - date_start)\n",
    "print('Average interval for single date:', str((date_finish - date_start) / len(idx_test_date_range)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
