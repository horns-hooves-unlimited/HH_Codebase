{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMTRADE DATASETS EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import gc\n",
    "import os\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:  3.7.4\n",
      "numpy version:  1.17.2\n",
      "pandas version:  0.25.3\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('python version: ', python_version())\n",
    "print('numpy version: ', np.__version__)\n",
    "print('pandas version: ', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: MAIN CONSTANTS\n",
    "\n",
    "### MultiIndex level slice constant:\n",
    "All = slice(None)\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Dates:\n",
    "str_date_end = '2022-12-31'\n",
    "date_start = pd.Timestamp('1989-12-29')\n",
    "date_end = pd.Timestamp(str_date_end)\n",
    "### NA for MS Excel files:\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable', '---']\n",
    "### UN Comtrade adopted data containers:\n",
    "str_path_unc_res_all_annual = 'Data_Files/Source_Files/unc_res_all_annual.h5'\n",
    "str_key_unc_res = 'unc_res'\n",
    "### File with aggregated flows:\n",
    "str_path_unc_res_flows = 'Data_Files/Source_Files/unc_res_flows.h5'\n",
    "### Universal HDF5 key:\n",
    "str_key_unc_res = 'unc_res'\n",
    "### Augmented bilateral export:\n",
    "str_path_export_bilateral = 'Data_Files/Source_Files/comtrade_export_bilateral.h5'\n",
    "### Export key:\n",
    "str_key_unc_export = 'export_augmented'\n",
    "### Augmented bilateral import:\n",
    "str_path_import_bilateral = 'Data_Files/Source_Files/comtrade_import_bilateral.h5'\n",
    "### Import key:\n",
    "str_key_unc_import = 'import_augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING COUNTRY CODES EXTRACTOR\n",
    "\n",
    "def get_country_codes(use_local_copy = False):  \n",
    "    ### In case if URL is unavailable:\n",
    "    if (use_local_copy):\n",
    "        url_country_code = 'Data_Files/Source_Files/countrycode.html'\n",
    "    ### Online extraction:\n",
    "    else:\n",
    "        url_country_code = 'https://countrycode.org/'\n",
    "    df_full_codes = pd.read_html(url_country_code, index_col = 'COUNTRY')[0]\n",
    "    df_full_codes[['ISO SHORT', 'ISO LONG']] = df_full_codes['ISO CODES'].str.split(' / ', expand = True)\n",
    "    df_result = df_full_codes[['ISO SHORT', 'ISO LONG']].sort_index()    \n",
    "    df_result.index = df_result.index.str.upper()\n",
    "    ### Results output:\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: CODELISTS LOADING (PRODUCT VERSION)\n",
    "\n",
    "### World Country Codes:\n",
    "df_country_codes = get_country_codes()\n",
    "### Codelists container:\n",
    "dict_codelist = {}\n",
    "### List of reference tables for categories:\n",
    "request_session = requests.Session()\n",
    "obj_unc_reference = request_session.get('https://comtradeapi.un.org/files/v1/app/reference/ListofReferences.json')\n",
    "df_cat_reference = pd.DataFrame(obj_unc_reference.json()['results']).set_index('category')\n",
    "### Description of categories\n",
    "df_cat_description = pd.DataFrame(request_session.get(df_cat_reference.loc['dataitem']['fileuri']).json()['results']).set_index('dataItem')\n",
    "### Parameters:\n",
    "ser_type_code = pd.Series(['Goods', 'Services'], index = ['C', 'S'])\n",
    "ser_type_code.index.names = ['id']\n",
    "ser_type_code.name = 'typeCode'\n",
    "dict_codelist['typeCode'] = ser_type_code\n",
    "### Frequency:\n",
    "ser_freq_code = pd.DataFrame(request_session.get(df_cat_reference.loc['freq']['fileuri']).json()['results']).set_index('id').squeeze()\n",
    "dict_codelist['freqCode'] = ser_freq_code\n",
    "### Commodity HS Codes:\n",
    "df_hs_comm = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:HS']['fileuri']).json()['results'])\n",
    "ser_hs_comm_ag2 = df_hs_comm[df_hs_comm['aggrLevel'] == 2].drop(['parent', 'isLeaf', 'aggrLevel'], axis = 1).set_index('id').squeeze().str[5: ]\n",
    "ser_hs_comm_ag2.name = 'clCode'\n",
    "dict_codelist['clCode'] = {}\n",
    "dict_codelist['clCode']['C'] = ser_hs_comm_ag2\n",
    "### Service EBOPS Codes:\n",
    "df_eb_serv = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:EB']['fileuri']).json()['results'])\n",
    "df_eb_serv_ag2 = df_eb_serv[df_eb_serv['parent'].isin(df_eb_serv[df_eb_serv['parent'] == '200']['id'])]\n",
    "ser_eb_serv_ag2 = df_eb_serv_ag2[df_eb_serv_ag2['id'].astype(int) <= 950].set_index('id')['text']\n",
    "ser_eb_serv_ag2.name = 'clCode'\n",
    "dict_codelist['clCode']['S'] = ser_eb_serv_ag2\n",
    "### United codes:\n",
    "dict_codelist['clCode']['T'] = pd.concat([dict_codelist['clCode']['C'], dict_codelist['clCode']['S']], axis = 0)\n",
    "### Reporter Codes:\n",
    "df_reporter_raw = pd.DataFrame(request_session.get(df_cat_reference.loc['reporter']['fileuri']).json()['results'])\n",
    "df_reporter_raw['id'] = df_reporter_raw['id'].astype(str).str.zfill(3)\n",
    "df_reporter_raw['entryEffectiveDate'] = pd.to_datetime(df_reporter_raw['entryEffectiveDate'])\n",
    "df_reporter_raw['entryExpiredDate'] = pd.to_datetime(df_reporter_raw['entryExpiredDate'])\n",
    "df_reporter_raw = df_reporter_raw[df_reporter_raw['reporterCodeIsoAlpha2'].isin(df_country_codes['ISO SHORT'])]\n",
    "df_reporter_raw = df_reporter_raw[df_reporter_raw['entryExpiredDate'].isna() | (df_reporter_raw['entryExpiredDate'] > date_start)]\n",
    "ser_reporter_code = df_reporter_raw.set_index('id')['reporterCodeIsoAlpha2'].squeeze()\n",
    "ser_reporter_code.name = 'Reporter'\n",
    "dict_codelist['reporterCode'] = ser_reporter_code\n",
    "### Partner Codes:\n",
    "df_partner_raw = pd.DataFrame(request_session.get(df_cat_reference.loc['partner']['fileuri']).json()['results'])\n",
    "df_partner_raw['id'] = df_partner_raw['id'].astype(str).str.zfill(3)\n",
    "df_partner_raw['entryEffectiveDate'] = pd.to_datetime(df_partner_raw['entryEffectiveDate'])\n",
    "df_partner_raw['entryExpiredDate'] = pd.to_datetime(df_partner_raw['entryExpiredDate'])\n",
    "df_partner_raw = df_partner_raw[df_partner_raw['PartnerCodeIsoAlpha2'].isin(df_country_codes['ISO SHORT'])]\n",
    "df_partner_raw = df_partner_raw[df_partner_raw['entryExpiredDate'].isna() | (df_partner_raw['entryExpiredDate'] > date_start)]\n",
    "ser_partner_code = df_partner_raw.set_index('id')['PartnerCodeIsoAlpha2'].squeeze()\n",
    "ser_partner_code.name = 'Partner'\n",
    "dict_codelist['partnerCode'] = ser_partner_code\n",
    "### Trade Flow Codes:\n",
    "ser_flow_code = pd.DataFrame(request_session.get(df_cat_reference.loc['flow']['fileuri']).json()['results']).set_index('id').squeeze()\n",
    "ser_flow_code.name = 'flowCode'\n",
    "dict_codelist['flowCode'] = ser_flow_code\n",
    "### Customs Codes:\n",
    "ser_customs_code = pd.DataFrame(request_session.get(df_cat_reference.loc['customs']['fileuri']).json()['results']).set_index('id').squeeze()\n",
    "ser_customs_code.name = 'customsCode'\n",
    "dict_codelist['customsCode'] = ser_customs_code\n",
    "### Transport Codes:\n",
    "ser_tansport_code = pd.DataFrame(request_session.get(df_cat_reference.loc['mot']['fileuri']).json()['results']).set_index('id').squeeze()\n",
    "ser_tansport_code.name = 'motCode'\n",
    "dict_codelist['motCode'] = ser_tansport_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: DATA REQUEST PARAMETERS\n",
    "\n",
    "#str_primary_key = 'e690550ab9414234a6b705220596677a'\n",
    "str_primary_key = '3f0b8d53d71b401e840d58c90a283176'\n",
    "str_primary_key = 'd79c218e2e9d464fade1810fd14347d8'\n",
    "str_goods_type = 'C'\n",
    "str_services_type = 'S'\n",
    "str_freq = 'A'\n",
    "str_goods_class = 'HS'\n",
    "str_services_class = 'EB'\n",
    "str_export_flow = 'X'\n",
    "str_import_flow = 'M'\n",
    "list_un_reporters = dict_codelist['reporterCode'].index.to_list()\n",
    "list_un_partners = dict_codelist['partnerCode'].index.to_list()\n",
    "list_un_goods_ag2 = dict_codelist['clCode'][str_goods_type].index.to_list()\n",
    "list_un_services_ag2 = dict_codelist['clCode'][str_services_type].index.to_list()\n",
    "list_periods = str_period = list(map(str, range(date_start.year, date_end.year + 1)))\n",
    "int_pause_short_sec = 10\n",
    "int_pause_long_sec = 10\n",
    "int_period_portion = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: DATA REQUEST EXECUTION\n",
    "\n",
    "def get_un_comtrade_data(str_type, str_freq, str_classification, str_trade_flow, list_reporters, list_partners, list_commodities, list_periods, str_api_key, \n",
    "                         int_limit = 99999):\n",
    "    ### Request preparation:\n",
    "    str_url_base = 'https://comtradeapi.un.org/data/v1/get/'\n",
    "    str_url_request = str_url_base + str_type + '/'\n",
    "    str_url_request = str_url_request + str_freq + '/' \n",
    "    str_url_request = str_url_request + str_classification + '?'   \n",
    "    str_url_request = str_url_request + 'flowCode=' + str_trade_flow \n",
    "    str_url_request = str_url_request + '&reporterCode=' + ','.join(list_reporters)\n",
    "    str_url_request = str_url_request + '&partnerCode=' + ','.join(list_partners)    \n",
    "    str_url_request = str_url_request + '&cmdCode=' + ','.join(list_commodities)\n",
    "    str_url_request = str_url_request + '&customsCode=' + 'C00'\n",
    "    str_url_request = str_url_request + '&motCode=' + '0'    \n",
    "    str_url_request = str_url_request + '&partner2Code=' + '000'       \n",
    "    str_url_request = str_url_request + '&period=' + ','.join(list_periods)  \n",
    "    str_url_request = str_url_request + '&maxrecords=' + str(int_limit)\n",
    "    ### Request sending:\n",
    "    bool_loaded = False\n",
    "    while (not bool_loaded):\n",
    "        request_session = requests.Session()\n",
    "        dict_request_headers = {}\n",
    "        dict_request_headers['Cache-Control'] = 'no-cache'\n",
    "        dict_request_headers['Ocp-Apim-Subscription-Key'] = str_api_key\n",
    "        request_session.headers.update(dict_request_headers)\n",
    "        ### Respond processing:\n",
    "        print(str_url_request)    \n",
    "        obj_unc_dataset = request_session.get(str_url_request)\n",
    "        if not ('count' in obj_unc_dataset.json()):\n",
    "            print(obj_unc_dataset.json()['error'])\n",
    "            int_dataset_length = -1\n",
    "        else:\n",
    "            int_dataset_length = obj_unc_dataset.json()['count']\n",
    "        if (int_dataset_length > 0):\n",
    "            df_dataset_raw = pd.DataFrame(obj_unc_dataset.json()['data'])\n",
    "            df_dataset_res = df_dataset_raw[['flowCode', 'typeCode', 'period', 'reporterCode', 'partnerCode', 'cmdCode', 'primaryValue']]\n",
    "            df_dataset_res.loc[:, 'Flow'] = df_dataset_res['flowCode'].replace(dict_codelist['flowCode']).astype('category').values\n",
    "            df_dataset_res['Flow'].cat.set_categories(sorted(dict_codelist['flowCode'].values), ordered = True, inplace = True)        \n",
    "            df_dataset_res.loc[:, 'Type'] = df_dataset_res['typeCode'].replace(dict_codelist['typeCode']).astype('category').values\n",
    "            df_dataset_res['Type'].cat.set_categories(sorted(dict_codelist['typeCode'].values), ordered = True, inplace = True)\n",
    "            df_dataset_res.loc[:, 'Date'] = (pd.to_datetime(df_dataset_raw['period']) + pd.offsets.BYearEnd()).values\n",
    "            df_dataset_res.loc[:, 'Reporter'] = df_dataset_res['reporterCode'].astype(str).str.zfill(3).replace(dict_codelist['reporterCode'])\\\n",
    "                                                                                                       .astype('category').values\n",
    "            df_dataset_res['Reporter'].cat.set_categories(sorted(dict_codelist['partnerCode'].unique()), ordered = True, inplace = True)\n",
    "            df_dataset_res.loc[:, 'Partner'] = df_dataset_res['partnerCode'].astype(str).str.zfill(3).replace(dict_codelist['partnerCode']).astype('category').values\n",
    "            df_dataset_res['Partner'].cat.set_categories(sorted(dict_codelist['partnerCode'].unique()), ordered = True, inplace = True)\n",
    "            df_dataset_res.loc[:, 'Commodity_ID'] = df_dataset_res['cmdCode'].astype('category').values\n",
    "            df_dataset_res['Commodity_ID'].cat.set_categories(sorted(dict_codelist['clCode']['T'].index), ordered = True, inplace = True)\n",
    "            df_dataset_res.loc[:, 'Value'] = (df_dataset_res['primaryValue'] / 1000).astype('int32').values\n",
    "            ### Data clearing:\n",
    "            df_dataset_res.drop(df_dataset_res[(df_dataset_res['Reporter'] == 'SA') & (df_dataset_res['Partner'] == 'TW')].index, inplace = True)\n",
    "            df_dataset_res.drop(df_dataset_res[df_dataset_res['Reporter'] == df_dataset_res['Partner']].index, inplace = True)\n",
    "            df_dataset_res['Value'].clip(lower = 0, inplace = True)\n",
    "            df_dataset_res = df_dataset_res[['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID', 'Value']]#.dropna()\n",
    "            print('Loaded Observations Number:', int_dataset_length)\n",
    "            bool_loaded = True\n",
    "        elif (int_dataset_length == 0):\n",
    "            print('Empty Dataset')\n",
    "            bool_loaded = True            \n",
    "            df_dataset_res = None            \n",
    "        else:\n",
    "            print('Loading Error. Let\\'s try once more...')\n",
    "    return df_dataset_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA LOADING ENGINE\n",
    "\n",
    "list_un_collection = []\n",
    "\n",
    "for iter_portion in range(-(-len(list_periods) // int_period_portion)):\n",
    "#for iter_portion in range(-(-len(list_periods) // int_period_portion))[3 : 4]:\n",
    "    gc.collect()\n",
    "    list_iter_periods = list_periods[iter_portion *  int_period_portion : (iter_portion + 1) *  int_period_portion]\n",
    "    ### Goods data loading:\n",
    "    for iter_comm_id in list_un_goods_ag2:\n",
    "        print(iter_comm_id, '/', list_iter_periods)        \n",
    "        ### Export of Goods:\n",
    "        df_iter_dataset = get_un_comtrade_data(str_goods_type, str_freq, str_goods_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                               [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "        if (df_iter_dataset is not None):\n",
    "            list_un_collection.append(df_iter_dataset)\n",
    "            time.sleep(int_pause_short_sec)\n",
    "        else:\n",
    "            time.sleep(int_pause_long_sec) \n",
    "        ### Import of Goods:\n",
    "        df_iter_dataset = get_un_comtrade_data(str_goods_type, str_freq, str_goods_class, str_import_flow, list_un_reporters, list_un_partners, \n",
    "                                               [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "        if (df_iter_dataset is not None):\n",
    "            list_un_collection.append(df_iter_dataset)\n",
    "            time.sleep(int_pause_short_sec)\n",
    "        else:\n",
    "            time.sleep(int_pause_long_sec)                    \n",
    "    ### Services data loading:\n",
    "    for iter_comm_id in list_un_services_ag2:      \n",
    "        print(iter_comm_id, '/', list_iter_periods)        \n",
    "        ### Export of Services:        \n",
    "        df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                               [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "        if (df_iter_dataset is not None):\n",
    "            list_un_collection.append(df_iter_dataset)\n",
    "            time.sleep(int_pause_short_sec)\n",
    "        else:\n",
    "            time.sleep(int_pause_long_sec)                    \n",
    "        ### Import of Services:        \n",
    "        df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_import_flow, list_un_reporters, list_un_partners, \n",
    "                                               [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "        if (df_iter_dataset is not None):\n",
    "            list_un_collection.append(df_iter_dataset)\n",
    "            time.sleep(int_pause_short_sec)\n",
    "        else:\n",
    "            time.sleep(int_pause_long_sec)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA CONCATENATION AND SAVING\n",
    "\n",
    "### Downloaded Data Concatenating:\n",
    "df_full_dataset = pd.concat(list_un_collection, axis = 0, sort = False, ignore_index = True)\n",
    "del list_un_collection\n",
    "ser_full_dataset = df_full_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "### Dataset saving:\n",
    "ser_full_dataset.to_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, mode = 'w', format = 'table', complevel = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPORT AND REVERTED IMPORT CONCATENATION\n",
    "\n",
    "gc.collect()\n",
    "### File deleting:\n",
    "if (os.path.exists(str_path_unc_res_flows)):\n",
    "    os.remove(str_path_unc_res_flows)\n",
    "### Results container:\n",
    "list_export_aug = []\n",
    "### Countries portion length:\n",
    "int_portion = 5\n",
    "list_unc_countries = sorted(dict_codelist['partnerCode'].unique())\n",
    "### Looping over countrie portions:\n",
    "for iter_num in range(len(list_unc_countries) // int_portion + 1):\n",
    "#for iter_num in range(2):\n",
    "    gc.collect()    \n",
    "    ### Portion of countries selecting:\n",
    "    list_iter_countries = list(list_unc_countries)[int_portion * iter_num : int_portion * (iter_num + 1)]\n",
    "    if (len(list_iter_countries) > 0):\n",
    "        print(list_iter_countries)\n",
    "        ### Export data loading:\n",
    "        ser_unc_export = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res,\n",
    "                                     where = \"(Flow = 'Export') & (Reporter in list_iter_countries) & (Partner != 'World')\").droplevel('Flow')\n",
    "        print('Export dataset loaded')\n",
    "        ### Import data loading:\n",
    "        ser_unc_import = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res, \n",
    "                                     where = \"(Flow = 'Import') & (Partner in list_iter_countries)\").droplevel('Flow')\n",
    "        print('Import dataset loaded')    \n",
    "        ### Import data reverting:\n",
    "        ser_unc_import.index.set_names('Partner_Inv', level = 1, inplace = True)\n",
    "        ser_unc_import.index.set_names('Reporter', level = 2, inplace = True)\n",
    "        ser_unc_import.index.set_names('Partner', level = 1, inplace = True)\n",
    "        ser_unc_import = ser_unc_import.swaplevel('Reporter', 'Partner').sort_index()\n",
    "        print('Import dataset reverted')\n",
    "        ### Datasets concatenation:\n",
    "        df_export_aug = pd.concat([ser_unc_export, ser_unc_import], axis = 1, names = 'Source Flow', keys = ['Export', 'Import']).astype('float32')\n",
    "        del ser_unc_export\n",
    "        del ser_unc_import    \n",
    "        gc.collect()    \n",
    "        print('Export and reverted Import dataset concatenated')\n",
    "        ### Columns categorization:\n",
    "        df_export_aug.to_hdf(str_path_unc_res_flows, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, append = True)                \n",
    "        print('Aggregated dataset added to container')\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIF COEFFICIENTS CALCULATION & IMPLEMENTATION\n",
    "\n",
    "gc.collect()\n",
    "### Files deleting:\n",
    "if (os.path.exists(str_path_export_bilateral)):\n",
    "    os.remove(str_path_export_bilateral)\n",
    "if (os.path.exists(str_path_import_bilateral)):\n",
    "    os.remove(str_path_import_bilateral)\n",
    "### Getting list of commodities:\n",
    "#str_date = '1998-12-31'\n",
    "str_date = '2020-12-31'\n",
    "list_commodity_id = sorted(pd.read_hdf(str_path_unc_res_flows, key = str_key_unc_res, where = \"Date in str_date\").index.get_level_values('Commodity_ID').unique())\n",
    "### Bounds to filter bilateral Import to Export ratio before median calculation:\n",
    "flo_lower_bound = 1.0\n",
    "flo_upper_bound = 2.0\n",
    "### Bilateral median calculation procedure:\n",
    "def get_obs_median(df_comm):\n",
    "    ### Export to Import ratio:\n",
    "    ser_obs_coeff = df_comm['Import'] / df_comm['Export']\n",
    "    ### Ratio filtering:\n",
    "    ser_obs_coeff = ser_obs_coeff.loc[(ser_obs_coeff >= flo_lower_bound) & (ser_obs_coeff <= flo_upper_bound)]\n",
    "    ### Filtered timeseries median as a result:\n",
    "    return ser_obs_coeff.median()\n",
    "### Calulation CIF coefficient for all commodities:\n",
    "for iter_commodity in list_commodity_id:\n",
    "    gc.collect()\n",
    "    df_iter_flows = pd.read_hdf(str_path_unc_res_flows, key = str_key_unc_res, where = \"Commodity_ID = iter_commodity\")\n",
    "    ser_cif_median = df_iter_flows.droplevel('Commodity_ID').groupby(['Reporter', 'Partner']).apply(get_obs_median)\n",
    "    ### General commodity median calculation:\n",
    "    flo_median = ser_cif_median.median()\n",
    "    print(iter_commodity, ':', flo_median)\n",
    "    ### Filling missed bilateral values with general commodity median:\n",
    "    if not (np.isnan(flo_median)):\n",
    "        ser_cif_median.fillna(flo_median, inplace = True)        \n",
    "    ser_cif_median.name = 'CIF_Coefficient'              \n",
    "    ### Adding CIF coefficients to dataset:\n",
    "    df_export_cif = df_iter_flows.merge(ser_cif_median, left_index = True, right_index = True)\n",
    "    df_export_cif = df_export_cif.reorder_levels(['Date', 'Reporter', 'Partner', 'Type', 'Commodity_ID'])\n",
    "    ### Import correction:\n",
    "    df_export_cif['Import_Corrected'] = df_export_cif['Import'] / df_export_cif['CIF_Coefficient']\n",
    "    ### Export correction:\n",
    "    df_export_cif['Export_Corrected'] = df_export_cif['Export'] * df_export_cif['CIF_Coefficient']\n",
    "    ### Combining Export & Import data:\n",
    "    ser_export_cif = df_export_cif['Export'].combine_first(df_export_cif['Import_Corrected']).astype('float32')\n",
    "    ser_import_cif = df_export_cif['Import'].combine_first(df_export_cif['Export_Corrected']).astype('float32')\n",
    "#    del df_export_cif\n",
    "    gc.collect()\n",
    "    ser_import_cif = ser_import_cif.reorder_levels(['Date', 'Partner', 'Reporter', 'Type', 'Commodity_ID']).sort_index()                               \n",
    "    ser_import_cif.index.names = ['Date', 'Reporter', 'Partner', 'Type', 'Commodity_ID']\n",
    "    gc.collect()\n",
    "    ### Incorporation options:\n",
    "    ser_export_cif.squeeze().to_hdf(str_path_export_bilateral, key = str_key_unc_export, mode = 'a', format = 'table', complevel = 9, append = True, \n",
    "                                    min_itemsize = {'Type': 8, 'Commodity_ID': 3})\n",
    "    ser_import_cif.squeeze().to_hdf(str_path_import_bilateral, key = str_key_unc_import, mode = 'a', format = 'table', complevel = 9, append = True, \n",
    "                                    min_itemsize = {'Type': 8, 'Commodity_ID': 3}) \n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
