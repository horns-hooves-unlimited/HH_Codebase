{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: COMTRADE DATASETS EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import gc\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import itertools\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:  3.7.4\n",
      "numpy version:  1.17.2\n",
      "pandas version:  0.25.3\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('python version: ', python_version())\n",
    "print('numpy version: ', np.__version__)\n",
    "print('pandas version: ', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: MAIN CONSTANTS\n",
    "\n",
    "### MultiIndex level slice constant:\n",
    "All = slice(None)\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Dates:\n",
    "str_date_end = '2022-12-31'\n",
    "date_start = pd.Timestamp('1989-12-29')\n",
    "date_end = pd.Timestamp(str_date_end)\n",
    "date_ison = pd.Timestamp('1994-12-31')\n",
    "### NA for MS Excel files:\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable', '---']\n",
    "### Checked EBOPS service IDs list (df_serv_to_gics['GICS Group Code']):\n",
    "list_services = ['206', '210', '214', '218', '219', '223', '227', '231', '232', '237', '240', '246', '247', '250', '251', '254', '255', '256', '257', '258', '263',\n",
    "                 '264', '269', '272', '273', '288', '289', '292', '293', '294', '310', '391', '431', '500', '888', '891', '892', '894', '950']\n",
    "### UN Comtrade authentication:\n",
    "unc_login = 'pavelb'\n",
    "unc_pass = 'bodoapux'\n",
    "unc_token = 'wqgBfTCn0Idq0LZWWAFKgj3YQYRKczgdnfmlQ3CkanmvQzoAlnL1oK1OJ0yVoCjSLjkUozAj0/dD4eCkSLJO/6pLCqK+iweXMqMazaADI+YqBOUPFySpbXM0CEZepZEuNl5bqxg50EPVB5lCrifsoA=='\n",
    "### UN Comtrade raw data containers:\n",
    "str_path_unc_raw_comm_annual = 'Data_Files/Source_Files/unc_raw_comm_annual.h5'\n",
    "str_path_unc_raw_serv_annual = 'Data_Files/Source_Files/unc_raw_serv_annual.h5'\n",
    "str_key_unc_raw = 'unc_raw'\n",
    "### UN Comtrade adopted data containers:\n",
    "str_path_unc_res_all_annual = 'Data_Files/Source_Files/unc_res_all_annual.h5'\n",
    "str_path_unc_res_all_zz = 'Data_Files/Source_Files/unc_res_all_world.h5'\n",
    "str_key_unc_res = 'unc_res'\n",
    "### File with aggregated flows:\n",
    "str_path_unc_res_flows = 'Data_Files/Source_Files/unc_res_flows.h5'\n",
    "### Universal HDF5 key:\n",
    "str_key_unc_res = 'unc_res'\n",
    "### Augmented bilateral export:\n",
    "str_path_export_bilateral = 'Data_Files/Source_Files/comtrade_export_bilateral.h5'\n",
    "### World export:\n",
    "str_path_export_world = 'Data_Files/Source_Files/comtrade_export_world.h5'\n",
    "### Export key:\n",
    "str_key_unc_export = 'export_augmented'\n",
    "### Augmented bilateral import:\n",
    "str_path_import_bilateral = 'Data_Files/Source_Files/comtrade_import_bilateral.h5'\n",
    "### World import:\n",
    "str_path_import_world = 'Data_Files/Source_Files/comtrade_import_world.h5'\n",
    "### Import key:\n",
    "str_key_unc_import = 'import_augmented'\n",
    "### Factor options:\n",
    "str_path_factor_xlsx = 'Data_Files/Source_Files/comtrade_factor.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE CALCULATOR\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = None, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if ser_weight is None:\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING COUNTRY CODES EXTRACTOR\n",
    "\n",
    "def get_country_codes(use_local_copy = False):  \n",
    "    ### In case if URL is unavailable:\n",
    "    if (use_local_copy):\n",
    "        url_country_code = 'Data_Files/Source_Files/countrycode.html'\n",
    "    ### Online extraction:\n",
    "    else:\n",
    "        url_country_code = 'https://countrycode.org/'\n",
    "    df_full_codes = pd.read_html(url_country_code, index_col = 'COUNTRY')[0]\n",
    "    df_full_codes[['ISO SHORT', 'ISO LONG']] = df_full_codes['ISO CODES'].str.split(' / ', expand = True)\n",
    "    df_result = df_full_codes[['ISO SHORT', 'ISO LONG']].sort_index()    \n",
    "    df_result.index = df_result.index.str.upper()\n",
    "    ### Results output:\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(engine = 'openpyxl', io = str_path_universe, sheet_name = 'Switchers', header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: COMMON DATA EXTRACTION STEPS\n",
    "\n",
    "### World Country Codes:\n",
    "df_country_codes = get_country_codes()\n",
    "### ISON membership history:\n",
    "ser_ison_membership = ison_membership_converting(str_path_universe, pd.to_datetime(str_date_end))\n",
    "ser_ison_membership.index.names = ['Date', 'Reporter']\n",
    "### ISON Members:\n",
    "list_ison_countries = sorted(ser_ison_membership.index.get_level_values('Reporter').unique())\n",
    "### ISON status for the last available date:\n",
    "ser_ison_status = ser_ison_membership.loc[ser_ison_membership.index[-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "int_unc_limit = 10 # 5\n",
    "int_seconds_to_sleep = 2\n",
    "### USA before 1981 code:\n",
    "str_usa_1981 = '841'\n",
    "### UN Comtrade country names to rename:\n",
    "dict_map_to_replace = {'BOLIVIA (PLURINATIONAL STATE OF)': 'BOLIVIA',\n",
    "                       'BOSNIA HERZEGOVINA': 'BOSNIA AND HERZEGOVINA',\n",
    "                       'BR. INDIAN OCEAN TERR.': 'BRITISH INDIAN OCEAN TERRITORY',\n",
    "                       'BR. VIRGIN ISDS': 'BRITISH VIRGIN ISLANDS',\n",
    "                       'BRUNEI DARUSSALAM': 'BRUNEI',\n",
    "                       'CABO VERDE': 'CAPE VERDE',\n",
    "                       'CAYMAN ISDS': 'CAYMAN ISLANDS',\n",
    "                       'CENTRAL AFRICAN REP.': 'CENTRAL AFRICAN REPUBLIC',\n",
    "                       'CHRISTMAS ISDS': 'CHRISTMAS ISLAND',\n",
    "                       'COCOS ISDS': 'COCOS ISLANDS',\n",
    "                       'COOK ISDS': 'COOK ISLANDS',                    \n",
    "                       'CURAÇAO': 'CURACAO',                          \n",
    "                       'CZECHIA': 'CZECH REPUBLIC',                    \n",
    "                       'DEM. REP. OF THE CONGO': 'DEMOCRATIC REPUBLIC OF THE CONGO',                          \n",
    "                       'DOMINICAN REP.': 'DOMINICAN REPUBLIC',                    \n",
    "                       'TIMOR-LESTE': 'EAST TIMOR',                          \n",
    "                       'FALKLAND ISDS (MALVINAS)': 'FALKLAND ISLANDS',                    \n",
    "                       'FAEROE ISDS': 'FAROE ISLANDS',                                           \n",
    "                       'CHINA, HONG KONG SAR': 'HONG KONG',                          \n",
    "                       'CÔTE D\\'IVOIRE': 'IVORY COAST',                                           \n",
    "                       'LAO PEOPLE\\'S DEM. REP.': 'LAOS',                                         \n",
    "                       'CHINA, MACAO SAR': 'MACAU',                          \n",
    "                       'TFYR OF MACEDONIA': 'MACEDONIA',                    \n",
    "                       'MARSHALL ISDS': 'MARSHALL ISLANDS',                          \n",
    "                       'FS MICRONESIA': 'MICRONESIA',                    \n",
    "                       'REP. OF MOLDOVA': 'MOLDOVA',                          \n",
    "                       'NETH. ANTILLES': 'NETHERLANDS ANTILLES',                          \n",
    "                       'DEM. PEOPLE\\'S REP. OF KOREA': 'NORTH KOREA',                          \n",
    "                       'N. MARIANA ISDS': 'NORTHERN MARIANA ISLANDS',                    \n",
    "                       'STATE OF PALESTINE': 'PALESTINE',                          \n",
    "                       'CONGO': 'REPUBLIC OF THE CONGO',                          \n",
    "                       'RÉUNION': 'REUNION',                    \n",
    "                       'RUSSIAN FEDERATION': 'RUSSIA',                          \n",
    "                       'SOLOMON ISDS': 'SOLOMON ISLANDS',                    \n",
    "                       'REP. OF KOREA': 'SOUTH KOREA',                                       \n",
    "                       'UNITED REP. OF TANZANIA': 'TANZANIA',     \n",
    "                       'OTHER ASIA, NES': 'TAIWAN',\n",
    "                       'TURKS AND CAICOS ISDS': 'TURKS AND CAICOS ISLANDS',                    \n",
    "                       'US VIRGIN ISDS': 'U.S. VIRGIN ISLANDS',                          \n",
    "                       'USA': 'UNITED STATES',                          \n",
    "                       'HOLY SEE (VATICAN CITY STATE)': 'VATICAN',                    \n",
    "                       'VIET NAM': 'VIETNAM',                          \n",
    "                       'WALLIS AND FUTUNA ISDS': 'WALLIS AND FUTUNA'\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: COUNTRIES DATA EXTRACTION AND MODIFICATION\n",
    "\n",
    "def get_un_comtrade_country_id(df_country_codes):\n",
    "    ### Getting UN Comtrade country info from post request:\n",
    "    str_UNC_countries_set = 'http://comtrade.un.org/data/cache/partnerAreas.json'\n",
    "    obj_UNC_countries_set = requests.post(str_UNC_countries_set)\n",
    "    ### Object to dataframe transformation:\n",
    "    list_UNC_countries = json.loads(obj_UNC_countries_set.text.encode().decode('utf-8-sig'))['results']\n",
    "    df_UNC_countries = pd.DataFrame(list_UNC_countries)\n",
    "    df_UNC_countries.columns = ['UNC ID', 'COUNTRY']\n",
    "    df_UNC_countries['COUNTRY'] = df_UNC_countries['COUNTRY'].str.upper()\n",
    "    df_UNC_countries.replace(dict_map_to_replace, inplace = True)\n",
    "    df_UNC_countries.set_index('COUNTRY', append = False, drop = True, inplace = True)\n",
    "    df_UNC_country_id = df_UNC_countries.join(df_country_codes, on = 'COUNTRY', how = 'left').dropna(how = 'any').reset_index(drop = True)\n",
    "    df_UNC_country_id.drop('ISO LONG', axis = 1, inplace = True)\n",
    "    df_UNC_country_id.columns = ['Comtrade_ID', 'Country']\n",
    "    ser_UNC_country_id = df_UNC_country_id.set_index('Country').squeeze().sort_index()\n",
    "    ### Results output:\n",
    "    return ser_UNC_country_id\n",
    "\n",
    "### Getting UN Comtrade country IDs:\n",
    "ser_UNC_country_id = get_un_comtrade_country_id(df_country_codes)\n",
    "#### Filtering ISON countries only & adding USA BEFORE 1981 Code & adding World Code:\n",
    "#ser_UNC_country_id = ser_UNC_country_id.reindex(ser_ison_membership.index.get_level_values(1).unique())\n",
    "#ser_UNC_country_id = ser_UNC_country_id.append(pd.Series(str_usa_1981, ['US'])).append(pd.Series('0', ['ZZ'])).sort_index() ### 'ZZ' ~ 'WORLD' TO BE LAST\n",
    "### Only adding World Code\n",
    "ser_UNC_country_id = ser_UNC_country_id.append(pd.Series('0', ['ZZ'])).sort_index() ### 'ZZ' ~ 'WORLD' TO BE LAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: DATA REQUEST EXECUTION\n",
    "\n",
    "def get_un_comtrade_data(str_rep_country_id, str_par_country_id, int_max_rec = 500000, str_type = 'C', str_freq = 'A', str_classification_system = 'S1', \n",
    "                         str_period = 'all', str_trade_flow = 'All', str_classification_code = 'TOTAL', str_token = unc_token):\n",
    "    ### Trade flows codification:\n",
    "    dict_trade_flow = {'All': 'all', 'Import': '1', 'Export': '2', 're-Export': '3', 're-Import': '4', 'Both': '1,2'}\n",
    "    ### URL prefix:\n",
    "    str_url_base = 'http://comtrade.un.org/api/get?'\n",
    "    ### Columns list:\n",
    "    list_dataset_columns = ['Date', 'Reporter_ID', 'Partner_ID', 'Flow_ID', 'Commodity_ID', 'Value']\n",
    "    ### Request URL preparation:\n",
    "    str_url_request = str_url_base\n",
    "    list_parameters = []\n",
    "    list_parameters.append('max=' + str(int_max_rec)) # Usage limit\n",
    "    list_parameters.append('type=' + str_type) # C = Commodities (merchandise trade data) / S = Services (trade in services data)\n",
    "    list_parameters.append('freq=' + str_freq) # A = Annual, M = Monthly\n",
    "    list_parameters.append('px=' + str_classification_system) # Trade data classification scheme. See list of valid classifications\n",
    "    list_parameters.append('ps=' + str_period) # Time period\n",
    "    list_parameters.append('r=' + str_rep_country_id) # Reporter country\n",
    "    list_parameters.append('p=' + str_par_country_id) # Partner country\n",
    "    list_parameters.append('rg=' + dict_trade_flow[str_trade_flow]) # Trade direction\n",
    "    list_parameters.append('cc=' + str_classification_code) # Commodity code\n",
    "    list_parameters.append('token=' + str_token) # Authorization code   \n",
    "    list_parameters.append('fmt=json') # Response data format\n",
    "    str_url_request += '&'.join(list_parameters)\n",
    "    ### Getting UN Comtrade data from post request:\n",
    "    request_session = requests.Session()\n",
    "    obj_unc_dataset = request_session.post(str_url_request)\n",
    "    print(str_url_request)\n",
    "    ### Object to dataframe transformation:        \n",
    "    if ('dataset' in obj_unc_dataset.json().keys()):\n",
    "        list_unc_dataset = obj_unc_dataset.json()['dataset']\n",
    "        if (len(list_unc_dataset) > 1):\n",
    "            df_unc_dataset = pd.DataFrame(list_unc_dataset)[['period', 'rtCode', 'ptCode', 'rgCode', 'cmdCode', 'TradeValue']]\n",
    "            df_unc_dataset.columns = list_dataset_columns\n",
    "            if (str_freq == 'M'):\n",
    "                df_unc_dataset['Date'] = pd.to_datetime(df_unc_dataset['Date'], format = '%Y%m') + pd.offsets.BMonthEnd()    \n",
    "            elif (str_freq == 'A'):\n",
    "                df_unc_dataset['Date'] = pd.to_datetime(df_unc_dataset['Date'], format = '%Y') + pd.offsets.BYearEnd()    \n",
    "            df_unc_dataset = df_unc_dataset[list_dataset_columns]\n",
    "        elif (len(list_unc_dataset) == 1):\n",
    "            df_unc_dataset = pd.DataFrame(columns = list_dataset_columns)    \n",
    "        else:\n",
    "            df_unc_dataset = pd.DataFrame([[np.NaN] * len(list_dataset_columns)], columns = list_dataset_columns)\n",
    "    else:\n",
    "        df_unc_dataset = pd.DataFrame(columns = list_dataset_columns)\n",
    "    ### Results output:\n",
    "    return df_unc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: SERVICES ANNUAL DATA EXTRACTION SCRIPT\n",
    "\n",
    "### Concatenation aggregator initializing:\n",
    "list_dataset = []\n",
    "### 5-length country pairs collection:\n",
    "list_empty_requests = list(itertools.product(range((len(ser_UNC_country_id.index) - 1) // int_unc_limit + 1), repeat = 2))\n",
    "### Looping over 5-length country pairs:\n",
    "while list_empty_requests:\n",
    "    iter_country_pair = list_empty_requests[0]\n",
    "    iter_reporter_group = iter_country_pair[0]    \n",
    "    iter_partner_group = iter_country_pair[1]\n",
    "    print(iter_reporter_group * int_unc_limit, '-', (iter_reporter_group + 1) * int_unc_limit - 1, '/', \n",
    "          iter_partner_group * int_unc_limit, '-', (iter_partner_group + 1) * int_unc_limit - 1)     \n",
    "    ### Country groups preparing:\n",
    "    list_reporter_group = ser_UNC_country_id.iloc[iter_reporter_group * int_unc_limit : (iter_reporter_group + 1) * int_unc_limit].to_list()\n",
    "    list_partner_group = ser_UNC_country_id.iloc[iter_partner_group * int_unc_limit : (iter_partner_group + 1) * int_unc_limit].to_list()    \n",
    "    str_reporter_group = ','.join(ser_UNC_country_id.iloc[iter_reporter_group * int_unc_limit : (iter_reporter_group + 1) * int_unc_limit].to_list())\n",
    "    str_partner_group = ','.join(ser_UNC_country_id.iloc[iter_partner_group * int_unc_limit : (iter_partner_group + 1) * int_unc_limit].to_list())    \n",
    "    ### Last single country list control to avoid endless loop:\n",
    "    if ((len(list_reporter_group) > 1) | (len(list_partner_group) > 1)):\n",
    "        ### Request performing:\n",
    "        df_iter_dataset = get_un_comtrade_data(str_reporter_group, str_partner_group, str_trade_flow = 'Both', str_type = 'S', \\\n",
    "                                               str_classification_code = 'all', str_classification_system = 'EB02',\n",
    "                                               str_period = ','.join(map(str, range(date_start.year, date_end.year + 1))))\n",
    "        if (len(df_iter_dataset) > 1):        \n",
    "            list_dataset += [df_iter_dataset]\n",
    "            print(len(df_iter_dataset), 'rows of data loaded successfully')\n",
    "            list_empty_requests.remove(iter_country_pair)\n",
    "        elif (df_iter_dataset.isna().sum().sum() == len(df_iter_dataset.columns)):\n",
    "            print('Empty response (no data found)')\n",
    "            list_empty_requests.remove(iter_country_pair)\n",
    "        else:\n",
    "            print('API error occured')\n",
    "        gc.collect()\n",
    "        time.sleep(int_seconds_to_sleep)    \n",
    "    else:\n",
    "        print('Two lists contains the same single country: no data')\n",
    "        list_empty_requests.remove(iter_country_pair)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: SERVICES ANNUAL RAW DATA SAVING\n",
    "\n",
    "### Raw data concatenating:\n",
    "df_loop_dataset = pd.concat(list_dataset, axis = 0, sort = False, ignore_index = True)\n",
    "del list_dataset\n",
    "gc.collect()\n",
    "### Raw data saving:\n",
    "df_loop_dataset.to_hdf(path_or_buf = str_path_unc_raw_serv_annual, key = str_key_unc_raw, mode = 'w', format = 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: GOODS ANNUAL DATA EXTRACTION SCRIPT\n",
    "\n",
    "gc.collect()\n",
    "### Concatenation aggregator initializing:\n",
    "list_dataset = []\n",
    "### 5-length country pairs collection:\n",
    "list_empty_requests = list(itertools.product(range((len(ser_UNC_country_id.index) - 1) // int_unc_limit + 1), repeat = 2))#[0 : 5]\n",
    "### Looping over 5-length country pairs:\n",
    "while list_empty_requests:\n",
    "    iter_country_pair = list_empty_requests[0]\n",
    "    iter_reporter_group = iter_country_pair[0]    \n",
    "    iter_partner_group = iter_country_pair[1]\n",
    "    print(iter_reporter_group * int_unc_limit, '-', (iter_reporter_group + 1) * int_unc_limit - 1, '/', \n",
    "          iter_partner_group * int_unc_limit, '-', (iter_partner_group + 1) * int_unc_limit - 1)     \n",
    "    ### Country groups preparing:\n",
    "    list_reporter_group = ser_UNC_country_id.iloc[iter_reporter_group * int_unc_limit : (iter_reporter_group + 1) * int_unc_limit].to_list()\n",
    "    list_partner_group = ser_UNC_country_id.iloc[iter_partner_group * int_unc_limit : (iter_partner_group + 1) * int_unc_limit].to_list()    \n",
    "    str_reporter_group = ','.join(ser_UNC_country_id.iloc[iter_reporter_group * int_unc_limit : (iter_reporter_group + 1) * int_unc_limit].to_list())\n",
    "    str_partner_group = ','.join(ser_UNC_country_id.iloc[iter_partner_group * int_unc_limit : (iter_partner_group + 1) * int_unc_limit].to_list())    \n",
    "    ### Last single country list control to avoid endless loop:\n",
    "    if ((len(list_reporter_group) > 1) | (len(list_partner_group) > 1)):\n",
    "        ### Request performing:\n",
    "        df_iter_dataset = get_un_comtrade_data(str_reporter_group, str_partner_group, str_trade_flow = 'Both', \n",
    "                                               str_classification_code = 'AG2', str_classification_system = 'H0', \n",
    "                                               str_period = ','.join(map(str, range(date_start.year, date_end.year + 1))))\n",
    "        if (len(df_iter_dataset) > 1):        \n",
    "            list_dataset += [df_iter_dataset]\n",
    "            print(len(df_iter_dataset), 'rows of data loaded successfully')\n",
    "            list_empty_requests.remove(iter_country_pair)\n",
    "        elif (df_iter_dataset.isna().sum().sum() == len(df_iter_dataset.columns)):\n",
    "            print('Empty response (no data found)')\n",
    "            list_empty_requests.remove(iter_country_pair)\n",
    "        else:\n",
    "            print('API error occured')\n",
    "        gc.collect()\n",
    "        time.sleep(int_seconds_to_sleep)    \n",
    "    else:\n",
    "        print('Two lists contains the same single country: no data')\n",
    "        list_empty_requests.remove(iter_country_pair)                                \n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: GOODS ANNUAL RAW DATA SAVING\n",
    "\n",
    "if (os.path.exists(str_path_unc_raw_comm_annual)):\n",
    "    os.remove(str_path_unc_raw_comm_annual)\n",
    "### Raw data saving:\n",
    "for df_iter in list_dataset:\n",
    "    df_iter.to_hdf(path_or_buf = str_path_unc_raw_comm_annual, key = str_key_unc_raw, mode = 'a', format = 'table', append = True)\n",
    "del list_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: SERVICES ANNUAL RAW DATA LOADING & CONVERTING\n",
    "\n",
    "list_chunks = []\n",
    "for df_iter_cast in pd.read_hdf(path_or_buf = str_path_unc_raw_serv_annual, key = str_key_unc_raw, chunksize = 1000000):\n",
    "    gc.collect()\n",
    "    ### Casting trade volume:\n",
    "    df_iter_cast['Value'] = (df_iter_cast['Value'] / 1000).astype('int32')\n",
    "    df_iter_cast['Value'].clip(lower = 0, inplace = True)\n",
    "#    df_iter_cast = df_iter_cast[df_iter_cast['Reporter_ID'] != df_iter_cast['Partner_ID']]\n",
    "    ### Replacing codes with values:\n",
    "    df_iter_cast['Reporter_ID'].replace(dict(zip(list(map(int, ser_UNC_country_id.values)), ser_UNC_country_id.index.to_list())), inplace = True)\n",
    "    df_iter_cast['Partner_ID'].replace(dict(zip(list(map(int, ser_UNC_country_id.values)), ser_UNC_country_id.index.to_list())), inplace = True)    \n",
    "    df_iter_cast.loc[All, 'Partner_ID'].replace({'ZZ': 'World'}, inplace = True)    \n",
    "    ### Dropping Saudi Arabia to Taiwan Export (https://unstats.un.org/wiki/display/comtrade/Taiwan%2C+Province+of+China+Trade+data) \n",
    "    df_iter_cast.drop(df_iter_cast[(df_iter_cast['Reporter_ID'] == 'SA') & (df_iter_cast['Partner_ID'] == 'TW')].index, inplace = True)    \n",
    "#    df_iter_cast['Commodity_ID'].replace(dict_comm_classification, inplace = True)\n",
    "    df_iter_cast['Flow_ID'].replace({1: 'Import', 2: 'Export'}, inplace = True)\n",
    "    ### Adding Type column:\n",
    "    df_iter_cast['Type'] = 'Services'\n",
    "    ### Categorization:    \n",
    "    df_iter_cast.columns = ['Date', 'Reporter', 'Partner', 'Flow', 'Commodity_ID', 'Value', 'Type']\n",
    "    df_iter_cast = df_iter_cast.astype({'Reporter': 'category', 'Partner': 'category', 'Flow': 'category', 'Type': 'category'})\n",
    "#    df_iter_cast = df_iter_cast.astype({'Reporter': 'category', 'Partner': 'category', 'Flow': 'category', 'Commodity_ID': 'category', 'Type': 'category'})    \n",
    "    df_iter_cast['Reporter'] = df_iter_cast['Reporter'].cat.set_categories(ser_UNC_country_id.index.to_list()[: -1] + ['World'])\n",
    "    df_iter_cast['Partner'] = df_iter_cast['Partner'].cat.set_categories(ser_UNC_country_id.index.to_list()[: -1] + ['World'])\n",
    "    df_iter_cast['Flow'] = df_iter_cast['Flow'].cat.set_categories(['Export', 'Import'])\n",
    "    df_iter_cast['Type'] = df_iter_cast['Type'].cat.set_categories(['Goods', 'Services'])    \n",
    "    ### Adding chunk to collection:\n",
    "    list_chunks.append(df_iter_cast)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: GOODS ANNUAL RAW DATA LOADING & CONVERTING\n",
    "\n",
    "#list_chunks = []\n",
    "for df_iter_cast in pd.read_hdf(path_or_buf = str_path_unc_raw_comm_annual, key = str_key_unc_raw, chunksize = 1000000):\n",
    "    gc.collect()\n",
    "    ### Casting trade volume:\n",
    "    df_iter_cast['Value'] = (df_iter_cast['Value'] / 1000).astype('int32')\n",
    "#    df_iter_cast = df_iter_cast[df_iter_cast['Reporter_ID'] != df_iter_cast['Partner_ID']]\n",
    "    ### Replacing codes with values:\n",
    "    df_iter_cast['Reporter_ID'].replace(dict(zip(list(map(int, ser_UNC_country_id.values)), ser_UNC_country_id.index.to_list())), inplace = True)\n",
    "    df_iter_cast['Partner_ID'].replace(dict(zip(list(map(int, ser_UNC_country_id.values)), ser_UNC_country_id.index.to_list())), inplace = True)    \n",
    "    df_iter_cast.loc[All, 'Partner_ID'].replace({'ZZ': 'World'}, inplace = True)    \n",
    "    ### Dropping Saudi Arabia to Taiwan Export (https://unstats.un.org/wiki/display/comtrade/Taiwan%2C+Province+of+China+Trade+data) \n",
    "    df_iter_cast.drop(df_iter_cast[(df_iter_cast['Reporter_ID'] == 'SA') & (df_iter_cast['Partner_ID'] == 'TW')].index, inplace = True)    \n",
    "#    df_iter_cast['Commodity_ID'].replace(dict_comm_classification, inplace = True)\n",
    "    df_iter_cast['Flow_ID'].replace({1: 'Import', 2: 'Export'}, inplace = True)\n",
    "    ### Adding Type column:\n",
    "    df_iter_cast['Type'] = 'Goods'\n",
    "    ### Categorization:    \n",
    "    df_iter_cast.columns = ['Date', 'Reporter', 'Partner', 'Flow', 'Commodity_ID', 'Value', 'Type']\n",
    "    df_iter_cast = df_iter_cast.astype({'Reporter': 'category', 'Partner': 'category', 'Flow': 'category', 'Type': 'category'})\n",
    "#    df_iter_cast = df_iter_cast.astype({'Reporter': 'category', 'Partner': 'category', 'Flow': 'category', 'Commodity_ID': 'category', 'Type': 'category'})    \n",
    "    df_iter_cast['Reporter'] = df_iter_cast['Reporter'].cat.set_categories(ser_UNC_country_id.index.to_list()[: -1] + ['World'])\n",
    "    df_iter_cast['Partner'] = df_iter_cast['Partner'].cat.set_categories(ser_UNC_country_id.index.to_list()[: -1] + ['World'])\n",
    "    df_iter_cast['Flow'] = df_iter_cast['Flow'].cat.set_categories(['Export', 'Import'])\n",
    "    df_iter_cast['Type'] = df_iter_cast['Type'].cat.set_categories(['Goods', 'Services'])    \n",
    "    ### Adding chunk to collection:    \n",
    "    list_chunks.append(df_iter_cast)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "948254266\n",
      "729443916\n",
      "474153092\n"
     ]
    }
   ],
   "source": [
    "### UN COMTRADE: SERVICES & GOODS CONVERTED ANNUAL DATA AGGREGATING & RE-SAVING\n",
    "\n",
    "gc.collect()\n",
    "df_cast_dataset = pd.concat(list_chunks, axis = 0, sort = False, ignore_index = True)\n",
    "del list_chunks\n",
    "gc.collect()\n",
    "print(df_cast_dataset.memory_usage().sum())\n",
    "df_cast_dataset['Commodity_ID'] = df_cast_dataset['Commodity_ID'].astype('category')\n",
    "print(df_cast_dataset.memory_usage().sum())\n",
    "ser_cast_dataset = df_cast_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "del df_cast_dataset\n",
    "gc.collect()\n",
    "print(ser_cast_dataset.memory_usage())\n",
    "ser_cast_dataset.to_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, mode = 'w', format = 'table', complevel = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: AGGREGATED TOTAL EXTRACTING TO USE IN ACADIAN STAND\n",
    "\n",
    "ser_zz_data = pd.read_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, where = \"(Partner = 'World') & ('Flow' in ['Export', 'Import'])\")\n",
    "ser_zz_data.groupby(['Date', 'Reporter', 'Flow']).sum().dropna().to_hdf(path_or_buf = str_path_unc_res_all_zz, key = str_key_unc_res, mode = 'w', format = 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "pd.read_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, stop = 10000)\n",
    "pd.read_hdf(path_or_buf = str_path_unc_res_all_zz, key = str_key_unc_res, stop = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPORT AND REVERTED IMPORT CONCATENATION\n",
    "\n",
    "gc.collect()\n",
    "### File deleting:\n",
    "if (os.path.exists(str_path_unc_res_flows)):\n",
    "    os.remove(str_path_unc_res_flows)\n",
    "### Results container:\n",
    "list_export_aug = []\n",
    "### Countries portion length:\n",
    "int_portion = 5\n",
    "### Looping over countrie portions:\n",
    "for iter_num in range(len(ser_UNC_country_id.index.to_list()) // int_portion + 1):\n",
    "#for iter_num in range(2):\n",
    "    gc.collect()    \n",
    "    ### Portion of countries selecting:\n",
    "    list_iter_countries = list(ser_UNC_country_id.index.to_list())[int_portion * iter_num : int_portion * (iter_num + 1)]\n",
    "    if (len(list_iter_countries) > 0):\n",
    "        print(list_iter_countries)\n",
    "        ### Export data loading:\n",
    "        ser_unc_export = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res,\n",
    "                                     where = \"(Flow = 'Export') & (Reporter in list_iter_countries) & (Partner != 'World')\").droplevel('Flow')\n",
    "        print('Export dataset loaded')\n",
    "        ### Import data loading:\n",
    "        ser_unc_import = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res, \n",
    "                                     where = \"(Flow = 'Import') & (Partner in list_iter_countries)\").droplevel('Flow')\n",
    "        print('Import dataset loaded')    \n",
    "        ### Import data reverting:\n",
    "        ser_unc_import.index.set_names('Partner_Inv', level = 1, inplace = True)\n",
    "        ser_unc_import.index.set_names('Reporter', level = 2, inplace = True)\n",
    "        ser_unc_import.index.set_names('Partner', level = 1, inplace = True)\n",
    "        ser_unc_import = ser_unc_import.swaplevel('Reporter', 'Partner').sort_index()\n",
    "        print('Import dataset reverted')\n",
    "        ### Datasets concatenation:\n",
    "        df_export_aug = pd.concat([ser_unc_export, ser_unc_import], axis = 1, names = 'Source Flow', keys = ['Export', 'Import']).astype('float32')\n",
    "        del ser_unc_export\n",
    "        del ser_unc_import    \n",
    "        gc.collect()    \n",
    "        print('Export and reverted Import dataset concatenated')\n",
    "        ### Columns categorization:\n",
    "        df_export_aug.to_hdf(str_path_unc_res_flows, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, append = True)                \n",
    "#        df_export_aug = df_export_aug.astype({'Export': 'Int32', 'Import': 'Int32'})\n",
    "#        df_export_reseted['Reporter'] = df_export_reseted['Reporter'].cat.set_categories(ser_UNC_country_id.index.to_list()[: -1] + ['World'])\n",
    "#        df_export_reseted['Partner'] = df_export_reseted['Partner'].cat.set_categories(ser_UNC_country_id.index.to_list()[: -1] + ['World'])\n",
    "#        df_export_reseted['Type'] = df_export_reseted['Type'].cat.set_categories(['Goods', 'Services'])\n",
    "#        df_export_aug = df_export_aug.join(ser_ison_status, on = 'Reporter').set_index('Market', append = True).sort_index()\n",
    "#        print('Concatenated dataset index sorted')\n",
    "#        list_export_aug.append(df_export_aug)\n",
    "        print('Aggregated dataset added to container')\n",
    "    #    break\n",
    "#### Container elements concatenation:\n",
    "#df_export_aug_full = pd.concat(list_export_aug, axis = 0, sort = False)\n",
    "#del list_export_aug\n",
    "#gc.collect()    \n",
    "#### Aggregated table indexation:\n",
    "#df_export_aug_full = df_export_aug_full.sort_index()\n",
    "#### Results saving:\n",
    "#df_export_aug_full.astype(float).to_hdf(str_path_unc_res_flows, key = str_key_unc_res, mode = 'w', format = 'table', complevel = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIF COEFFICIENTS CALCULATION & IMPLEMENTATION\n",
    "\n",
    "gc.collect()\n",
    "### Files deleting:\n",
    "if (os.path.exists(str_path_export_bilateral)):\n",
    "    os.remove(str_path_export_bilateral)\n",
    "if (os.path.exists(str_path_import_bilateral)):\n",
    "    os.remove(str_path_import_bilateral)\n",
    "### Getting list of commodities:\n",
    "str_date = '2020-12-31'\n",
    "list_commodity_id = sorted(pd.read_hdf(str_path_unc_res_flows, key = str_key_unc_res, where = \"Date in str_date\").index.get_level_values('Commodity_ID').unique())\n",
    "### Bounds to filter bilateral Import to Export ratio before median calculation:\n",
    "flo_lower_bound = 1.0\n",
    "flo_upper_bound = 2.0\n",
    "### Bilateral median calculation procedure:\n",
    "def get_obs_median(df_comm):\n",
    "    ### Export to Import ratio:\n",
    "    ser_obs_coeff = df_comm['Import'] / df_comm['Export']\n",
    "    ### Ratio filtering:\n",
    "    ser_obs_coeff = ser_obs_coeff.loc[(ser_obs_coeff >= flo_lower_bound) & (ser_obs_coeff <= flo_upper_bound)]\n",
    "    ### Filtered timeseries median as a result:\n",
    "    return ser_obs_coeff.median()\n",
    "### Calulation CIF coefficient for all commodities:\n",
    "for iter_commodity in list_commodity_id:\n",
    "    gc.collect()\n",
    "    df_iter_flows = pd.read_hdf(str_path_unc_res_flows, key = str_key_unc_res, where = \"Commodity_ID = iter_commodity\")\n",
    "    ser_cif_median = df_iter_flows.droplevel('Commodity_ID').groupby(['Reporter', 'Partner']).apply(get_obs_median)\n",
    "    ### General commodity median calculation:\n",
    "    flo_median = ser_cif_median.median()\n",
    "    print(iter_commodity, ':', flo_median)\n",
    "    ### Filling missed bilateral values with general commodity median:\n",
    "    if not (np.isnan(flo_median)):\n",
    "        ser_cif_median.fillna(flo_median, inplace = True)        \n",
    "    ser_cif_median.name = 'CIF_Coefficient'              \n",
    "    ### Adding CIF coefficients to dataset:\n",
    "    df_export_cif = df_iter_flows.merge(ser_cif_median, left_index = True, right_index = True)\n",
    "    df_export_cif = df_export_cif.reorder_levels(['Date', 'Reporter', 'Partner', 'Type', 'Commodity_ID'])\n",
    "    ### Import correction:\n",
    "    df_export_cif['Import_Corrected'] = df_export_cif['Import'] / df_export_cif['CIF_Coefficient']\n",
    "    ### Export correction:\n",
    "    df_export_cif['Export_Corrected'] = df_export_cif['Export'] * df_export_cif['CIF_Coefficient']\n",
    "    ### Combining Export & Import data:\n",
    "    ser_export_cif = df_export_cif['Export'].combine_first(df_export_cif['Import_Corrected']).astype('float32')\n",
    "    ser_import_cif = df_export_cif['Import'].combine_first(df_export_cif['Export_Corrected']).astype('float32')\n",
    "#    del df_export_cif\n",
    "    gc.collect()\n",
    "    ser_import_cif = ser_import_cif.reorder_levels(['Date', 'Partner', 'Reporter', 'Type', 'Commodity_ID']).sort_index()                               \n",
    "    ser_import_cif.index.names = ['Date', 'Reporter', 'Partner', 'Type', 'Commodity_ID']\n",
    "    gc.collect()\n",
    "    ### Incorporation options:\n",
    "    ser_export_cif.squeeze().to_hdf(str_path_export_bilateral, key = str_key_unc_export, mode = 'a', format = 'table', complevel = 9, append = True, \n",
    "                                    min_itemsize = {'Type': 8, 'Commodity_ID': 3})\n",
    "    ser_import_cif.squeeze().to_hdf(str_path_import_bilateral, key = str_key_unc_import, mode = 'a', format = 'table', complevel = 9, append = True, \n",
    "                                    min_itemsize = {'Type': 8, 'Commodity_ID': 3}) \n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BILATERAL EXPORT DATA LOADING TO PERFORM FACTOR CALCULATION\n",
    "\n",
    "gc.collect()\n",
    "list_export_chunks = []\n",
    "for num_iter_number, ser_iter_chunk in enumerate(pd.read_hdf(str_path_export_bilateral, key = str_key_unc_export, chunksize = 1000000)):\n",
    "    gc.collect()\n",
    "    print(num_iter_number, ': Extraction started')\n",
    "    ser_iter_chunk = ser_iter_chunk[ser_iter_chunk > 0.0].astype('int32')\n",
    "    df_iter_chunk = ser_iter_chunk.reset_index()\n",
    "    df_iter_chunk = df_iter_chunk[(df_iter_chunk['Reporter'] != df_iter_chunk['Partner']) & \\\n",
    "                                ((df_iter_chunk['Type'] == 'Goods') | df_iter_chunk['Commodity_ID'].isin(list_services))]\\\n",
    "                               .drop('Type', axis = 1)    \n",
    "    print(num_iter_number, ': Filtering performed')    \n",
    "    ser_iter_chunk = df_iter_chunk.set_index(['Date', 'Reporter', 'Partner', 'Commodity_ID']).squeeze().sort_index()\n",
    "    del df_iter_chunk\n",
    "    gc.collect()\n",
    "    list_export_chunks.append(ser_iter_chunk)\n",
    "    print(num_iter_number, ': Chunk added to container')    \n",
    "ser_bilateral_export = pd.concat(list_export_chunks, axis = 0, sort = False).sort_index()\n",
    "ser_bilateral_export.name = 'Export'\n",
    "del list_export_chunks\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### BILATERAL IMPORT DATA LOADING TO PERFORM FACTOR CALCULATION\n",
    "\n",
    "gc.collect()\n",
    "list_import_chunks = []\n",
    "for num_iter_number, ser_iter_chunk in enumerate(pd.read_hdf(str_path_import_bilateral, key = str_key_unc_import, chunksize = 1000000)):\n",
    "    gc.collect()\n",
    "    print(num_iter_number, ': Extraction started')\n",
    "    ser_iter_chunk = ser_iter_chunk[ser_iter_chunk > 0.0].astype('int32')\n",
    "    df_iter_chunk = ser_iter_chunk.reset_index()\n",
    "    df_iter_chunk = df_iter_chunk[(df_iter_chunk['Reporter'] != df_iter_chunk['Partner']) & \\\n",
    "                                ((df_iter_chunk['Type'] == 'Goods') | df_iter_chunk['Commodity_ID'].isin(list_services))]\\\n",
    "                               .drop('Type', axis = 1)    \n",
    "    print(num_iter_number, ': Filtering performed')    \n",
    "    ser_iter_chunk = df_iter_chunk.set_index(['Date', 'Reporter', 'Partner', 'Commodity_ID']).squeeze().sort_index()\n",
    "    del df_iter_chunk\n",
    "    gc.collect()\n",
    "    list_import_chunks.append(ser_iter_chunk)\n",
    "    print(num_iter_number, ': Chunk added to container')    \n",
    "ser_bilateral_import = pd.concat(list_import_chunks, axis = 0, sort = False).sort_index()\n",
    "ser_bilateral_import.name = 'Import'\n",
    "del list_import_chunks\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPORTER / COMMODITY BY DATE TOTAL EXPORT & IMPORT & TRADE\n",
    "\n",
    "gc.collect()\n",
    "### Export totals:\n",
    "ser_country_comm_export = ser_bilateral_export.groupby(['Date', 'Reporter', 'Commodity_ID']).sum().dropna()\n",
    "ser_country_comm_export.name = 'Export'\n",
    "### Import totals:\n",
    "ser_country_comm_import = ser_bilateral_import.groupby(['Date', 'Reporter', 'Commodity_ID']).sum().dropna()\n",
    "ser_country_comm_import.name = 'Import'\n",
    "### Adding trade totals:\n",
    "df_country_comm_trade = pd.concat([ser_country_comm_export, ser_country_comm_import], axis = 1)\n",
    "df_country_comm_trade = df_country_comm_trade.unstack('Date').stack('Date', dropna = False)\n",
    "df_country_comm_trade = df_country_comm_trade.unstack('Reporter').stack('Reporter', dropna = False)\n",
    "df_country_comm_trade = df_country_comm_trade.unstack('Commodity_ID').stack('Commodity_ID', dropna = False).fillna(0.0)\n",
    "df_country_comm_trade['Trade'] = df_country_comm_trade['Export'] + df_country_comm_trade['Import']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TOTAL COMMODITY BY DATE TRADE VOLUME & CROSS-SECTIONAL WEIGHT\n",
    "\n",
    "### Total commodity volume:\n",
    "ser_commodity_trade = df_country_comm_trade.groupby(['Date', 'Commodity_ID'])['Trade'].sum().swaplevel().sort_index()\n",
    "ser_commodity_trade.name = 'Commodity_Total'\n",
    "df_commodity_trade = ser_commodity_trade.to_frame()\n",
    "### Total commodity weight:\n",
    "df_commodity_trade['Commodity_Weight'] = df_commodity_trade['Commodity_Total'].groupby('Date').transform(lambda df_group: df_group / df_group.sum())\n",
    "### Adding missed years:\n",
    "def reindex_annual(df_group):\n",
    "    df_result = df_group.droplevel('Commodity_ID').resample('BY').last()\n",
    "    df_result = df_result.reindex(pd.date_range(df_result.index[0], str_date_end, freq = 'BY'))\n",
    "    return df_result\n",
    "df_commodity_trade = df_commodity_trade.groupby('Commodity_ID').apply(reindex_annual)\n",
    "### Resampling to monthly data:\n",
    "def reindex_monthly(df_group):\n",
    "    df_result = df_group.droplevel('Commodity_ID').resample('BM').last()\n",
    "    df_result = df_result.reindex(pd.date_range(df_result.index[0], str_date_end, freq = 'BM'))\n",
    "    df_result = df_result.ffill()\n",
    "    return df_result\n",
    "df_commodity_trade = df_commodity_trade.groupby('Commodity_ID').apply(reindex_monthly)\n",
    "df_commodity_trade.index.names = ['Commodity_ID', 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPORTER / COMMODITY BY DATE PAGE RANK\n",
    "\n",
    "gc.collect()\n",
    "def get_pagerank(df_group):\n",
    "    nx_graph = nx.from_pandas_edgelist(df_group, 'Reporter', 'Partner', edge_attr = 'Export', create_using = nx.DiGraph)\n",
    "    dict_pagerank = nx.pagerank(nx_graph)\n",
    "    ser_pagerank = pd.Series(dict_pagerank)\n",
    "    return ser_pagerank\n",
    "    \n",
    "ser_export_pagerank = ser_bilateral_export.reset_index().groupby(['Date', 'Commodity_ID']).apply(get_pagerank)\n",
    "ser_export_pagerank.name = 'PG_Rank_Local'\n",
    "ser_export_pagerank.index.names = ['Date', 'Commodity_ID', 'Reporter']\n",
    "ser_export_pagerank = ser_export_pagerank.reorder_levels(['Date', 'Reporter', 'Commodity_ID']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JOINING PAGE RANK TO REPORTER / COMMODITY BY DATE TRADE & REINDEXAITION / RESAMPLING TO MONTHLY FREQUENCY\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "df_country_comm_ranked = df_country_comm_trade['Trade'].to_frame().join(ser_export_pagerank, how = 'right')\\\n",
    "                                                       .reorder_levels(['Commodity_ID', 'Reporter', 'Date']).sort_index()\n",
    "del df_country_comm_trade\n",
    "gc.collect()\n",
    "### Adding missed years:\n",
    "def reindex_annual(df_group):\n",
    "    df_result = df_group.droplevel(['Commodity_ID', 'Reporter']).resample('BY').last()\n",
    "    df_result = df_result.reindex(pd.date_range(df_result.index[0], str_date_end, freq = 'BY'))\n",
    "    return df_result\n",
    "df_country_comm_ranked = df_country_comm_ranked.groupby(['Commodity_ID', 'Reporter']).apply(reindex_annual).fillna(-1.0)\n",
    "### Resampling to monthly data:\n",
    "def reindex_monthly(df_group):\n",
    "    df_result = df_group.droplevel(['Commodity_ID', 'Reporter']).resample('BM').last()\n",
    "    df_result = df_result.reindex(pd.date_range(df_result.index[0], str_date_end, freq = 'BM'))\n",
    "    df_result = df_result.ffill()\n",
    "    return df_result\n",
    "df_country_comm_ranked = df_country_comm_ranked.groupby(['Commodity_ID', 'Reporter']).apply(reindex_monthly).replace({-1.0: np.NaN})\n",
    "df_country_comm_ranked.index.names = ['Commodity_ID', 'Reporter', 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADDING ISON REGIONS AND COMMODITY TOTAL TRADE VOLUME\n",
    "\n",
    "### HERE WE CAN SELECT TO USE BY DATE ISON MEMBERSHIP OR LAST DATE (CURRENT) ISON STATUS !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "gc.collect()\n",
    "### Adding Regions by Date:\n",
    "#df_country_comm_isoned = df_country_comm_ranked.join(ser_ison_membership.swaplevel().sort_index())\\\n",
    "#                                               .dropna(subset = ['Market']).reorder_levels(['Commodity_ID', 'Date', 'Reporter']).sort_index()\n",
    "#### Adding last date Regions:\n",
    "#df_country_comm_isoned = df_country_comm_ranked.join(ser_ison_status).dropna(subset = ['Market']).reorder_levels(['Commodity_ID', 'Date', 'Reporter']).sort_index()\n",
    "#df_country_comm_isoned = df_country_comm_isoned.join(df_commodity_trade['Commodity_Weight']).reorder_levels(['Commodity_ID', 'Reporter', 'Date']).sort_index()\n",
    "#df_country_comm_isoned = df_country_comm_isoned.set_index('Market', append = True)\n",
    "### Not adding Markets:\n",
    "df_country_comm_isoned = df_country_comm_ranked.reorder_levels(['Commodity_ID', 'Date', 'Reporter']).sort_index()\n",
    "del df_country_comm_ranked\n",
    "gc.collect()\n",
    "df_country_comm_isoned = df_country_comm_isoned.join(df_commodity_trade['Commodity_Weight']).reorder_levels(['Commodity_ID', 'Reporter', 'Date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADDING ALTERNATIVE PAGERANK & CALCULATING OF WEIGHTED AVERAGE FOR BOTH OPTIONS:\n",
    "\n",
    "### GLOBAL RANK IS A LOCAL ONE MULTIPLIED BY TOTAL COMMODITY CROSS-SECTIONAL WEIGHT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "gc.collect()\n",
    "### Page Rank Weighted by Commodity by Date Weight:\n",
    "df_country_comm_isoned['PG_Rank_Global'] = df_country_comm_isoned['PG_Rank_Local'] * df_country_comm_isoned['Commodity_Weight']\n",
    "### Page Ranks Weighting:\n",
    "df_pg_rank_mean = pd.DataFrame()\n",
    "for iter_pg_rank in ['PG_Rank_Local', 'PG_Rank_Global']:\n",
    "#    df_pg_rank_mean[iter_pg_rank] = df_country_comm_isoned.groupby(['Reporter', 'Date', 'Market'])\\\n",
    "#                                                          .apply(lambda df_group: weighted_average(df_group[iter_pg_rank], df_group['Trade']))\n",
    "    df_pg_rank_mean[iter_pg_rank] = df_country_comm_isoned.groupby(['Reporter', 'Date'])\\\n",
    "                                                          .apply(lambda df_group: weighted_average(df_group[iter_pg_rank], df_group['Trade']))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FACTOR SAVING\n",
    "\n",
    "df_pg_rank_mean.to_excel(str_path_factor_xlsx, merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        Reporter  Commodity_ID\n",
       "1989-12-29  AD        01              0.003644\n",
       "                      02              0.003563\n",
       "                      03              0.002449\n",
       "                      04              0.003163\n",
       "                      05              0.002600\n",
       "                                        ...   \n",
       "2022-12-30  ZW        94              0.002828\n",
       "                      95              0.001660\n",
       "                      96              0.002464\n",
       "                      97              0.001832\n",
       "                      99              0.002258\n",
       "Name: PG_Rank_Local, Length: 744944, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_export_pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
