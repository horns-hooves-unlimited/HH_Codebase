{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMTRADE DATASETS EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import gc\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISABLING OF WARNINGS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS (RESEARCH VERSION ONLY)\n",
    "\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### NA for MS Excel files:\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable', '---']\n",
    "### UN Comtrade adopted data containers:\n",
    "str_path_unc_res_all_annual = 'Data_Files/Source_Files/unc_res_all_annual.h5'\n",
    "str_path_unc_res_services_annual = 'Data_Files/Source_Files/unc_res_services_annual.h5'\n",
    "str_key_unc_res = 'unc_res'\n",
    "### File with aggregated flows:\n",
    "str_path_unc_res_flows = 'Data_Files/Source_Files/unc_res_flows.h5'\n",
    "### Universal HDF5 key:\n",
    "str_key_unc_res = 'unc_res'\n",
    "### Augmented bilateral export:\n",
    "str_path_export_bilateral = 'Data_Files/Source_Files/comtrade_export_bilateral.h5'\n",
    "### Export key:\n",
    "str_key_unc_export = 'export_augmented'\n",
    "### Augmented bilateral import:\n",
    "str_path_import_bilateral = 'Data_Files/Source_Files/comtrade_import_bilateral.h5'\n",
    "### Import key:\n",
    "str_key_unc_import = 'import_augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN CONSTANTS\n",
    "\n",
    "### Dates:\n",
    "date_start = pd.Timestamp('1989-12-29')\n",
    "date_end = pd.Timestamp('2022-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING & LAUNCH COUNTRY CODES EXTRACTOR (RESEARCH VERSION ONLY)\n",
    "\n",
    "def get_country_codes(use_local_copy = False):  \n",
    "    ### In case if URL is unavailable:\n",
    "    if (use_local_copy):\n",
    "        url_country_code = 'Data_Files/Source_Files/countrycode.html'\n",
    "    ### Online extraction:\n",
    "    else:\n",
    "        url_country_code = 'https://countrycode.org/'\n",
    "    df_full_codes = pd.read_html(url_country_code, index_col = 'COUNTRY')[0]\n",
    "    df_full_codes[['ISO SHORT', 'ISO LONG']] = df_full_codes['ISO CODES'].str.split(' / ', expand = True)\n",
    "    df_result = df_full_codes[['ISO SHORT', 'ISO LONG']].sort_index()    \n",
    "    df_result.index = df_result.index.str.upper()\n",
    "    ### Results output:\n",
    "    return df_result\n",
    "\n",
    "### World Country Codes:\n",
    "df_country_codes = get_country_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: CODELISTS LOADING (PRODUCT VERSION)\n",
    "\n",
    "### Codelists container:\n",
    "dict_codelist = {}\n",
    "### List of reference tables for categories (request parameters) loading:\n",
    "request_session = requests.Session()\n",
    "obj_unc_reference = request_session.get('https://comtradeapi.un.org/files/v1/app/reference/ListofReferences.json')\n",
    "df_cat_reference = pd.DataFrame(obj_unc_reference.json()['results']).set_index('category')\n",
    "### Parameters:\n",
    "ser_type_code = pd.Series(['Goods', 'Services'], index = ['C', 'S'])\n",
    "ser_type_code.index.names = ['id']\n",
    "ser_type_code.name = 'typeCode'\n",
    "dict_codelist['typeCode'] = ser_type_code\n",
    "### Commodity HS Codes:\n",
    "df_hs_comm = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:HS']['fileuri']).json()['results'])\n",
    "### Filtering needed level of coomodity groups aggregation:\n",
    "ser_hs_comm_ag2 = df_hs_comm[df_hs_comm['aggrLevel'] == 2].drop(['parent', 'isLeaf', 'aggrLevel'], axis = 1).set_index('id').squeeze().str[5: ]\n",
    "ser_hs_comm_ag2.name = 'clCode'\n",
    "dict_codelist['clCode'] = {}\n",
    "dict_codelist['clCode']['C'] = ser_hs_comm_ag2\n",
    "### Service EBOPS Codes:\n",
    "df_eb_serv = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:EB']['fileuri']).json()['results'])\n",
    "### Filtering needed level of services groups aggregation:\n",
    "df_eb_serv_ag2 = df_eb_serv[df_eb_serv['parent'].isin(df_eb_serv[df_eb_serv['parent'] == '200']['id'])]\n",
    "ser_eb_serv_ag2 = df_eb_serv_ag2[df_eb_serv_ag2['id'].astype(int) <= 950].set_index('id')['text']\n",
    "ser_eb_serv_ag2.name = 'clCode'\n",
    "dict_codelist['clCode']['S'] = ser_eb_serv_ag2\n",
    "### United codes:\n",
    "dict_codelist['clCode']['T'] = pd.concat([dict_codelist['clCode']['C'], dict_codelist['clCode']['S']], axis = 0)\n",
    "### Reporter Codes:\n",
    "df_reporter_raw = pd.DataFrame(request_session.get(df_cat_reference.loc['reporter']['fileuri']).json()['results'])\n",
    "df_reporter_raw['id'] = df_reporter_raw['id'].astype(str).str.zfill(3)\n",
    "df_reporter_raw['entryEffectiveDate'] = pd.to_datetime(df_reporter_raw['entryEffectiveDate'])\n",
    "df_reporter_raw['entryExpiredDate'] = pd.to_datetime(df_reporter_raw['entryExpiredDate'])\n",
    "### Reporters filtering to exclude aggregated and regional values (need to be replaced with SQL-based code):\n",
    "df_reporter_raw = df_reporter_raw[df_reporter_raw['reporterCodeIsoAlpha2'].isin(df_country_codes['ISO SHORT'])]\n",
    "### Non-actual country codes filtering out:\n",
    "df_reporter_raw = df_reporter_raw[df_reporter_raw['entryExpiredDate'].isna() | (df_reporter_raw['entryExpiredDate'] > date_start)]\n",
    "ser_reporter_code = df_reporter_raw.set_index('id')['reporterCodeIsoAlpha2'].squeeze()\n",
    "ser_reporter_code.name = 'Reporter'\n",
    "dict_codelist['reporterCode'] = ser_reporter_code\n",
    "### Partner Codes:\n",
    "df_partner_raw = pd.DataFrame(request_session.get(df_cat_reference.loc['partner']['fileuri']).json()['results'])\n",
    "df_partner_raw['id'] = df_partner_raw['id'].astype(str).str.zfill(3)\n",
    "df_partner_raw['entryEffectiveDate'] = pd.to_datetime(df_partner_raw['entryEffectiveDate'])\n",
    "df_partner_raw['entryExpiredDate'] = pd.to_datetime(df_partner_raw['entryExpiredDate'])\n",
    "### Partners filtering to exclude aggregated and regional values (need to be replaced with SQL-based code):\n",
    "df_partner_raw = df_partner_raw[df_partner_raw['PartnerCodeIsoAlpha2'].isin(df_country_codes['ISO SHORT'])]\n",
    "### Non-actual country codes filtering out:\n",
    "df_partner_raw = df_partner_raw[df_partner_raw['entryExpiredDate'].isna() | (df_partner_raw['entryExpiredDate'] > date_start)]\n",
    "ser_partner_code = df_partner_raw.set_index('id')['PartnerCodeIsoAlpha2'].squeeze()\n",
    "ser_partner_code.name = 'Partner'\n",
    "dict_codelist['partnerCode'] = ser_partner_code\n",
    "### Trade Flow Codes:\n",
    "ser_flow_code = pd.DataFrame(request_session.get(df_cat_reference.loc['flow']['fileuri']).json()['results']).set_index('id').squeeze()\n",
    "ser_flow_code.name = 'flowCode'\n",
    "dict_codelist['flowCode'] = ser_flow_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: DATA REQUEST EXECUTION\n",
    "\n",
    "def get_un_comtrade_data(str_type, str_freq, str_classification, str_trade_flow, list_reporters, list_partners, list_commodities, list_periods, str_api_key, \n",
    "                         int_limit = 99999):\n",
    "    ### Request preparation:\n",
    "    str_url_base = 'https://comtradeapi.un.org/data/v1/get/'\n",
    "    str_url_request = str_url_base + str_type + '/'\n",
    "    str_url_request = str_url_request + str_freq + '/' \n",
    "    str_url_request = str_url_request + str_classification + '?'   \n",
    "    str_url_request = str_url_request + 'flowCode=' + str_trade_flow \n",
    "    str_url_request = str_url_request + '&reporterCode=' + ','.join(list_reporters)\n",
    "    str_url_request = str_url_request + '&partnerCode=' + ','.join(list_partners)    \n",
    "    str_url_request = str_url_request + '&cmdCode=' + ','.join(list_commodities)\n",
    "    str_url_request = str_url_request + '&customsCode=' + 'C00'\n",
    "    str_url_request = str_url_request + '&motCode=' + '0'    \n",
    "    str_url_request = str_url_request + '&partner2Code=' + '000'       \n",
    "    str_url_request = str_url_request + '&period=' + ','.join(list_periods)  \n",
    "    str_url_request = str_url_request + '&maxrecords=' + str(int_limit)\n",
    "    ### Request sending:\n",
    "    bool_loaded = False\n",
    "    while (not bool_loaded):\n",
    "        request_session = requests.Session()\n",
    "        dict_request_headers = {}\n",
    "        dict_request_headers['Cache-Control'] = 'no-cache'\n",
    "        dict_request_headers['Ocp-Apim-Subscription-Key'] = str_api_key\n",
    "        request_session.headers.update(dict_request_headers)\n",
    "        ### Respond processing:\n",
    "        print(str_url_request)    \n",
    "        obj_unc_dataset = request_session.get(str_url_request)\n",
    "        print(obj_unc_dataset.json())\n",
    "        ### Request error marker:\n",
    "        if not ('count' in obj_unc_dataset.json()):\n",
    "#            print(obj_unc_dataset.json())\n",
    "            print(obj_unc_dataset.json()['error'])\n",
    "            int_dataset_length = -1\n",
    "        else:\n",
    "            int_dataset_length = obj_unc_dataset.json()['count']\n",
    "        ### Respond result transformation:\n",
    "        if (int_dataset_length > 0):\n",
    "            df_dataset_raw = pd.DataFrame(obj_unc_dataset.json()['data'])\n",
    "            ### Selecting columns:\n",
    "            df_dataset_res = df_dataset_raw[['flowCode', 'typeCode', 'period', 'reporterCode', 'partnerCode', 'cmdCode', 'primaryValue']]        \n",
    "            ### Replacing Flow codes to Flow names with categorization:\n",
    "            df_dataset_res.loc[:, 'Flow'] = df_dataset_res['flowCode'].replace(dict_codelist['flowCode']).astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Flow'].cat.set_categories(sorted(dict_codelist['flowCode'].values), ordered = True, inplace = True)\n",
    "            ### Replacing flow codes to flow names with categorization:\n",
    "            df_dataset_res.loc[:, 'Type'] = df_dataset_res['typeCode'].replace(dict_codelist['typeCode']).astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Type'].cat.set_categories(sorted(dict_codelist['typeCode'].values), ordered = True, inplace = True)\n",
    "            ### Year to Date transformation:\n",
    "            df_dataset_res.loc[:, 'Date'] = (pd.to_datetime(df_dataset_raw['period']) + pd.offsets.BYearEnd()).values\n",
    "            ### Replacing Reporter codes to ISON IDs with categorization:\n",
    "            df_dataset_res.loc[:, 'Reporter'] = df_dataset_res['reporterCode'].astype(str).str.zfill(3).replace(dict_codelist['reporterCode'])\\\n",
    "                                                                                                       .astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Reporter'].cat.set_categories(sorted(dict_codelist['partnerCode'].unique()), ordered = True, inplace = True)\n",
    "            ### Replacing Partner codes to ISON IDs with categorization:\n",
    "            df_dataset_res.loc[:, 'Partner'] = df_dataset_res['partnerCode'].astype(str).str.zfill(3).replace(dict_codelist['partnerCode']).astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Partner'].cat.set_categories(sorted(dict_codelist['partnerCode'].unique()), ordered = True, inplace = True)\n",
    "            ### Commodity Type categorization:\n",
    "            df_dataset_res.loc[:, 'Commodity_ID'] = df_dataset_res['cmdCode'].astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Commodity_ID'].cat.set_categories(sorted(dict_codelist['clCode']['T'].index), ordered = True, inplace = True)\n",
    "            ### Values scaling and transformation to integer:\n",
    "            df_dataset_res.loc[:, 'Value'] = (df_dataset_res['primaryValue'] / 1000).astype('int32').values\n",
    "            ### Data clearing:\n",
    "            df_dataset_res.drop(df_dataset_res[(df_dataset_res['Reporter'] == 'SA') & (df_dataset_res['Partner'] == 'TW')].index, inplace = True)\n",
    "            df_dataset_res.drop(df_dataset_res[df_dataset_res['Reporter'] == df_dataset_res['Partner']].index, inplace = True)\n",
    "            df_dataset_res['Value'].clip(lower = 0, inplace = True)\n",
    "            ### Dropping extra columns:\n",
    "            df_dataset_res = df_dataset_res[['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID', 'Value']]#.dropna()\n",
    "            print('Loaded Observations Number:', int_dataset_length)\n",
    "            bool_loaded = True\n",
    "        elif (int_dataset_length == 0):\n",
    "            print('Empty Dataset')\n",
    "            bool_loaded = True            \n",
    "            df_dataset_res = None            \n",
    "        else:\n",
    "            print('Loading Error. Let\\'s try once more...')\n",
    "    return df_dataset_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: GOODS DATA REQUEST PARAMETERS\n",
    "\n",
    "### Primary key to authorize:\n",
    "#str_primary_key = 'e690550ab9414234a6b705220596677a'\n",
    "#str_primary_key = 'd79c218e2e9d464fade1810fd14347d8'\n",
    "str_primary_key = '3f0b8d53d71b401e840d58c90a283176'\n",
    "### Type: Goods\n",
    "str_goods_type = 'C'\n",
    "### Type: Services\n",
    "str_services_type = 'S'\n",
    "### Annual frequency:\n",
    "str_freq = 'A'\n",
    "### Goods classification:\n",
    "str_goods_class = 'HS'\n",
    "### Services classification classification:\n",
    "str_services_class = 'EB'\n",
    "### Flow: Export\n",
    "str_export_flow = 'X'\n",
    "### Flow: Import\n",
    "str_import_flow = 'M'\n",
    "### Reporters list:\n",
    "list_un_reporters = dict_codelist['reporterCode'].index.to_list()\n",
    "### Partners list:\n",
    "list_un_partners = dict_codelist['partnerCode'].index.to_list()\n",
    "### Goods classification codes:\n",
    "list_un_goods_ag2 = dict_codelist['clCode'][str_goods_type].index.to_list()\n",
    "### Services classification codes:\n",
    "list_un_services_ag2 = dict_codelist['clCode'][str_services_type].index.to_list()\n",
    "#### United classification codes:\n",
    "list_un_commodities_ag2 = list_un_goods_ag2 + list_un_services_ag2\n",
    "### Years to collect data:\n",
    "list_periods = list(map(str, range(date_start.year, date_end.year + 1)))\n",
    "### Request tuning:\n",
    "int_pause_short_sec = 10\n",
    "int_pause_long_sec = 10\n",
    "int_goods_period_portion = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last saved observation options: 2022 / 99\n"
     ]
    }
   ],
   "source": [
    "### GOODS ONLY DATA LOADING ENGINE\n",
    "\n",
    "gc.collect()\n",
    "### Checking of file status & loading last observation saved:\n",
    "if (os.path.exists(str_path_unc_res_all_annual)):\n",
    "    ser_last_row = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res, start = -1)\n",
    "    str_last_comm_id = ser_last_row.index[0][5]\n",
    "    date_last_year = ser_last_row.index[0][0]\n",
    "    bool_break_flag = True\n",
    "    print('Last saved observation options:', date_last_year.year, '/', str_last_comm_id)\n",
    "else:\n",
    "    date_last_year = pd.to_datetime('1900-01-01')\n",
    "    print(date_last_year.year)\n",
    "    bool_break_flag = False\n",
    "    \n",
    "### Looping over period portions:\n",
    "for iter_portion in range(-(-len(list_periods) // int_goods_period_portion)):\n",
    "    gc.collect()\n",
    "    ### Selecting periods:\n",
    "    list_iter_periods = list_periods[iter_portion *  int_goods_period_portion : (iter_portion + 1) *  int_goods_period_portion]\n",
    "    if (int(list_iter_periods[-1]) < date_last_year.year):\n",
    "        continue\n",
    "    else:         \n",
    "        ### Commodities data loading:\n",
    "        for iter_comm_id in list_un_goods_ag2:\n",
    "            ### Starting point searching:\n",
    "            if bool_break_flag:\n",
    "                if (list_un_goods_ag2.index(str_last_comm_id) < list_un_goods_ag2.index(iter_comm_id)):\n",
    "                    continue\n",
    "                elif (list_un_goods_ag2.index(str_last_comm_id) == list_un_goods_ag2.index(iter_comm_id)):\n",
    "                    bool_break_flag = False\n",
    "                    continue\n",
    "            ### Loading procedure:\n",
    "            else:\n",
    "                ### Container initialization:\n",
    "                list_un_collection = []                \n",
    "                ### Export Data Requests:                \n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "                ### Export of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_goods_type, str_freq, str_goods_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec) \n",
    "                ### Import Data Requests:\n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Import: Loading through the API')\n",
    "                ### Import of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_goods_type, str_freq, str_goods_class, str_import_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec)             \n",
    "                ### Downloaded Data concatenation and indexation:\n",
    "                if (len(list_un_collection) > 0):\n",
    "                    df_full_dataset = pd.concat(list_un_collection, axis = 0, sort = False, ignore_index = True)\n",
    "                    ser_full_dataset = df_full_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "                    ### Dataset saving (need to be replaced with SQL Request):\n",
    "                    ser_full_dataset.to_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, \n",
    "                                            append = True)\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Flows saved to database')\n",
    "                else:\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Both flow\\'s datasets are empty')\n",
    "#            break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "df_eb_serv = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:EB10']['fileuri']).json()['results'])\n",
    "### Filtering needed level of services groups aggregation:\n",
    "df_eb_serv_ag2 = df_eb_serv[df_eb_serv['parent'].isin(df_eb_serv[df_eb_serv['parent'] == '200']['id'])]\n",
    "#display(df_eb_serv_ag2)\n",
    "df_eb_serv_ag2.merge(df_eb_serv, left_on = 'parent', right_on = 'id', suffixes = ['_child', '_parent']).drop('parent_child', axis = 1)\\\n",
    "#              .to_excel('Data_Files/Test_Files/EBOPS_2010.xlsx')\n",
    "#df_eb_serv[df_eb_serv['parent'] == '200']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.4</td>\n",
       "      <td>6.4 Pension and standardized guarantee services</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             text parent\n",
       "120  6.4  6.4 Pension and standardized guarantee services      6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "### Service EBOPS Codes:\n",
    "df_eb_serv = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:EB10']['fileuri']).json()['results'])\n",
    "### Filtering needed level of services groups aggregation:\n",
    "df_eb_serv_ag2 = df_eb_serv[df_eb_serv['parent'].isin(df_eb_serv[df_eb_serv['parent'] == '200']['id'])]\n",
    "df_eb_serv[df_eb_serv['id'] == '6.4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>parent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>206</td>\n",
       "      <td>1.1 Sea transport</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>210</td>\n",
       "      <td>1.2 Air transport</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>214</td>\n",
       "      <td>1.3 Other transport</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>218</td>\n",
       "      <td>1.4 Other transport of which: Space transport</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>219</td>\n",
       "      <td>1.5 Other transport of which: Rail transport</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>223</td>\n",
       "      <td>1.6 Other transport of which: Road transport</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>227</td>\n",
       "      <td>1.7 Other transport of which: Inland waterway ...</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>231</td>\n",
       "      <td>1.8 Other transport of which: Pipeline transpo...</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>232</td>\n",
       "      <td>1.9 Other supporting an auxiliary transport se...</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>237</td>\n",
       "      <td>2.1 Business travel</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>240</td>\n",
       "      <td>2.2 Personal travel</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>246</td>\n",
       "      <td>3.1 Postal and courier services</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>247</td>\n",
       "      <td>3.2 Telecommunications services</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>250</td>\n",
       "      <td>4.1 Construction abroad</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>251</td>\n",
       "      <td>4.2 Construction in the compiling economy</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>254</td>\n",
       "      <td>5.1 Life insurance and pension funding</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>255</td>\n",
       "      <td>5.2 Freight insurance</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>256</td>\n",
       "      <td>5.3 Other direct insurance</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>257</td>\n",
       "      <td>5.4 Reinsurance</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>258</td>\n",
       "      <td>5.5 Auxiliary services</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>263</td>\n",
       "      <td>7.1 Computer services</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>264</td>\n",
       "      <td>7.2 Information services</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>269</td>\n",
       "      <td>9.1 Merchanting and other trade-related services</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>272</td>\n",
       "      <td>9.2 Operational leasing services</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>273</td>\n",
       "      <td>9.3 Miscellaneous business, professional, and ...</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>288</td>\n",
       "      <td>10.1 Audiovisual and related services</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>289</td>\n",
       "      <td>10.2 Other personal, cultural, and recreationa...</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>292</td>\n",
       "      <td>11.1 Embassies and consulates</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>293</td>\n",
       "      <td>11.2 Military units and agencies</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>294</td>\n",
       "      <td>11.3 Other government services</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>310</td>\n",
       "      <td>Compensation of employees</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>391</td>\n",
       "      <td>Workers' remittance</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>431</td>\n",
       "      <td>Migrant's transfers</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>500</td>\n",
       "      <td>Direct investment</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>887</td>\n",
       "      <td>5. Financial intermediation services indirectl...</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>888</td>\n",
       "      <td>6. Financial servies including FISIM</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>891</td>\n",
       "      <td>8.1 Franchises and similar rights</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>892</td>\n",
       "      <td>8.2 Other royalties and license fees</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>894</td>\n",
       "      <td>8. Audiovisual transactions</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>950</td>\n",
       "      <td>1. Freight transportation on merchandise, valu...</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>960</td>\n",
       "      <td>3. Gross insurance premiums</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>961</td>\n",
       "      <td>4. Gross insurance claims</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>962</td>\n",
       "      <td>7. Merchanting gross flows</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>970</td>\n",
       "      <td>2. Travel</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text parent\n",
       "3    206                                  1.1 Sea transport    205\n",
       "7    210                                  1.2 Air transport    205\n",
       "11   214                                1.3 Other transport    205\n",
       "15   218      1.4 Other transport of which: Space transport    205\n",
       "16   219       1.5 Other transport of which: Rail transport    205\n",
       "20   223       1.6 Other transport of which: Road transport    205\n",
       "24   227  1.7 Other transport of which: Inland waterway ...    205\n",
       "28   231  1.8 Other transport of which: Pipeline transpo...    205\n",
       "29   232  1.9 Other supporting an auxiliary transport se...    205\n",
       "31   237                                2.1 Business travel    236\n",
       "34   240                                2.2 Personal travel    236\n",
       "39   246                    3.1 Postal and courier services    245\n",
       "40   247                    3.2 Telecommunications services    245\n",
       "42   250                            4.1 Construction abroad    249\n",
       "43   251          4.2 Construction in the compiling economy    249\n",
       "45   254             5.1 Life insurance and pension funding    253\n",
       "46   255                              5.2 Freight insurance    253\n",
       "47   256                         5.3 Other direct insurance    253\n",
       "48   257                                    5.4 Reinsurance    253\n",
       "49   258                             5.5 Auxiliary services    253\n",
       "52   263                              7.1 Computer services    262\n",
       "53   264                           7.2 Information services    262\n",
       "56   269   9.1 Merchanting and other trade-related services    268\n",
       "59   272                   9.2 Operational leasing services    268\n",
       "60   273  9.3 Miscellaneous business, professional, and ...    268\n",
       "74   288              10.1 Audiovisual and related services    287\n",
       "75   289  10.2 Other personal, cultural, and recreationa...    287\n",
       "77   292                      11.1 Embassies and consulates    291\n",
       "78   293                   11.2 Military units and agencies    291\n",
       "79   294                     11.3 Other government services    291\n",
       "80   310                          Compensation of employees    900\n",
       "81   391                                Workers' remittance    900\n",
       "82   431                                Migrant's transfers    900\n",
       "83   500                                  Direct investment    900\n",
       "90   887  5. Financial intermediation services indirectl...    999\n",
       "91   888               6. Financial servies including FISIM    999\n",
       "94   891                  8.1 Franchises and similar rights    266\n",
       "95   892               8.2 Other royalties and license fees    266\n",
       "96   894                        8. Audiovisual transactions    999\n",
       "101  950  1. Freight transportation on merchandise, valu...    999\n",
       "107  960                        3. Gross insurance premiums    999\n",
       "108  961                          4. Gross insurance claims    999\n",
       "109  962                         7. Merchanting gross flows    999\n",
       "110  970                                          2. Travel    999"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "df_eb_serv = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:EB']['fileuri']).json()['results'])\n",
    "### Filtering needed level of services groups aggregation:\n",
    "df_eb_serv_ag2 = df_eb_serv[df_eb_serv['parent'].isin(df_eb_serv[df_eb_serv['parent'] == '200']['id'])]\n",
    "df_eb_serv_ag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "int_period_portion = 5\n",
    "str_services_class = 'EB02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: GOODS DATA REQUEST PARAMETERS\n",
    "\n",
    "### Primary key to authorize:\n",
    "#str_primary_key = 'e690550ab9414234a6b705220596677a'\n",
    "#str_primary_key = 'd79c218e2e9d464fade1810fd14347d8'\n",
    "str_primary_key = '3f0b8d53d71b401e840d58c90a283176'\n",
    "### Type: Goods\n",
    "str_goods_type = 'C'\n",
    "### Type: Services\n",
    "str_services_type = 'S'\n",
    "### Annual frequency:\n",
    "str_freq = 'A'\n",
    "### Goods classification:\n",
    "str_goods_class = 'HS'\n",
    "### Services classification classification:\n",
    "str_services_class = 'EB'\n",
    "### Flow: Export\n",
    "str_export_flow = 'X'\n",
    "### Flow: Import\n",
    "str_import_flow = 'M'\n",
    "### Reporters list:\n",
    "list_un_reporters = dict_codelist['reporterCode'].index.to_list()\n",
    "### Partners list:\n",
    "list_un_partners = dict_codelist['partnerCode'].index.to_list()\n",
    "### Goods classification codes:\n",
    "list_un_goods_ag2 = dict_codelist['clCode'][str_goods_type].index.to_list()\n",
    "### Services classification codes:\n",
    "list_un_services_ag2 = dict_codelist['clCode'][str_services_type].index.to_list()\n",
    "#### United classification codes:\n",
    "list_un_commodities_ag2 = list_un_goods_ag2 + list_un_services_ag2\n",
    "### Years to collect data:\n",
    "list_periods = list(map(str, range(date_start.year, date_end.year + 1)))\n",
    "### Request tuning:\n",
    "int_pause_short_sec = 10\n",
    "int_pause_long_sec = 10\n",
    "int_goods_period_portion = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last saved observation options: 2022 / 99\n"
     ]
    }
   ],
   "source": [
    "### GOODS ONLY DATA LOADING ENGINE\n",
    "\n",
    "gc.collect()\n",
    "### Checking of file status & loading last observation saved:\n",
    "if (os.path.exists(str_path_unc_res_all_annual)):\n",
    "    ser_last_row = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res, start = -1)\n",
    "    str_last_comm_id = ser_last_row.index[0][5]\n",
    "    date_last_year = ser_last_row.index[0][0]\n",
    "    bool_break_flag = True\n",
    "    print('Last saved observation options:', date_last_year.year, '/', str_last_comm_id)\n",
    "else:\n",
    "    date_last_year = pd.to_datetime('1900-01-01')\n",
    "    print(date_last_year.year)\n",
    "    bool_break_flag = False\n",
    "    \n",
    "### Looping over period portions:\n",
    "for iter_portion in range(-(-len(list_periods) // int_goods_period_portion)):\n",
    "    gc.collect()\n",
    "    ### Selecting periods:\n",
    "    list_iter_periods = list_periods[iter_portion *  int_goods_period_portion : (iter_portion + 1) *  int_goods_period_portion]\n",
    "    if (int(list_iter_periods[-1]) < date_last_year.year):\n",
    "        continue\n",
    "    else:         \n",
    "        ### Commodities data loading:\n",
    "        for iter_comm_id in list_un_goods_ag2:\n",
    "            ### Starting point searching:\n",
    "            if bool_break_flag:\n",
    "                if (list_un_goods_ag2.index(str_last_comm_id) < list_un_goods_ag2.index(iter_comm_id)):\n",
    "                    continue\n",
    "                elif (list_un_goods_ag2.index(str_last_comm_id) == list_un_goods_ag2.index(iter_comm_id)):\n",
    "                    bool_break_flag = False\n",
    "                    continue\n",
    "            ### Loading procedure:\n",
    "            else:\n",
    "                ### Container initialization:\n",
    "                list_un_collection = []                \n",
    "                ### Export Data Requests:                \n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "                ### Export of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_goods_type, str_freq, str_goods_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec) \n",
    "                ### Import Data Requests:\n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Import: Loading through the API')\n",
    "                ### Import of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_goods_type, str_freq, str_goods_class, str_import_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec)             \n",
    "                ### Downloaded Data concatenation and indexation:\n",
    "                if (len(list_un_collection) > 0):\n",
    "                    df_full_dataset = pd.concat(list_un_collection, axis = 0, sort = False, ignore_index = True)\n",
    "                    ser_full_dataset = df_full_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "                    ### Dataset saving (need to be replaced with SQL Request):\n",
    "                    ser_full_dataset.to_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, \n",
    "                                            append = True)\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Flows saved to database')\n",
    "                else:\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Both flow\\'s datasets are empty')\n",
    "#            break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "dict_test = {}\n",
    "dict_test['2010_EB'] = df_iter_dataset\n",
    "#dict_test['2010_EB10'] = df_iter_dataset\n",
    "#dict_test['2010_EB2'] = df_iter_dataset\n",
    "#print(len(dict_test['2010_EB10']))\n",
    "#print(len(dict_test['2010_EB']))\n",
    "#print(len(set(dict_test['2010_EB10'].set_index(['Reporter', 'Partner', 'Flow'])['Value'].index) - \\\n",
    "#set(dict_test['2010_EB'].set_index(['Reporter', 'Partner', 'Flow'])['Value'].index)))\n",
    "#print(len(set(dict_test['2010_EB'].set_index(['Reporter', 'Partner', 'Flow'])['Value'].index) - \\\n",
    "#set(dict_test['2010_EB10'].set_index(['Reporter', 'Partner', 'Flow'])['Value'].index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AL, AT, BE, HR, DK, ..., ME, NL, PL, RS, SE]\n",
       "Length: 14\n",
       "Categories (14, object): [AL < AT < BE < DK ... NL < PL < RS < SE]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[BY, RU, UA, US]\n",
       "Categories (4, object): [BY < RU < UA < US]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "display(dict_test['2010_EB10']['Reporter'].unique())\n",
    "display(dict_test['2010_EB']['Reporter'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reporter  Partner  Type      Commodity_ID\n",
       "BT        IN       Services  210             17318\n",
       "                             237              5702\n",
       "                             257              1763\n",
       "                             292               354\n",
       "Name: Value, dtype: int32"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "#ser_unc_export = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res,\n",
    "#                                     where = \"(Flow = 'Export') & (Type == 'Services')\").droplevel('Flow')\n",
    "\n",
    "ser_unc_export['2015-12-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA CONCATENATION AND SAVING\n",
    "\n",
    "#### Downloaded Data concatenation and indexation:\n",
    "#df_full_dataset = pd.concat(list_un_collection, axis = 0, sort = False, ignore_index = True)\n",
    "#del list_un_collection\n",
    "#ser_full_dataset = df_full_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "#### Dataset saving (need to be replaced with SQL Request):\n",
    "#ser_full_dataset.to_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, mode = 'w', format = 'table', complevel = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPORT AND REVERTED IMPORT CONCATENATION\n",
    "\n",
    "gc.collect()\n",
    "### File deleting (need to be replaced with SQL Request):\n",
    "if (os.path.exists(str_path_unc_res_flows)):\n",
    "    os.remove(str_path_unc_res_flows)\n",
    "### Results container:\n",
    "list_export_aug = []\n",
    "### Countries portion length:\n",
    "int_portion = 5\n",
    "list_unc_countries = sorted(dict_codelist['partnerCode'].unique())\n",
    "### Looping over countries portions:\n",
    "for iter_num in range(len(list_unc_countries) // int_portion + 1):\n",
    "    gc.collect()    \n",
    "    ### Portion of countries selecting:\n",
    "    list_iter_countries = list(list_unc_countries)[int_portion * iter_num : int_portion * (iter_num + 1)]\n",
    "    if (len(list_iter_countries) > 0):\n",
    "        print(list_iter_countries)\n",
    "        ### Export data loading:\n",
    "        ser_unc_export = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res,\n",
    "                                     where = \"(Flow = 'Export') & (Reporter in list_iter_countries) & (Partner != 'World')\").droplevel('Flow')\n",
    "        print('Export dataset loaded')\n",
    "        ### Import data loading:\n",
    "        ser_unc_import = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res, \n",
    "                                     where = \"(Flow = 'Import') & (Partner in list_iter_countries)\").droplevel('Flow')\n",
    "        print('Import dataset loaded')    \n",
    "        ### Import data reverting:\n",
    "        ser_unc_import.index.set_names('Partner_Inv', level = 1, inplace = True)\n",
    "        ser_unc_import.index.set_names('Reporter', level = 2, inplace = True)\n",
    "        ser_unc_import.index.set_names('Partner', level = 1, inplace = True)\n",
    "        ser_unc_import = ser_unc_import.swaplevel('Reporter', 'Partner').sort_index()\n",
    "        print('Import dataset reverted')\n",
    "        ### Datasets concatenation:\n",
    "        df_export_aug = pd.concat([ser_unc_export, ser_unc_import], axis = 1, names = 'Source Flow', keys = ['Export', 'Import']).astype('float32')\n",
    "        del ser_unc_export\n",
    "        del ser_unc_import    \n",
    "        gc.collect()    \n",
    "        print('Export and reverted Import dataset concatenated')\n",
    "        ### Aggregateddataset saving (need to be replaced with SQL Request):\n",
    "        df_export_aug.to_hdf(str_path_unc_res_flows, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, append = True)                \n",
    "        print('Aggregated dataset added to database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIF COEFFICIENTS CALCULATION & IMPLEMENTATION\n",
    "\n",
    "gc.collect()\n",
    "### Files deleting (need to be replaced with SQL Request):\n",
    "if (os.path.exists(str_path_export_bilateral)):\n",
    "    os.remove(str_path_export_bilateral)\n",
    "if (os.path.exists(str_path_import_bilateral)):\n",
    "    os.remove(str_path_import_bilateral)\n",
    "### Getting full list of commodities:\n",
    "str_date = '2020-12-31'\n",
    "list_commodity_id = sorted(pd.read_hdf(str_path_unc_res_flows, key = str_key_unc_res, where = \"Date in str_date\").index.get_level_values('Commodity_ID').unique())\n",
    "### Bounds to filter bilateral Import to Export ratio before median calculation:\n",
    "flo_lower_bound = 1.0\n",
    "flo_upper_bound = 2.0\n",
    "### Bilateral median calculation procedure:\n",
    "def get_obs_median(df_comm):\n",
    "    ### Export to Import ratio:\n",
    "    ser_obs_coeff = df_comm['Import'] / df_comm['Export']\n",
    "    ### Ratio filtering:\n",
    "    ser_obs_coeff = ser_obs_coeff.loc[(ser_obs_coeff >= flo_lower_bound) & (ser_obs_coeff <= flo_upper_bound)]\n",
    "    ### Filtered timeseries median as a result:\n",
    "    return ser_obs_coeff.median()\n",
    "### Calulation CIF coefficient for each commodity:\n",
    "for iter_commodity in list_commodity_id:\n",
    "    gc.collect()\n",
    "    ### Commodity flows loading (need to be replaced with SQL Request):\n",
    "    df_iter_flows = pd.read_hdf(str_path_unc_res_flows, key = str_key_unc_res, where = \"Commodity_ID = iter_commodity\")\n",
    "    ### Bilateral Commodity CIF Median calculation:\n",
    "    ser_cif_median = df_iter_flows.droplevel('Commodity_ID').groupby(['Reporter', 'Partner']).apply(get_obs_median)\n",
    "    ### General Commodity Median calculation:\n",
    "    flo_median = ser_cif_median.median()\n",
    "    print(iter_commodity, ':', flo_median)\n",
    "    ### Filling missed bilateral values with general commodity median:\n",
    "    if not (np.isnan(flo_median)):\n",
    "        ser_cif_median.fillna(flo_median, inplace = True)        \n",
    "    ser_cif_median.name = 'CIF_Coefficient'              \n",
    "    ### Adding CIF coefficients to dataset:\n",
    "    df_export_cif = df_iter_flows.merge(ser_cif_median, left_index = True, right_index = True)\n",
    "    df_export_cif = df_export_cif.reorder_levels(['Date', 'Reporter', 'Partner', 'Type', 'Commodity_ID'])\n",
    "    ### Import correction:\n",
    "    df_export_cif['Import_Corrected'] = df_export_cif['Import'] / df_export_cif['CIF_Coefficient']\n",
    "    ### Export correction:\n",
    "    df_export_cif['Export_Corrected'] = df_export_cif['Export'] * df_export_cif['CIF_Coefficient']\n",
    "    ### Combining Export & Import data:\n",
    "    ser_export_cif = df_export_cif['Export'].combine_first(df_export_cif['Import_Corrected']).astype('float32')\n",
    "    ser_import_cif = df_export_cif['Import'].combine_first(df_export_cif['Export_Corrected']).astype('float32')\n",
    "    gc.collect()\n",
    "    ### Import data reverting back to original order:\n",
    "    ser_import_cif = ser_import_cif.reorder_levels(['Date', 'Partner', 'Reporter', 'Type', 'Commodity_ID']).sort_index()                               \n",
    "    ser_import_cif.index.names = ['Date', 'Reporter', 'Partner', 'Type', 'Commodity_ID']\n",
    "    gc.collect()\n",
    "    ### Augmented flows saving (need to be replaced with SQL Request):\n",
    "    ser_export_cif.squeeze().to_hdf(str_path_export_bilateral, key = str_key_unc_export, mode = 'a', format = 'table', complevel = 9, append = True, \n",
    "                                    min_itemsize = {'Type': 8, 'Commodity_ID': 3})\n",
    "    ser_import_cif.squeeze().to_hdf(str_path_import_bilateral, key = str_key_unc_import, mode = 'a', format = 'table', complevel = 9, append = True, \n",
    "                                    min_itemsize = {'Type': 8, 'Commodity_ID': 3}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
