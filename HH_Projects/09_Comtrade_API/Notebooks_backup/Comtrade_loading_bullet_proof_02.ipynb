{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMTRADE DATASETS EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import gc\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DISABLING OF WARNINGS\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS (RESEARCH VERSION ONLY)\n",
    "\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Service codes convertion path:\n",
    "str_path_ebops = 'Data_Files/Source_Files/goods_to_industries_2023.xlsx'\n",
    "str_ebops_2002 = 'EBOPS_2002'\n",
    "str_ebops_2010 = 'EBOPS_2010'\n",
    "### NA for MS Excel files:\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable', '---']\n",
    "### UN Comtrade adopted data containers:\n",
    "str_path_unc_res_all_annual = 'Data_Files/Source_Files/unc_res_all_annual.h5'\n",
    "str_path_unc_eb_am_services_annual = 'Data_Files/Source_Files/unc_eb_am_services_annual.h5'\n",
    "str_path_unc_eb_pm_services_annual = 'Data_Files/Source_Files/unc_eb_pm_services_annual.h5'\n",
    "str_path_unc_eb10_pm_services_annual = 'Data_Files/Source_Files/unc_eb10_pm_services_annual.h5'\n",
    "str_key_unc_res = 'unc_res'\n",
    "### File with aggregated flows:\n",
    "str_path_unc_res_flows = 'Data_Files/Source_Files/unc_res_flows.h5'\n",
    "### Universal HDF5 key:\n",
    "str_key_unc_res = 'unc_res'\n",
    "### Augmented bilateral export:\n",
    "str_path_export_bilateral = 'Data_Files/Source_Files/comtrade_export_bilateral.h5'\n",
    "### Export key:\n",
    "str_key_unc_export = 'export_augmented'\n",
    "### Augmented bilateral import:\n",
    "str_path_import_bilateral = 'Data_Files/Source_Files/comtrade_import_bilateral.h5'\n",
    "### Import key:\n",
    "str_key_unc_import = 'import_augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN CONSTANTS\n",
    "\n",
    "### Dates:\n",
    "date_start = pd.Timestamp('1989-12-29')\n",
    "date_end = pd.Timestamp('2022-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EBOPS SERVICE'S CODES PREPARATION\n",
    "\n",
    "### EBOPS 2010 codes:\n",
    "list_ebops_2010 = pd.read_excel(engine = 'openpyxl', io = str_path_ebops, sheet_name = str_ebops_2010, header = 0, index_col = 0, \n",
    "                               na_values = list_na_excel_values + ['None'], keep_default_na = False)['GICS Group Code'].dropna().sort_index().index.to_list()\n",
    "### EBOPS 2002 codes mapped with EBOPS 2010:\n",
    "ser_ebops_2002 = pd.read_excel(engine = 'openpyxl', io = str_path_ebops, sheet_name = str_ebops_2002, header = 0, index_col = 0, dtype = str,\n",
    "                               na_values = list_na_excel_values + ['None'], keep_default_na = False)['EBOPS 2010 Correspondent ID'].dropna().sort_index()\n",
    "ser_ebops_2002.index = ser_ebops_2002.index.astype(str)\n",
    "ser_ebops_2002 = ser_ebops_2002[ser_ebops_2002.isin(list_ebops_2010)]\n",
    "ser_ebops_2002.name = 'ebops_mapping'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING & LAUNCH COUNTRY CODES EXTRACTOR (RESEARCH VERSION ONLY)\n",
    "\n",
    "def get_country_codes(use_local_copy = False):  \n",
    "    ### In case if URL is unavailable:\n",
    "    if (use_local_copy):\n",
    "        url_country_code = 'Data_Files/Source_Files/countrycode.html'\n",
    "    ### Online extraction:\n",
    "    else:\n",
    "        url_country_code = 'https://countrycode.org/'\n",
    "    df_full_codes = pd.read_html(url_country_code, index_col = 'COUNTRY')[0]\n",
    "    df_full_codes[['ISO SHORT', 'ISO LONG']] = df_full_codes['ISO CODES'].str.split(' / ', expand = True)\n",
    "    df_result = df_full_codes[['ISO SHORT', 'ISO LONG']].sort_index()    \n",
    "    df_result.index = df_result.index.str.upper()\n",
    "    ### Results output:\n",
    "    return df_result\n",
    "\n",
    "### World Country Codes:\n",
    "df_country_codes = get_country_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: CODELISTS LOADING (PRODUCT VERSION)\n",
    "\n",
    "### Codelists container:\n",
    "dict_codelist = {}\n",
    "### List of reference tables for categories (request parameters) loading:\n",
    "request_session = requests.Session()\n",
    "obj_unc_reference = request_session.get('https://comtradeapi.un.org/files/v1/app/reference/ListofReferences.json')\n",
    "df_cat_reference = pd.DataFrame(obj_unc_reference.json()['results']).set_index('category')\n",
    "### Parameters:\n",
    "ser_type_code = pd.Series(['Goods', 'Services'], index = ['C', 'S'])\n",
    "ser_type_code.index.names = ['id']\n",
    "ser_type_code.name = 'typeCode'\n",
    "dict_codelist['typeCode'] = ser_type_code\n",
    "### Commodity HS Codes:\n",
    "df_hs_comm = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:HS']['fileuri']).json()['results'])\n",
    "### Filtering needed level of coomodity groups aggregation:\n",
    "ser_hs_comm_ag2 = df_hs_comm[df_hs_comm['aggrLevel'] == 2].drop(['parent', 'isLeaf', 'aggrLevel'], axis = 1).set_index('id').squeeze().str[5: ]\n",
    "ser_hs_comm_ag2.name = 'clCode'\n",
    "dict_codelist['clCode'] = {}\n",
    "dict_codelist['clCode']['C'] = ser_hs_comm_ag2\n",
    "### Service EBOPS 2002 Codes:\n",
    "df_eb_serv_02 = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:EB02']['fileuri']).json()['results'])\n",
    "### Filtering needed level of services groups aggregation:\n",
    "df_eb_serv_ag2 = df_eb_serv_02[df_eb_serv_02['parent'].isin(df_eb_serv_02[df_eb_serv_02['parent'] == '200']['id'])]\n",
    "ser_eb_serv_ag2 = df_eb_serv_02[df_eb_serv_02['id'].isin(ser_ebops_2002.index)].set_index('id')['text']\n",
    "ser_eb_serv_ag2.name = 'clCode'\n",
    "dict_codelist['clCode']['S_old'] = ser_eb_serv_ag2\n",
    "### Service EBOPS 2010 Codes:\n",
    "df_eb_serv_02 = pd.DataFrame(request_session.get(df_cat_reference.loc['cmd:EB10']['fileuri']).json()['results'])\n",
    "### Filtering needed level of services groups aggregation:\n",
    "df_eb_serv_ag2 = df_eb_serv_02[df_eb_serv_02['parent'].isin(df_eb_serv_02[df_eb_serv_02['parent'] == '200']['id'])]\n",
    "ser_eb_serv_ag2 = df_eb_serv_02[df_eb_serv_02['id'].isin(list_ebops_2010)].set_index('id')['text']\n",
    "ser_eb_serv_ag2.name = 'clCode'\n",
    "dict_codelist['clCode']['S_new'] = ser_eb_serv_ag2\n",
    "### United codes:\n",
    "dict_codelist['clCode']['T'] = pd.concat([dict_codelist['clCode']['C'], dict_codelist['clCode']['S_old'], dict_codelist['clCode']['S_new']], axis = 0)\n",
    "### Reporter Codes:\n",
    "df_reporter_raw = pd.DataFrame(request_session.get(df_cat_reference.loc['reporter']['fileuri']).json()['results'])\n",
    "df_reporter_raw['id'] = df_reporter_raw['id'].astype(str).str.zfill(3)\n",
    "df_reporter_raw['entryEffectiveDate'] = pd.to_datetime(df_reporter_raw['entryEffectiveDate'])\n",
    "df_reporter_raw['entryExpiredDate'] = pd.to_datetime(df_reporter_raw['entryExpiredDate'])\n",
    "### Reporters filtering to exclude aggregated and regional values (need to be replaced with SQL-based code):\n",
    "df_reporter_raw = df_reporter_raw[df_reporter_raw['reporterCodeIsoAlpha2'].isin(df_country_codes['ISO SHORT'])]\n",
    "### Non-actual country codes filtering out:\n",
    "df_reporter_raw = df_reporter_raw[df_reporter_raw['entryExpiredDate'].isna() | (df_reporter_raw['entryExpiredDate'] > date_start)]\n",
    "ser_reporter_code = df_reporter_raw.set_index('id')['reporterCodeIsoAlpha2'].squeeze()\n",
    "ser_reporter_code.name = 'Reporter'\n",
    "dict_codelist['reporterCode'] = ser_reporter_code\n",
    "### Partner Codes:\n",
    "df_partner_raw = pd.DataFrame(request_session.get(df_cat_reference.loc['partner']['fileuri']).json()['results'])\n",
    "df_partner_raw['id'] = df_partner_raw['id'].astype(str).str.zfill(3)\n",
    "df_partner_raw['entryEffectiveDate'] = pd.to_datetime(df_partner_raw['entryEffectiveDate'])\n",
    "df_partner_raw['entryExpiredDate'] = pd.to_datetime(df_partner_raw['entryExpiredDate'])\n",
    "### Partners filtering to exclude aggregated and regional values (need to be replaced with SQL-based code):\n",
    "df_partner_raw = df_partner_raw[df_partner_raw['PartnerCodeIsoAlpha2'].isin(df_country_codes['ISO SHORT'])]\n",
    "### Non-actual country codes filtering out:\n",
    "df_partner_raw = df_partner_raw[df_partner_raw['entryExpiredDate'].isna() | (df_partner_raw['entryExpiredDate'] > date_start)]\n",
    "ser_partner_code = df_partner_raw.set_index('id')['PartnerCodeIsoAlpha2'].squeeze()\n",
    "ser_partner_code.name = 'Partner'\n",
    "dict_codelist['partnerCode'] = ser_partner_code\n",
    "### Trade Flow Codes:\n",
    "ser_flow_code = pd.DataFrame(request_session.get(df_cat_reference.loc['flow']['fileuri']).json()['results']).set_index('id').squeeze()\n",
    "ser_flow_code.name = 'flowCode'\n",
    "dict_codelist['flowCode'] = ser_flow_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: DATA REQUEST EXECUTION\n",
    "\n",
    "def get_un_comtrade_data(str_type, str_freq, str_classification, str_trade_flow, list_reporters, list_partners, list_commodities, list_periods, str_api_key, \n",
    "                         int_limit = 99999):\n",
    "    ### Request preparation:\n",
    "    str_url_base = 'https://comtradeapi.un.org/data/v1/get/'\n",
    "    str_url_request = str_url_base + str_type + '/'\n",
    "    str_url_request = str_url_request + str_freq + '/' \n",
    "    str_url_request = str_url_request + str_classification + '?'   \n",
    "    str_url_request = str_url_request + 'flowCode=' + str_trade_flow \n",
    "    str_url_request = str_url_request + '&reporterCode=' + ','.join(list_reporters)\n",
    "    str_url_request = str_url_request + '&partnerCode=' + ','.join(list_partners)    \n",
    "    str_url_request = str_url_request + '&cmdCode=' + ','.join(list_commodities)\n",
    "    str_url_request = str_url_request + '&customsCode=' + 'C00'\n",
    "    str_url_request = str_url_request + '&motCode=' + '0'    \n",
    "    str_url_request = str_url_request + '&partner2Code=' + '000'       \n",
    "    str_url_request = str_url_request + '&period=' + ','.join(list_periods)  \n",
    "    str_url_request = str_url_request + '&maxrecords=' + str(int_limit)\n",
    "    ### Request sending:\n",
    "    bool_loaded = False\n",
    "    while (not bool_loaded):\n",
    "        request_session = requests.Session()\n",
    "        dict_request_headers = {}\n",
    "        dict_request_headers['Cache-Control'] = 'no-cache'\n",
    "        dict_request_headers['Ocp-Apim-Subscription-Key'] = str_api_key\n",
    "        request_session.headers.update(dict_request_headers)\n",
    "        ### Respond processing:\n",
    "        print(str_url_request)    \n",
    "        obj_unc_dataset = request_session.get(str_url_request)\n",
    "#        print(obj_unc_dataset.json())\n",
    "        ### Request error marker:\n",
    "        if not ('count' in obj_unc_dataset.json()):\n",
    "#            print(obj_unc_dataset.json())\n",
    "            print(obj_unc_dataset.json()['error'])\n",
    "            int_dataset_length = -1\n",
    "        else:\n",
    "            int_dataset_length = obj_unc_dataset.json()['count']\n",
    "        ### Respond result transformation:\n",
    "        if (int_dataset_length > 0):\n",
    "            df_dataset_raw = pd.DataFrame(obj_unc_dataset.json()['data'])\n",
    "            ### Selecting columns:\n",
    "            df_dataset_res = df_dataset_raw[['flowCode', 'typeCode', 'period', 'reporterCode', 'partnerCode', 'cmdCode', 'primaryValue']]        \n",
    "            ### Replacing Flow codes to Flow names with categorization:\n",
    "            df_dataset_res.loc[:, 'Flow'] = df_dataset_res['flowCode'].replace(dict_codelist['flowCode']).astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Flow'].cat.set_categories(sorted(dict_codelist['flowCode'].values), ordered = True, inplace = True)\n",
    "            ### Replacing flow codes to flow names with categorization:\n",
    "            df_dataset_res.loc[:, 'Type'] = df_dataset_res['typeCode'].replace(dict_codelist['typeCode']).astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Type'].cat.set_categories(sorted(dict_codelist['typeCode'].values), ordered = True, inplace = True)\n",
    "            ### Year to Date transformation:\n",
    "            df_dataset_res.loc[:, 'Date'] = (pd.to_datetime(df_dataset_raw['period']) + pd.offsets.BYearEnd()).values\n",
    "            ### Replacing Reporter codes to ISON IDs with categorization:\n",
    "            df_dataset_res.loc[:, 'Reporter'] = df_dataset_res['reporterCode'].astype(str).str.zfill(3).replace(dict_codelist['reporterCode'])\\\n",
    "                                                                                                       .astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Reporter'].cat.set_categories(sorted(dict_codelist['partnerCode'].unique()), ordered = True, inplace = True)\n",
    "            ### Replacing Partner codes to ISON IDs with categorization:\n",
    "            df_dataset_res.loc[:, 'Partner'] = df_dataset_res['partnerCode'].astype(str).str.zfill(3).replace(dict_codelist['partnerCode']).astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Partner'].cat.set_categories(sorted(dict_codelist['partnerCode'].unique()), ordered = True, inplace = True)\n",
    "            ### Commodity Type categorization:\n",
    "#            if ((str_type == 'S') & (list_commodities[0].find('.') == -1)):\n",
    "#                df_dataset_res['cmdCode'] = df_dataset_res['cmdCode'].replace(ser_ebops_2002.to_dict())\n",
    "            df_dataset_res.loc[:, 'Commodity_ID'] = df_dataset_res['cmdCode'].astype('category').values\n",
    "            ### Expanding categorical list to full list of possible values:\n",
    "            df_dataset_res['Commodity_ID'].cat.set_categories(sorted(dict_codelist['clCode']['T'].index), ordered = True, inplace = True)\n",
    "            ### Values scaling and transformation to integer:\n",
    "            df_dataset_res.loc[:, 'Value'] = (df_dataset_res['primaryValue'] / 1000).astype('int32').values\n",
    "            ### Data clearing:\n",
    "            df_dataset_res.drop(df_dataset_res[(df_dataset_res['Reporter'] == 'SA') & (df_dataset_res['Partner'] == 'TW')].index, inplace = True)\n",
    "            df_dataset_res.drop(df_dataset_res[df_dataset_res['Reporter'] == df_dataset_res['Partner']].index, inplace = True)\n",
    "            df_dataset_res['Value'].clip(lower = 0, inplace = True)\n",
    "            ### Dropping extra columns:\n",
    "            df_dataset_res = df_dataset_res[['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID', 'Value']]#.dropna()\n",
    "            print('Loaded Observations Number:', int_dataset_length)\n",
    "            bool_loaded = True\n",
    "        elif (int_dataset_length == 0):\n",
    "            print('Empty Dataset')\n",
    "            bool_loaded = True            \n",
    "            df_dataset_res = None            \n",
    "        else:\n",
    "            print('Loading Error. Let\\'s try once more...')\n",
    "    return df_dataset_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### UN COMTRADE: GOODS DATA REQUEST PARAMETERS\n",
    "\n",
    "### Primary key to authorize:\n",
    "#str_primary_key = 'e690550ab9414234a6b705220596677a'\n",
    "str_primary_key = 'd79c218e2e9d464fade1810fd14347d8'\n",
    "#str_primary_key = '3f0b8d53d71b401e840d58c90a283176'\n",
    "### Type: Goods\n",
    "str_goods_type = 'C'\n",
    "### Annual frequency:\n",
    "str_freq = 'A'\n",
    "### Goods classification:\n",
    "str_goods_class = 'HS'\n",
    "### Flow: Export\n",
    "str_export_flow = 'X'\n",
    "### Flow: Import\n",
    "str_import_flow = 'M'\n",
    "### Reporters list:\n",
    "list_un_reporters = dict_codelist['reporterCode'].index.to_list()\n",
    "### Partners list:\n",
    "list_un_partners = dict_codelist['partnerCode'].index.to_list()\n",
    "### Goods classification codes:\n",
    "list_un_goods_ag2 = dict_codelist['clCode'][str_goods_type].index.to_list()\n",
    "### Years to collect data:\n",
    "list_periods = list(map(str, range(date_start.year, date_end.year + 1)))\n",
    "### Request tuning:\n",
    "int_pause_short_sec = 10\n",
    "int_pause_long_sec = 10\n",
    "int_goods_period_portion = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last saved observation options: 2022 / 99\n"
     ]
    }
   ],
   "source": [
    "### GOODS ONLY DATA LOADING ENGINE\n",
    "\n",
    "gc.collect()\n",
    "### Checking of file status & loading last observation saved:\n",
    "if (os.path.exists(str_path_unc_res_all_annual)):\n",
    "    ser_last_row = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res, start = -1)\n",
    "    str_last_comm_id = ser_last_row.index[0][5]\n",
    "    date_last_year = ser_last_row.index[0][0]\n",
    "    bool_break_flag = True\n",
    "    print('Last saved observation options:', date_last_year.year, '/', str_last_comm_id)\n",
    "else:\n",
    "    date_last_year = pd.to_datetime('1900-01-01')\n",
    "    print(date_last_year.year)\n",
    "    bool_break_flag = False\n",
    "    \n",
    "### Looping over period portions:\n",
    "for iter_portion in range(-(-len(list_periods) // int_goods_period_portion)):\n",
    "    gc.collect()\n",
    "    ### Selecting periods:\n",
    "    list_iter_periods = list_periods[iter_portion *  int_goods_period_portion : (iter_portion + 1) *  int_goods_period_portion]\n",
    "    if (int(list_iter_periods[-1]) < date_last_year.year):\n",
    "        continue\n",
    "    else:         \n",
    "        ### Commodities data loading:\n",
    "        for iter_comm_id in list_un_goods_ag2:\n",
    "            ### Starting point searching:\n",
    "            if bool_break_flag:\n",
    "                if (list_un_goods_ag2.index(str_last_comm_id) < list_un_goods_ag2.index(iter_comm_id)):\n",
    "                    continue\n",
    "                elif (list_un_goods_ag2.index(str_last_comm_id) == list_un_goods_ag2.index(iter_comm_id)):\n",
    "                    bool_break_flag = False\n",
    "                    continue\n",
    "            ### Loading procedure:\n",
    "            else:\n",
    "                ### Container initialization:\n",
    "                list_un_collection = []                \n",
    "                ### Export Data Requests:                \n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "                ### Export of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_goods_type, str_freq, str_goods_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec) \n",
    "                ### Import Data Requests:\n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Import: Loading through the API')\n",
    "                ### Import of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_goods_type, str_freq, str_goods_class, str_import_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec)             \n",
    "                ### Downloaded Data concatenation and indexation:\n",
    "                if (len(list_un_collection) > 0):\n",
    "                    df_full_dataset = pd.concat(list_un_collection, axis = 0, sort = False, ignore_index = True)\n",
    "                    ser_full_dataset = df_full_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "                    ### Dataset saving (need to be replaced with SQL Request):\n",
    "                    ser_full_dataset.to_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, \n",
    "                                            append = True)\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Flows saved to database')\n",
    "                else:\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Both flow\\'s datasets are empty')\n",
    "#            break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### UN COMTRADE: OLD STYLE CLASSIFIED SERVICES DATA REQUEST PARAMETERS\n",
    "\n",
    "### Primary key to authorize:\n",
    "str_primary_key = 'e690550ab9414234a6b705220596677a'\n",
    "#str_primary_key = 'd79c218e2e9d464fade1810fd14347d8'\n",
    "#str_primary_key = '3f0b8d53d71b401e840d58c90a283176'\n",
    "### Type: Services\n",
    "str_services_type = 'S'\n",
    "### Annual frequency:\n",
    "str_freq = 'A'\n",
    "### Services classification:\n",
    "str_services_class = 'EB'\n",
    "### Flow: Export\n",
    "str_export_flow = 'X'\n",
    "### Flow: Import\n",
    "str_import_flow = 'M'\n",
    "### Reporters list:\n",
    "list_un_reporters = dict_codelist['reporterCode'].index.to_list()\n",
    "### Partners list:\n",
    "list_un_partners = dict_codelist['partnerCode'].index.to_list()\n",
    "### Services classification codes:\n",
    "list_un_services_ag2 = dict_codelist['clCode']['S_old'].index.to_list()\n",
    "### Years to collect data:\n",
    "list_periods = list(map(str, range(2000, 2014)))\n",
    "### Request tuning:\n",
    "int_pause_short_sec = 10\n",
    "int_pause_long_sec = 10\n",
    "int_services_period_portion = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last saved observation options: 2013 / 289\n"
     ]
    }
   ],
   "source": [
    "### OLD STYLE CLASSIFIED SERVICES ONLY DATA LOADING ENGINE\n",
    "\n",
    "gc.collect()\n",
    "### Checking of file status & loading last observation saved:\n",
    "if (os.path.exists(str_path_unc_eb_am_services_annual)):\n",
    "    ser_last_row = pd.read_hdf(str_path_unc_eb_am_services_annual, key = str_key_unc_res, start = -1)\n",
    "    str_last_comm_id = ser_last_row.index[0][5]\n",
    "    date_last_year = ser_last_row.index[0][0]\n",
    "    bool_break_flag = True\n",
    "    print('Last saved observation options:', date_last_year.year, '/', str_last_comm_id)\n",
    "else:\n",
    "    date_last_year = pd.to_datetime('1900-01-01')\n",
    "    print(date_last_year.year)\n",
    "    bool_break_flag = False\n",
    "    \n",
    "### Looping over period portions:\n",
    "for iter_portion in range(-(-len(list_periods) // int_services_period_portion)):\n",
    "    gc.collect()\n",
    "    ### Selecting periods:\n",
    "    list_iter_periods = list_periods[iter_portion *  int_services_period_portion : (iter_portion + 1) *  int_services_period_portion]\n",
    "    if (int(list_iter_periods[-1]) < date_last_year.year):\n",
    "        continue\n",
    "    else:         \n",
    "        ### Commodities data loading:\n",
    "        for iter_comm_id in list_un_services_ag2:\n",
    "            ### Starting point searching:\n",
    "            if bool_break_flag:\n",
    "                if (list_un_services_ag2.index(str_last_comm_id) < list_un_services_ag2.index(iter_comm_id)):\n",
    "                    continue\n",
    "                elif (list_un_services_ag2.index(str_last_comm_id) == list_un_services_ag2.index(iter_comm_id)):\n",
    "                    bool_break_flag = False\n",
    "                    continue\n",
    "            ### Loading procedure:\n",
    "            else:\n",
    "                ### Container initialization:\n",
    "                list_un_collection = []                \n",
    "                ### Export Data Requests:                \n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "                ### Export of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec) \n",
    "                ### Import Data Requests:\n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Import: Loading through the API')\n",
    "                ### Import of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_import_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec)             \n",
    "                ### Downloaded Data concatenation and indexation:\n",
    "                if (len(list_un_collection) > 0):\n",
    "                    df_full_dataset = pd.concat(list_un_collection, axis = 0, sort = False, ignore_index = True)\n",
    "                    ser_full_dataset = df_full_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "                    ### Dataset saving (need to be replaced with SQL Request):\n",
    "                    ser_full_dataset.to_hdf(path_or_buf = str_path_unc_eb_am_services_annual, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, \n",
    "                                            append = True)\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Flows saved to database')\n",
    "                else:\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Both flow\\'s datasets are empty')\n",
    "#            break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### UN COMTRADE: NEW STYLE CLASSIFIED AS REPORTED SERVICES DATA REQUEST PARAMETERS\n",
    "\n",
    "### Primary key to authorize:\n",
    "#str_primary_key = 'e690550ab9414234a6b705220596677a'\n",
    "str_primary_key = 'd79c218e2e9d464fade1810fd14347d8'\n",
    "#str_primary_key = '3f0b8d53d71b401e840d58c90a283176'\n",
    "### Type: Services\n",
    "str_services_type = 'S'\n",
    "### Annual frequency:\n",
    "str_freq = 'A'\n",
    "### Services classification:\n",
    "str_services_class = 'EB'\n",
    "### Flow: Export\n",
    "str_export_flow = 'X'\n",
    "### Flow: Import\n",
    "str_import_flow = 'M'\n",
    "### Reporters list:\n",
    "list_un_reporters = dict_codelist['reporterCode'].index.to_list()\n",
    "### Partners list:\n",
    "list_un_partners = dict_codelist['partnerCode'].index.to_list()\n",
    "### Services classification codes:\n",
    "list_un_services_ag2 = dict_codelist['clCode']['S_new'].index.to_list()\n",
    "### Responded as invalide codes:\n",
    "list_un_services_ag2.remove('3.10')\n",
    "list_un_services_ag2.remove('3.11')\n",
    "### Years to collect data:\n",
    "list_periods = list(map(str, range(2010, date_end.year + 1)))\n",
    "### Request tuning:\n",
    "int_pause_short_sec = 10\n",
    "int_pause_long_sec = 10\n",
    "int_services_period_portion = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last saved observation options: 2021 / 9.3\n"
     ]
    }
   ],
   "source": [
    "### NEW STYLE CLASSIFIED AS REPORTED SERVICES ONLY DATA LOADING ENGINE\n",
    "\n",
    "gc.collect()\n",
    "### Checking of file status & loading last observation saved:\n",
    "if (os.path.exists(str_path_unc_eb_pm_services_annual)):\n",
    "    ser_last_row = pd.read_hdf(str_path_unc_eb_pm_services_annual, key = str_key_unc_res, start = -1)\n",
    "    str_last_comm_id = ser_last_row.index[0][5]\n",
    "    date_last_year = ser_last_row.index[0][0]\n",
    "    bool_break_flag = True\n",
    "    print('Last saved observation options:', date_last_year.year, '/', str_last_comm_id)\n",
    "else:\n",
    "    date_last_year = pd.to_datetime('1900-01-01')\n",
    "    print(date_last_year.year)\n",
    "    bool_break_flag = False\n",
    "    \n",
    "### Looping over period portions:\n",
    "for iter_portion in range(-(-len(list_periods) // int_services_period_portion)):\n",
    "    gc.collect()\n",
    "    ### Selecting periods:\n",
    "    list_iter_periods = list_periods[iter_portion *  int_services_period_portion : (iter_portion + 1) *  int_services_period_portion]\n",
    "    if (int(list_iter_periods[-1]) < date_last_year.year):\n",
    "        continue\n",
    "    else:         \n",
    "        ### Commodities data loading:\n",
    "        for iter_comm_id in list_un_services_ag2:\n",
    "            ### Starting point searching:\n",
    "            if bool_break_flag:\n",
    "                if (list_un_services_ag2.index(str_last_comm_id) < list_un_services_ag2.index(iter_comm_id)):\n",
    "                    continue\n",
    "                elif (list_un_services_ag2.index(str_last_comm_id) == list_un_services_ag2.index(iter_comm_id)):\n",
    "                    bool_break_flag = False\n",
    "                    continue\n",
    "            ### Loading procedure:\n",
    "            else:\n",
    "                ### Container initialization:\n",
    "                list_un_collection = []                \n",
    "                ### Export Data Requests:                \n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "                ### Export of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec) \n",
    "                ### Import Data Requests:\n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Import: Loading through the API')\n",
    "                ### Import of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_import_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec)             \n",
    "                ### Downloaded Data concatenation and indexation:\n",
    "                if (len(list_un_collection) > 0):\n",
    "                    df_full_dataset = pd.concat(list_un_collection, axis = 0, sort = False, ignore_index = True)\n",
    "                    ser_full_dataset = df_full_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "                    ### Dataset saving (need to be replaced with SQL Request):\n",
    "                    ser_full_dataset.to_hdf(path_or_buf = str_path_unc_eb_pm_services_annual, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, \n",
    "                                            append = True)\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Flows saved to database')\n",
    "                else:\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Both flow\\'s datasets are empty')\n",
    "#            break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UN COMTRADE: NEW STYLE CLASSIFIED EB10 SERVICES DATA REQUEST PARAMETERS\n",
    "\n",
    "### Primary key to authorize:\n",
    "str_primary_key = 'e690550ab9414234a6b705220596677a'\n",
    "#str_primary_key = 'd79c218e2e9d464fade1810fd14347d8'\n",
    "#str_primary_key = '3f0b8d53d71b401e840d58c90a283176'\n",
    "### Type: Services\n",
    "str_services_type = 'S'\n",
    "### Annual frequency:\n",
    "str_freq = 'A'\n",
    "### Services classification:\n",
    "str_services_class = 'EB10'\n",
    "### Flow: Export\n",
    "str_export_flow = 'X'\n",
    "### Flow: Import\n",
    "str_import_flow = 'M'\n",
    "### Reporters list:\n",
    "list_un_reporters = dict_codelist['reporterCode'].index.to_list()\n",
    "### Partners list:\n",
    "list_un_partners = dict_codelist['partnerCode'].index.to_list()\n",
    "### Services classification codes:\n",
    "list_un_services_ag2 = dict_codelist['clCode']['S_new'].index.to_list()\n",
    "### Responded as invalide codes:\n",
    "list_un_services_ag2.remove('3.10')\n",
    "list_un_services_ag2.remove('3.11')\n",
    "### Years to collect data:\n",
    "list_periods = list(map(str, range(2010, date_end.year + 1)))\n",
    "### Request tuning:\n",
    "int_pause_short_sec = 10\n",
    "int_pause_long_sec = 10\n",
    "int_services_period_portion = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW STYLE CLASSIFIED EB10 SERVICES ONLY DATA LOADING ENGINE\n",
    "\n",
    "gc.collect()\n",
    "### Checking of file status & loading last observation saved:\n",
    "if (os.path.exists(str_path_unc_eb10_pm_services_annual)):\n",
    "    ser_last_row = pd.read_hdf(str_path_unc_eb10_pm_services_annual, key = str_key_unc_res, start = -1)\n",
    "    str_last_comm_id = ser_last_row.index[0][5]\n",
    "    date_last_year = ser_last_row.index[0][0]\n",
    "    bool_break_flag = True\n",
    "    print('Last saved observation options:', date_last_year.year, '/', str_last_comm_id)\n",
    "else:\n",
    "    date_last_year = pd.to_datetime('1900-01-01')\n",
    "    print(date_last_year.year)\n",
    "    bool_break_flag = False\n",
    "    \n",
    "### Looping over period portions:\n",
    "for iter_portion in range(-(-len(list_periods) // int_services_period_portion)):\n",
    "    gc.collect()\n",
    "    ### Selecting periods:\n",
    "    list_iter_periods = list_periods[iter_portion *  int_services_period_portion : (iter_portion + 1) *  int_services_period_portion]\n",
    "    if (int(list_iter_periods[-1]) < date_last_year.year):\n",
    "        continue\n",
    "    else:         \n",
    "        ### Commodities data loading:\n",
    "        for iter_comm_id in list_un_services_ag2:\n",
    "            ### Starting point searching:\n",
    "            if bool_break_flag:\n",
    "                if (list_un_services_ag2.index(str_last_comm_id) < list_un_services_ag2.index(iter_comm_id)):\n",
    "                    continue\n",
    "                elif (list_un_services_ag2.index(str_last_comm_id) == list_un_services_ag2.index(iter_comm_id)):\n",
    "                    bool_break_flag = False\n",
    "                    continue\n",
    "            ### Loading procedure:\n",
    "            else:\n",
    "                ### Container initialization:\n",
    "                list_un_collection = []                \n",
    "                ### Export Data Requests:                \n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "                ### Export of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec) \n",
    "                ### Import Data Requests:\n",
    "                print(iter_comm_id, '/', list_iter_periods, '/ Import: Loading through the API')\n",
    "                ### Import of Goods:\n",
    "                df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_import_flow, list_un_reporters, list_un_partners, \n",
    "                                                       [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "                ### Adding dataset to container:\n",
    "                if (df_iter_dataset is not None):\n",
    "                    list_un_collection.append(df_iter_dataset)\n",
    "                    time.sleep(int_pause_short_sec)\n",
    "                else:\n",
    "                    time.sleep(int_pause_long_sec)             \n",
    "                ### Downloaded Data concatenation and indexation:\n",
    "                if (len(list_un_collection) > 0):\n",
    "                    df_full_dataset = pd.concat(list_un_collection, axis = 0, sort = False, ignore_index = True)\n",
    "                    ser_full_dataset = df_full_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "                    ### Dataset saving (need to be replaced with SQL Request):\n",
    "                    ser_full_dataset.to_hdf(path_or_buf = str_path_unc_eb10_pm_services_annual, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, \n",
    "                                            append = True)\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Flows saved to database')\n",
    "                else:\n",
    "                    print(iter_comm_id, '/', list_iter_periods, ': Both flow\\'s datasets are empty')\n",
    "#            break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_eb_am = pd.read_hdf(str_path_unc_eb_am_services_annual)\n",
    "ser_eb_pm = pd.read_hdf(str_path_unc_eb_pm_services_annual)\n",
    "ser_eb10_pm = pd.read_hdf(str_path_unc_eb10_pm_services_annual)\n",
    "ser_eb_am.reset_index('Commodity_ID').replace(ser_ebops_2002.to_dict()).set_index('Commodity_ID', append = True).squeeze().sort_index()\n",
    "ser_services = pd.concat([ser_eb_am, ser_eb_pm, ser_eb10_pm]).sort_index()\n",
    "ser_services.to_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, append = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEVICES DATA RESEARCH : 1989 - 1999 TEST\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "str_am_id = '206' # '247'\n",
    "str_pm_id = '3.1' # '9.1'\n",
    "\n",
    "list_iter_periods = list(map(str, range(1989, 2000)))\n",
    "### Container initialization:\n",
    "list_un_collection = []    \n",
    "\n",
    "for iter_tup in [('EB', str_am_id), ('EB02', str_am_id), ('EB10', str_am_id), ('EB', str_pm_id), ('EB02', str_pm_id), ('EB10', str_pm_id)]:\n",
    "    str_services_class = iter_tup[0]\n",
    "    iter_comm_id = iter_tup[1]\n",
    "            \n",
    "    ### Export Data Requests:                \n",
    "    print(str_services_class, '/', iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "    ### Export of Goods:\n",
    "    df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                           [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "    ### Adding dataset to container:\n",
    "    if (df_iter_dataset is not None):\n",
    "        ser_iter_dataset = df_iter_dataset.set_index(['Date', 'Reporter', 'Partner'])['Value'].squeeze().sort_index()\n",
    "        ser_iter_dataset = pd.concat({str_services_class : pd.concat({iter_comm_id : ser_iter_dataset}, names = ['ID'])}, names = ['Class'])\n",
    "        list_un_collection.append(ser_iter_dataset)\n",
    "    time.sleep(int_pause_long_sec)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "### SEVICES DATA RESEARCH : 1989 - 1999 TEST\n",
    "\n",
    "print(list_un_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEVICES DATA RESEARCH : 2000 - 2009 TEST\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "str_am_id = '206' # '247'\n",
    "str_pm_id = '3.1' # '9.1'\n",
    "\n",
    "list_iter_periods = list(map(str, range(2000, 2010)))\n",
    "### Container initialization:\n",
    "list_un_collection = []    \n",
    "\n",
    "for iter_tup in [('EB', str_am_id), ('EB02', str_am_id), ('EB10', str_am_id), ('EB', str_pm_id), ('EB02', str_pm_id), ('EB10', str_pm_id)]:\n",
    "    str_services_class = iter_tup[0]\n",
    "    iter_comm_id = iter_tup[1]\n",
    "            \n",
    "    ### Export Data Requests:                \n",
    "    print(str_services_class, '/', iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "    ### Export of Goods:\n",
    "    df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                           [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "    ### Adding dataset to container:\n",
    "    if (df_iter_dataset is not None):\n",
    "        ser_iter_dataset = df_iter_dataset.set_index(['Date', 'Reporter', 'Partner'])['Value'].squeeze().sort_index()\n",
    "        ser_iter_dataset = pd.concat({str_services_class : pd.concat({iter_comm_id : ser_iter_dataset}, names = ['ID'])}, names = ['Class'])\n",
    "        list_un_collection.append(ser_iter_dataset)\n",
    "    time.sleep(int_pause_long_sec)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>2000-12-29</th>\n",
       "      <th>2001-12-31</th>\n",
       "      <th>2002-12-31</th>\n",
       "      <th>2003-12-31</th>\n",
       "      <th>2004-12-31</th>\n",
       "      <th>2005-12-30</th>\n",
       "      <th>2006-12-29</th>\n",
       "      <th>2007-12-31</th>\n",
       "      <th>2008-12-31</th>\n",
       "      <th>2009-12-31</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EB</th>\n",
       "      <th>206</th>\n",
       "      <td>84</td>\n",
       "      <td>108</td>\n",
       "      <td>226</td>\n",
       "      <td>251</td>\n",
       "      <td>438</td>\n",
       "      <td>524</td>\n",
       "      <td>602</td>\n",
       "      <td>568</td>\n",
       "      <td>594</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Date       2000-12-29  2001-12-31  2002-12-31  2003-12-31  2004-12-31  \\\n",
       "Class ID                                                                \n",
       "EB    206          84         108         226         251         438   \n",
       "\n",
       "Date       2005-12-30  2006-12-29  2007-12-31  2008-12-31  2009-12-31  \n",
       "Class ID                                                               \n",
       "EB    206         524         602         568         594         550  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### SEVICES DATA RESEARCH : 2000 - 2009 TEST\n",
    "\n",
    "ser_test_id = pd.concat(list_un_collection)#\n",
    "display(ser_test_id.groupby(['Class', 'ID', 'Date']).apply(len).unstack('Date'))\n",
    "#ser_test_id.unstack(['Class', 'ID']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEVICES DATA RESEARCH : 2010 - 2015 TEST\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "str_am_id = '206' # '247'\n",
    "str_pm_id = '3.1' # '9.1'\n",
    "\n",
    "list_iter_periods = list(map(str, range(2010, 2016)))\n",
    "### Container initialization:\n",
    "list_un_collection = []    \n",
    "\n",
    "for iter_tup in [('EB', str_am_id), ('EB02', str_am_id), ('EB10', str_am_id), ('EB', str_pm_id), ('EB02', str_pm_id), ('EB10', str_pm_id)]:\n",
    "    str_services_class = iter_tup[0]\n",
    "    iter_comm_id = iter_tup[1]\n",
    "            \n",
    "    ### Export Data Requests:                \n",
    "    print(str_services_class, '/', iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "    ### Export of Goods:\n",
    "    df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                           [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "    ### Adding dataset to container:\n",
    "    if (df_iter_dataset is not None):\n",
    "        ser_iter_dataset = df_iter_dataset.set_index(['Date', 'Reporter', 'Partner'])['Value'].squeeze().sort_index()\n",
    "        ser_iter_dataset = pd.concat({str_services_class : pd.concat({iter_comm_id : ser_iter_dataset}, names = ['ID'])}, names = ['Class'])\n",
    "        list_un_collection.append(ser_iter_dataset)\n",
    "    time.sleep(int_pause_long_sec)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>2010-12-31</th>\n",
       "      <th>2011-12-30</th>\n",
       "      <th>2012-12-31</th>\n",
       "      <th>2013-12-31</th>\n",
       "      <th>2014-12-31</th>\n",
       "      <th>2015-12-31</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">EB</th>\n",
       "      <th>206</th>\n",
       "      <td>48.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.1</th>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EB02</th>\n",
       "      <th>206</th>\n",
       "      <td>349.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>609.0</td>\n",
       "      <td>611.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EB10</th>\n",
       "      <th>3.1</th>\n",
       "      <td>346.0</td>\n",
       "      <td>349.0</td>\n",
       "      <td>484.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Date       2010-12-31  2011-12-30  2012-12-31  2013-12-31  2014-12-31  \\\n",
       "Class ID                                                                \n",
       "EB    206        48.0        58.0         6.0         NaN         NaN   \n",
       "      3.1        74.0        74.0        74.0        80.0        80.0   \n",
       "EB02  206       349.0       352.0       496.0       560.0       609.0   \n",
       "EB10  3.1       346.0       349.0       484.0       548.0       597.0   \n",
       "\n",
       "Date       2015-12-31  \n",
       "Class ID               \n",
       "EB    206         NaN  \n",
       "      3.1        79.0  \n",
       "EB02  206       611.0  \n",
       "EB10  3.1       600.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>EB</th>\n",
       "      <th>EB02</th>\n",
       "      <th>EB</th>\n",
       "      <th>EB10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>206</th>\n",
       "      <th>206</th>\n",
       "      <th>3.1</th>\n",
       "      <th>3.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Reporter</th>\n",
       "      <th>Partner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(EB, 206), (EB02, 206), (EB, 3.1), (EB10, 3.1)]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SEVICES DATA RESEARCH : 2010 - 2015 TEST\n",
    "\n",
    "ser_test_id = pd.concat(list_un_collection)\n",
    "df_test_id = ser_test_id.unstack(['Class', 'ID'])\n",
    "print('Number of values:')\n",
    "display(ser_test_id.groupby(['Class', 'ID', 'Date']).apply(len).unstack('Date'))\n",
    "\n",
    "df_test_id[(df_test_id[('EB', str_am_id)].isna() & df_test_id[('EB', str_pm_id)].isna() & df_test_id[('EB10', str_pm_id)].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SEVICES DATA RESEARCH : 2016 - 2022 TEST\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "str_am_id = '206' # '247'\n",
    "str_pm_id = '3.1' # '9.1'\n",
    "\n",
    "list_iter_periods = list(map(str, range(2016, 2023)))\n",
    "### Container initialization:\n",
    "list_un_collection = []    \n",
    "\n",
    "for iter_tup in [('EB', str_am_id), ('EB02', str_am_id), ('EB10', str_am_id), ('EB', str_pm_id), ('EB02', str_pm_id), ('EB10', str_pm_id)]:\n",
    "    str_services_class = iter_tup[0]\n",
    "    iter_comm_id = iter_tup[1]\n",
    "            \n",
    "    ### Export Data Requests:                \n",
    "    print(str_services_class, '/', iter_comm_id, '/', list_iter_periods, '/ Export: Loading through the API')\n",
    "    ### Export of Goods:\n",
    "    df_iter_dataset = get_un_comtrade_data(str_services_type, str_freq, str_services_class, str_export_flow, list_un_reporters, list_un_partners, \n",
    "                                           [iter_comm_id], list_iter_periods, str_primary_key)\n",
    "    ### Adding dataset to container:\n",
    "    if (df_iter_dataset is not None):\n",
    "        ser_iter_dataset = df_iter_dataset.set_index(['Date', 'Reporter', 'Partner'])['Value'].squeeze().sort_index()\n",
    "        ser_iter_dataset = pd.concat({str_services_class : pd.concat({iter_comm_id : ser_iter_dataset}, names = ['ID'])}, names = ['Class'])\n",
    "        list_un_collection.append(ser_iter_dataset)\n",
    "    time.sleep(int_pause_long_sec)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>2016-12-30</th>\n",
       "      <th>2017-12-29</th>\n",
       "      <th>2018-12-31</th>\n",
       "      <th>2019-12-31</th>\n",
       "      <th>2020-12-31</th>\n",
       "      <th>2021-12-31</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EB</th>\n",
       "      <th>3.1</th>\n",
       "      <td>81</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "      <td>307</td>\n",
       "      <td>312</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EB02</th>\n",
       "      <th>206</th>\n",
       "      <td>610</td>\n",
       "      <td>742</td>\n",
       "      <td>721</td>\n",
       "      <td>618</td>\n",
       "      <td>633</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EB10</th>\n",
       "      <th>3.1</th>\n",
       "      <td>599</td>\n",
       "      <td>618</td>\n",
       "      <td>605</td>\n",
       "      <td>506</td>\n",
       "      <td>527</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Date       2016-12-30  2017-12-29  2018-12-31  2019-12-31  2020-12-31  \\\n",
       "Class ID                                                                \n",
       "EB    3.1          81         194         194         307         312   \n",
       "EB02  206         610         742         721         618         633   \n",
       "EB10  3.1         599         618         605         506         527   \n",
       "\n",
       "Date       2021-12-31  \n",
       "Class ID               \n",
       "EB    3.1         314  \n",
       "EB02  206         896  \n",
       "EB10  3.1         582  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>EB02</th>\n",
       "      <th>EB</th>\n",
       "      <th>EB10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>206</th>\n",
       "      <th>3.1</th>\n",
       "      <th>3.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Reporter</th>\n",
       "      <th>Partner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(EB02, 206), (EB, 3.1), (EB10, 3.1)]\n",
       "Index: []"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SEVICES DATA RESEARCH : 2016 - 2022 TEST\n",
    "\n",
    "ser_test_id = pd.concat(list_un_collection)\n",
    "df_test_id = ser_test_id.unstack(['Class', 'ID'])\n",
    "print('Number of values:')\n",
    "display(ser_test_id.groupby(['Class', 'ID', 'Date']).apply(len).unstack('Date'))\n",
    "\n",
    "df_test_id[(df_test_id[('EB', str_pm_id)].isna() & df_test_id[('EB10', str_pm_id)].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>2016-12-30</th>\n",
       "      <th>2017-12-29</th>\n",
       "      <th>2018-12-31</th>\n",
       "      <th>2019-12-31</th>\n",
       "      <th>2020-12-31</th>\n",
       "      <th>2021-12-31</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EB</th>\n",
       "      <th>3.1</th>\n",
       "      <td>81</td>\n",
       "      <td>194</td>\n",
       "      <td>194</td>\n",
       "      <td>307</td>\n",
       "      <td>312</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EB02</th>\n",
       "      <th>206</th>\n",
       "      <td>610</td>\n",
       "      <td>742</td>\n",
       "      <td>721</td>\n",
       "      <td>618</td>\n",
       "      <td>633</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EB10</th>\n",
       "      <th>3.1</th>\n",
       "      <td>599</td>\n",
       "      <td>618</td>\n",
       "      <td>605</td>\n",
       "      <td>506</td>\n",
       "      <td>527</td>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Date       2016-12-30  2017-12-29  2018-12-31  2019-12-31  2020-12-31  \\\n",
       "Class ID                                                                \n",
       "EB    3.1          81         194         194         307         312   \n",
       "EB02  206         610         742         721         618         633   \n",
       "EB10  3.1         599         618         605         506         527   \n",
       "\n",
       "Date       2021-12-31  \n",
       "Class ID               \n",
       "EB    3.1         314  \n",
       "EB02  206         896  \n",
       "EB10  3.1         582  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique EB02 | AM values:\n",
      "0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>EB02</th>\n",
       "      <th>EB</th>\n",
       "      <th>EB10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>206</th>\n",
       "      <th>3.1</th>\n",
       "      <th>3.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Reporter</th>\n",
       "      <th>Partner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(EB02, 206), (EB, 3.1), (EB10, 3.1)]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique EB | PM values:\n",
      "619\n",
      "0.44151212553495006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>EB02</th>\n",
       "      <th>EB</th>\n",
       "      <th>EB10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>206</th>\n",
       "      <th>3.1</th>\n",
       "      <th>3.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Reporter</th>\n",
       "      <th>Partner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2016-12-30</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">US</th>\n",
       "      <th>CA</th>\n",
       "      <td>NaN</td>\n",
       "      <td>331000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CH</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1249000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CL</th>\n",
       "      <td>NaN</td>\n",
       "      <td>73000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CY</th>\n",
       "      <td>NaN</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2020-12-31</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">UA</th>\n",
       "      <th>CY</th>\n",
       "      <td>NaN</td>\n",
       "      <td>17911.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19434.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GB</th>\n",
       "      <td>NaN</td>\n",
       "      <td>12018.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>42551.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9577.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>619 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Class                       EB02         EB EB10\n",
       "ID                           206        3.1  3.1\n",
       "Date       Reporter Partner                     \n",
       "2016-12-30 US       CA       NaN   331000.0  NaN\n",
       "                    CH       NaN  1249000.0  NaN\n",
       "                    CL       NaN    73000.0  NaN\n",
       "                    CN       NaN  1200000.0  NaN\n",
       "                    CY       NaN    52000.0  NaN\n",
       "...                          ...        ...  ...\n",
       "2020-12-31 UA       CY       NaN    17911.0  NaN\n",
       "                    DE       NaN    19434.0  NaN\n",
       "                    GB       NaN    12018.0  NaN\n",
       "                    TR       NaN    42551.0  NaN\n",
       "                    US       NaN     9577.0  NaN\n",
       "\n",
       "[619 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique EB10 | PM values:\n",
      "0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>EB02</th>\n",
       "      <th>EB</th>\n",
       "      <th>EB10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>206</th>\n",
       "      <th>3.1</th>\n",
       "      <th>3.1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Reporter</th>\n",
       "      <th>Partner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(EB02, 206), (EB, 3.1), (EB10, 3.1)]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### SEVICES DATA RESEARCH : 2016 - 2022 TEST\n",
    "\n",
    "ser_test_id = pd.concat(list_un_collection)\n",
    "df_test_id = ser_test_id.unstack(['Class', 'ID'])\n",
    "print('Number of values:')\n",
    "display(ser_test_id.groupby(['Class', 'ID', 'Date']).apply(len).unstack('Date'))\n",
    "print('Number of unique EB02 | AM values:')\n",
    "ser_test_unique = df_test_id.dropna(subset = [('EB02', '206')]).notna().sum(axis = 1)\n",
    "print(len(ser_test_unique[ser_test_unique == 1]))\n",
    "print(len(ser_test_unique[ser_test_unique == 1]) / len(df_test_id.dropna(subset = [('EB02', '206')])))\n",
    "display(df_test_id.loc[ser_test_unique[ser_test_unique == 1].index])\n",
    "print('Number of unique EB | PM values:')\n",
    "ser_test_unique = df_test_id.dropna(subset = [('EB', '3.1')]).notna().sum(axis = 1)\n",
    "print(len(ser_test_unique[ser_test_unique == 1]))\n",
    "print(len(ser_test_unique[ser_test_unique == 1]) / len(df_test_id.dropna(subset = [('EB', '3.1')])))\n",
    "display(df_test_id.loc[ser_test_unique[ser_test_unique == 1].index])\n",
    "print('Number of unique EB10 | PM values:')\n",
    "ser_test_unique = df_test_id.dropna(subset = [('EB10', '3.1')]).notna().sum(axis = 1)\n",
    "print(len(ser_test_unique[ser_test_unique == 1]))\n",
    "print(len(ser_test_unique[ser_test_unique == 1]) / len(df_test_id.dropna(subset = [('EB10', '3.1')])))\n",
    "display(df_test_id.loc[ser_test_unique[ser_test_unique == 1].index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA CONCATENATION AND SAVING\n",
    "\n",
    "#### Downloaded Data concatenation and indexation:\n",
    "#df_full_dataset = pd.concat(list_un_collection, axis = 0, sort = False, ignore_index = True)\n",
    "#del list_un_collection\n",
    "#ser_full_dataset = df_full_dataset.set_index(['Date', 'Reporter', 'Partner', 'Flow', 'Type', 'Commodity_ID']).squeeze().sort_index()\n",
    "#### Dataset saving (need to be replaced with SQL Request):\n",
    "#ser_full_dataset.to_hdf(path_or_buf = str_path_unc_res_all_annual, key = str_key_unc_res, mode = 'w', format = 'table', complevel = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPORT AND REVERTED IMPORT CONCATENATION\n",
    "\n",
    "gc.collect()\n",
    "### File deleting (need to be replaced with SQL Request):\n",
    "if (os.path.exists(str_path_unc_res_flows)):\n",
    "    os.remove(str_path_unc_res_flows)\n",
    "### Results container:\n",
    "list_export_aug = []\n",
    "### Countries portion length:\n",
    "int_portion = 5\n",
    "list_unc_countries = sorted(dict_codelist['partnerCode'].unique())\n",
    "### Looping over countries portions:\n",
    "for iter_num in range(len(list_unc_countries) // int_portion + 1):\n",
    "    gc.collect()    \n",
    "    ### Portion of countries selecting:\n",
    "    list_iter_countries = list(list_unc_countries)[int_portion * iter_num : int_portion * (iter_num + 1)]\n",
    "    if (len(list_iter_countries) > 0):\n",
    "        print(list_iter_countries)\n",
    "        ### Export data loading:\n",
    "        ser_unc_export = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res,\n",
    "                                     where = \"(Flow = 'Export') & (Reporter in list_iter_countries) & (Partner != 'World')\").droplevel('Flow')\n",
    "        print('Export dataset loaded')\n",
    "        ### Import data loading:\n",
    "        ser_unc_import = pd.read_hdf(str_path_unc_res_all_annual, key = str_key_unc_res, \n",
    "                                     where = \"(Flow = 'Import') & (Partner in list_iter_countries)\").droplevel('Flow')\n",
    "        print('Import dataset loaded')    \n",
    "        ### Import data reverting:\n",
    "        ser_unc_import.index.set_names('Partner_Inv', level = 1, inplace = True)\n",
    "        ser_unc_import.index.set_names('Reporter', level = 2, inplace = True)\n",
    "        ser_unc_import.index.set_names('Partner', level = 1, inplace = True)\n",
    "        ser_unc_import = ser_unc_import.swaplevel('Reporter', 'Partner').sort_index()\n",
    "        print('Import dataset reverted')\n",
    "        ### Datasets concatenation:\n",
    "        df_export_aug = pd.concat([ser_unc_export, ser_unc_import], axis = 1, names = 'Source Flow', keys = ['Export', 'Import']).astype('float32')\n",
    "        del ser_unc_export\n",
    "        del ser_unc_import    \n",
    "        gc.collect()    \n",
    "        print('Export and reverted Import dataset concatenated')\n",
    "        ### Aggregateddataset saving (need to be replaced with SQL Request):\n",
    "        df_export_aug.to_hdf(str_path_unc_res_flows, key = str_key_unc_res, mode = 'a', format = 'table', complevel = 9, append = True)                \n",
    "        print('Aggregated dataset added to database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CIF COEFFICIENTS CALCULATION & IMPLEMENTATION\n",
    "\n",
    "gc.collect()\n",
    "### Files deleting (need to be replaced with SQL Request):\n",
    "if (os.path.exists(str_path_export_bilateral)):\n",
    "    os.remove(str_path_export_bilateral)\n",
    "if (os.path.exists(str_path_import_bilateral)):\n",
    "    os.remove(str_path_import_bilateral)\n",
    "### Getting full list of commodities:\n",
    "str_date = '2020-12-31'\n",
    "list_commodity_id = sorted(pd.read_hdf(str_path_unc_res_flows, key = str_key_unc_res, where = \"Date in str_date\").index.get_level_values('Commodity_ID').unique())\n",
    "### Bounds to filter bilateral Import to Export ratio before median calculation:\n",
    "flo_lower_bound = 1.0\n",
    "flo_upper_bound = 2.0\n",
    "### Bilateral median calculation procedure:\n",
    "def get_obs_median(df_comm):\n",
    "    ### Export to Import ratio:\n",
    "    ser_obs_coeff = df_comm['Import'] / df_comm['Export']\n",
    "    ### Ratio filtering:\n",
    "    ser_obs_coeff = ser_obs_coeff.loc[(ser_obs_coeff >= flo_lower_bound) & (ser_obs_coeff <= flo_upper_bound)]\n",
    "    ### Filtered timeseries median as a result:\n",
    "    return ser_obs_coeff.median()\n",
    "### Calulation CIF coefficient for each commodity:\n",
    "for iter_commodity in list_commodity_id:\n",
    "    gc.collect()\n",
    "    ### Commodity flows loading (need to be replaced with SQL Request):\n",
    "    df_iter_flows = pd.read_hdf(str_path_unc_res_flows, key = str_key_unc_res, where = \"Commodity_ID = iter_commodity\")\n",
    "    ### Bilateral Commodity CIF Median calculation:\n",
    "    ser_cif_median = df_iter_flows.droplevel('Commodity_ID').groupby(['Reporter', 'Partner']).apply(get_obs_median)\n",
    "    ### General Commodity Median calculation:\n",
    "    flo_median = ser_cif_median.median()\n",
    "    print(iter_commodity, ':', flo_median)\n",
    "    ### Filling missed bilateral values with general commodity median:\n",
    "    if not (np.isnan(flo_median)):\n",
    "        ser_cif_median.fillna(flo_median, inplace = True)        \n",
    "    ser_cif_median.name = 'CIF_Coefficient'              \n",
    "    ### Adding CIF coefficients to dataset:\n",
    "    df_export_cif = df_iter_flows.merge(ser_cif_median, left_index = True, right_index = True)\n",
    "    df_export_cif = df_export_cif.reorder_levels(['Date', 'Reporter', 'Partner', 'Type', 'Commodity_ID'])\n",
    "    ### Import correction:\n",
    "    df_export_cif['Import_Corrected'] = df_export_cif['Import'] / df_export_cif['CIF_Coefficient']\n",
    "    ### Export correction:\n",
    "    df_export_cif['Export_Corrected'] = df_export_cif['Export'] * df_export_cif['CIF_Coefficient']\n",
    "    ### Combining Export & Import data:\n",
    "    ser_export_cif = df_export_cif['Export'].combine_first(df_export_cif['Import_Corrected']).astype('float32')\n",
    "    ser_import_cif = df_export_cif['Import'].combine_first(df_export_cif['Export_Corrected']).astype('float32')\n",
    "    gc.collect()\n",
    "    ### Import data reverting back to original order:\n",
    "    ser_import_cif = ser_import_cif.reorder_levels(['Date', 'Partner', 'Reporter', 'Type', 'Commodity_ID']).sort_index()                               \n",
    "    ser_import_cif.index.names = ['Date', 'Reporter', 'Partner', 'Type', 'Commodity_ID']\n",
    "    gc.collect()\n",
    "    ### Augmented flows saving (need to be replaced with SQL Request):\n",
    "    ser_export_cif.squeeze().to_hdf(str_path_export_bilateral, key = str_key_unc_export, mode = 'a', format = 'table', complevel = 9, append = True, \n",
    "                                    min_itemsize = {'Type': 8, 'Commodity_ID': 3})\n",
    "    ser_import_cif.squeeze().to_hdf(str_path_import_bilateral, key = str_key_unc_import, mode = 'a', format = 'table', complevel = 9, append = True, \n",
    "                                    min_itemsize = {'Type': 8, 'Commodity_ID': 3}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
