{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: GRAVITY SOURCE DATASETS EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json ### To correct JSON structure before unpacking\n",
    "import gc\n",
    "import os\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: DEFINING COUNTRY CODES EXTRACTOR\n",
    "\n",
    "def get_country_codes(use_local_copy = False):  \n",
    "    ### In case if URL is unavailable:\n",
    "    if (use_local_copy):\n",
    "        url_country_code = 'Data_Files/Source_Files/countrycode.html'\n",
    "    ### Online extraction:\n",
    "    else:\n",
    "        url_country_code = 'https://countrycode.org/'\n",
    "    df_full_codes = pd.read_html(url_country_code, index_col = 'COUNTRY')[0]\n",
    "    df_full_codes[['ISO SHORT', 'ISO LONG']] = df_full_codes['ISO CODES'].str.split(' / ', expand = True)\n",
    "    df_result = df_full_codes[['ISO SHORT', 'ISO LONG']].sort_index()    \n",
    "    df_result.index = df_result.index.str.upper()\n",
    "    ### Results output:\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(engine = 'openpyxl', io = str_path_universe, sheet_name = 'Switchers', header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: COMMON DATA EXTRACTION STEPS\n",
    "\n",
    "### MultiIndex level slice constant:\n",
    "All = slice(None)\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Technical Constants:\n",
    "str_date_end = '2022-10-31'\n",
    "### World Country Codes:\n",
    "df_country_codes = get_country_codes()\n",
    "### ISON membership history:\n",
    "ser_ison_membership = ison_membership_converting(str_path_universe, pd.to_datetime(str_date_end))\n",
    "### ISON SHORT IDs list:\n",
    "list_ison_countries = sorted(list(map(str, ser_ison_membership.index.get_level_values(1).unique())))\n",
    "### ISON LONG IDs list:\n",
    "list_ison_long = list(df_country_codes.loc[df_country_codes['ISO SHORT'].isin(ser_ison_membership.index.get_level_values('Country').unique()), 'ISO LONG'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: BILATERAL EXPORTS & IMPORTS (MILLIONS OF USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: IMF DOTS: DATA SAVING PARAMETERS\n",
    "\n",
    "### Loaded dataset:\n",
    "str_path_imf_dots_dataset = 'Data_Files/Source_Files/dots_dataset.h5'\n",
    "str_key_imf_dots_export = 'dots_export'\n",
    "str_key_imf_dots_import_inverted = 'dots_import_inverted'\n",
    "### Resulting dataset:\n",
    "str_path_imf_dots_augmented = 'Data_Files/Source_Files/dots_augmented_unconditional.h5'\n",
    "str_key_imf_dots_augmented = 'dots_export_augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: LOADING PARAMETERS PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "str_imf_base_url = 'http://dataservices.imf.org/REST/SDMX_JSON.svc/'\n",
    "str_imf_dataset_add = 'CompactData/'\n",
    "str_imf_dots_id = 'DOT'\n",
    "str_dots_freq = 'M'\n",
    "### Максимальное количество стран в запросе (для соблюдения ограничений API):\n",
    "int_imf_country_limit = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: EXPORT DATA EXTRACTION: BILATERAL FLOWS\n",
    "\n",
    "str_dots_const_url = str_imf_base_url + str_imf_dataset_add + str_imf_dots_id + '/' # Beginning of request URL\n",
    "str_dots_indicator = 'TXG_FOB_USD'\n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "### List of bilateral dataframes for future concatenation\n",
    "list_dots_bilateral = [] \n",
    "### Looping over reporters:\n",
    "for str_reporter in list_ison_countries:\n",
    "#for str_reporter in ['AU', 'US']:\n",
    "    ### Generating complete request URL:\n",
    "    str_dots_full_url = str_dots_const_url + '.'.join([str_dots_freq, str_reporter, str_dots_indicator])\n",
    "    ### Receiving DOTS dataset from IMF API:\n",
    "    print(str_reporter, ' / ', str_dots_indicator)\n",
    "    obj_dots_set = request_session.get(str_dots_full_url)\n",
    "    ### Data reading as JSON:\n",
    "    dict_dots_set = json.loads(obj_dots_set.text)\n",
    "    if ('Series' in dict_dots_set['CompactData']['DataSet']):\n",
    "        ### Converting each bilateral dataset to dataframe and it's mungling:\n",
    "        for dict_dots_pair in dict_dots_set['CompactData']['DataSet']['Series']:\n",
    "            if isinstance(dict_dots_pair['Obs'], list):\n",
    "                df_dots_bilateral = pd.DataFrame(dict_dots_pair['Obs'])\n",
    "            else:\n",
    "                df_dots_bilateral = pd.DataFrame([dict_dots_pair['Obs']])\n",
    "            ### Markers checking:\n",
    "            if '@OBS_STATUS' not in df_dots_bilateral.columns:\n",
    "                df_dots_bilateral['@OBS_STATUS'] = np.NaN\n",
    "            ### Data extracting and mungling:\n",
    "            df_dots_bilateral = df_dots_bilateral[['@TIME_PERIOD', '@OBS_VALUE', '@OBS_STATUS']]\n",
    "            df_dots_bilateral.columns = ['Date', 'Value', 'Status']\n",
    "            df_dots_bilateral = df_dots_bilateral.assign(Reporter_ID = dict_dots_pair['@REF_AREA'])\n",
    "            df_dots_bilateral = df_dots_bilateral.assign(Partner_ID = dict_dots_pair['@COUNTERPART_AREA'])\n",
    "            list_dots_bilateral.append(df_dots_bilateral)\n",
    "### Flow level data aggregation:\n",
    "df_dots_indicator = pd.concat(list_dots_bilateral, axis = 0, ignore_index = True)\n",
    "df_dots_indicator['Date'] = pd.to_datetime(df_dots_indicator['Date']) + pd.offsets.BMonthEnd()\n",
    "df_dots_indicator = df_dots_indicator[df_dots_indicator['Partner_ID'].isin(df_country_codes['ISO SHORT'].values)].drop('Status', axis = 1)\n",
    "df_dots_indicator.rename({'Reporter_ID': 'Reporter', 'Partner_ID': 'Partner'}, axis = 1, inplace = True)\n",
    "### Data saving:\n",
    "ser_dots_export = df_dots_indicator.set_index(['Date', 'Reporter', 'Partner'])['Value'].sort_index().astype('float16')\n",
    "del df_dots_indicator\n",
    "gc.collect()\n",
    "ser_dots_export.to_hdf(path_or_buf = str_path_imf_dots_dataset, key = str_key_imf_dots_export, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: IMPORT DATA EXTRACTION: BILATERAL FLOWS\n",
    "\n",
    "str_dots_const_url = str_imf_base_url + str_imf_dataset_add + str_imf_dots_id + '/' # Beginning of request URL\n",
    "str_dots_indicator = 'TMG_CIF_USD'\n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "### List of bilateral dataframes for future concatenation\n",
    "list_dots_bilateral = [] \n",
    "### Looping over reporters:\n",
    "for str_reporter in list_ison_countries:\n",
    "#for str_reporter in ['AU', 'US']:\n",
    "    ### Generating complete request URL:\n",
    "    str_dots_full_url = str_dots_const_url + '.'.join([str_dots_freq, '', str_dots_indicator, str_reporter])\n",
    "    ### Receiving DOTS dataset from IMF API:\n",
    "    print(str_reporter, ' / ', str_dots_indicator)\n",
    "    obj_dots_set = request_session.get(str_dots_full_url)\n",
    "    ### Data reading as JSON:\n",
    "    dict_dots_set = json.loads(obj_dots_set.text)\n",
    "    if ('Series' in dict_dots_set['CompactData']['DataSet']):\n",
    "        ### Converting each bilateral dataset to dataframe and it's mungling:\n",
    "        for dict_dots_pair in dict_dots_set['CompactData']['DataSet']['Series']:\n",
    "            if isinstance(dict_dots_pair['Obs'], list):\n",
    "                df_dots_bilateral = pd.DataFrame(dict_dots_pair['Obs'])\n",
    "            else:\n",
    "                df_dots_bilateral = pd.DataFrame([dict_dots_pair['Obs']])\n",
    "            ### Markers checking:\n",
    "            if '@OBS_STATUS' not in df_dots_bilateral.columns:\n",
    "                df_dots_bilateral['@OBS_STATUS'] = np.NaN\n",
    "            ### Data extracting and mungling:\n",
    "            df_dots_bilateral = df_dots_bilateral[['@TIME_PERIOD', '@OBS_VALUE', '@OBS_STATUS']]\n",
    "            df_dots_bilateral.columns = ['Date', 'Value', 'Status']\n",
    "            df_dots_bilateral = df_dots_bilateral.assign(Reporter_ID = dict_dots_pair['@REF_AREA'])\n",
    "            df_dots_bilateral = df_dots_bilateral.assign(Partner_ID = dict_dots_pair['@COUNTERPART_AREA'])\n",
    "            list_dots_bilateral.append(df_dots_bilateral)\n",
    "### Flow level data aggregation:\n",
    "df_dots_indicator = pd.concat(list_dots_bilateral, axis = 0, ignore_index = True)\n",
    "df_dots_indicator['Date'] = pd.to_datetime(df_dots_indicator['Date']) + pd.offsets.BMonthEnd()\n",
    "df_dots_indicator = df_dots_indicator[df_dots_indicator['Reporter_ID'].isin(df_country_codes['ISO SHORT'].values)].drop('Status', axis = 1)\n",
    "df_dots_indicator.rename({'Reporter_ID': 'Partner', 'Partner_ID': 'Reporter'}, axis = 1, inplace = True)\n",
    "### Data saving:\n",
    "ser_dots_import_inv = df_dots_indicator.set_index(['Date', 'Reporter', 'Partner'])['Value'].sort_index().astype('float16')\n",
    "del df_dots_indicator\n",
    "gc.collect()\n",
    "ser_dots_import_inv.to_hdf(path_or_buf = str_path_imf_dots_dataset, key = str_key_imf_dots_import_inverted, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: EXPORT & IMPORT DATA AGGREGATION: DATASETS LOADING\n",
    "\n",
    "### Создаем DataFrame, в который складываем две колонки: Export и Inverted Import:\n",
    "gc.collect()\n",
    "ser_dots_export = pd.read_hdf(path_or_buf = str_path_imf_dots_dataset, key = str_key_imf_dots_export)\n",
    "ser_dots_import_inv = pd.read_hdf(path_or_buf = str_path_imf_dots_dataset, key = str_key_imf_dots_import_inverted)\n",
    "df_export_aug = pd.concat([ser_dots_export, ser_dots_import_inv], axis = 1, names = 'Source Flow', keys = ['Export', 'Import'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: CIF COEFFICIENTS CALCULATION\n",
    "\n",
    "### Поскольку мы забираем CIF Import, то, для того, чтобы он был сравним с FOB Export нам нужно учесть расходы на CIF и нивелировать их.\n",
    "gc.collect()\n",
    "### Bounds to filter bilateral Import to Export ratio before median calculation: \n",
    "flo_lower_bound = 1.0\n",
    "flo_upper_bound = 2.0\n",
    "### Bilateral median calculation procedure:\n",
    "def get_obs_median(df_comm):\n",
    "    ### Export to Import ratio:\n",
    "    ser_obs_coeff = df_comm['Import'] / df_comm['Export']\n",
    "    ### Ratio filtering:\n",
    "    ser_obs_coeff = ser_obs_coeff.loc[(ser_obs_coeff >= flo_lower_bound) & (ser_obs_coeff <= flo_upper_bound)]\n",
    "    ### Filtered timeseries median as a result:\n",
    "    return round(ser_obs_coeff.median(), 2)\n",
    "\n",
    "### Calulation CIF coefficient for all commodities:\n",
    "### Для каждой пары Exporter / Importer делаем следующее:\n",
    "### Выбираем только те даты, где соотношение CIF Import / FOB Export у пары находится в диапазоне от 1 до 2.\n",
    "### Считаем медианное значение CIF Coefficient = CIF Import / FOB Export\n",
    "ser_cif_median = df_export_aug.groupby(['Reporter', 'Partner']).apply(get_obs_median)\n",
    "### Для тех пар, у которых такого значения не существует, определяем в качестве CIF Coefficient медиану среди медиан\n",
    "ser_cif_median.fillna(ser_cif_median.median(), inplace = True)\n",
    "ser_cif_median.name = 'CIF_Coefficient'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: IMPORT DATA CORRECTION:\n",
    "\n",
    "### Adding CIF coefficients to dataset:\n",
    "### Дополняем DataFrame колонкой CIF Coefficient:\n",
    "df_export_cif = df_export_aug.merge(ser_cif_median, left_index = True, right_index = True)\n",
    "del df_export_aug\n",
    "gc.collect()\n",
    "df_export_cif = df_export_cif.reorder_levels(['Date', 'Reporter', 'Partner']).sort_index()\n",
    "### Import correction:\n",
    "### Считаем Corrected Import как CIF Import / CIF Coefficient\n",
    "df_export_cif['Import_Corrected'] = df_export_cif['Import'] / df_export_cif['CIF_Coefficient'].astype('float16')\n",
    "df_export_cif.drop(['Import', 'CIF_Coefficient'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: UNCONDITIONAL COMBINATION\n",
    "\n",
    "### Combining Export & Import data:\n",
    "### Наним нулевые значения Export, чтобы они не помешали нам забрать ненулевой Import\n",
    "df_export_cif.loc[df_export_cif['Export'] == 0.0, 'Export'] = np.NaN \n",
    "### Дополняем Export данными Corrected Import:\n",
    "ser_export_augmented = df_export_cif['Export'].combine_first(df_export_cif['Import_Corrected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: IMF DOTS: RESULTS SAVING\n",
    "\n",
    "ser_export_augmented.to_hdf(path_or_buf = str_path_imf_dots_augmented, key = str_key_imf_dots_augmented, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CPIS: BILATERAL EQUITY INVESTMENT POSITIONS (MILLIONS OF USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: IMF CPIS: DATA SAVING PARAMETERS\n",
    "\n",
    "### Loaded dataset:\n",
    "str_path_imf_cpis_detailed_raw = 'Data_Files/Source_Files/cpis_detailed_raw.h5'\n",
    "str_key_imf_cpis_assets = 'cpis_detailed_assets'\n",
    "str_key_imf_cpis_liabilities = 'cpis_detailed_liabilities'\n",
    "### Filtered dataset:\n",
    "str_path_imf_cpis_filtered = 'Data_Files/Source_Files/cpis_filtered.h5'\n",
    "str_key_imf_cpis_filtered_asset = 'cpis_filtered_asset'\n",
    "str_key_imf_cpis_filtered_liability = 'cpis_filtered_liability'\n",
    "### Resulting dataset:\n",
    "str_path_imf_cpis_augmented = 'Data_Files/Source_Files/cpis_augmented_unconditional.h5'\n",
    "str_key_imf_cpis_augmented = 'cpis_asset_augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CPIS: LOADING PARAMETERS PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "str_imf_base_url = 'http://dataservices.imf.org/REST/SDMX_JSON.svc/'\n",
    "str_imf_dataset_add = 'CompactData/'\n",
    "str_imf_cpis_id = 'CPIS'\n",
    "str_cpis_freq = 'A'\n",
    "int_seconds_to_sleep = 1\n",
    "int_imf_country_limit = 30\n",
    "list_sector_filtered = ['T', 'CB', 'GG', 'HH', 'NP']\n",
    "dict_indicator = {'I_A_E_T_T_BP6_USD': 'Assets, Equity, BPM6, US Dollars', 'I_L_E_T_T_BP6_USD': 'Liabilities, Equity, BPM6, US Dollars'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CPIS : REPORTED PORTFOLIO INVESTMENT ASSETS DATASET RETRIEVING\n",
    "\n",
    "gc.collect()\n",
    "### List of bilateral dataframes for future concatenation:\n",
    "list_cpis_bilateral = [] \n",
    "### Beggining of request URL:\n",
    "str_cpis_const_url = str_imf_base_url + str_imf_dataset_add + str_imf_cpis_id + '/'\n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "### Looping over reporter:\n",
    "for iter_investor in list_ison_countries:\n",
    "#for iter_investor in ['AU', 'US']:  \n",
    "    ### Looping over indicator:\n",
    "    for iter_indicator in dict_indicator:        \n",
    "        if (iter_indicator[2] == 'A'):\n",
    "            str_reporter_sector = '+'.join(list_sector_filtered)        \n",
    "            str_partner_sector = '+'.join(list_sector_filtered)\n",
    "            str_cpis_full_url = str_cpis_const_url + '.'.join([str_cpis_freq, iter_investor, iter_indicator, str_reporter_sector, str_partner_sector])\n",
    "            obj_cpis_set = request_session.get(str_cpis_full_url)\n",
    "            ### Data reading as JSON:\n",
    "            dict_cpis_set = json.loads(obj_cpis_set.text.replace('@OBS_STATUS', '@OBS_VALUE'))\n",
    "            ### Converting each bilateral dataset to dataframe and it's mungling:\n",
    "            if ('Series' in dict_cpis_set['CompactData']['DataSet']):\n",
    "                if isinstance(dict_cpis_set['CompactData']['DataSet']['Series'], list):\n",
    "                    list_series = dict_cpis_set['CompactData']['DataSet']['Series']\n",
    "                else:\n",
    "                    list_series = [dict_cpis_set['CompactData']['DataSet']['Series']]\n",
    "                for dict_cpis_pair in list_series:\n",
    "                    if isinstance(dict_cpis_pair['Obs'], list):\n",
    "                        dict_bilateral = dict_cpis_pair['Obs']\n",
    "                    else:\n",
    "                        dict_bilateral = [dict_cpis_pair['Obs']]\n",
    "                    df_cpis_bilateral = pd.DataFrame(dict_bilateral)\n",
    "                    df_cpis_bilateral = df_cpis_bilateral[['@TIME_PERIOD', '@OBS_VALUE']]\n",
    "                    df_cpis_bilateral.columns = ['Date', 'Value']\n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Indicator = dict_cpis_pair['@INDICATOR'])\n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Reporter_Sector = dict_cpis_pair['@REF_SECTOR'])\n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Partner_Sector = dict_cpis_pair['@COUNTERPART_SECTOR'])                    \n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Reporter_ID = dict_cpis_pair['@REF_AREA'])\n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Partner_ID = dict_cpis_pair['@COUNTERPART_AREA'])\n",
    "                    list_cpis_bilateral.append(df_cpis_bilateral)  \n",
    "            else:\n",
    "                print('No data in response of the next request:\\n', str_cpis_full_url)\n",
    "            time.sleep(int_seconds_to_sleep)                    \n",
    "#        break\n",
    "    print(iter_investor, ': loading completed')\n",
    "#    break\n",
    "### Bilateral datasets aggregating:\n",
    "df_cpis_raw = pd.concat(list_cpis_bilateral, axis = 0, ignore_index = True)\n",
    "df_cpis_raw['Date'] = pd.to_datetime(df_cpis_raw['Date']) + pd.offsets.BYearEnd()\n",
    "df_cpis_raw.loc[df_cpis_raw['Value'] == 'C', 'Value'] = np.NaN\n",
    "df_cpis_raw.loc[df_cpis_raw['Value'] == '-', 'Value'] = np.NaN\n",
    "df_cpis_raw = df_cpis_raw[df_cpis_raw['Reporter_ID'] != df_cpis_raw['Partner_ID']]\n",
    "df_cpis_raw = df_cpis_raw[df_cpis_raw['Partner_ID'].isin(df_country_codes['ISO SHORT'].values)]\n",
    "df_cpis_raw.rename({'Reporter_ID': 'Reporter', 'Partner_ID': 'Partner'}, axis = 1, inplace = True)\n",
    "df_cpis_raw = df_cpis_raw.astype({'Indicator': 'str', 'Reporter_Sector': 'str', 'Partner_Sector': 'str', 'Reporter': 'str', 'Partner': 'str', \n",
    "                                  'Value': 'float32'})    \n",
    "### Data saving:\n",
    "ser_cpis_asset = df_cpis_raw.set_index(['Date', 'Indicator', 'Reporter_Sector', 'Partner_Sector', 'Reporter', 'Partner'])['Value'].sort_index().astype('float32')\n",
    "del df_cpis_raw\n",
    "gc.collect()\n",
    "ser_cpis_asset.to_hdf(path_or_buf = str_path_imf_cpis_detailed_raw, key = str_key_imf_cpis_assets, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CPIS : REPORTED PORTFOLIO INVESTMENT LIABILITIES DATASET RETRIEVING\n",
    "\n",
    "gc.collect()\n",
    "### List of bilateral dataframes for future concatenation:\n",
    "list_cpis_bilateral = [] \n",
    "### Beggining of request URL:\n",
    "str_cpis_const_url = str_imf_base_url + str_imf_dataset_add + str_imf_cpis_id + '/' \n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "### Looping over reporter:\n",
    "for iter_investor in list_ison_countries:\n",
    "#for iter_investor in ['AU', 'US']:   \n",
    "    ### Looping over indicator:\n",
    "    for iter_indicator in dict_indicator:        \n",
    "        if (iter_indicator[2] == 'L'):\n",
    "            str_reporter_sector = '+'.join(list_sector_filtered)        \n",
    "            str_partner_sector = '+'.join(list_sector_filtered)\n",
    "            str_cpis_full_url = str_cpis_const_url + '.'.join([str_cpis_freq, '', iter_indicator, str_reporter_sector, str_partner_sector, iter_investor])\n",
    "            obj_cpis_set = request_session.get(str_cpis_full_url)\n",
    "            ### Data reading as JSON:\n",
    "            dict_cpis_set = json.loads(obj_cpis_set.text.replace('@OBS_STATUS', '@OBS_VALUE'))\n",
    "            ### Converting each bilateral dataset to dataframe and it's mungling:\n",
    "            if ('Series' in dict_cpis_set['CompactData']['DataSet']):\n",
    "                if isinstance(dict_cpis_set['CompactData']['DataSet']['Series'], list):\n",
    "                    list_series = dict_cpis_set['CompactData']['DataSet']['Series']\n",
    "                else:\n",
    "                    list_series = [dict_cpis_set['CompactData']['DataSet']['Series']]\n",
    "                for dict_cpis_pair in list_series:\n",
    "                    if isinstance(dict_cpis_pair['Obs'], list):\n",
    "                        dict_bilateral = dict_cpis_pair['Obs']\n",
    "                    else:\n",
    "                        dict_bilateral = [dict_cpis_pair['Obs']]\n",
    "                    df_cpis_bilateral = pd.DataFrame(dict_bilateral)\n",
    "                    df_cpis_bilateral = df_cpis_bilateral[['@TIME_PERIOD', '@OBS_VALUE']]\n",
    "                    df_cpis_bilateral.columns = ['Date', 'Value']\n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Indicator = dict_cpis_pair['@INDICATOR'])\n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Reporter_S = dict_cpis_pair['@REF_SECTOR'])\n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Partner_S = dict_cpis_pair['@COUNTERPART_SECTOR'])                    \n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Reporter_ID = dict_cpis_pair['@REF_AREA'])\n",
    "                    df_cpis_bilateral = df_cpis_bilateral.assign(Partner_ID = dict_cpis_pair['@COUNTERPART_AREA'])\n",
    "                    list_cpis_bilateral.append(df_cpis_bilateral)  \n",
    "            else:\n",
    "                print('No data in response of the next request:\\n', str_cpis_full_url)\n",
    "            time.sleep(int_seconds_to_sleep)                    \n",
    "#        break\n",
    "    print(iter_investor, ': loading completed')\n",
    "#    break\n",
    "### Bilateral datasets aggregating:\n",
    "df_cpis_raw = pd.concat(list_cpis_bilateral, axis = 0, ignore_index = True)\n",
    "df_cpis_raw['Date'] = pd.to_datetime(df_cpis_raw['Date']) + pd.offsets.BYearEnd()\n",
    "df_cpis_raw.loc[df_cpis_raw['Value'] == 'C', 'Value'] = np.NaN\n",
    "df_cpis_raw.loc[df_cpis_raw['Value'] == '-', 'Value'] = np.NaN\n",
    "df_cpis_raw = df_cpis_raw[df_cpis_raw['Reporter_ID'] != df_cpis_raw['Partner_ID']]\n",
    "df_cpis_raw = df_cpis_raw[df_cpis_raw['Reporter_ID'].isin(df_country_codes['ISO SHORT'].values)]\n",
    "df_cpis_raw.rename({'Reporter_ID': 'Partner', 'Partner_ID': 'Reporter', 'Reporter_S': 'Partner_Sector', 'Partner_S': 'Reporter_Sector'}, axis = 1, inplace = True)\n",
    "df_cpis_raw = df_cpis_raw.astype({'Indicator': 'str', 'Reporter_Sector': 'str', 'Partner_Sector': 'str', 'Reporter': 'str', 'Partner': 'str', \n",
    "                                  'Value': 'float32'})    \n",
    "### Data saving:\n",
    "ser_cpis_liability_inv = df_cpis_raw.set_index(['Date', 'Indicator', 'Reporter_Sector', 'Partner_Sector', 'Reporter', 'Partner'])['Value'].sort_index()\\\n",
    "                                    .astype('float32')\n",
    "del df_cpis_raw\n",
    "gc.collect()\n",
    "ser_cpis_liability_inv.to_hdf(path_or_buf = str_path_imf_cpis_detailed_raw, key = str_key_imf_cpis_liabilities, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### IMF CPIS: RAW ASSET DATA FILTERING\n",
    "\n",
    "### Мы хотим отфильтровать сектора откуда инвестирует и куда инвестируют, а также оставить только инвесторов-членов ISON\n",
    "### Со стороны партнеров мы хотим отфильтровать аггрегирующие значения (оставить в качестве партнеров только страны)\n",
    "gc.collect()\n",
    "ser_cpis_asset = pd.read_hdf(path_or_buf = str_path_imf_cpis_detailed_raw, key = str_key_imf_cpis_assets)\n",
    "\n",
    "### Интересующие нас сектора:\n",
    "list_valid_reporter_sectors = ['T', 'CB', 'GG']\n",
    "list_valid_partner_sectors = ['T', 'CB', 'GG']\n",
    "### Интересующие нас партнеры:\n",
    "list_valid_partners = df_country_codes['ISO SHORT'].values\n",
    "### Фильтруем сектора на обеих сторонах и страны на обеих сторонах\n",
    "ser_asset_to_filter = ser_cpis_asset.loc[:, 'I_A_E_T_T_BP6_USD', list_valid_reporter_sectors, list_valid_partner_sectors, :, list_valid_partners]\\\n",
    "                                    .reorder_levels([0, 1, 4, 5, 2, 3]).sort_index().astype('float32')\n",
    "### Считаем значение для билатеральных пар и разных секторов инвесторов: \n",
    "### от значений для партнерского сектора Total values отнимаем значения для партнерских секторов Central Bank и General Government\n",
    "df_partner_sector_unstacked = ser_asset_to_filter.unstack('Partner_Sector').fillna(0.0)\n",
    "ser_partner_sector_filtered = df_partner_sector_unstacked['T'] - (df_partner_sector_unstacked['CB'] + df_partner_sector_unstacked['GG'])\n",
    "### Считаем значение для билатеральных пар: \n",
    "### от значений для инвесторского сектора Total values отнимаем значения для инвесторских секторов Central Bank и General Government\n",
    "df_reporter_sector_unstacked = ser_partner_sector_filtered.unstack('Reporter_Sector').fillna(0.0)\n",
    "ser_reporter_sector_filtered = df_reporter_sector_unstacked['T'] - (df_reporter_sector_unstacked['CB'] + df_reporter_sector_unstacked['GG'])\n",
    "ser_asset_filtered = ser_reporter_sector_filtered.droplevel('Indicator').sort_index()\n",
    "\n",
    "del ser_cpis_asset\n",
    "del df_partner_sector_unstacked\n",
    "del df_reporter_sector_unstacked\n",
    "del ser_reporter_sector_filtered\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: IMF CPIS: FILTERED ASSET DATASET SAVING\n",
    "\n",
    "ser_asset_filtered.replace({0.0: np.NaN}).to_hdf(path_or_buf = str_path_imf_cpis_filtered, key = str_key_imf_cpis_filtered_asset, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### IMF CPIS: RAW LIABILITY DATA FILTERING\n",
    "\n",
    "### Информация о Liabilities содержит только данные о Total сектора с обех сторон. Поэтом задача сводится к тому, чтобы  оставить только инвесторов-членов ISON\n",
    "### Со стороны партнеров мы по-прежнему хотим отфильтровать аггрегирующие значения (оставить в качестве партнеров только страны)\n",
    "\n",
    "gc.collect()\n",
    "ser_cpis_liability_inv = pd.read_hdf(path_or_buf = str_path_imf_cpis_detailed_raw, key = str_key_imf_cpis_liabilities)\n",
    "\n",
    "list_valid_reporter_sectors = ['T', 'CB', 'GG']\n",
    "list_valid_partner_sectors = ['T', 'CB', 'GG']\n",
    "list_valid_partners = df_country_codes['ISO SHORT'].values\n",
    "ser_liability_to_filter = ser_cpis_liability_inv.loc[:, 'I_L_E_T_T_BP6_USD', list_valid_reporter_sectors, list_valid_partner_sectors, :, list_valid_partners]\\\n",
    "                                                .reorder_levels([0, 1, 4, 5, 2, 3]).sort_index().astype('float32')\n",
    "ser_liability_filtered = ser_liability_to_filter.fillna(0.0).droplevel(['Reporter_Sector', 'Partner_Sector']).droplevel('Indicator').sort_index()\n",
    "del ser_liability_to_filter\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: IMF CPIS: FILTERED LIABILITY DATASET SAVING\n",
    "\n",
    "ser_liability_filtered.replace({0.0: np.NaN}).to_hdf(path_or_buf = str_path_imf_cpis_filtered, key = str_key_imf_cpis_filtered_liability, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CPIS: FILTERED DATA AGGREGATION: DATASETS LOADING\n",
    "\n",
    "### Объединяем Assets и Inverted Liabilities в один DataFrame (фактически - это приведение двух векторов к общему индексу)\n",
    "gc.collect()\n",
    "\n",
    "ser_cpis_asset_filtered = pd.read_hdf(path_or_buf = str_path_imf_cpis_filtered, key = str_key_imf_cpis_filtered_asset)\n",
    "ser_cpis_asset_filtered.name = 'Asset'\n",
    "ser_cpis_liability_filtered = pd.read_hdf(path_or_buf = str_path_imf_cpis_filtered, key = str_key_imf_cpis_filtered_liability)\n",
    "ser_cpis_liability_filtered.name = 'Liability_Inverted'\n",
    "df_cpis_total = pd.concat([ser_cpis_asset_filtered, ser_cpis_liability_filtered], axis = 1, names = 'Data Source').astype('float32').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CPIS: UNCONDITIONAL COMBINATION\n",
    "\n",
    "### Combining Export & Import data:\n",
    "### Дополняем Assets значениями Inverted Liabilities:\n",
    "df_cpis_total.loc[df_cpis_total['Asset'] == 0.0, 'Asset'] = np.NaN \n",
    "ser_cpis_augmented = df_cpis_total['Asset'].combine_first(df_cpis_total['Liability_Inverted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: IMF CPIS: RESULTS SAVING\n",
    "\n",
    "ser_cpis_augmented.to_hdf(path_or_buf = str_path_imf_cpis_augmented, key = str_key_imf_cpis_augmented, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: BILATERAL DIRECT INVESTMENT POSITIONS (MILLIONS OF USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: IMF CDIS: DATA SAVING PARAMETERS\n",
    "\n",
    "### Filtered dataset:\n",
    "str_path_imf_cdis_dataset = 'Data_Files/Source_Files/cdis_assets.h5'\n",
    "str_key_do_debt_imf_cdis_dataset = 'cdis_debt_outward_assets'\n",
    "str_key_di_debt_imf_cdis_dataset = 'cdis_debt_inward_assets'\n",
    "### Resulting dataset:\n",
    "str_path_imf_cdis_augmented = 'Data_Files/Source_Files/cdis_augmented_unconditional.h5'\n",
    "str_key_imf_cdis_augmented = 'cdis_asset_augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: LOADING PARAMETERS PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "str_imf_base_url = 'http://dataservices.imf.org/REST/SDMX_JSON.svc/'\n",
    "str_imf_dataset_add = 'CompactData/'\n",
    "str_imf_cdis_id = 'CDIS'\n",
    "str_cdis_freq = 'A'\n",
    "int_seconds_to_sleep = 3\n",
    "int_imf_country_limit = 30\n",
    "dict_to_download = {'IOWDA_BP6_USD': 'Outward Debt Instruments Assets Positions (Gross), US Dollars',\n",
    "                    'IOWDL_BP6_USD': 'Outward Debt Instruments Liabilities Positions (Gross), US Dollars',\n",
    "                    'IOWD_BP6_USD': 'Outward Debt Instruments Positions (Net), US Dollars',\n",
    "                    'IIWDA_BP6_USD': 'Inward Debt Instruments Assets Positions (Gross), US Dollars',\n",
    "                    'IIWDL_BP6_USD': 'Inward Debt Instruments Liabilities Positions (Gross), US Dollars',\n",
    "                    'IIWD_BP6_USD': 'Inward Debt Instruments Positions (Net), US Dollars'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "df_cdis_raw = pd.concat(list_cdis_bilateral, axis = 0, ignore_index = True, sort = False)\n",
    "df_cdis_raw['Date'] = pd.to_datetime(df_cdis_raw['Date']) + pd.offsets.BYearEnd()\n",
    "df_cdis_raw.loc[df_cdis_raw['Value'] == 'C', 'Value'] = np.NaN\n",
    "df_cdis_raw.loc[df_cdis_raw['Value'] == '-', 'Value'] = np.NaN\n",
    "df_cdis_raw = df_cdis_raw[df_cdis_raw['Reporter_ID'] != df_cdis_raw['Partner_ID']]\n",
    "df_cdis_raw = df_cdis_raw[df_cdis_raw['Partner_ID'].isin(df_country_codes['ISO SHORT'].values)]\n",
    "df_cdis_raw.rename({'Reporter_ID': 'Reporter', 'Partner_ID': 'Partner'}, axis = 1, inplace = True)\n",
    "df_cdis_raw = df_cdis_raw.astype({'Indicator': 'str', 'Reporter': 'str', 'Partner': 'str', \n",
    "                                  'Value': 'float32'})\n",
    "#df_cdis_raw['Value'].clip(lower = 0.0, inplace = True)\n",
    "df_cdis_raw['Indicator'].replace(dict_to_download, inplace = True)\n",
    "df_cdis_raw['Direction'] = df_cdis_raw['Indicator'].str.partition(' ')[0]\n",
    "df_cdis_raw['Type'] = df_cdis_raw['Indicator'].str.partition(' ')[2].str.partition(' ')[0]\n",
    "df_cdis_raw['Account'] = np.NaN\n",
    "df_cdis_raw.loc[df_cdis_raw['Indicator'].str.contains('(Net)'), 'Account'] = 'Net'\n",
    "df_cdis_raw.loc[df_cdis_raw['Indicator'].str.contains('Asset'), 'Account'] = 'Asset'\n",
    "df_cdis_raw.loc[df_cdis_raw['Indicator'].str.contains('Liabilit'), 'Account'] = 'Liability'\n",
    "df_cdis_raw[df_cdis_raw['Reporter'] != df_cdis_raw['Partner']]\n",
    "#ser_cdis_raw = df_cdis_raw.set_index(['Type', 'Direction', 'Account', 'Date', 'Reporter', 'Partner'])['Value'].sort_index()\n",
    "#ser_cdis_raw.name = 'CDIS Positions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: REPORTED DIRECT INVESTMENT NET VOLUMES RETRIEVING\n",
    "\n",
    "gc.collect()\n",
    "### List of bilateral dataframes for future concatenation:\n",
    "list_cdis_bilateral = [] \n",
    "### Beggining of request URL:\n",
    "str_cdis_const_url = str_imf_base_url + str_imf_dataset_add + str_imf_cdis_id + '/' \n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "### Looping over reporter:\n",
    "for iter_investor in list_ison_countries:\n",
    "#for iter_investor in ['AU']:\n",
    "    ### Looping over indicator:\n",
    "    for iter_indicator in dict_to_download:        \n",
    "#    for iter_indicator in ['IOWDA_BP6_USD']:\n",
    "        if (iter_indicator[1] == 'O'):\n",
    "            str_cdis_full_url = str_cdis_const_url + '.'.join([str_cdis_freq, iter_investor, iter_indicator, ''])\n",
    "        else:\n",
    "            str_cdis_full_url = str_cdis_const_url + '.'.join([str_cdis_freq, '', iter_indicator, iter_investor])\n",
    "        print(str_cdis_full_url)\n",
    "        obj_cdis_set = request_session.get(str_cdis_full_url)\n",
    "        ### Data reading as JSON:\n",
    "        dict_cdis_set = json.loads(obj_cdis_set.text.replace('@OBS_STATUS', '@OBS_VALUE'))\n",
    "        ### Converting each bilateral dataset to dataframe and it's mungling:\n",
    "        if ('Series' in dict_cdis_set['CompactData']['DataSet']):\n",
    "            if isinstance(dict_cdis_set['CompactData']['DataSet']['Series'], list):\n",
    "                list_series = dict_cdis_set['CompactData']['DataSet']['Series']\n",
    "            else:\n",
    "                list_series = [dict_cdis_set['CompactData']['DataSet']['Series']]\n",
    "            for dict_cdis_pair in list_series:\n",
    "                if isinstance(dict_cdis_pair['Obs'], list):\n",
    "                    dict_bilateral = dict_cdis_pair['Obs']\n",
    "                else:\n",
    "                    dict_bilateral = [dict_cdis_pair['Obs']]\n",
    "                df_cdis_bilateral = pd.DataFrame(dict_bilateral)\n",
    "                if '@OBS_VALUE' in df_cdis_bilateral.columns:\n",
    "                    df_cdis_bilateral = df_cdis_bilateral[['@TIME_PERIOD', '@OBS_VALUE']]\n",
    "                    df_cdis_bilateral.columns = ['Date', 'Value']\n",
    "                    df_cdis_bilateral = df_cdis_bilateral.assign(Indicator = dict_cdis_pair['@INDICATOR'])\n",
    "                    df_cdis_bilateral = df_cdis_bilateral.assign(Reporter_ID = dict_cdis_pair['@REF_AREA'])\n",
    "                    df_cdis_bilateral = df_cdis_bilateral.assign(Partner_ID = dict_cdis_pair['@COUNTERPART_AREA'])\n",
    "                    list_cdis_bilateral.append(df_cdis_bilateral)  \n",
    "        else:\n",
    "            print('No data in response of the next request:\\n', str_cdis_full_url)\n",
    "        time.sleep(int_seconds_to_sleep)                    \n",
    "#        break            \n",
    "    print(iter_investor, ': loading completed')\n",
    "#    break\n",
    "### Bilateral datasets aggregating:\n",
    "df_cdis_raw = pd.concat(list_cdis_bilateral, axis = 0, ignore_index = True, sort = False)\n",
    "df_cdis_raw['Date'] = pd.to_datetime(df_cdis_raw['Date']) + pd.offsets.BYearEnd()\n",
    "df_cdis_raw.loc[df_cdis_raw['Value'] == 'C', 'Value'] = np.NaN\n",
    "df_cdis_raw.loc[df_cdis_raw['Value'] == '-', 'Value'] = np.NaN\n",
    "df_cdis_raw = df_cdis_raw[df_cdis_raw['Reporter_ID'] != df_cdis_raw['Partner_ID']]\n",
    "df_cdis_raw = df_cdis_raw[df_cdis_raw['Partner_ID'].isin(df_country_codes['ISO SHORT'].values)]\n",
    "df_cdis_raw.rename({'Reporter_ID': 'Reporter', 'Partner_ID': 'Partner'}, axis = 1, inplace = True)\n",
    "df_cdis_raw = df_cdis_raw.astype({'Indicator': 'str', 'Reporter': 'str', 'Partner': 'str', \n",
    "                                  'Value': 'float32'})\n",
    "#df_cdis_raw['Value'].clip(lower = 0.0, inplace = True)\n",
    "df_cdis_raw['Indicator'].replace(dict_to_download, inplace = True)\n",
    "df_cdis_raw['Direction'] = df_cdis_raw['Indicator'].str.partition(' ')[0]\n",
    "df_cdis_raw['Type'] = df_cdis_raw['Indicator'].str.partition(' ')[2].str.partition(' ')[0]\n",
    "df_cdis_raw['Account'] = np.NaN\n",
    "df_cdis_raw.loc[df_cdis_raw['Indicator'].str.contains('(Net)'), 'Account'] = 'Net'\n",
    "df_cdis_raw.loc[df_cdis_raw['Indicator'].str.contains('Asset'), 'Account'] = 'Asset'\n",
    "df_cdis_raw.loc[df_cdis_raw['Indicator'].str.contains('Liabilit'), 'Account'] = 'Liability'\n",
    "df_cdis_raw[df_cdis_raw['Reporter'] != df_cdis_raw['Partner']]\n",
    "ser_cdis_raw = df_cdis_raw.set_index(['Type', 'Direction', 'Account', 'Date', 'Reporter', 'Partner'])['Value'].sort_index()\n",
    "ser_cdis_raw.name = 'CDIS Positions'\n",
    "\n",
    "del df_cdis_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### IMF CDIS: REPLACING EMPTY POSITION VALUES WITH NET\n",
    "\n",
    "### Специфика данных IMF CDIS такова, что нам не гарантировано, что для каждой пары в каждую даты для каждого потока будут все три значения: Net, Asset, Liability\n",
    "### Поэтому мы проверяем различные сочетания отсутствующих и имеющихся данных, чтобы заполнять пустоты.\n",
    "### Например, если для Otward потока есть негативное Net Value, а Asset и Liability отсутствуют, мы считаем, что Liability Value = -Net Value\n",
    "### Например, если для Otward потока есть позитивное Net Value, а Asset и Liability отсутствуют, мы считаем, что Asset Value = Net Value\n",
    "### Если мы знаем два значения из трех, то считаем третье, исходя из имеющихся.\n",
    "\n",
    "df_cdis_pos_acc = ser_cdis_raw.unstack('Account')\n",
    "\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Outward') & (df_cdis_pos_acc['Net'] < 0.0) & \\\n",
    "            (df_cdis_pos_acc['Asset'].isna() & df_cdis_pos_acc['Liability'].isna())\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Liability'] = -df_cdis_pos_acc.loc[idx_fill, 'Net'].values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Outward') & (df_cdis_pos_acc['Net'] >= 0.0) & \\\n",
    "            (df_cdis_pos_acc['Asset'].isna() & df_cdis_pos_acc['Liability'].isna())\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Asset'] = df_cdis_pos_acc.loc[idx_fill, 'Net'].values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Inward') & (df_cdis_pos_acc['Net'] < 0.0) & \\\n",
    "            (df_cdis_pos_acc['Asset'].isna() & df_cdis_pos_acc['Liability'].isna())\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Asset'] = -df_cdis_pos_acc.loc[idx_fill, 'Net'].values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Inward') & (df_cdis_pos_acc['Net'] >= 0.0) & \\\n",
    "            (df_cdis_pos_acc['Asset'].isna() & df_cdis_pos_acc['Liability'].isna())\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Liability'] = df_cdis_pos_acc.loc[idx_fill, 'Net'].values\n",
    "\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Outward') & df_cdis_pos_acc['Net'].isna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Net'] = (df_cdis_pos_acc.loc[idx_fill, 'Asset'] - df_cdis_pos_acc.loc[idx_fill, 'Liability']).values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Inward') & df_cdis_pos_acc['Net'].isna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Net'] = (df_cdis_pos_acc.loc[idx_fill, 'Liability'] - df_cdis_pos_acc.loc[idx_fill, 'Asset']).values\n",
    "\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Outward') & df_cdis_pos_acc['Net'].isna() & df_cdis_pos_acc['Asset'].notna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Net'] = df_cdis_pos_acc.loc[idx_fill, 'Asset'].values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Outward') & df_cdis_pos_acc['Net'].isna() & df_cdis_pos_acc['Liability'].notna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Net'] = -df_cdis_pos_acc.loc[idx_fill, 'Liability'].values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Inward') & df_cdis_pos_acc['Net'].isna() & df_cdis_pos_acc['Asset'].notna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Net'] = -df_cdis_pos_acc.loc[idx_fill, 'Asset'].values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Inward') & df_cdis_pos_acc['Net'].isna() & df_cdis_pos_acc['Liability'].notna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Net'] = df_cdis_pos_acc.loc[idx_fill, 'Liability'].values\n",
    "\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Outward') & df_cdis_pos_acc['Net'].notna() & \\\n",
    "            df_cdis_pos_acc['Asset'].notna() & df_cdis_pos_acc['Liability'].isna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Liability'] = (df_cdis_pos_acc.loc[idx_fill, 'Asset'] - df_cdis_pos_acc.loc[idx_fill, 'Net']).values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Outward') & df_cdis_pos_acc['Net'].notna() & \\\n",
    "            df_cdis_pos_acc['Asset'].isna() & df_cdis_pos_acc['Liability'].notna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Asset'] = (df_cdis_pos_acc.loc[idx_fill, 'Net'] + df_cdis_pos_acc.loc[idx_fill, 'Liability']).values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Inward') & df_cdis_pos_acc['Net'].notna() & \\\n",
    "            df_cdis_pos_acc['Asset'].notna() & df_cdis_pos_acc['Liability'].isna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Liability'] = (df_cdis_pos_acc.loc[idx_fill, 'Net'] + df_cdis_pos_acc.loc[idx_fill, 'Asset']).values\n",
    "idx_fill = (df_cdis_pos_acc.index.get_level_values('Direction') == 'Inward') & df_cdis_pos_acc['Net'].notna() & \\\n",
    "            df_cdis_pos_acc['Asset'].isna() & df_cdis_pos_acc['Liability'].notna()\n",
    "df_cdis_pos_acc.loc[idx_fill, 'Asset'] = (df_cdis_pos_acc.loc[idx_fill, 'Liability'] - df_cdis_pos_acc.loc[idx_fill, 'Net']).values\n",
    "\n",
    "ser_cdis_full = df_cdis_pos_acc.stack('Account', dropna = False).reorder_levels([0, 1, 5, 2, 3, 4]).sort_index()\n",
    "\n",
    "del df_cdis_pos_acc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: FDI POSITION DATASETS SAVING\n",
    "\n",
    "### В качестве основного dataset используем Outward Debt Assets\n",
    "### Для него фильтруем списки Инвеcторов (ISON страны) и Партнеров (убираем аггрегаторы)\n",
    "ser_debt_do = ser_cdis_full.loc['Debt', 'Outward', 'Asset', :, list_ison_countries, df_country_codes['ISO SHORT'].values]\\\n",
    ".droplevel(['Type', 'Direction', 'Account']).sort_index()\n",
    "ser_debt_do.to_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_do_debt_imf_cdis_dataset, mode = 'w', format = 'fixed')\n",
    "### В качестве дополнябщего dataset используем Inward Debt Liabilities\n",
    "### Для него фильтруем списки Репортеров (убираем аггрегаторы) и Инвесторов (только ISON страны)\n",
    "### не забываем поменять местами Reporter и Partner, чтобы инвертировать Import\n",
    "ser_debt_di = ser_cdis_full.loc['Debt', 'Inward', 'Liability', :, df_country_codes['ISO SHORT'].values, list_ison_countries]\\\n",
    "                           .droplevel(['Type', 'Direction', 'Account'])\n",
    "ser_debt_di.index.names = ['Date', 'Partner', 'Reporter']\n",
    "ser_debt_di = ser_debt_di.reorder_levels(['Date', 'Reporter', 'Partner']).sort_index()\n",
    "ser_debt_di.to_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_di_debt_imf_cdis_dataset, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: DEBT DATA AGGREGATION: DATASETS LOADING\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "### Убираем отрицательные значения и формируем DataFrame из двух колонок:\n",
    "ser_cdis_asset = pd.read_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_do_debt_imf_cdis_dataset)\n",
    "ser_cdis_asset[ser_cdis_asset < 0.0] = 0.0\n",
    "ser_cdis_asset.name = 'Asset'\n",
    "ser_cdis_liability_inv = pd.read_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_di_debt_imf_cdis_dataset)\n",
    "ser_cdis_liability_inv[ser_cdis_liability_inv < 0.0] = 0.0\n",
    "ser_cdis_liability_inv.name = 'Liability_Inverted'\n",
    "df_cdis_debt = pd.concat([ser_cdis_asset, ser_cdis_liability_inv], axis = 1, names = 'Data Source').astype('float32').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: UNCONDITIONAL COMBINATION\n",
    "\n",
    "### Combining Export & Import data:\n",
    "### Дополняем данные Outward Debt Assets данными Inward Debt Liabilities:\n",
    "df_cdis_debt.loc[df_cdis_debt['Asset'] == 0.0, 'Asset'] = np.NaN \n",
    "ser_cdis_augmented = df_cdis_debt['Asset'].combine_first(df_cdis_debt['Liability_Inverted']).replace({0.0: np.NaN})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: IMF CDIS: RESULTS SAVING\n",
    "\n",
    "ser_cdis_augmented.to_hdf(path_or_buf = str_path_imf_cdis_augmented, key = str_key_imf_cdis_augmented, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OECD FDI: FOREIGN DIRECT INVESTMENT (MILLIONS OF USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: OECD FDI: DATA SAVING PARAMETERS\n",
    "\n",
    "### Filtered dataset:\n",
    "str_path_oecd_fdi_dataset = 'Data_Files/Source_Files/oecd_assets.h5'\n",
    "str_key_do_total_oecd_fdi_dataset = 'fdi_total_outward_assets'\n",
    "str_key_di_total_oecd_fdi_dataset = 'fdi_total_inward_assets'\n",
    "### Resulting dataset:\n",
    "str_path_oecd_fdi_augmented = 'Data_Files/Source_Files/oecd_augmented_unconditional.h5'\n",
    "str_key_oecd_fdi_augmented = 'oecd_asset_augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OECD FDI: LOADING PARAMETERS PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "date_start = pd.Timestamp('1989-12-29')\n",
    "date_end = pd.Timestamp('2022-10-31')\n",
    "\n",
    "str_oecd_base_url = 'https://stats.oecd.org/sdmx-json/data/'\n",
    "str_oecd_structure_url = 'https://stats.oecd.org/restsdmx/sdmx.ashx/GetDataStructure/'\n",
    "str_fdi_pos_dataset_add = 'FDI_POS_CTRY'\n",
    "### Currency:\n",
    "str_measure = 'USD'\n",
    "### Direction:\n",
    "str_direction = '+'.join(['DI', 'DO']) # 'DO' # \n",
    "### Investment type:\n",
    "str_fdi_type = '+'.join(['LE_FA_F']) # 'LE_FA_F5' # \n",
    "### Residence defining:\n",
    "str_residence = 'ALL'\n",
    "### Accounting way:\n",
    "str_accounting =  '+'.join(['A', 'NET', 'L']) # '+'.join(['A', 'L']) # 'NET' # \n",
    "### Level counterpart(???):\n",
    "str_counterpart = 'IMC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OECD FDI: FDI POSITION REQUEST CONSTRUCTING\n",
    "\n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "\n",
    "str_fdi_pos_request_params = '.'.join(['', str_measure, str_direction, str_fdi_type, str_residence, str_accounting, str_counterpart, ''])\n",
    "str_fdi_pos_request = str_oecd_base_url + str_fdi_pos_dataset_add + '/' + str_fdi_pos_request_params + '/all?startTime=' + str(date_start.year) + \\\n",
    "                      '&endTime=' + str(date_end.year) + '&detail=DataOnly'\n",
    "obj_fdi_pos_dataset = request_session.get(str_fdi_pos_request).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OECD FDI: FDI POSITION INDEX DATA COLLECTING:\n",
    "\n",
    "### Dates:\n",
    "list_idx_dates = []\n",
    "for tup_date in obj_fdi_pos_dataset['structure']['dimensions']['observation'][0]['values']:\n",
    "    list_idx_dates.append(pd.to_datetime(tup_date['id']) + pd.offsets.BYearEnd())\n",
    "### Parameters:    \n",
    "list_idx_library = []\n",
    "for iter_position in obj_fdi_pos_dataset['structure']['dimensions']['series']:\n",
    "    list_param_values = []\n",
    "    for tup_parameter in iter_position['values']:\n",
    "        list_param_values.append(tup_parameter['id'])            \n",
    "    list_idx_library.append(list_param_values)\n",
    "### Result:\n",
    "list_idx_library.append(list_idx_dates)\n",
    "### Converting to dictionary for future replacing:\n",
    "list_idx_dict = []\n",
    "for iter_list in list_idx_library:\n",
    "    list_idx_dict.append(dict(zip(map(str, range(len(iter_list))), iter_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OECD FDI: FDI POSITION DATASET RESAMPLING\n",
    "\n",
    "dict_datasets_res = {}\n",
    "dict_datasets_source = obj_fdi_pos_dataset['dataSets'][0]['series']\n",
    "### Parameters and date indexes integration:\n",
    "for iter_dataset in dict_datasets_source:\n",
    "    dict_observations = dict_datasets_source[iter_dataset]['observations']\n",
    "    for iter_observation in dict_observations:\n",
    "        str_iter_idx = iter_dataset + ':' + iter_observation\n",
    "        flo_iter_value = dict_observations[iter_observation][0]\n",
    "        dict_datasets_res[str_iter_idx] = flo_iter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OECD FDI: FDI POSITION DATASET REINDEXATION\n",
    "\n",
    "gc.collect()\n",
    "df_fdi_pos_data = pd.Series(dict_datasets_res)\n",
    "df_fdi_pos_data.index = pd.MultiIndex.from_arrays(zip(*df_fdi_pos_data.index.str.split(':')))\n",
    "int_levels_number = df_fdi_pos_data.index.nlevels\n",
    "df_fdi_pos_data = df_fdi_pos_data.reset_index()\n",
    "### Replacing numbers with parameter values:\n",
    "for iter_level in range(int_levels_number):\n",
    "    df_fdi_pos_data['level_' + str(iter_level)].replace(list_idx_dict[iter_level], inplace = True)\n",
    "    ### Replacing long ISO names with short ISO names:\n",
    "    if (iter_level == 0):\n",
    "        df_fdi_pos_data['level_' + str(iter_level)].replace(dict(zip(df_country_codes['ISO LONG'].values, df_country_codes['ISO SHORT'].values)), inplace = True)\n",
    "    elif (iter_level == 7):\n",
    "        df_fdi_pos_data['level_' + str(iter_level)].replace(dict(zip(df_country_codes['ISO LONG'].values, df_country_codes['ISO SHORT'].values)), inplace = True)\n",
    "    ### Directions renaming:\n",
    "    elif (iter_level == 2):\n",
    "        df_fdi_pos_data['level_' + str(iter_level)].replace({'DI': 'Inward', 'DO': 'Outward'}, inplace = True)\n",
    "    ### Investment types renaming:\n",
    "    elif (iter_level == 3):\n",
    "        df_fdi_pos_data['level_' + str(iter_level)].replace({'LE_FA_F': 'Total', 'LE_FA_F5': 'Equity'}, inplace = True)         \n",
    "    ### Flow types renaming:\n",
    "    elif (iter_level == 5):\n",
    "        df_fdi_pos_data['level_' + str(iter_level)].replace({'NET': 'Net', 'A': 'Asset', 'L': 'Liability'}, inplace = True)      \n",
    "\n",
    "### Intergated observations dropping:\n",
    "df_fdi_pos_data = df_fdi_pos_data.loc[\n",
    "                                      df_fdi_pos_data['level_0'].isin(df_country_codes['ISO SHORT'].values) & \n",
    "                                      df_fdi_pos_data['level_7'].isin(df_country_codes['ISO SHORT'].values)\n",
    "                                     ]\n",
    "### Indexes defining:\n",
    "ser_fdi_pos_data = df_fdi_pos_data.drop(['level_1', 'level_4', 'level_6'], axis = 1)\\\n",
    "                    .set_index(['level_3', 'level_2', 'level_5', 'level_8', 'level_0', 'level_7']).squeeze()\n",
    "ser_fdi_pos_data.index.names = ['Type', 'Direction', 'Account', 'Date', 'Reporter', 'Partner']\n",
    "ser_fdi_pos_data.sort_index(inplace = True)\n",
    "ser_fdi_pos_data = ser_fdi_pos_data[ser_fdi_pos_data.index.get_level_values('Reporter') != ser_fdi_pos_data.index.get_level_values('Partner')]\n",
    "#ser_fdi_pos_data[ser_fdi_pos_data < 0.0] = 0.0\n",
    "ser_fdi_pos_data.name = 'FDI Positions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OECD FDI: REPLACING EMPTY POSITION VALUES WITH NET\n",
    "\n",
    "### Специфика данных OECD FDI такова, что нам не гарантировано, что для каждой пары в каждую даты для каждого потока будут все три значения: Net, Asset, Liability\n",
    "### Поэтому мы проверяем различные сочетания отсутствующих и имеющихся данных, чтобы заполнять пустоты.\n",
    "### Например, если для Otward потока есть негативное Net Value, а Asset и Liability отсутствуют, мы считаем, что Liability Value = -Net Value\n",
    "### Например, если для Otward потока есть позитивное Net Value, а Asset и Liability отсутствуют, мы считаем, что Asset Value = Net Value\n",
    "### Если мы знаем два значения из трех, то считаем третье, исходя из имеющихся.\n",
    "\n",
    "df_fdi_pos_acc = ser_fdi_pos_data.unstack('Account')\n",
    "\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Outward') & (df_fdi_pos_acc['Net'] < 0.0) & \\\n",
    "            (df_fdi_pos_acc['Asset'].isna() & df_fdi_pos_acc['Liability'].isna())\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Liability'] = -df_fdi_pos_acc.loc[idx_fill, 'Net'].values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Outward') & (df_fdi_pos_acc['Net'] >= 0.0) & \\\n",
    "            (df_fdi_pos_acc['Asset'].isna() & df_fdi_pos_acc['Liability'].isna())\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Asset'] = df_fdi_pos_acc.loc[idx_fill, 'Net'].values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Inward') & (df_fdi_pos_acc['Net'] < 0.0) & \\\n",
    "            (df_fdi_pos_acc['Asset'].isna() & df_fdi_pos_acc['Liability'].isna())\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Asset'] = -df_fdi_pos_acc.loc[idx_fill, 'Net'].values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Inward') & (df_fdi_pos_acc['Net'] >= 0.0) & \\\n",
    "            (df_fdi_pos_acc['Asset'].isna() & df_fdi_pos_acc['Liability'].isna())\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Liability'] = df_fdi_pos_acc.loc[idx_fill, 'Net'].values\n",
    "\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Outward') & df_fdi_pos_acc['Net'].isna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Net'] = (df_fdi_pos_acc.loc[idx_fill, 'Asset'] - df_fdi_pos_acc.loc[idx_fill, 'Liability']).values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Inward') & df_fdi_pos_acc['Net'].isna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Net'] = (df_fdi_pos_acc.loc[idx_fill, 'Liability'] - df_fdi_pos_acc.loc[idx_fill, 'Asset']).values\n",
    "\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Outward') & df_fdi_pos_acc['Net'].isna() & df_fdi_pos_acc['Asset'].notna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Net'] = df_fdi_pos_acc.loc[idx_fill, 'Asset'].values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Outward') & df_fdi_pos_acc['Net'].isna() & df_fdi_pos_acc['Liability'].notna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Net'] = -df_fdi_pos_acc.loc[idx_fill, 'Liability'].values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Inward') & df_fdi_pos_acc['Net'].isna() & df_fdi_pos_acc['Asset'].notna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Net'] = -df_fdi_pos_acc.loc[idx_fill, 'Asset'].values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Inward') & df_fdi_pos_acc['Net'].isna() & df_fdi_pos_acc['Liability'].notna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Net'] = df_fdi_pos_acc.loc[idx_fill, 'Liability'].values\n",
    "\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Outward') & df_fdi_pos_acc['Net'].notna() & \\\n",
    "            df_fdi_pos_acc['Asset'].notna() & df_fdi_pos_acc['Liability'].isna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Liability'] = (df_fdi_pos_acc.loc[idx_fill, 'Asset'] - df_fdi_pos_acc.loc[idx_fill, 'Net']).values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Outward') & df_fdi_pos_acc['Net'].notna() & \\\n",
    "            df_fdi_pos_acc['Asset'].isna() & df_fdi_pos_acc['Liability'].notna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Asset'] = (df_fdi_pos_acc.loc[idx_fill, 'Net'] + df_fdi_pos_acc.loc[idx_fill, 'Liability']).values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Inward') & df_fdi_pos_acc['Net'].notna() & \\\n",
    "            df_fdi_pos_acc['Asset'].notna() & df_fdi_pos_acc['Liability'].isna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Liability'] = (df_fdi_pos_acc.loc[idx_fill, 'Net'] + df_fdi_pos_acc.loc[idx_fill, 'Asset']).values\n",
    "idx_fill = (df_fdi_pos_acc.index.get_level_values('Direction') == 'Inward') & df_fdi_pos_acc['Net'].notna() & \\\n",
    "            df_fdi_pos_acc['Asset'].isna() & df_fdi_pos_acc['Liability'].notna()\n",
    "df_fdi_pos_acc.loc[idx_fill, 'Asset'] = (df_fdi_pos_acc.loc[idx_fill, 'Liability'] - df_fdi_pos_acc.loc[idx_fill, 'Net']).values\n",
    "\n",
    "ser_fdi_pos_data = df_fdi_pos_acc.stack('Account', dropna = False).reorder_levels([0, 1, 5, 2, 3, 4]).sort_index()\n",
    "ser_fdi_pos_data[ser_fdi_pos_data < 0.0] = 0.0\n",
    "\n",
    "del df_fdi_pos_acc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OECD FDI: FDI POSITION DATASETS SAVING\n",
    "\n",
    "### В качестве основного dataset используем Outward Total Assets\n",
    "### Для него фильтруем списки Инвеcторов (ISON страны)\n",
    "ser_total_do = ser_fdi_pos_data.loc['Total', 'Outward', 'Asset', :, list_ison_countries, :].droplevel(['Type', 'Direction', 'Account']).sort_index()\n",
    "ser_total_do.to_hdf(path_or_buf = str_path_oecd_fdi_dataset, key = str_key_do_total_oecd_fdi_dataset, mode = 'w', format = 'fixed')\n",
    "### В качестве дополняющего dataset используем Inward Total Liabilities\n",
    "### Для него фильтруем списки Инвесторов (только ISON страны)\n",
    "### не забываем поменять местами Reporter и Partner, чтобы инвертировать Import\n",
    "ser_total_di = ser_fdi_pos_data.loc['Total', 'Inward', 'Liability', :, :, list_ison_countries].droplevel(['Type', 'Direction', 'Account'])\n",
    "ser_total_di.index.names = ['Date', 'Partner', 'Reporter']\n",
    "ser_total_di = ser_total_di.reorder_levels(['Date', 'Reporter', 'Partner']).sort_index()\n",
    "ser_total_di.to_hdf(path_or_buf = str_path_oecd_fdi_dataset, key = str_key_di_total_oecd_fdi_dataset, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: OECD FDI: TOTAL DATA AGGREGATION: DATASETS LOADING\n",
    "\n",
    "### Объединем два вектора в общий DataFrame:\n",
    "gc.collect()\n",
    "ser_oecd_asset = pd.read_hdf(path_or_buf = str_path_oecd_fdi_dataset, key = str_key_do_total_oecd_fdi_dataset)\n",
    "ser_oecd_asset.name = 'Asset'\n",
    "ser_oecd_liability_inv = pd.read_hdf(path_or_buf = str_path_oecd_fdi_dataset, key = str_key_di_total_oecd_fdi_dataset)\n",
    "ser_oecd_liability_inv.name = 'Liability_Inverted'\n",
    "df_oecd_total = pd.concat([ser_oecd_asset, ser_oecd_liability_inv], axis = 1, names = 'Data Source').astype('float32').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OECD FDI: UNCONDITIONAL COMBINATION\n",
    "\n",
    "### Combining Export & Import data:\n",
    "### Дополняем основной вектор дополнительным:\n",
    "df_oecd_total.loc[df_oecd_total['Asset'] == 0.0, 'Asset'] = np.NaN \n",
    "ser_oecd_augmented = df_oecd_total['Asset'].combine_first(df_oecd_total['Liability_Inverted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: OECD FDI: RESULTS SAVING\n",
    "\n",
    "ser_oecd_augmented.to_hdf(path_or_buf = str_path_oecd_fdi_augmented, key = str_key_oecd_fdi_augmented, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRAVITY DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: GRAVITY: DATA SAVING PARAMETERS\n",
    "\n",
    "### GDP Dataset:\n",
    "str_path_wb_gdp_dataset = 'Data_Files/Source_Files/gdp_dataset.h5'\n",
    "str_wb_gdp_dataset = 'gdp_dataset'\n",
    "### CEPII Dataste:\n",
    "str_path_cepii_dataset = 'Data_Files/Source_Files/cepii_dataset.h5'\n",
    "str_distance_dataset = 'distance_dataset'\n",
    "### Resulting dataset:\n",
    "str_path_gravity = 'Data_Files/Source_Files/gravity_constructed.h5'\n",
    "str_key_gravity = 'gravity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CEPII DISTANCES: LOADING PARAMETERS PREPARATION\n",
    "\n",
    "### MultiIndex level slice constant:\n",
    "All = slice(None)\n",
    "### Path to MS Excel file:\n",
    "str_path_cepii_source = 'Data_Files/Source_Files/CEPII Distance Data/dist_cepii.xls'\n",
    "### Saved dataset:\n",
    "str_path_cepii_dataset = 'Data_Files/Source_Files/cepii_dataset.h5'\n",
    "str_distance_dataset = 'distance_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CEPII DISTANCES: DATA EXPORT AND REPACKING\n",
    "\n",
    "### Расстояния хранятся в стационарном файле. Его URL: http://www.cepii.fr/distance/dist_cepii.zip\n",
    "### Мы оперируем непосредственно с одним файлом из этого архива - dist_cepii.xls\n",
    "### Для целей данног опроекта нас инетерсует только показатель distw\n",
    "\n",
    "### Constants:\n",
    "str_path_cepii_source = 'Data_Files/Source_Files/CEPII Distance Data/dist_cepii.xls'\n",
    "### Source data export:\n",
    "df_distance_source = pd.read_excel(str_path_cepii_source, index_col = [0, 1])\n",
    "### Long to Short Country ID's converting:\n",
    "df_distance_data = df_distance_source.join(df_country_codes.set_index('ISO LONG').squeeze(), on = 'iso_o')\n",
    "df_distance_data.rename({'ISO SHORT': 'From_ID'}, axis = 1, inplace = True)\n",
    "df_distance_data = df_distance_data.join(df_country_codes.set_index('ISO LONG').squeeze(), on = 'iso_d')\n",
    "df_distance_data.rename({'ISO SHORT': 'To_ID'}, axis = 1, inplace = True)\n",
    "### ISON countries filtering:\n",
    "df_distance_data = df_distance_data.dropna().set_index(['From_ID', 'To_ID']).loc[(list_ison_countries, list_ison_countries), ['dist', 'distcap', 'distw', 'distwces']]\n",
    "df_distance_data = df_distance_data.astype(int)\n",
    "### Result saving:\n",
    "df_distance_data.to_hdf(path_or_buf = str_path_cepii_dataset, key = str_distance_dataset, mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORLD BANK: WDI: GDP: GENERAL DATA PREPARATION\n",
    "\n",
    "### MultiIndex level slice constant:\n",
    "All = slice(None)\n",
    "\n",
    "str_wdi_base_url = 'http://api.worldbank.org/v2/'\n",
    "str_wdi_request_format = '?format=json&per_page=29999'\n",
    "str_gdp_dataset = 'NY.GDP.MKTP.CD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORLD BANK: WDI: GDP: DATA EXTRACTING\n",
    "\n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "### List of ISON countries converting:\n",
    "str_reporters_all = ';'.join(sorted(list_ison_long))\n",
    "### URL for API request:\n",
    "str_gdp_url = str_wdi_base_url + 'country/' + str_reporters_all + '/indicator/' + str_gdp_dataset + \\\n",
    "              str_wdi_request_format\n",
    "### API response:\n",
    "obj_gdp_dataset = request_session.get(str_gdp_url)\n",
    "### Data converting from JSON to pandas:\n",
    "ser_country_id = pd.DataFrame(obj_gdp_dataset.json()[1])['country'].apply(pd.Series)['id']\n",
    "df_raw_dataset = pd.concat([ser_country_id, pd.DataFrame(obj_gdp_dataset.json()[1])[['date', 'value']]], axis = 1)\n",
    "df_raw_dataset.columns = ['Country', 'Year', 'Value']\n",
    "df_raw_dataset['Date'] = pd.to_datetime(df_raw_dataset['Year']) + pd.offsets.BYearEnd()\n",
    "### Adding data to container:\n",
    "ser_full_gdp = df_raw_dataset.set_index(['Date', 'Country'])['Value'].sort_index()\n",
    "### Data saving:\n",
    "ser_full_gdp.to_hdf(path_or_buf = str_path_wb_gdp_dataset, key = str_wb_gdp_dataset, mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: DATASETS LOADING:\n",
    "\n",
    "### WB WDI GDP dataset:\n",
    "ser_gdp = pd.read_hdf(path_or_buf = str_path_wb_gdp_dataset, key = str_wb_gdp_dataset)\n",
    "### CEPII Distances dataset:\n",
    "ser_dist = pd.read_hdf(path_or_buf = str_path_cepii_dataset, key = str_distance_dataset)['distw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRAVITY: DATASET CONSTRUCTION\n",
    "\n",
    "### Мы определяем Gravity как соотношения произведений GDP Reporter и GDP Partner к расстоянию между ними.\n",
    "### Для калькуляции фактора мы цепляем к вектору расстояний GDP Reporter и отдельно - GDP Partner. Затем объединяем два DataFrames в один.\n",
    "\n",
    "### Construction constants:\n",
    "flo_dist_power = 1\n",
    "### Distances naming:\n",
    "ser_dist.index.names = ['Reporter', 'Partner']\n",
    "ser_dist.name = 'Distance'\n",
    "### Dropping internal distances:\n",
    "df_dist = ser_dist.reset_index()\n",
    "df_dist.drop(df_dist[df_dist['Reporter'] == df_dist['Partner']].index, inplace = True)\n",
    "ser_dist_cleared = df_dist.set_index(['Reporter', 'Partner']).squeeze().sort_index()\n",
    "### GDP duplicating:\n",
    "ser_gdp_reporter = ser_gdp[:]\n",
    "ser_gdp_reporter.index.names = ['Date', 'Reporter']\n",
    "ser_gdp_reporter.name = 'GDP_Reporter'\n",
    "ser_gdp_partner = ser_gdp[:]\n",
    "ser_gdp_partner.index.names = ['Date', 'Partner']\n",
    "ser_gdp_partner.name = 'GDP_Partner'\n",
    "### Reporters data connecting:\n",
    "df_reporter = ser_dist_cleared.to_frame().join(ser_gdp_reporter).sort_index()\n",
    "### Partners data connecting:\n",
    "df_partner = ser_dist_cleared.to_frame().join(ser_gdp_partner).drop('Distance', axis = 1).sort_index()\n",
    "df_partner = df_partner.reorder_levels([1, 0, 2])\n",
    "### Joining data and Gravity calculation:\n",
    "df_gravity = pd.concat([df_reporter, df_partner], axis = 1)\n",
    "df_gravity = df_gravity.reset_index('Date').dropna(subset = ['Date']).set_index('Date', append = True).reorder_levels([2, 0, 1]).sort_index()\n",
    "ser_gravity = (df_gravity['GDP_Reporter'] / 10 ** 9) * (df_gravity['GDP_Partner'] / 10 ** 9) / (df_gravity['Distance'] ** flo_dist_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEED TO BE ADOPTED: GRAVITY: RESULTS SAVING\n",
    "\n",
    "ser_gravity.to_hdf(path_or_buf = str_path_gravity, key = str_key_gravity, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
