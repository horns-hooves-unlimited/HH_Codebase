{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CPIS DOTS DATA EXPORT : PARTNERS FROM THE WHOLE WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "pd.set_option('display.max_colwidth', -1) ### To display long strings\n",
    "register_matplotlib_converters()\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "import requests\n",
    "import json ### To correct JSON structure before unpacking\n",
    "import gc\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  0.25.3\n",
      "python version:  3.7.4\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: MAIN CONSTANTS\n",
    "\n",
    "### MultiIndex level slice constant:\n",
    "All = slice(None)\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### IMF DOTS dataset:\n",
    "str_path_imf_dots_world = 'Data_Files/Source_Files/dots_world_export.h5'\n",
    "str_full_imf_dots_world = 'dots_world_export'\n",
    "str_path_imf_dots_dataset = 'Data_Files/Source_Files/dots_dataset.h5'\n",
    "str_key_imf_dots_export = 'dots_export'\n",
    "str_key_imf_dots_import_inverted = 'dots_import_inverted'\n",
    "str_path_imf_dots_combined = 'Data_Files/Source_Files/dots_combined.h5'\n",
    "str_key_imf_dots_full = 'dots_full_combined'\n",
    "str_path_imf_dots_options = 'Data_Files/Source_Files/dots_options.h5'\n",
    "str_key_do_total_imf_dots_options = 'dots_export_options'\n",
    "### Technical Constants:\n",
    "str_date_end = '2022-10-31'\n",
    "date_start = pd.Timestamp('1989-12-29')\n",
    "date_end = pd.Timestamp(str_date_end)\n",
    "date_ison = pd.Timestamp('1994-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING COUNTRY CODES EXTRACTOR\n",
    "\n",
    "def get_country_codes(use_local_copy = False):  \n",
    "    ### In case if URL is unavailable:\n",
    "    if (use_local_copy):\n",
    "        url_country_code = 'Data_Files/Source_Files/countrycode.html'\n",
    "    ### Online extraction:\n",
    "    else:\n",
    "        url_country_code = 'https://countrycode.org/'\n",
    "    df_full_codes = pd.read_html(url_country_code, index_col = 'COUNTRY')[0]\n",
    "    df_full_codes[['ISO SHORT', 'ISO LONG']] = df_full_codes['ISO CODES'].str.split(' / ', expand = True)\n",
    "    df_result = df_full_codes[['ISO SHORT', 'ISO LONG']].sort_index()    \n",
    "    df_result.index = df_result.index.str.upper()\n",
    "    ### Results output:\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(engine = 'openpyxl', io = str_path_universe, sheet_name = 'Switchers', header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: COMMON DATA EXTRACTION STEPS\n",
    "\n",
    "### World Country Codes:\n",
    "df_country_codes = get_country_codes()\n",
    "### ISON membership history:\n",
    "ser_ison_membership = ison_membership_converting(str_path_universe, pd.to_datetime(str_date_end))\n",
    "### ISON LONG IDs list:\n",
    "list_ison_long = list(df_country_codes.loc[df_country_codes['ISO SHORT'].isin(ser_ison_membership.index.get_level_values('Country').unique()), 'ISO LONG'].values)\n",
    "### ISON current status:\n",
    "ser_ison_status = ser_ison_membership.loc[str_date_end].droplevel('Date')\n",
    "### ISON stats:\n",
    "int_ison_number = len(list_ison_long)\n",
    "list_regions = ['DM', 'EM', 'FM']\n",
    "dict_ison_len = {}\n",
    "dict_ison_len['Full Universe'] = int_ison_number\n",
    "for iter_region in list_regions:\n",
    "    dict_ison_len[iter_region] = len(ser_ison_status[ser_ison_status == iter_region])\n",
    "ser_market_len = pd.Series(dict_ison_len)\n",
    "ser_market_len.index.names = ['Market']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: BILATERAL EXPORTS & IMPORTS (MILLIONS OF USD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "dict_request_headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'}\n",
    "str_imf_base_url = 'http://dataservices.imf.org/REST/SDMX_JSON.svc/'\n",
    "str_imf_dataflow_add = 'DataFlow'\n",
    "str_imf_datastructure_add = 'DataStructure/'\n",
    "str_imf_codelist_add = 'CodeList/'\n",
    "str_imf_dataset_add = 'CompactData/'\n",
    "int_seconds_to_sleep = 6\n",
    "int_imf_country_limit = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: REQUESTS SESSION INITIALIZING\n",
    "\n",
    "request_session = requests.Session()\n",
    "### For avoiding data request errors from IMF Data Service:\n",
    "request_session.headers.update(dict_request_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOT\n"
     ]
    }
   ],
   "source": [
    "### IMF DOTS: DATAFLOW SEARCHING\n",
    "\n",
    "obj_imf_dataflow_list = request_session.get(str_imf_base_url + str_imf_dataflow_add).json()\n",
    "df_imf_dataflow = pd.DataFrame(obj_imf_dataflow_list['Structure']['Dataflows']['Dataflow'])\n",
    "df_imf_dataflow = df_imf_dataflow.assign(Description = df_imf_dataflow['Name'].apply(pd.Series)['#text'].values)[['@id', 'Description']]\n",
    "ser_imf_dataflow = df_imf_dataflow.set_index('@id', drop = True).squeeze()\n",
    "### Searching DataFlow code for further requests:\n",
    "str_imf_dots_id = ser_imf_dataflow[ser_imf_dataflow.str.endswith('(DOTS)')].index[0].replace('DS-', '')\n",
    "print(str_imf_dots_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        @conceptRef                @codelist @isFrequencyDimension\n",
      "0  FREQ              CL_FREQ                  true                \n",
      "1  REF_AREA          CL_AREA_DOT              NaN                 \n",
      "2  INDICATOR         CL_INDICATOR_DOT         NaN                 \n",
      "3  COUNTERPART_AREA  CL_COUNTERPART_AREA_DOT  NaN                 \n"
     ]
    }
   ],
   "source": [
    "### IMF DOTS: DATASTRUCTURE SEARCHING\n",
    "\n",
    "obj_imf_dots_structure = request_session.get(str_imf_base_url + str_imf_datastructure_add + str_imf_dots_id).json()\n",
    "df_imf_dots_params = pd.DataFrame(obj_imf_dots_structure['Structure']['KeyFamilies']['KeyFamily']['Components']['Dimension'])\\\n",
    "                                [['@conceptRef', '@codelist', '@isFrequencyDimension']]\n",
    "### Receiving DataFlow parameters and code lists for each of them:\n",
    "print(df_imf_dots_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 : INDICATOR : CL_INDICATOR_DOT :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@value</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXG_FOB_USD</td>\n",
       "      <td>Goods, Value of Exports, Free on board (FOB), US Dollars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TMG_CIF_USD</td>\n",
       "      <td>Goods, Value of Imports, Cost, Insurance, Freight (CIF), US Dollars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TMG_FOB_USD</td>\n",
       "      <td>Goods, Value of Imports, Free on board (FOB), US Dollars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TBG_USD</td>\n",
       "      <td>Goods, Value of Trade Balance, US Dollars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        @value  \\\n",
       "0  TXG_FOB_USD   \n",
       "1  TMG_CIF_USD   \n",
       "2  TMG_FOB_USD   \n",
       "3  TBG_USD       \n",
       "\n",
       "                                                                  Text  \n",
       "0  Goods, Value of Exports, Free on board (FOB), US Dollars             \n",
       "1  Goods, Value of Imports, Cost, Insurance, Freight (CIF), US Dollars  \n",
       "2  Goods, Value of Imports, Free on board (FOB), US Dollars             \n",
       "3  Goods, Value of Trade Balance, US Dollars                            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### IMF DOTS: CODES DESCRIPTION SEARCHING\n",
    "\n",
    "for int_counter, str_param_code in enumerate(df_imf_dots_params['@codelist']):\n",
    "    if (int_counter == 2):\n",
    "        time.sleep(int_seconds_to_sleep)    \n",
    "        obj_imf_dots_param = request_session.get(str_imf_base_url + str_imf_codelist_add + str_param_code).json()\n",
    "        df_imf_dots_param =  pd.DataFrame(obj_imf_dots_param['Structure']['CodeLists']['CodeList']['Code'])\n",
    "        ### Receiving values for each code list:\n",
    "        df_imf_dots_param = df_imf_dots_param.assign(Text = df_imf_dots_param['Description'].apply(pd.Series)['#text'].values)[['@value', 'Text']]\n",
    "        print(int_counter, ':', df_imf_dots_params.iloc[int_counter, All]['@conceptRef'], ':', str_param_code, ':') \n",
    "        display(df_imf_dots_param)\n",
    "    \n",
    "str_dots_freq = 'M' # 'A' # 'B' # \n",
    "#str_dots_indicator = \n",
    "list_dots_indicator = ['TXG_FOB_USD', 'TMG_CIF_USD', 'TMG_FOB_USD', 'TBG_USD']\n",
    "list_ison_countries = sorted(list(map(str, ser_ison_membership.index.get_level_values(1).unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: INDICATORS EXTRACTION: WORLD EXPORT\n",
    "\n",
    "dict_dots_bilateral = {} # Global container\n",
    "str_dots_const_url = str_imf_base_url + str_imf_dataset_add + str_imf_dots_id + '/' # Beginning of request URL\n",
    "### Export Indicator defining:\n",
    "str_dots_indicator = 'TXG_FOB_USD'\n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "request_session.headers.update(dict_request_headers)    \n",
    "### List of bilateral dataframes for future concatenation\n",
    "list_dots_bilateral = [] \n",
    "### Looping for reporter country groups:\n",
    "for int_ison_reporter_part in range(0, - (len(list_ison_countries) // ( - int_imf_country_limit))):\n",
    "    str_dots_reporters = '+'.join(list_ison_countries[int_ison_reporter_part * int_imf_country_limit : (int_ison_reporter_part + 1) * int_imf_country_limit])\n",
    "    ### World as a partner:\n",
    "    str_dots_partners = 'W00'\n",
    "    ### Generating complete request URL:\n",
    "    str_dots_full_url = str_dots_const_url + '.'.join([str_dots_freq, str_dots_reporters, str_dots_indicator, str_dots_partners])\n",
    "    ### Receiving DOTS dataset from IMF API:\n",
    "    print(str_dots_indicator, '(WORLD) :', int_ison_reporter_part * int_imf_country_limit, '-', (int_ison_reporter_part + 1) * int_imf_country_limit)        \n",
    "    obj_dots_set = request_session.get(str_dots_full_url)\n",
    "    ### Data reading as JSON:\n",
    "    dict_dots_set = json.loads(obj_dots_set.text)\n",
    "    ### Converting each bilateral dataset to dataframe and it's mungling:\n",
    "    for dict_dots_pair in dict_dots_set['CompactData']['DataSet']['Series']:\n",
    "        if isinstance(dict_dots_pair['Obs'], list):\n",
    "            df_dots_bilateral = pd.DataFrame(dict_dots_pair['Obs'])\n",
    "        else:\n",
    "            df_dots_bilateral = pd.DataFrame([dict_dots_pair['Obs']])\n",
    "        ### Markers checking:\n",
    "        if '@OBS_STATUS' not in df_dots_bilateral.columns:\n",
    "            df_dots_bilateral['@OBS_STATUS'] = np.NaN\n",
    "        ### Data extracting and mungling:\n",
    "        df_dots_bilateral = df_dots_bilateral[['@TIME_PERIOD', '@OBS_VALUE', '@OBS_STATUS']]\n",
    "        df_dots_bilateral.columns = ['Date', 'Value', 'Status']\n",
    "        df_dots_bilateral = df_dots_bilateral.assign(Reporter_ID = dict_dots_pair['@REF_AREA'])\n",
    "        df_dots_bilateral = df_dots_bilateral.assign(Partner_ID = dict_dots_pair['@COUNTERPART_AREA'])\n",
    "        list_dots_bilateral.append(df_dots_bilateral)\n",
    "#        break\n",
    "#    break\n",
    "### Flow level data aggregation:\n",
    "df_dots_indicator = pd.concat(list_dots_bilateral, axis = 0, ignore_index = True)\n",
    "df_dots_indicator['Date'] = pd.to_datetime(df_dots_indicator['Date']) + pd.offsets.BMonthEnd()\n",
    "ser_dots_indicator = df_dots_indicator.set_index(['Date', 'Reporter_ID']).sort_index().drop(['Partner_ID', 'Status'], axis = 1).squeeze().astype(float)\n",
    "ser_dots_indicator.index.names = ['Date', 'Reporter']\n",
    "ser_dots_indicator.name = 'World_Export'\n",
    "### Dataset saving:\n",
    "ser_dots_indicator.to_hdf(path_or_buf = str_path_imf_dots_world, key = str_full_imf_dots_world, mode = 'w', format = 'fixed')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: EXPORT DATA EXTRACTION: BILATERAL FLOWS\n",
    "\n",
    "dict_dots_bilateral = {} # Global container\n",
    "str_dots_const_url = str_imf_base_url + str_imf_dataset_add + str_imf_dots_id + '/' # Beginning of request URL\n",
    "str_dots_indicator = list_dots_indicator[0]\n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "request_session.headers.update(dict_request_headers)    \n",
    "### List of bilateral dataframes for future concatenation\n",
    "list_dots_bilateral = [] \n",
    "### Looping over reporters:\n",
    "for str_reporter in list_ison_countries:\n",
    "    ### Generating complete request URL:\n",
    "    str_dots_full_url = str_dots_const_url + '.'.join([str_dots_freq, str_reporter, str_dots_indicator])\n",
    "    ### Receiving DOTS dataset from IMF API:\n",
    "    print(str_reporter, ' / ', str_dots_indicator)\n",
    "    obj_dots_set = request_session.get(str_dots_full_url)\n",
    "    ### Data reading as JSON:\n",
    "    dict_dots_set = json.loads(obj_dots_set.text)\n",
    "    if ('Series' in dict_dots_set['CompactData']['DataSet']):\n",
    "        ### Converting each bilateral dataset to dataframe and it's mungling:\n",
    "        for dict_dots_pair in dict_dots_set['CompactData']['DataSet']['Series']:\n",
    "            if isinstance(dict_dots_pair['Obs'], list):\n",
    "                df_dots_bilateral = pd.DataFrame(dict_dots_pair['Obs'])\n",
    "            else:\n",
    "                df_dots_bilateral = pd.DataFrame([dict_dots_pair['Obs']])\n",
    "            ### Markers checking:\n",
    "            if '@OBS_STATUS' not in df_dots_bilateral.columns:\n",
    "                df_dots_bilateral['@OBS_STATUS'] = np.NaN\n",
    "            ### Data extracting and mungling:\n",
    "            df_dots_bilateral = df_dots_bilateral[['@TIME_PERIOD', '@OBS_VALUE', '@OBS_STATUS']]\n",
    "            df_dots_bilateral.columns = ['Date', 'Value', 'Status']\n",
    "            df_dots_bilateral = df_dots_bilateral.assign(Reporter_ID = dict_dots_pair['@REF_AREA'])\n",
    "            df_dots_bilateral = df_dots_bilateral.assign(Partner_ID = dict_dots_pair['@COUNTERPART_AREA'])\n",
    "            list_dots_bilateral.append(df_dots_bilateral)\n",
    "### Flow level data aggregation:\n",
    "df_dots_indicator = pd.concat(list_dots_bilateral, axis = 0, ignore_index = True)\n",
    "df_dots_indicator['Date'] = pd.to_datetime(df_dots_indicator['Date']) + pd.offsets.BMonthEnd()\n",
    "df_dots_indicator = df_dots_indicator[df_dots_indicator['Partner_ID'].isin(df_country_codes['ISO SHORT'].values)].drop('Status', axis = 1)\n",
    "print('Unique partners number:', len(df_dots_indicator['Partner_ID'].unique()))\n",
    "df_dots_indicator.rename({'Reporter_ID': 'Reporter', 'Partner_ID': 'Partner'}, axis = 1, inplace = True)\n",
    "### Data saving:\n",
    "ser_dots_export = df_dots_indicator.set_index(['Date', 'Reporter', 'Partner'])['Value'].sort_index().astype('float16')\n",
    "del df_dots_indicator\n",
    "gc.collect()\n",
    "ser_dots_export.to_hdf(path_or_buf = str_path_imf_dots_dataset, key = str_key_imf_dots_export, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: IMPORT DATA EXTRACTION: BILATERAL FLOWS\n",
    "\n",
    "gc.collect()\n",
    "dict_dots_bilateral = {} # Global container\n",
    "str_dots_const_url = str_imf_base_url + str_imf_dataset_add + str_imf_dots_id + '/' # Beginning of request URL\n",
    "str_dots_indicator = list_dots_indicator[1]\n",
    "### Session initializing:\n",
    "request_session = requests.Session()\n",
    "request_session.headers.update(dict_request_headers)    \n",
    "### List of bilateral dataframes for future concatenation\n",
    "list_dots_bilateral = [] \n",
    "### Looping over reporters:\n",
    "for str_reporter in list_ison_countries:\n",
    "    ### Generating complete request URL:\n",
    "    str_dots_full_url = str_dots_const_url + '.'.join([str_dots_freq, '', str_dots_indicator, str_reporter])\n",
    "    ### Receiving DOTS dataset from IMF API:\n",
    "    print(str_reporter, ' / ', str_dots_indicator)\n",
    "    obj_dots_set = request_session.get(str_dots_full_url)\n",
    "    ### Data reading as JSON:\n",
    "    dict_dots_set = json.loads(obj_dots_set.text)\n",
    "    if ('Series' in dict_dots_set['CompactData']['DataSet']):\n",
    "        ### Converting each bilateral dataset to dataframe and it's mungling:\n",
    "        for dict_dots_pair in dict_dots_set['CompactData']['DataSet']['Series']:\n",
    "            if isinstance(dict_dots_pair['Obs'], list):\n",
    "                df_dots_bilateral = pd.DataFrame(dict_dots_pair['Obs'])\n",
    "            else:\n",
    "                df_dots_bilateral = pd.DataFrame([dict_dots_pair['Obs']])\n",
    "            ### Markers checking:\n",
    "            if '@OBS_STATUS' not in df_dots_bilateral.columns:\n",
    "                df_dots_bilateral['@OBS_STATUS'] = np.NaN\n",
    "            ### Data extracting and mungling:\n",
    "            df_dots_bilateral = df_dots_bilateral[['@TIME_PERIOD', '@OBS_VALUE', '@OBS_STATUS']]\n",
    "            df_dots_bilateral.columns = ['Date', 'Value', 'Status']\n",
    "            df_dots_bilateral = df_dots_bilateral.assign(Reporter_ID = dict_dots_pair['@REF_AREA'])\n",
    "            df_dots_bilateral = df_dots_bilateral.assign(Partner_ID = dict_dots_pair['@COUNTERPART_AREA'])\n",
    "            list_dots_bilateral.append(df_dots_bilateral)\n",
    "### Flow level data aggregation:\n",
    "df_dots_indicator = pd.concat(list_dots_bilateral, axis = 0, ignore_index = True)\n",
    "df_dots_indicator['Date'] = pd.to_datetime(df_dots_indicator['Date']) + pd.offsets.BMonthEnd()\n",
    "df_dots_indicator = df_dots_indicator[df_dots_indicator['Reporter_ID'].isin(df_country_codes['ISO SHORT'].values)].drop('Status', axis = 1)\n",
    "print('Unique reporters number:', len(df_dots_indicator['Reporter_ID'].unique()))\n",
    "df_dots_indicator.rename({'Reporter_ID': 'Partner', 'Partner_ID': 'Reporter'}, axis = 1, inplace = True)\n",
    "### Data saving:\n",
    "ser_dots_import_inv = df_dots_indicator.set_index(['Date', 'Reporter', 'Partner'])['Value'].sort_index().astype('float16')\n",
    "del df_dots_indicator\n",
    "gc.collect()\n",
    "ser_dots_import_inv.to_hdf(path_or_buf = str_path_imf_dots_dataset, key = str_key_imf_dots_import_inverted, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: EXPORT & IMPORT DATA AGGREGATION: DATASETS LOADING\n",
    "\n",
    "gc.collect()\n",
    "ser_dots_export = pd.read_hdf(path_or_buf = str_path_imf_dots_dataset, key = str_key_imf_dots_export)\n",
    "ser_dots_import_inv = pd.read_hdf(path_or_buf = str_path_imf_dots_dataset, key = str_key_imf_dots_import_inverted)\n",
    "df_export_aug = pd.concat([ser_dots_export, ser_dots_import_inv], axis = 1, names = 'Source Flow', keys = ['Export', 'Import'])\n",
    "#df_export_aug = df_export_aug.join(ser_ison_status, on = 'Reporter').set_index('Market', append = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: CIF COEFFICIENTS CALCULATION\n",
    "\n",
    "gc.collect()\n",
    "### Bounds to filter bilateral Import to Export ratio before median calculation:\n",
    "flo_lower_bound = 1.0\n",
    "flo_upper_bound = 2.0\n",
    "### Bilateral median calculation procedure:\n",
    "def get_obs_median(df_comm):\n",
    "    ### Export to Import ratio:\n",
    "    ser_obs_coeff = df_comm['Import'] / df_comm['Export']\n",
    "    ### Ratio filtering:\n",
    "    ser_obs_coeff = ser_obs_coeff.loc[(ser_obs_coeff >= flo_lower_bound) & (ser_obs_coeff <= flo_upper_bound)]\n",
    "    ### Filtered timeseries median as a result:\n",
    "    return round(ser_obs_coeff.median(), 2)\n",
    "\n",
    "### Calulation CIF coefficient for all commodities:\n",
    "ser_cif_median = df_export_aug.groupby(['Reporter', 'Partner']).apply(get_obs_median)\n",
    "ser_cif_median.fillna(ser_cif_median.median(), inplace = True)\n",
    "ser_cif_median.name = 'CIF_Coefficient'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: IMPORT DATA CORRECTION:\n",
    "\n",
    "### Adding CIF coefficients to dataset:\n",
    "df_export_cif = df_export_aug.merge(ser_cif_median, left_index = True, right_index = True)\n",
    "del df_export_aug\n",
    "gc.collect()\n",
    "df_export_cif = df_export_cif.reorder_levels(['Date', 'Reporter', 'Partner'])\n",
    "### Import correction:\n",
    "df_export_cif['Import_Corrected'] = df_export_cif['Import'] / df_export_cif['CIF_Coefficient'].astype('float16')\n",
    "df_export_cif.drop(['Import', 'CIF_Coefficient'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: SIMPLE COMBINATION (OLD VERSION)\n",
    "\n",
    "### Combining Export & Import data:\n",
    "df_export_cif['Export_Augmented'] = df_export_cif['Export'].combine_first(df_export_cif['Import_Corrected'])\n",
    "df_export_cif.drop('CIF_Coefficient', axis = 1, inplace = True)\n",
    "df_export_cif = df_export_cif.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### IMF DOTS: IMPORT DATA INCORPORATION: RESULTS SAVING (OLD VERSION)\n",
    "\n",
    "### Dataset saving:\n",
    "df_export_cif[['Export', 'Export_Augmented']].to_hdf(path_or_buf = str_path_imf_dots_combined, key = str_key_imf_dots_full, mode = 'w', format = 'fixed')\n",
    "del df_export_cif\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in half_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "### IMF DOTS: DATA AGGREGATION: EXPORT QUALITY RATIOS\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "### Defining similarity for exporters by date\n",
    "def get_exporter_ratio(df_group):\n",
    "    df_group.fillna(0.0, inplace = True)    \n",
    "    df_both = df_group.dropna()    \n",
    "    if (df_both['Export'].sum() > 0.0):\n",
    "        flo_result = (df_both['Export'] - df_both['Import_Corrected']).abs().clip(upper = df_group['Export'].max()).sum() / df_group['Export'].sum() / len(df_group)\n",
    "    else:\n",
    "        flo_result = np.NaN    \n",
    "    return flo_result\n",
    "\n",
    "### Similarity values calculation:\n",
    "ser_exporter_ratio = df_export_cif.groupby(['Date', 'Reporter']).apply(get_exporter_ratio)\n",
    "ser_exporter_ratio.name = 'Exporter_Ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 / (Timestamp('2000-06-30 00:00:00'), 'US')\n",
      "1.0 / (Timestamp('1960-01-29 00:00:00'), 'MA')\n"
     ]
    }
   ],
   "source": [
    "### IMF DOTS: DATA AGGREGATION: SIMILARITY TEST\n",
    "\n",
    "print(round(ser_exporter_ratio.min(), 4), '/', ser_exporter_ratio.idxmin())\n",
    "print(round(ser_exporter_ratio.max(), 4), '/', ser_exporter_ratio.idxmax())\n",
    "\n",
    "#display(df_export_cif.loc[('2000-06-30', 'US', All), :].dropna())\n",
    "#display(df_export_cif.loc[('1960-01-29', 'MA', All), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in half_scalars\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "### IMF DOTS: DATA AGGREGATION: IMPORT QUALITY RATIOS\n",
    "\n",
    "gc.collect()\n",
    "### Defining similarity for importers by date\n",
    "def get_importer_ratio(df_group):\n",
    "#    df_group['Liability_Inverted'].fillna(0.0, inplace = True)\n",
    "    df_group.fillna(0.0, inplace = True)     \n",
    "    df_both = df_group.dropna()\n",
    "    if (df_both['Import_Corrected'].sum() > 0.0):\n",
    "        flo_result = (df_both['Export'] - df_both['Import_Corrected']).abs().clip(upper = df_group['Import_Corrected'].max()).sum() \\\n",
    "                                                                        / df_group['Import_Corrected'].sum() / len(df_group)\n",
    "    else:\n",
    "        flo_result = np.NaN    \n",
    "    return flo_result\n",
    "\n",
    "### Similarity values calculation:\n",
    "ser_importer_ratio = df_export_cif.groupby(['Date', 'Partner']).apply(get_importer_ratio)\n",
    "ser_importer_ratio.name = 'Importer_Ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 / (Timestamp('1981-04-30 00:00:00'), 'KN')\n",
      "1.0007 / (Timestamp('1966-09-30 00:00:00'), 'NG')\n"
     ]
    }
   ],
   "source": [
    "### IMF DOTS: DATA AGGREGATION: SIMILARITY TEST\n",
    "\n",
    "print(round(ser_importer_ratio.min(), 4), '/', ser_importer_ratio.idxmin())\n",
    "print(round(ser_importer_ratio.max(), 4), '/', ser_importer_ratio.idxmax())\n",
    "\n",
    "#display(df_export_cif.loc[('1981-04-30', All, 'KN'), :])\n",
    "#display(df_export_cif.loc[('1966-09-30', All, 'NG'), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: DATA AGGREGATION: ADDING RATIOS\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "df_dots_to_augment = df_export_cif.join(ser_exporter_ratio).join(ser_importer_ratio)\n",
    "df_dots_to_augment['Export_Augmented'] = np.NaN # -999 # \n",
    "df_dots_to_augment = df_dots_to_augment.reorder_levels(['Date', 'Reporter', 'Partner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: DATA AGGREGATION: CONDITIONAL REPLACING\n",
    "\n",
    "gc.collect()\n",
    "def augment_by_date(df_date, int_option = -1):\n",
    "    '''\n",
    "       -1 : Replace NaN Asset values unconditionally\n",
    "        0 : Replace NaN Asset values when Investor's Ratio > Borrower's Ratio\n",
    "        1 : Replace NaN or zero Asset values when Investor's Ratio > Borrower's Ratio\n",
    "        2 : Replace any Asset values when Investor's Ratio > Borrower's Ratio\n",
    "    '''\n",
    "    if (int_option == -1):\n",
    "        ### Replacing zero Export & Import values with NaN:\n",
    "        df_date.loc[df_date['Export'] == 0.0, 'Export'] = np.NaN        \n",
    "        df_date['Export_Augmented'] = df_date['Export'].combine_first(df_date['Import_Corrected'])\n",
    "    elif (int_option == 0):\n",
    "        ### Fill resulting column with not NaN Export values:\n",
    "        df_date.loc[df_date['Export'].notna(), 'Export_Augmented'] = df_date[df_date['Export'].notna()]['Export'].values\n",
    "        ### Fill resulting column with Import value if Export value is NaN & Investor Ratio is NaN (and doesn't matter if Borrower Ratio is NaN):\n",
    "        df_date.loc[df_date['Export'].isna() & df_date['Exporter_Ratio'].isna(), 'Export_Augmented'] = \\\n",
    "            df_date[df_date['Export'].isna() & df_date['Exporter_Ratio'].isna()]['Import_Corrected'].values\n",
    "        ### Fill resulting column with Import value if Export value is NaN & Investor Ratio is bigger than Borrower Ratio:\n",
    "        df_date.loc[df_date['Export'].isna() & df_date['Exporter_Ratio'].notna() & df_date['Importer_Ratio'].notna() & \\\n",
    "                    (df_date['Exporter_Ratio'] > df_date['Importer_Ratio']), 'Export_Augmented'] = \\\n",
    "            df_date[df_date['Export'].isna() & df_date['Exporter_Ratio'].notna() & df_date['Importer_Ratio'].notna() & \\\n",
    "                    (df_date['Exporter_Ratio'] > df_date['Importer_Ratio'])]['Import_Corrected'].values\n",
    "    elif (int_option == 1):\n",
    "        ### Replacing zero Export values with NaN:\n",
    "        df_date.loc[df_date['Export'] == 0.0, 'Export'] = np.NaN\n",
    "        ### Fill resulting column with not NaN Export values:\n",
    "        df_date.loc[df_date['Export'].notna(), 'Export_Augmented'] = df_date.loc[df_date['Export'].notna(), 'Export'].values\n",
    "        ### Fill resulting column with Import value if Export value is NaN & Investor Ratio is NaN (and doesn't matter if Borrower Ratio is NaN):\n",
    "        df_date.loc[df_date['Export'].isna() & df_date['Exporter_Ratio'].isna(), 'Export_Augmented'] = \\\n",
    "            df_date[df_date['Export'].isna() & df_date['Exporter_Ratio'].isna()]['Import_Corrected'].values\n",
    "        ### Fill resulting column with Import value if Export value is NaN & Investor Ratio is bigger than Borrower Ratio:\n",
    "        df_date.loc[df_date['Export'].isna() & df_date['Exporter_Ratio'].notna() & df_date['Importer_Ratio'].notna() & \\\n",
    "                    (df_date['Exporter_Ratio'] > df_date['Importer_Ratio']), 'Export_Augmented'] = \\\n",
    "            df_date[df_date['Export'].isna() & df_date['Exporter_Ratio'].notna() & df_date['Importer_Ratio'].notna() & \\\n",
    "                    (df_date['Exporter_Ratio'] > df_date['Importer_Ratio'])]['Import_Corrected'].values\n",
    "    else:\n",
    "        ### Replacing zero Export & Import values with NaN:\n",
    "        df_date.loc[df_date['Export'] == 0.0, 'Export'] = np.NaN        \n",
    "        df_date.loc[df_date['Import_Corrected'] == 0.0, 'Import_Corrected'] = np.NaN\n",
    "        ### Ratios preparation:\n",
    "        df_date.loc[df_date['Exporter_Ratio'].isna(), 'Exporter_Ratio'] = 999.0\n",
    "        df_date.loc[df_date['Importer_Ratio'].isna(), 'Importer_Ratio'] = 1000.0\n",
    "        ### Ratios comparision:\n",
    "        df_date.loc[df_date['Exporter_Ratio'] <= df_date['Importer_Ratio'], 'Export_Augmented'] = \\\n",
    "            df_date[df_date['Exporter_Ratio'] <= df_date['Importer_Ratio']]['Export'].values\n",
    "        df_date.loc[df_date['Exporter_Ratio'] > df_date['Importer_Ratio'], 'Export_Augmented'] = \\\n",
    "            df_date[df_date['Exporter_Ratio'] > df_date['Importer_Ratio']]['Import_Corrected'].values                                       \n",
    "    return df_date\n",
    "\n",
    "dict_dots_augmented = {}\n",
    "dict_dots_augmented[-1] = df_dots_to_augment.groupby('Date').apply(augment_by_date, -1)\n",
    "#dict_dots_augmented[0] = df_dots_to_augment.groupby('Date').apply(augment_by_date, 0)\n",
    "#dict_dots_augmented[1] = df_dots_to_augment.groupby('Date').apply(augment_by_date, 1)\n",
    "dict_dots_augmented[2] = df_dots_to_augment.groupby('Date').apply(augment_by_date, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF DOTS: TOTAL DATA AGGREGATION: RESULTS CONSOLIDATION TO DATAFRAME AND SAVING\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "df_augmentation_way = pd.concat([df_dots_to_augment['Export'].replace({0.0: np.NaN}), \n",
    "                                 dict_dots_augmented[-1]['Export_Augmented'], \n",
    "                                 dict_dots_augmented[2]['Export_Augmented']], \n",
    "                                axis = 1, keys = ['Exports_Only', 'Unconditional', 'Option_2'], names = 'Augmentation_Way')\n",
    "df_augmentation_way.to_hdf(path_or_buf = str_path_imf_dots_options, key = str_key_do_total_imf_dots_options, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
