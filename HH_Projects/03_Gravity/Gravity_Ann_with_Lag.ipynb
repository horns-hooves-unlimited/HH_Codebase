{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: GRAVITY DATASETS EXPLORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import gc\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  0.25.3\n",
      "python version:  3.7.4\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: MAIN CONSTANTS\n",
    "\n",
    "### MultiIndex level slice constant:\n",
    "All = slice(None)\n",
    "### Regions list:\n",
    "list_regions = ['DM', 'EM', 'FM']\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Activities naming:\n",
    "dict_activity = {}\n",
    "dict_activity['imf_dots'] = 'Trade Export'\n",
    "dict_activity['imf_cpis'] = 'Portfolio Investment'\n",
    "dict_activity['oecd_fdi'] = 'Direct Investment'\n",
    "dict_activity['bis_lbs'] = 'Bank Lending'\n",
    "dict_activity['gravity'] = 'Gravity'\n",
    "### CEPII dataset:\n",
    "str_path_cepii_dataset = 'Data_Files/Source_Files/cepii_dataset.h5'\n",
    "str_distance_dataset = 'distance_dataset'\n",
    "### WB WDI GDP dataset:\n",
    "str_path_wb_gdp_dataset = 'Data_Files/Source_Files/gdp_dataset.h5'\n",
    "str_wb_gdp_dataset = 'gdp_dataset'\n",
    "### BIS Loans dataset:\n",
    "str_path_bis_lbs_combined = 'Data_Files/Source_Files/bis_combined.h5'\n",
    "str_full_bis_lbs_combined = 'bis_full_combined'\n",
    "### IMF CPIS dataset:\n",
    "str_path_imf_cpis_combined = 'Data_Files/Source_Files/cpis_combined.h5'\n",
    "str_full_imf_cpis_combined = 'cpis_full_combined'\n",
    "### Filtered IMF CPIS dataset:\n",
    "str_path_imf_cpis_filtered = 'Data_Files/Source_Files/cpis_filtered.h5'\n",
    "str_key_imf_cpis_filtered = 'cpis_filtered'\n",
    "### IMF DOTS datasets:\n",
    "str_path_imf_dots_combined = 'Data_Files/Source_Files/dots_combined.h5'\n",
    "str_full_imf_dots_combined = 'dots_full_combined'\n",
    "str_path_imf_dots_world = 'Data_Files/Source_Files/dots_world_export.h5'\n",
    "str_full_imf_dots_world = 'dots_world_export'\n",
    "### OECD FDI dataset:\n",
    "str_path_oecd_fdi_combined = 'Data_Files/Source_Files/oecd_combined.h5'\n",
    "str_full_oecd_fdi_combined = 'oecd_full_combined'\n",
    "str_path_direct_out_net = 'Data_Files/Source_Files/direct_outward_net.h5'\n",
    "str_key_direct_out_net = 'direct_outward'\n",
    "### Technical Constants:\n",
    "date_start = pd.Timestamp('1989-12-29')\n",
    "str_date_end = '2022-12-31'\n",
    "date_end = pd.Timestamp(str_date_end)\n",
    "date_ison = pd.Timestamp('1994-12-31')\n",
    "### Distance power for gravity calculation:\n",
    "flo_dist_power = 1 / 2\n",
    "### Bloomberg structured data extraction parameters:\n",
    "str_path_bb_hdf = 'Data_Files/Source_Files/Bloomberg_prepared.h5'\n",
    "str_key_ret_daily = 'bb_ret_daily'\n",
    "str_ret_daily_csv_path = 'Data_Files/Source_Files/ret_daily.csv'\n",
    "### Saved annualized activities:\n",
    "str_path_act_annualized = 'Data_Files/Source_Files/datasets_annualized.h5'\n",
    "str_path_act_weights = 'Data_Files/Source_Files/datasets_weights.h5'\n",
    "str_path_world_export_annualized = 'Data_Files/Source_Files/world_export_annualized.h5'\n",
    "str_key_world_export_annualized = 'world_export_ann'\n",
    "### Herfindahl index threshold:\n",
    "flo_hi_threshold = 0.0 # 1.1\n",
    "### Saved herfindahl values:\n",
    "str_path_herfindahl = 'Data_Files/Source_Files/herfindahl_indices.h5'\n",
    "### Returns average parameters:\n",
    "int_ave_months = 6\n",
    "int_halflife_months = 2\n",
    "### Thresholds for weighted cross-sectional average returns calculation:\n",
    "int_select_top = 3\n",
    "flo_select_share = 0.05\n",
    "### Saved weighted returns:\n",
    "str_path_ret_weighted = 'Data_Files/Source_Files/returns_weighted.h5'\n",
    "str_key_weighted = 'ret_weighted'\n",
    "### Saved export data:\n",
    "str_path_gravity_results = 'Data_Files/Source_Files/gravity_export.h5'\n",
    "str_key_activity_sum = 'activity_sum'\n",
    "str_key_activity_share = 'activity_share'\n",
    "str_key_gdp_total = 'gdp_total'\n",
    "str_key_herfindahl = 'herfindahl_index'\n",
    "str_key_openess = 'openess_measure'\n",
    "str_key_ret_weighted = 'ret_weighted'\n",
    "### CSV to Export data:\n",
    "str_activity_sum_csv_path = 'Data_Files/Test_Files/activity_sum.csv'\n",
    "str_activity_share_csv_path = 'Data_Files/Test_Files/activity_share.csv'\n",
    "str_gdp_total_csv_path = 'Data_Files/Test_Files/gdp_total.csv'\n",
    "str_herfindahl_csv_path = 'Data_Files/Test_Files/herfindahl.csv'\n",
    "str_openess_csv_path = 'Data_Files/Test_Files/openess.csv'\n",
    "str_ret_weighted_csv_path = 'Data_Files/Test_Files/ret_weighted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RUN EVERY TIME: DATASETS LOADING\n",
    "\n",
    "dict_dataset = {}\n",
    "dict_dataset['imf_dots'] = pd.read_hdf(path_or_buf = str_path_imf_dots_combined, key = str_full_imf_dots_combined).droplevel('Market').sort_index()['Export_Augmented']\n",
    "#dict_dataset['imf_cpis'] = pd.read_hdf(path_or_buf = str_path_imf_cpis_combined, key = str_full_imf_cpis_combined).droplevel('Market').sort_index()['Asset_Augmented']\n",
    "#dict_dataset['imf_cpis'] = pd.read_hdf(path_or_buf = str_path_imf_cpis_filtered, key = str_key_imf_cpis_filtered)\n",
    "#dict_dataset['imf_cpis'].loc[dict_dataset['imf_cpis'] < 0.0] = np.NaN\n",
    "#dict_dataset['oecd_fdi'] = pd.read_hdf(path_or_buf = str_path_oecd_fdi_combined, key = str_full_oecd_fdi_combined).droplevel('Market').sort_index()['Asset']\n",
    "#dict_dataset['oecd_fdi'].loc[dict_dataset['oecd_fdi'] < 0.0] = np.NaN\n",
    "#dict_dataset['oecd_fdi'] = pd.read_hdf(path_or_buf = str_path_direct_out_net, key = str_key_direct_out_net)\n",
    "#dict_dataset['oecd_fdi'].loc[dict_dataset['oecd_fdi'] < 0.0] = np.NaN\n",
    "#dict_dataset['bis_lbs'] = pd.read_hdf(path_or_buf = str_path_bis_lbs_combined, key = str_full_bis_lbs_combined)\\\n",
    "#                            .set_index(['Date', 'Reporter', 'Partner']).sort_index()['Claim_Augmented']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: GRAVITY DATASET CONSTRUCTION\n",
    "\n",
    "### GDP loading:\n",
    "ser_gdp = pd.read_hdf(path_or_buf = str_path_wb_gdp_dataset, key = str_wb_gdp_dataset)\n",
    "### Distances loading:\n",
    "ser_dist = pd.read_hdf(path_or_buf = str_path_cepii_dataset, key = str_distance_dataset)['distw']\n",
    "### Distances naming:\n",
    "ser_dist.index.names = ['Reporter', 'Partner']\n",
    "ser_dist.name = 'Distance'\n",
    "### Dropping internal distances:\n",
    "df_dist = ser_dist.reset_index()\n",
    "df_dist.drop(df_dist[df_dist['Reporter'] == df_dist['Partner']].index, inplace = True)\n",
    "ser_dist = df_dist.set_index(['Reporter', 'Partner']).squeeze().sort_index()\n",
    "### GDP duplicating:\n",
    "ser_gdp_reporter = ser_gdp[:]\n",
    "ser_gdp_reporter.index.names = ['Date', 'Reporter']\n",
    "ser_gdp_reporter.name = 'GDP_Reporter'\n",
    "ser_gdp_partner = ser_gdp[:]\n",
    "ser_gdp_partner.index.names = ['Date', 'Partner']\n",
    "ser_gdp_partner.name = 'GDP_Partner'\n",
    "### Reporters data connecting:\n",
    "df_reporter = ser_dist.to_frame().join(ser_gdp_reporter).sort_index()\n",
    "### Partners data connecting:\n",
    "df_partner = ser_dist.to_frame().join(ser_gdp_partner).drop('Distance', axis = 1).sort_index()\n",
    "df_partner = df_partner.reorder_levels([1, 0, 2])\n",
    "### Joining data and Gravity calculation:\n",
    "df_gravity = pd.concat([df_reporter, df_partner], axis = 1)\n",
    "df_gravity = df_gravity.reset_index('Date').dropna(subset = ['Date']).set_index('Date', append = True).reorder_levels([2, 0, 1]).sort_index()\n",
    "#display(df_gravity.loc[('2020-12-31', ['US', 'IL'], ['US', 'IL']), :])\n",
    "ser_gravity = (df_gravity['GDP_Reporter'] / 10 ** 9) * (df_gravity['GDP_Partner'] / 10 ** 9) / (df_gravity['Distance'] ** flo_dist_power)\n",
    "### Adding gravity to activities:\n",
    "dict_dataset['gravity'] = ser_gravity.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANNUALIZATION CONSTANTS INITIALIZATION\n",
    "\n",
    "gc.collect()\n",
    "### Lags initialization:\n",
    "dict_lag = {}\n",
    "dict_lag['imf_dots'] = 3\n",
    "#dict_lag['imf_cpis'] = 6\n",
    "#dict_lag['oecd_fdi'] = 24\n",
    "#dict_lag['bis_lbs'] = 6\n",
    "dict_lag['gravity'] = 9\n",
    "### Periods to fill initialization:\n",
    "dict_ffill = {}\n",
    "dict_ffill['imf_dots'] = 1\n",
    "#dict_ffill['imf_cpis'] = 12\n",
    "#dict_ffill['oecd_fdi'] = 12\n",
    "#dict_ffill['bis_lbs'] = 3\n",
    "dict_ffill['gravity'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "iter_activity = 'imf_dots'\n",
    "#iter_activity = 'bis_lbs'\n",
    "#iter_activity = 'gravity'\n",
    "int_lag = dict_lag[iter_activity]\n",
    "int_period = dict_ffill[iter_activity]\n",
    "\n",
    "ser_group = dict_dataset[iter_activity].loc[:, ['US'], ['CN']]\n",
    "\n",
    "if True:\n",
    "    ser_group = ser_group.droplevel(['Reporter', 'Partner'])\n",
    "    ser_group = ser_group.append(pd.Series(np.NaN, index = [ser_group.index[0] - pd.offsets.BMonthEnd(int_period)])).sort_index()\n",
    "    idx_dates = pd.date_range(ser_group.index[0], date_end, freq = 'BM')\n",
    "    ser_result = pd.Series(np.NaN, index = idx_dates)    \n",
    "    for iter_date in idx_dates:\n",
    "#    for iter_date in idx_dates[-5 :]:        \n",
    "#    for iter_date in [pd.to_datetime('1973-07-31')]:\n",
    "        ### Dates defining:\n",
    "        date_bm_end = iter_date + pd.offsets.BMonthEnd(0)\n",
    "        if (date_bm_end > iter_date):\n",
    "            date_bm_end = date_bm_end - pd.offsets.BMonthEnd(1)      \n",
    "        date_known_end = date_bm_end - pd.offsets.BMonthEnd(int_lag)\n",
    "        ser_to_date = ser_group.loc[date_known_end - pd.offsets.BMonthEnd(12 + int_period): date_known_end]\n",
    "        if len(ser_to_date >= 12):\n",
    "            ser_resampled = (ser_to_date / int_period).resample('BM').bfill()\n",
    "            ser_prolonged = ser_resampled[:-1].append(pd.Series(ser_resampled[-1], index = pd.date_range(ser_resampled.index[-1], date_bm_end, freq = 'BM')))\n",
    "            ser_annualized = ser_prolonged.rolling(12, 12).sum()\n",
    "            flo_result = ser_annualized[-1]\n",
    "            ser_result[iter_date] = ser_annualized[-1]      \n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "#print(iter_date.date())\n",
    "#print(ser_to_date)\n",
    "#print(ser_resampled)\n",
    "#print(ser_prolonged)\n",
    "#print(flo_result)\n",
    "#print(ser_prolonged[-12 :].sum())\n",
    "#ser_result.dropna()[-20 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Files/Source_Files/datasets_annualized.h5 File removed\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'iter_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5596/2356291941.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#    break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mser_iter_ann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Reporter'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Partner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mser_iter_ann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter_dataset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_ann'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mser_iter_ann\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_hdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_path_act_annualized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m': annualizing done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'iter_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "def annualize_with_lag(ser_group, int_lag, int_period):\n",
    "    ser_group = ser_group.droplevel(['Reporter', 'Partner'])\n",
    "    ser_group = ser_group.append(pd.Series(np.NaN, index = [ser_group.index[0] - pd.offsets.BMonthEnd(int_period)])).sort_index()\n",
    "    idx_dates = pd.date_range(ser_group.index[0], date_end, freq = 'BM')\n",
    "    ser_result = pd.Series(np.NaN, index = idx_dates)    \n",
    "    for iter_date in idx_dates:\n",
    "#    for iter_date in idx_dates[-5 :]:        \n",
    "#    for iter_date in [pd.to_datetime('1973-07-31')]:\n",
    "        ### Dates defining:\n",
    "        date_bm_end = iter_date + pd.offsets.BMonthEnd(0)\n",
    "        if (date_bm_end > iter_date):\n",
    "            date_bm_end = date_bm_end - pd.offsets.BMonthEnd(1)      \n",
    "        date_known_end = date_bm_end - pd.offsets.BMonthEnd(int_lag)\n",
    "        ser_to_date = ser_group.loc[date_known_end - pd.offsets.BMonthEnd(12 + int_period): date_known_end]\n",
    "        if len(ser_to_date >= 12):\n",
    "            ser_resampled = (ser_to_date / int_period).resample('BM').bfill()\n",
    "            ser_prolonged = ser_resampled[:-1].append(pd.Series(ser_resampled[-1], index = pd.date_range(ser_resampled.index[-1], date_bm_end, freq = 'BM')))\n",
    "            ser_annualized = ser_prolonged.rolling(12, 12).sum()\n",
    "#            flo_result = ser_annualized[-1]\n",
    "            ser_result[iter_date] = ser_annualized[-1]      \n",
    "#        break\n",
    "    ### Results output:\n",
    "    return ser_result\n",
    "### Deleting existing file with annualized data:\n",
    "if os.path.exists(str_path_act_annualized):\n",
    "    os.remove(str_path_act_annualized)\n",
    "    print(str_path_act_annualized, 'File removed')\n",
    "### Looping over activities:    \n",
    "for iter_activity in dict_lag:\n",
    "    ser_iter_raw = dict_dataset[iter_activity]#.loc[:, ['US'], ['CN']]\n",
    "    ser_iter_ann = ser_iter_raw.groupby(['Reporter', 'Partner']).apply(annualize_with_lag, dict_lag[iter_activity], dict_ffill[iter_activity])\\\n",
    "                                                                .astype('float32').reorder_levels([2, 0, 1]).sort_index()\n",
    "#    break\n",
    "    ser_iter_ann.index.names = ['Date', 'Reporter', 'Partner']\n",
    "    ser_iter_ann.name = iter_dataset + '_ann'\n",
    "    ser_iter_ann.to_hdf(str_path_act_annualized, iter_activity, mode = 'a', format = 'table')\n",
    "    print(iter_dataset, ': annualizing done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_iter_ann.to_hdf(str_path_act_annualized, iter_activity, mode = 'a', format = 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: GDP & OPENESS MEASURE ANNUALIZATION\n",
    "\n",
    "### Annualization of differewnt frequency data definition:\n",
    "def annualize_with_lag(ser_group, int_lag, int_period):\n",
    "    ser_group = ser_group.droplevel(['Country'])\n",
    "    ser_group = ser_group.append(pd.Series(np.NaN, index = [ser_group.index[0] - pd.offsets.BMonthEnd(int_period)])).sort_index()\n",
    "    idx_dates = pd.date_range(ser_group.index[0], date_end, freq = 'BM')\n",
    "    ser_result = pd.Series(np.NaN, index = idx_dates)    \n",
    "    for iter_date in idx_dates:\n",
    "#    for iter_date in idx_dates[-5 :]:        \n",
    "#    for iter_date in [pd.to_datetime('1973-07-31')]:\n",
    "        ### Dates defining:\n",
    "        date_bm_end = iter_date + pd.offsets.BMonthEnd(0)\n",
    "        if (date_bm_end > iter_date):\n",
    "            date_bm_end = date_bm_end - pd.offsets.BMonthEnd(1)      \n",
    "        date_known_end = date_bm_end - pd.offsets.BMonthEnd(int_lag)\n",
    "        ser_to_date = ser_group.loc[date_known_end - pd.offsets.BMonthEnd(12 + int_period): date_known_end]\n",
    "        if len(ser_to_date >= 12):\n",
    "            ser_resampled = (ser_to_date / int_period).resample('BM').bfill()\n",
    "            ser_prolonged = ser_resampled[:-1].append(pd.Series(ser_resampled[-1], index = pd.date_range(ser_resampled.index[-1], date_bm_end, freq = 'BM')))\n",
    "            ser_annualized = ser_prolonged.rolling(12, 12).sum()\n",
    "#            flo_result = ser_annualized[-1]\n",
    "            ser_result[iter_date] = ser_annualized[-1]      \n",
    "#        break\n",
    "    ### Results output:\n",
    "    return ser_result\n",
    "### GDP Loading:\n",
    "ser_gdp = pd.read_hdf(path_or_buf = str_path_wb_gdp_dataset, key = str_wb_gdp_dataset)\n",
    "### GDP annualization:\n",
    "ser_gdp_ann = (ser_gdp.groupby(['Country']).apply(annualize_with_lag, dict_lag['gravity'], dict_ffill['gravity'])\\\n",
    "                                           .astype('float32') / 1000000).reorder_levels([1, 0]).sort_index()\n",
    "ser_gdp_ann.index.names = ['Date', 'Country']\n",
    "ser_gdp_ann.to_hdf(str_path_gravity_results, str_key_gdp_total, mode = 'a')\n",
    "### Export data loading & converting:\n",
    "ser_export_ann = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_dots')\n",
    "### Export sum calculation:\n",
    "ser_export_sum = ser_export_ann.groupby(['Date', 'Reporter']).sum()\n",
    "ser_export_sum.index.names = ['Date', 'Country']\n",
    "### Openess measure saving:\n",
    "(ser_export_sum / ser_gdp_ann).to_hdf(str_path_gravity_results, str_key_openess, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: WORLD EXPORT ANNUALIZATION\n",
    "\n",
    "### Annualization of different frequency data definition:\n",
    "def annualize_with_lag(ser_group, int_lag, int_period):\n",
    "    ser_group = ser_group.droplevel(['Reporter'])\n",
    "    ser_group = ser_group.append(pd.Series(np.NaN, index = [ser_group.index[0] - pd.offsets.BMonthEnd(int_period)])).sort_index()\n",
    "    idx_dates = pd.date_range(ser_group.index[0], date_end, freq = 'BM')\n",
    "    ser_result = pd.Series(np.NaN, index = idx_dates)    \n",
    "    for iter_date in idx_dates:\n",
    "#    for iter_date in idx_dates[-5 :]:        \n",
    "#    for iter_date in [pd.to_datetime('1973-07-31')]:\n",
    "        ### Dates defining:\n",
    "        date_bm_end = iter_date + pd.offsets.BMonthEnd(0)\n",
    "        if (date_bm_end > iter_date):\n",
    "            date_bm_end = date_bm_end - pd.offsets.BMonthEnd(1)      \n",
    "        date_known_end = date_bm_end - pd.offsets.BMonthEnd(int_lag)\n",
    "        ser_to_date = ser_group.loc[date_known_end - pd.offsets.BMonthEnd(12 + int_period): date_known_end]\n",
    "        if len(ser_to_date >= 12):\n",
    "            ser_resampled = (ser_to_date / int_period).resample('BM').bfill()\n",
    "            ser_prolonged = ser_resampled[:-1].append(pd.Series(ser_resampled[-1], index = pd.date_range(ser_resampled.index[-1], date_bm_end, freq = 'BM')))\n",
    "            ser_annualized = ser_prolonged.rolling(12, 12).sum()\n",
    "#            flo_result = ser_annualized[-1]\n",
    "            ser_result[iter_date] = ser_annualized[-1]      \n",
    "#        break\n",
    "    ### Results output:\n",
    "    return ser_result\n",
    "### World Export Loading:\n",
    "ser_world_export = pd.read_hdf(path_or_buf = str_path_imf_dots_world, key = str_full_imf_dots_world)\n",
    "### World Export annualization:\n",
    "ser_world_export_ann = ser_world_export.groupby(['Reporter']).apply(annualize_with_lag, dict_lag['imf_dots'], dict_ffill['imf_dots'])\\\n",
    "                                                             .astype('float32').reorder_levels([1, 0]).sort_index()\n",
    "ser_world_export_ann.index.names = ['Date', 'Reporter']\n",
    "ser_world_export_ann.to_hdf(str_path_world_export_annualized, str_key_world_export_annualized, mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
