{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: GRAVITY DATASETS EXPLORING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import gc\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  0.25.3\n",
      "python version:  3.7.4\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: MAIN CONSTANTS\n",
    "\n",
    "### MultiIndex level slice constant:\n",
    "All = slice(None)\n",
    "### Regions list:\n",
    "list_regions = ['DM', 'EM', 'FM']\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Activities naming:\n",
    "dict_activity = {}\n",
    "dict_activity['imf_dots'] = 'Trade Export'\n",
    "dict_activity['imf_cpis'] = 'Portfolio Investment'\n",
    "dict_activity['oecd_fdi'] = 'Direct Investment'\n",
    "dict_activity['bis_lbs'] = 'Bank Lending'\n",
    "dict_activity['gravity'] = 'Gravity'\n",
    "### CEPII dataset:\n",
    "str_path_cepii_dataset = 'Data_Files/Source_Files/cepii_dataset.h5'\n",
    "str_distance_dataset = 'distance_dataset'\n",
    "### WB WDI GDP dataset:\n",
    "str_path_wb_gdp_dataset = 'Data_Files/Source_Files/gdp_dataset.h5'\n",
    "str_wb_gdp_dataset = 'gdp_dataset'\n",
    "### BIS Loans dataset:\n",
    "str_path_bis_lbs_combined = 'Data_Files/Source_Files/bis_combined.h5'\n",
    "str_full_bis_lbs_combined = 'bis_full_combined'\n",
    "### IMF CPIS dataset:\n",
    "str_path_imf_cpis_combined = 'Data_Files/Source_Files/cpis_combined.h5'\n",
    "str_full_imf_cpis_combined = 'cpis_full_combined'\n",
    "### Filtered IMF CPIS dataset:\n",
    "str_path_imf_cpis_filtered = 'Data_Files/Source_Files/cpis_filtered.h5'\n",
    "str_key_imf_cpis_filtered = 'cpis_filtered'\n",
    "### IMF DOTS datasets:\n",
    "str_path_imf_dots_combined = 'Data_Files/Source_Files/dots_combined.h5'\n",
    "str_full_imf_dots_combined = 'dots_full_combined'\n",
    "str_path_imf_dots_world = 'Data_Files/Source_Files/dots_world_export.h5'\n",
    "str_full_imf_dots_world = 'dots_world_export'\n",
    "### OECD FDI dataset:\n",
    "str_path_oecd_fdi_combined = 'Data_Files/Source_Files/oecd_combined.h5'\n",
    "str_full_oecd_fdi_combined = 'oecd_full_combined'\n",
    "str_path_direct_out_net = 'Data_Files/Source_Files/direct_outward_net.h5'\n",
    "str_key_direct_out_net = 'direct_outward'\n",
    "### Technical Constants:\n",
    "date_start = pd.Timestamp('1989-12-29')\n",
    "str_date_end = '2021-12-31'\n",
    "date_end = pd.Timestamp(str_date_end)\n",
    "date_ison = pd.Timestamp('1994-12-31')\n",
    "### Distance power for gravity calculation:\n",
    "flo_dist_power = 1 / 2\n",
    "### Bloomberg structured data extraction parameters:\n",
    "str_path_bb_hdf = 'Data_Files/Source_Files/Bloomberg_prepared.h5'\n",
    "str_key_ret_daily = 'bb_ret_daily'\n",
    "str_ret_daily_csv_path = 'Data_Files/Source_Files/ret_daily.csv'\n",
    "### Saved annualized activities:\n",
    "str_path_act_annualized = 'Data_Files/Source_Files/datasets_annualized.h5'\n",
    "str_path_act_weights = 'Data_Files/Source_Files/datasets_weights.h5'\n",
    "str_path_world_export_annualized = 'Data_Files/Source_Files/world_export_annualized.h5'\n",
    "str_key_world_export_annualized = 'world_export_ann'\n",
    "### Herfindahl index threshold:\n",
    "flo_hi_threshold = 0.0 # 1.1\n",
    "### Saved herfindahl values:\n",
    "str_path_herfindahl = 'Data_Files/Source_Files/herfindahl_indices.h5'\n",
    "### Returns average parameters:\n",
    "int_ave_months = 6\n",
    "int_halflife_months = 2\n",
    "### Thresholds for weighted cross-sectional average returns calculation:\n",
    "int_select_top = 3\n",
    "flo_select_share = 0.05\n",
    "### Saved weighted returns:\n",
    "str_path_ret_weighted = 'Data_Files/Source_Files/returns_weighted.h5'\n",
    "str_key_weighted = 'ret_weighted'\n",
    "### Saved export data:\n",
    "str_path_gravity_results = 'Data_Files/Source_Files/gravity_export.h5'\n",
    "str_key_activity_sum = 'activity_sum'\n",
    "str_key_activity_share = 'activity_share'\n",
    "str_key_gdp_total = 'gdp_total'\n",
    "str_key_herfindahl = 'herfindahl_index'\n",
    "str_key_openess = 'openess_measure'\n",
    "str_key_ret_weighted = 'ret_weighted'\n",
    "### CSV to Export data:\n",
    "str_activity_sum_csv_path = 'Data_Files/Test_Files/activity_sum.csv'\n",
    "str_activity_share_csv_path = 'Data_Files/Test_Files/activity_share.csv'\n",
    "str_gdp_total_csv_path = 'Data_Files/Test_Files/gdp_total.csv'\n",
    "str_herfindahl_csv_path = 'Data_Files/Test_Files/herfindahl.csv'\n",
    "str_openess_csv_path = 'Data_Files/Test_Files/openess.csv'\n",
    "str_ret_weighted_csv_path = 'Data_Files/Test_Files/ret_weighted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING COUNTRY CODES EXTRACTOR\n",
    "\n",
    "def get_country_codes(use_local_copy = False):  \n",
    "    ### In case if URL is unavailable:\n",
    "    if (use_local_copy):\n",
    "        url_country_code = 'Data_Files/Source_Files/countrycode.html'\n",
    "    ### Online extraction:\n",
    "    else:\n",
    "        url_country_code = 'https://countrycode.org/'\n",
    "    df_full_codes = pd.read_html(url_country_code, index_col = 'COUNTRY')[0]\n",
    "    df_full_codes[['ISO SHORT', 'ISO LONG']] = df_full_codes['ISO CODES'].str.split(' / ', expand = True)\n",
    "    df_result = df_full_codes[['ISO SHORT', 'ISO LONG']].sort_index()    \n",
    "    df_result.index = df_result.index.str.upper()\n",
    "    ### Results output:\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(engine = 'openpyxl', io = str_path_universe, sheet_name = 'Switchers', header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RUN EVERY TIME: DATASETS LOADING\n",
    "\n",
    "dict_dataset = {}\n",
    "dict_dataset['imf_dots'] = pd.read_hdf(path_or_buf = str_path_imf_dots_combined, key = str_full_imf_dots_combined).droplevel('Market').sort_index()['Export_Augmented']\n",
    "dict_dataset['imf_cpis'] = pd.read_hdf(path_or_buf = str_path_imf_cpis_combined, key = str_full_imf_cpis_combined).droplevel('Market').sort_index()['Asset_Augmented']\n",
    "#dict_dataset['imf_cpis'] = pd.read_hdf(path_or_buf = str_path_imf_cpis_filtered, key = str_key_imf_cpis_filtered)\n",
    "#dict_dataset['imf_cpis'].loc[dict_dataset['imf_cpis'] < 0.0] = np.NaN\n",
    "#dict_dataset['oecd_fdi'] = pd.read_hdf(path_or_buf = str_path_oecd_fdi_combined, key = str_full_oecd_fdi_combined).droplevel('Market').sort_index()['Asset']\n",
    "#dict_dataset['oecd_fdi'].loc[dict_dataset['oecd_fdi'] < 0.0] = np.NaN\n",
    "#dict_dataset['oecd_fdi'] = pd.read_hdf(path_or_buf = str_path_direct_out_net, key = str_key_direct_out_net)\n",
    "#dict_dataset['oecd_fdi'].loc[dict_dataset['oecd_fdi'] < 0.0] = np.NaN\n",
    "#dict_dataset['bis_lbs'] = pd.read_hdf(path_or_buf = str_path_bis_lbs_combined, key = str_full_bis_lbs_combined)\\\n",
    "#    .set_index(['Date', 'Reporter', 'Partner']).sort_index()['Claim_Augmented']\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Distance</th>\n",
       "      <th>GDP_Reporter</th>\n",
       "      <th>GDP_Partner</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Reporter</th>\n",
       "      <th>Partner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2020-12-31</th>\n",
       "      <th>IL</th>\n",
       "      <th>US</th>\n",
       "      <td>10516.0</td>\n",
       "      <td>4.132677e+11</td>\n",
       "      <td>2.106047e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US</th>\n",
       "      <th>IL</th>\n",
       "      <td>10516.0</td>\n",
       "      <td>2.106047e+13</td>\n",
       "      <td>4.132677e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Distance  GDP_Reporter   GDP_Partner\n",
       "Date       Reporter Partner                                      \n",
       "2020-12-31 IL       US        10516.0  4.132677e+11  2.106047e+13\n",
       "           US       IL        10516.0  2.106047e+13  4.132677e+11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### RUN EVERY TIME: GRAVITY DATASET CONSTRUCTION\n",
    "\n",
    "### GDP loading:\n",
    "ser_gdp = pd.read_hdf(path_or_buf = str_path_wb_gdp_dataset, key = str_wb_gdp_dataset)\n",
    "### Distances loading:\n",
    "ser_dist = pd.read_hdf(path_or_buf = str_path_cepii_dataset, key = str_distance_dataset)['distw']\n",
    "### Distances naming:\n",
    "ser_dist.index.names = ['Reporter', 'Partner']\n",
    "ser_dist.name = 'Distance'\n",
    "### Dropping internal distances:\n",
    "df_dist = ser_dist.reset_index()\n",
    "df_dist.drop(df_dist[df_dist['Reporter'] == df_dist['Partner']].index, inplace = True)\n",
    "ser_dist = df_dist.set_index(['Reporter', 'Partner']).squeeze().sort_index()\n",
    "### GDP duplicating:\n",
    "ser_gdp_reporter = ser_gdp[:]\n",
    "ser_gdp_reporter.index.names = ['Date', 'Reporter']\n",
    "ser_gdp_reporter.name = 'GDP_Reporter'\n",
    "ser_gdp_partner = ser_gdp[:]\n",
    "ser_gdp_partner.index.names = ['Date', 'Partner']\n",
    "ser_gdp_partner.name = 'GDP_Partner'\n",
    "### Reporters data connecting:\n",
    "df_reporter = ser_dist.to_frame().join(ser_gdp_reporter).sort_index()\n",
    "### Partners data connecting:\n",
    "df_partner = ser_dist.to_frame().join(ser_gdp_partner).drop('Distance', axis = 1).sort_index()\n",
    "df_partner = df_partner.reorder_levels([1, 0, 2])\n",
    "### Joining data and Gravity calculation:\n",
    "df_gravity = pd.concat([df_reporter, df_partner], axis = 1)\n",
    "df_gravity = df_gravity.reset_index('Date').dropna(subset = ['Date']).set_index('Date', append = True).reorder_levels([2, 0, 1]).sort_index()\n",
    "display(df_gravity.loc[('2020-12-31', ['US', 'IL'], ['US', 'IL']), :])\n",
    "ser_gravity = (df_gravity['GDP_Reporter'] / 10 ** 9) * (df_gravity['GDP_Partner'] / 10 ** 9) / (df_gravity['Distance'] ** flo_dist_power)\n",
    "### Adding gravity to activities:\n",
    "dict_dataset['gravity'] = ser_gravity.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Reporter  Partner\n",
       "1989-04-28  CI        FI              NaN\n",
       "1989-05-31  CI        FI              NaN\n",
       "1989-06-30  CI        FI              NaN\n",
       "1989-07-31  CI        FI              NaN\n",
       "1989-08-31  CI        FI              NaN\n",
       "                                   ...   \n",
       "2022-03-31  ZM        PH            0.307\n",
       "                      SE            4.323\n",
       "                      TW            0.559\n",
       "                      US         3721.000\n",
       "                      ZA          960.000\n",
       "Name: Claim_Augmented, Length: 886292, dtype: float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "gc.collect()\n",
    "dict_annualized = {}\n",
    "def annualize(ser_group):\n",
    "    ser_ann = ser_group.droplevel(['Reporter', 'Partner'])\n",
    "    ### Filter not empty pairs:\n",
    "    if (ser_ann.count() > 0):\n",
    "        ### Data frequency calculation (observations per year):\n",
    "        ser_freq = ser_ann.dropna().resample('BY').count()\n",
    "        int_freq = int(ser_freq[ser_freq > 0].median())\n",
    "        ### Data periodicity definition (months number covered per one observation):\n",
    "        int_period = 12 // int_freq\n",
    "        ### Convert data to monthly frequency:\n",
    "        if (int_period > 1):\n",
    "            ### Prepending one more period to backfill first valid observation:\n",
    "            ser_ann = ser_ann.append(pd.Series(np.NaN, index = [ser_ann.index[0] - pd.offsets.BMonthEnd(int_period)])).sort_index()    \n",
    "            ### Replace period value to monthly value:\n",
    "            ser_ann = (ser_ann / int_period)\n",
    "            ### Backfill monthly value for a whole period:\n",
    "            ser_ann = ser_ann.resample('BM').bfill(limit = int_period)\n",
    "            ### Drop dummy observation:\n",
    "            ser_ann.drop(ser_ann.index[0], inplace = True)\n",
    "        ### Annualize data:\n",
    "        ser_ann = ser_ann.rolling(window = 12, min_periods = 12).sum()\n",
    "    ### Results output:\n",
    "    return ser_ann\n",
    "\n",
    "dict_dataset['bis_lbs'].groupby(['Reporter', 'Partner']).apply(annualize).astype('float32').reorder_levels([2, 0, 1]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Files/Source_Files/datasets_annualized.h5 File removed\n",
      "imf_dots : annualizing started\n",
      "imf_dots : annualizing done\n",
      "imf_cpis : annualizing started\n",
      "imf_cpis : annualizing done\n",
      "oecd_fdi : annualizing started\n",
      "oecd_fdi : annualizing done\n",
      "gravity : annualizing started\n",
      "gravity : annualizing done\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: ACTIVITIES ANNUALIZATION\n",
    "\n",
    "gc.collect()\n",
    "dict_annualized = {}\n",
    "def annualize(ser_group):\n",
    "    ser_ann = ser_group.droplevel(['Reporter', 'Partner'])\n",
    "    ### Filter not empty pairs:\n",
    "    if (ser_ann.count() > 0):\n",
    "        ### Data frequency calculation (observations per year):\n",
    "        ser_freq = ser_ann.dropna().resample('BY').count()\n",
    "        int_freq = int(ser_freq[ser_freq > 0].median())\n",
    "        ### Data periodicity definition (months number covered per one observation):\n",
    "        int_period = 12 // int_freq\n",
    "        ### Convert data to monthly frequency:\n",
    "        if (int_period > 1):\n",
    "            ### Prepending one more period to backfill first valid observation:\n",
    "            ser_ann = ser_ann.append(pd.Series(np.NaN, index = [ser_ann.index[0] - pd.offsets.BMonthEnd(int_period)])).sort_index()    \n",
    "            ### Replace period value to monthly value:\n",
    "            ser_ann = (ser_ann / int_period)\n",
    "            ### Backfill monthly value for a whole period:\n",
    "            ser_ann = ser_ann.resample('BM').bfill(limit = int_period - 1)\n",
    "            ### Drop dummy observation:\n",
    "            ser_ann.drop(ser_ann.index[0], inplace = True)\n",
    "        ### Annualize data:\n",
    "        ser_ann = ser_ann.rolling(window = 12, min_periods = 12).sum()\n",
    "    ### Results output:\n",
    "    return ser_ann\n",
    "### Deleting existing file with annualized data:\n",
    "if os.path.exists(str_path_act_annualized):\n",
    "    os.remove(str_path_act_annualized)\n",
    "    print(str_path_act_annualized, 'File removed')\n",
    "### Looping over activities:\n",
    "for iter_dataset in dict_dataset:\n",
    "#for iter_dataset in ['imf_cpis', 'oecd_fdi']:\n",
    "    gc.collect()\n",
    "    print(iter_dataset, ': annualizing started')    \n",
    "    ser_iter = dict_dataset[iter_dataset]\n",
    "    ser_iter_ann = ser_iter.groupby(['Reporter', 'Partner']).apply(annualize).astype('float32').reorder_levels([2, 0, 1]).sort_index()\n",
    "    ser_iter_ann.index.names = ['Date', 'Reporter', 'Partner']\n",
    "    ser_iter_ann.name = iter_dataset + '_ann'\n",
    "    ser_iter_ann.to_hdf(str_path_act_annualized, iter_dataset, mode = 'a', format = 'table')\n",
    "    print(iter_dataset, ': annualizing done')    \n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: WEIGHTED AVERAGE OF RETURNS CALCULATION: DATA PREPARATION\n",
    "\n",
    "gc.collect()\n",
    "### Lags initialization:\n",
    "dict_lag = {}\n",
    "dict_lag['imf_dots'] = 3\n",
    "dict_lag['imf_cpis'] = 6\n",
    "dict_lag['oecd_fdi'] = 24\n",
    "#dict_lag['bis_lbs'] = 6\n",
    "dict_lag['gravity'] = 9\n",
    "### Periods to fill initialization:\n",
    "dict_ffill = {}\n",
    "dict_ffill['imf_dots'] = 1\n",
    "dict_ffill['imf_cpis'] = 12\n",
    "dict_ffill['oecd_fdi'] = 12\n",
    "#dict_ffill['bis_lbs'] = 3\n",
    "dict_ffill['gravity'] = 12\n",
    "### Annualized data loading:\n",
    "dict_annual = {}\n",
    "dict_annual['imf_dots'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_dots')\n",
    "dict_annual['imf_cpis'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_cpis')\n",
    "dict_annual['oecd_fdi'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'oecd_fdi')\n",
    "#dict_annual['bis_lbs'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'bis_lbs')\n",
    "dict_annual['gravity'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'gravity')\n",
    "### Returns loading & shifting:\n",
    "### HDF version:\n",
    "#ser_ret_usd = pd.read_hdf(str_path_bb_hdf, str_key_ret_daily).loc['USD']\n",
    "### CSV version:\n",
    "df_ret_usd = pd.read_csv(str_ret_daily_csv_path, header = None, sep = ';', parse_dates = [0])\n",
    "df_ret_usd.columns = ['Date', 'Country', 'Value']\n",
    "ser_ret_usd = df_ret_usd.set_index(['Date', 'Country']).squeeze()\n",
    "###\n",
    "ser_ret_shifted = ser_ret_usd.groupby('Country').shift()\n",
    "ser_ret_shifted.index.names = ['Date', 'Partner']\n",
    "ser_ret_shifted.name = 'ret_usd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: WEIGHTED AVERAGE OF RETURNS CALCULATION: AVERAGE RETURNS PREPARATION\n",
    "\n",
    "gc.collect()\n",
    "### Weighted mean for end-of-bmonth dates defining:\n",
    "def get_weighted_mean(ser_group):\n",
    "    ser_values = ser_group.droplevel('Partner')\n",
    "    if (ser_values.index[-1] == ser_values.index[-1] + pd.offsets.BMonthEnd(0)):\n",
    "        ser_weights = pd.Series(list_weight[-len(ser_values.index) : ], ser_values.index)\n",
    "        flo_result = weighted_average(ser_values, ser_weights)\n",
    "    else:\n",
    "        flo_result = np.NaN\n",
    "    return flo_result\n",
    "### Weights defining:\n",
    "list_weight = list(map(lambda iter_num: exp_weight_single(int_halflife_months * 22, iter_num), range(int_ave_months * 22)))[::-1]\n",
    "### Mean returns calculation:\n",
    "#ser_test = ser_ret_shifted.loc[:, ['US', 'IL']]#[-132 : ]\n",
    "ser_ret_ave = ser_ret_shifted.groupby('Partner', group_keys = False)\\\n",
    "                             .rolling(window = int_ave_months * 22, min_periods = int_ave_months * 22 // 2)\\\n",
    "                             .apply(get_weighted_mean, raw = False).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: GDP & OPENESS MEASURE ANNUALIZATION\n",
    "\n",
    "### Annualization of differewnt frequency data definition:\n",
    "def annualize(ser_group):\n",
    "    ser_ann = ser_group.droplevel(['Country'])\n",
    "    ### Filter not empty pairs:\n",
    "    if (ser_ann.count() > 0):\n",
    "        ### Data frequency calculation (observations per year):\n",
    "        ser_freq = ser_ann.dropna().resample('BY').count()\n",
    "        int_freq = int(ser_freq[ser_freq > 0].median())\n",
    "        ### Data periodicity definition (months number covered per one observation):\n",
    "        int_period = 12 // int_freq\n",
    "        ### Convert data to monthly frequency:\n",
    "        if (int_period > 1):\n",
    "            ### Prepending one more period to backfill first valid observation:\n",
    "            ser_ann = ser_ann.append(pd.Series(np.NaN, index = [ser_ann.index[0] - pd.offsets.BMonthEnd(int_period)])).sort_index()    \n",
    "            ### Replace period value to monthly value:\n",
    "            ser_ann = (ser_ann / int_period)\n",
    "            ### Backfill monthly value for a whole period:\n",
    "            ser_ann = ser_ann.resample('BM').bfill(limit = int_period - 1)\n",
    "            ### Drop dummy observation:\n",
    "            ser_ann.drop(ser_ann.index[0], inplace = True)\n",
    "        ### Annualize data:\n",
    "        ser_ann = ser_ann.rolling(window = 12, min_periods = 12).sum()\n",
    "    ### Results output:\n",
    "    return ser_ann\n",
    "### GDP Loading:\n",
    "ser_gdp = pd.read_hdf(path_or_buf = str_path_wb_gdp_dataset, key = str_wb_gdp_dataset)\n",
    "### GDP annualization:\n",
    "ser_gdp_ann = (ser_gdp.groupby(['Country']).apply(annualize).astype('float32') / 1000000).reorder_levels([1, 0]).sort_index()\n",
    "ser_gdp_ann.index.names = ['Date', 'Country']\n",
    "ser_gdp_ann.to_hdf(str_path_gravity_results, str_key_gdp_total, mode = 'a')\n",
    "### Export data loading & converting:\n",
    "ser_export_ann = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_dots')\n",
    "### Added to exclude trade bilaterals for partners that don't have returns data:\n",
    "idx_ret_exist = ser_ret_ave.dropna().index\n",
    "ser_export_ann = ser_export_ann.groupby('Reporter').apply(lambda ser_country: ser_country.droplevel('Reporter').reindex(idx_ret_exist))\\\n",
    "                               .reorder_levels(['Date', 'Reporter', 'Partner']).sort_index()\n",
    "### Export sum calculation:\n",
    "ser_export_sum = ser_export_ann.groupby(['Date', 'Reporter']).sum()\n",
    "ser_export_sum.index.names = ['Date', 'Country']\n",
    "### Openess measure saving:\n",
    "(ser_export_sum / ser_gdp_ann).to_hdf(str_path_gravity_results, str_key_openess, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: WORLD EXPORT ANNUALIZATION\n",
    "\n",
    "### Annualization of different frequency data definition:\n",
    "def annualize(ser_group):\n",
    "    ser_ann = ser_group.droplevel(['Reporter'])\n",
    "    ### Filter not empty pairs:\n",
    "    if (ser_ann.count() > 0):\n",
    "        ### Data frequency calculation (observations per year):\n",
    "        ser_freq = ser_ann.dropna().resample('BY').count()\n",
    "        int_freq = int(ser_freq[ser_freq > 0].median())\n",
    "        ### Data periodicity definition (months number covered per one observation):\n",
    "        int_period = 12 // int_freq\n",
    "        ### Convert data to monthly frequency:\n",
    "        if (int_period > 1):\n",
    "            ### Prepending one more period to backfill first valid observation:\n",
    "            ser_ann = ser_ann.append(pd.Series(np.NaN, index = [ser_ann.index[0] - pd.offsets.BMonthEnd(int_period)])).sort_index()    \n",
    "            ### Replace period value to monthly value:\n",
    "            ser_ann = (ser_ann / int_period)\n",
    "            ### Backfill monthly value for a whole period:\n",
    "            ser_ann = ser_ann.resample('BM').bfill(limit = int_period - 1)\n",
    "            ### Drop dummy observation:\n",
    "            ser_ann.drop(ser_ann.index[0], inplace = True)\n",
    "        ### Annualize data:\n",
    "        ser_ann = ser_ann.rolling(window = 12, min_periods = 12).sum()\n",
    "    ### Results output:\n",
    "    return ser_ann\n",
    "### World Export Loading:\n",
    "ser_world_export = pd.read_hdf(path_or_buf = str_path_imf_dots_world, key = str_full_imf_dots_world)\n",
    "### World Export annualization:\n",
    "ser_world_export_ann = (ser_world_export.groupby(['Reporter']).apply(annualize).astype('float32')).reorder_levels([1, 0]).sort_index()\n",
    "ser_world_export_ann.index.names = ['Date', 'Reporter']\n",
    "ser_world_export_ann.to_hdf(str_path_world_export_annualized, str_key_world_export_annualized, mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Closing remaining open files:Data_Files/Source_Files/datasets_annualized.h5...done\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: CROSS SECTIONAL ACTIVITY TOTALS & WEIGHTS BY COUNTRY SAVING\n",
    "\n",
    "gc.collect()\n",
    "### Annualized data loading:\n",
    "dict_annual = {}\n",
    "dict_annual['imf_dots'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_dots')\n",
    "dict_annual['imf_cpis'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_cpis')\n",
    "dict_annual['oecd_fdi'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'oecd_fdi')\n",
    "#dict_annual['bis_lbs'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'bis_lbs')\n",
    "#dict_annual['gravity'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'gravity')\n",
    "### X-sectional sum by Country:\n",
    "dict_country_sum = {}\n",
    "for iter_activity in dict_annual:\n",
    "    ### Modified to exlcude partners that don't have returns:\n",
    "    idx_ret_exist = ser_ret_ave.dropna().index\n",
    "    ser_iter_activity = dict_annual[iter_activity]\n",
    "    ser_iter_activity = ser_iter_activity.groupby('Reporter').apply(lambda ser_country: ser_country.droplevel('Reporter').reindex(idx_ret_exist))\\\n",
    "                                         .reorder_levels(['Date', 'Reporter', 'Partner']).sort_index()    \n",
    "    df_iter_sum = ser_iter_activity.groupby(['Date', 'Reporter']).sum().reset_index('Reporter')\n",
    "#    df_iter_sum = dict_annual[iter_activity].groupby(['Date', 'Reporter']).sum().reset_index('Reporter')\n",
    "    df_iter_sum['Reporter'] = df_iter_sum['Reporter'].astype(str)\n",
    "    ser_iter_sum = df_iter_sum.set_index('Reporter', append = True).squeeze().sort_index()\n",
    "    ser_iter_sum.name = 'Volume'\n",
    "    dict_country_sum[iter_activity] = ser_iter_sum\n",
    "#    break\n",
    "### Reporter data aggregation:\n",
    "ser_country_sum = pd.concat(dict_country_sum, axis = 0, sort = False, names = ['Activity'])\n",
    "ser_country_sum.reorder_levels([1, 2, 0]).to_hdf(str_path_gravity_results, key = str_key_activity_sum, mode = 'a')\n",
    "ser_country_share = ser_country_sum.loc[list(dict_country_sum.keys())[:-1]].groupby(['Date', 'Reporter']).apply(lambda ser_group: ser_group / ser_group.sum())\n",
    "ser_country_share.reorder_levels([1, 2, 0]).to_hdf(str_path_gravity_results, key = str_key_activity_share, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Files/Source_Files/herfindahl_indices.h5 File removed\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: HERFINDAHL INDEX CALСULATION\n",
    "\n",
    "gc.collect()\n",
    "### Annualized data loading:\n",
    "dict_annual = {}\n",
    "dict_annual['imf_dots'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_dots')\n",
    "dict_annual['imf_cpis'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_cpis')\n",
    "dict_annual['oecd_fdi'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'oecd_fdi')\n",
    "#dict_annual['bis_lbs'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'bis_lbs')\n",
    "dict_annual['gravity'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'gravity')\n",
    "\n",
    "### Defining Herfindahl index calculation:\n",
    "def get_herfindahl(ser_group):\n",
    "    if (ser_group.count() > 0):\n",
    "        ser_norm = ser_group / ser_group.sum()\n",
    "        flo_herfindahl = 1 / ((ser_norm ** 2).sum() ** (1 / 2))\n",
    "    else:\n",
    "        flo_herfindahl = np.NaN\n",
    "    return flo_herfindahl\n",
    "### Deleting existing file with annualized data:\n",
    "if os.path.exists(str_path_herfindahl):\n",
    "    os.remove(str_path_herfindahl)\n",
    "    print(str_path_herfindahl, 'File removed')\n",
    "### Looping over activities datasets:\n",
    "for iter_dataset in dict_annual:\n",
    "#for iter_dataset in ['imf_cpis']:    \n",
    "    ### Herfindahl index calculation:\n",
    "    ser_herfindahl_full = dict_annual[iter_dataset].groupby(['Date', 'Reporter']).apply(get_herfindahl)\n",
    "    ser_herfindahl_full.to_hdf(str_path_herfindahl, iter_dataset, mode = 'a', format = 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: ADDING HERFINDAHL INDICES TO DATA COLLECTION\n",
    "\n",
    "dict_herfindahl = {}\n",
    "for iter_activity in dict_activity:\n",
    "    ser_iter_herfindahl = pd.read_hdf(str_path_herfindahl, iter_activity)\n",
    "    df_iter_herfindahl = ser_iter_herfindahl.reset_index('Reporter')\n",
    "    df_iter_herfindahl['Country'] = df_iter_herfindahl['Reporter'].astype(str)\n",
    "    dict_herfindahl[iter_activity] = df_iter_herfindahl.drop('Reporter', axis = 1).set_index('Country', append = True).squeeze()\n",
    "ser_full_herfindahl = pd.concat(dict_herfindahl, axis = 0).reorder_levels([1, 2, 0])\n",
    "ser_full_herfindahl.index.names = ['Date', 'Country', 'Activity']\n",
    "ser_full_herfindahl.to_hdf(str_path_gravity_results, str_key_herfindahl, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data_Files/Source_Files/datasets_weights.h5 File removed\n",
      "imf_dots : weights calculation started\n",
      "imf_dots : weights calculation done\n",
      "imf_cpis : weights calculation started\n",
      "imf_cpis : weights calculation done\n",
      "oecd_fdi : weights calculation started\n",
      "oecd_fdi : weights calculation done\n",
      "bis_lbs : weights calculation started\n",
      "bis_lbs : weights calculation done\n",
      "gravity : weights calculation started\n",
      "gravity : weights calculation done\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: REPLACING VALUES WITH WEIGHTS WHILE CONTROLLING HERFINDAL INDEX VALUE\n",
    "\n",
    "gc.collect()\n",
    "### Annualized data loading:\n",
    "dict_annual = {}\n",
    "dict_annual['imf_dots'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_dots')\n",
    "dict_annual['imf_cpis'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'imf_cpis')\n",
    "dict_annual['oecd_fdi'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'oecd_fdi')\n",
    "dict_annual['bis_lbs'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'bis_lbs')\n",
    "dict_annual['gravity'] = pd.read_hdf(path_or_buf = str_path_act_annualized, key = 'gravity')\n",
    "### Weights container:\n",
    "dict_weights = {}\n",
    "### Defining Herfindahl index calculation:\n",
    "def set_weights(ser_group, flo_limit = 1.0):\n",
    "    if (ser_group.count() > 0):\n",
    "        ser_norm = ser_group / ser_group.sum()\n",
    "        flo_herfindahl = 1 / ((ser_norm ** 2).sum() ** (1 / 2))\n",
    "        if (flo_herfindahl < flo_limit):\n",
    "            ser_weights = pd.Series(np.NaN, index = ser_group.index)\n",
    "        else:\n",
    "            ser_weights = ser_norm\n",
    "    else:\n",
    "        ser_weights = ser_group\n",
    "    return ser_weights\n",
    "### Deleting existing file with weights:\n",
    "if os.path.exists(str_path_act_weights):\n",
    "    os.remove(str_path_act_weights)\n",
    "    print(str_path_act_weights, 'File removed')\n",
    "### Looping over activities datasets:\n",
    "for iter_dataset in dict_annual:\n",
    "#for iter_dataset in ['imf_cpis']:    \n",
    "    ### Weights calculation:\n",
    "    gc.collect()\n",
    "    print(iter_dataset, ': weights calculation started')    \n",
    "    ser_iter_weights = dict_annual[iter_dataset].groupby(['Date', 'Reporter']).apply(set_weights, flo_hi_threshold)\n",
    "    ser_iter_weights.to_hdf(str_path_act_weights, iter_dataset, mode = 'a', format = 'table')\n",
    "    print(iter_dataset, ': weights calculation done')\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 00:00:00 : 1\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: WEIGHTED AVERAGE OF RETURNS CALCULATION\n",
    "\n",
    "gc.collect()\n",
    "### Weighted returns calculator:\n",
    "def get_weighted_ret(df_group, int_select_top = None, flo_select_share = None):\n",
    "    df_group = df_group.dropna()\n",
    "    if (int_select_top is not None):\n",
    "        df_group = df_group.sort_values('Weight', ascending = False)[ : int_select_top]\n",
    "    if (flo_select_share is not None):\n",
    "        df_group = df_group[df_group['Weight'] > flo_select_share]\n",
    "    flo_weighted = np.NaN\n",
    "    if (len(df_group.index) > 0):\n",
    "        flo_weighted = (df_group['Weight'] * df_group['Ret_USD']).sum() / df_group['Weight'].sum()\n",
    "    return flo_weighted\n",
    "### Global container:\n",
    "dict_all_weighted = {}\n",
    "### Number of datasets active:\n",
    "int_len = -1\n",
    "### Looping over returns dates:\n",
    "#for iter_date in sorted(ser_ret_ave.dropna().index.get_level_values('Date').unique()):\n",
    "for iter_date in [pd.to_datetime('2020-03-31'), pd.to_datetime('2020-04-30')]:\n",
    "    ### Dates defining:\n",
    "    date_bm_end = iter_date + pd.offsets.BMonthEnd(0)\n",
    "    if (date_bm_end > iter_date):\n",
    "        date_bm_end = date_bm_end - pd.offsets.BMonthEnd(1)\n",
    "#    print(iter_date, '/', date_bm_end)\n",
    "    ### Daily returns extraction:\n",
    "    ser_iter_ret = ser_ret_ave.loc[iter_date]\n",
    "    ### Daily container:\n",
    "    dict_iter_weighted = {}\n",
    "#    for iter_dataset in dict_dataset:\n",
    "    for iter_dataset in ['imf_dots']:\n",
    "        ### Loading raw dataset:\n",
    "        ser_iter_raw = dict_dataset[iter_dataset].loc[: (date_bm_end - pd.offsets.BMonthEnd(dict_lag[iter_dataset]))]\n",
    "        if (ser_iter_raw.count() > 0):\n",
    "            ### Defining last date by lagging original dataset:\n",
    "            date_last = ser_iter_raw.index.get_level_values('Date')[-1]\n",
    "#            date_last = (date_bm_end - pd.offsets.BMonthEnd(dict_lag[iter_dataset]))          \n",
    "            date_prev = date_last - pd.offsets.BMonthEnd(dict_ffill[iter_dataset])\n",
    "    #        print(iter_date, '/', date_bm_end, '/', date_prev, ':', date_last)\n",
    "            ### Perform lagging & forward filling on annualized dataset:\n",
    "            ser_iter_ann = dict_annual[iter_dataset].loc[date_prev : date_last].groupby(['Reporter', 'Partner']).ffill(limit = dict_ffill[iter_dataset])\n",
    "            ### Taking last date value:\n",
    "            ser_iter_last = ser_iter_ann[date_last]\n",
    "            ### Calculating of weighted average of returns:\n",
    "            df_to_weight = ser_iter_last.to_frame().join(ser_iter_ret)\n",
    "            df_to_weight.columns = ['Weight', 'Ret_USD']\n",
    "            df_to_weight['Weight'] = df_to_weight['Weight'].groupby('Reporter').apply(lambda ser_group: ser_group / ser_group.sum())\n",
    "#            print(df_to_weight.loc['AE'])\n",
    "            ### Simple weighted average:\n",
    "#            dict_iter_weighted[iter_dataset] = df_to_weight.groupby('Reporter').apply(get_weighted_ret)            \n",
    "            ### Weighted average with threshold by partners number:\n",
    "#            dict_iter_weighted[iter_dataset] = df_to_weight.groupby('Reporter').apply(get_weighted_ret, int_select_top)\n",
    "            ### Weighted average with threshold by share:\n",
    "            dict_iter_weighted[iter_dataset] = df_to_weight.groupby('Reporter').apply(get_weighted_ret, None, flo_select_share)\n",
    "            ### Weighted average with both thresholds:\n",
    "#            dict_iter_weighted[iter_dataset] = df_to_weight.groupby('Reporter').apply(get_weighted_ret, int_select_top, flo_select_share)\n",
    "    if (len(dict_iter_weighted) > 0):\n",
    "        ### Daily results aggregation:\n",
    "        dict_all_weighted[iter_date] = pd.concat(dict_iter_weighted, axis = 1, sort = False)\n",
    "    if (int_len != len(dict_iter_weighted)):\n",
    "        int_len = len(dict_iter_weighted)\n",
    "        print(iter_date, ':', str(int_len))\n",
    "### Global results aggregation:\n",
    "df_all_weighted = pd.concat(dict_all_weighted, axis = 0, sort = False) \n",
    "df_all_weighted.index.names = ['Date', 'Country']\n",
    "### Global results saving:\n",
    "#df_all_weighted.to_hdf(str_path_ret_weighted, str_key_weighted, mode = 'w', format = 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-31 00:00:00 : 1\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: ALTERNATIVE WEIGHTED AVERAGE OF RETURNS CALCULATION FOR IMF DOTS\n",
    "\n",
    "gc.collect()\n",
    "### Weighted returns calculator:\n",
    "def get_weighted_ret(df_group, int_select_top = None, flo_select_share = None):\n",
    "    df_group = df_group.dropna()\n",
    "    if (int_select_top is not None):\n",
    "        df_group = df_group.sort_values('Weight', ascending = False)[ : int_select_top]\n",
    "    if (flo_select_share is not None):\n",
    "        df_group = df_group[df_group['Weight'] > flo_select_share]\n",
    "    flo_weighted = np.NaN\n",
    "    if (len(df_group.index) > 0):\n",
    "        flo_weighted = (df_group['Weight'] * df_group['Ret_USD']).sum() / df_group['Weight'].sum()\n",
    "    return flo_weighted\n",
    "### Global container:\n",
    "dict_all_weighted = {}\n",
    "### Loading World Export Annualized:\n",
    "ser_world_export_ann = pd.read_hdf(str_path_world_export_annualized, str_key_world_export_annualized)\n",
    "### Number of datasets active:\n",
    "int_len = -1\n",
    "### Looping over returns dates:\n",
    "for iter_date in sorted(ser_ret_ave.dropna().index.get_level_values('Date').unique()):\n",
    "#for iter_date in [pd.to_datetime('2020-03-31'), pd.to_datetime('2020-04-30')]:\n",
    "    ### Dates defining:\n",
    "    date_bm_end = iter_date + pd.offsets.BMonthEnd(0)\n",
    "    if (date_bm_end > iter_date):\n",
    "        date_bm_end = date_bm_end - pd.offsets.BMonthEnd(1)\n",
    "#    print(iter_date, '/', date_bm_end)\n",
    "    ### Daily returns extraction:\n",
    "    ser_iter_ret = ser_ret_ave.loc[iter_date]\n",
    "    ### Daily container:\n",
    "    dict_iter_weighted = {}\n",
    "#    for iter_dataset in dict_dataset:\n",
    "    for iter_dataset in ['imf_dots']:\n",
    "        ### Loading raw dataset:\n",
    "        ser_iter_raw = dict_dataset[iter_dataset].loc[: (date_bm_end - pd.offsets.BMonthEnd(dict_lag[iter_dataset]))]\n",
    "        if (ser_iter_raw.count() > 0):\n",
    "            ### Defining last date by lagging original dataset:\n",
    "            date_last = ser_iter_raw.index.get_level_values('Date')[-1]\n",
    "#            date_last = (date_bm_end - pd.offsets.BMonthEnd(dict_lag[iter_dataset]))          \n",
    "            date_prev = date_last - pd.offsets.BMonthEnd(dict_ffill[iter_dataset])\n",
    "    #        print(iter_date, '/', date_bm_end, '/', date_prev, ':', date_last)\n",
    "            ### Perform lagging & forward filling on annualized dataset:\n",
    "            ser_iter_ann = dict_annual[iter_dataset].loc[date_prev : date_last].groupby(['Reporter', 'Partner']).ffill(limit = dict_ffill[iter_dataset])\n",
    "            ser_iter_world_ann = ser_world_export_ann.loc[date_prev : date_last].groupby(['Reporter']).ffill(limit = dict_ffill[iter_dataset])\n",
    "            ### Taking last date value:\n",
    "            ser_iter_last = ser_iter_ann[date_last]\n",
    "            ser_iter_world_last = ser_iter_world_ann[date_last]\n",
    "            ### Calculating of weighted average of returns:\n",
    "            df_to_weight = ser_iter_last.to_frame().join(ser_iter_ret).join(ser_iter_world_last)\n",
    "            df_to_weight.columns = ['Country_Export', 'Ret_USD', 'World_Export']\n",
    "            df_to_weight['Weight'] = df_to_weight.groupby('Reporter', group_keys = False).apply(lambda df_group: df_group['Country_Export'] / df_group['World_Export'])\n",
    "#            print(df_to_weight.loc['US'])\n",
    "            ### Simple weighted average:\n",
    "#            dict_iter_weighted[iter_dataset] = df_to_weight.groupby('Reporter').apply(get_weighted_ret)            \n",
    "            ### Weighted average with threshold by partners number:\n",
    "#            dict_iter_weighted[iter_dataset] = df_to_weight.groupby('Reporter').apply(get_weighted_ret, int_select_top)\n",
    "            ### Weighted average with threshold by share:\n",
    "            dict_iter_weighted[iter_dataset] = df_to_weight.groupby('Reporter').apply(get_weighted_ret, None, flo_select_share)\n",
    "            ### Weighted average with both thresholds:\n",
    "#            dict_iter_weighted[iter_dataset] = df_to_weight.groupby('Reporter').apply(get_weighted_ret, int_select_top, flo_select_share)\n",
    "    if (len(dict_iter_weighted) > 0):\n",
    "        ### Daily results aggregation:\n",
    "        dict_all_weighted[iter_date] = pd.concat(dict_iter_weighted, axis = 1, sort = False)\n",
    "    if (int_len != len(dict_iter_weighted)):\n",
    "        int_len = len(dict_iter_weighted)\n",
    "        print(iter_date, ':', str(int_len))\n",
    "### Global results aggregation:\n",
    "ser_all_weighted = pd.concat(dict_all_weighted, axis = 0, sort = False).squeeze()\n",
    "ser_all_weighted.index.names = ['Date', 'Country']\n",
    "### Global results saving:\n",
    "df_all_weighted['imf_dots'] = ser_all_weighted\n",
    "df_all_weighted.to_hdf(str_path_ret_weighted, str_key_weighted, mode = 'w', format = 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: ADDING WEIGHTED RETURNS TO DATA COLLECTION\n",
    "\n",
    "df_all_weighted = pd.read_hdf(str_path_ret_weighted, str_key_weighted).reset_index('Country')\n",
    "df_all_weighted['Country'] = df_all_weighted['Country'].astype(str)\n",
    "ser_weighted = df_all_weighted.set_index('Country', append = True).stack()\n",
    "ser_weighted.index.names = ['Date', 'Country', 'Activity']\n",
    "ser_weighted.to_hdf(str_path_gravity_results, str_key_ret_weighted, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: HDF TO CSV\n",
    "\n",
    "ser_activity_sum = pd.read_hdf(str_path_gravity_results, str_key_activity_sum)\n",
    "ser_activity_sum.fillna(0.0).to_csv(str_activity_sum_csv_path, sep = ';', header = False)\n",
    "ser_activity_share = pd.read_hdf(str_path_gravity_results, str_key_activity_share)\n",
    "ser_activity_share.replace(0.0, np.NaN).to_csv(str_activity_share_csv_path, sep = ';', header = False)\n",
    "ser_gdp_total = pd.read_hdf(str_path_gravity_results, str_key_gdp_total)\n",
    "ser_gdp_total.to_csv(str_gdp_total_csv_path, sep = ';', header = False)\n",
    "ser_herfindahl = pd.read_hdf(str_path_gravity_results, str_key_herfindahl)\n",
    "ser_herfindahl.to_csv(str_herfindahl_csv_path, sep = ';', header = False)\n",
    "ser_openess = pd.read_hdf(str_path_gravity_results, str_key_openess)\n",
    "ser_openess.to_csv(str_openess_csv_path, sep = ';', header = False)\n",
    "ser_ret_weighted = pd.read_hdf(str_path_gravity_results, str_key_ret_weighted)\n",
    "ser_ret_weighted.to_csv(str_ret_weighted_csv_path, sep = ';', header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "pd.read_csv(str_ret_weighted_csv_path, sep = ';', header = None, parse_dates = [0]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_ret_weighted = pd.read_hdf(str_path_gravity_results, str_key_ret_weighted)\n",
    "ser_ret_weighted.to_csv(str_ret_weighted_csv_path, sep = ';', header = False)\n",
    "#ser_ret_weighted['2000-12-29', 'US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "### RUN EVERY TIME: WEIGHTED AVERAGE OF RETURNS CALCULATION\n",
    "\n",
    "gc.collect()\n",
    "### Weighted returns calculator:\n",
    "def get_weighted_ret(df_group, int_select_top = None, flo_select_share = None):\n",
    "    df_group = df_group.dropna()\n",
    "    if (int_select_top is not None):\n",
    "        df_group = df_group.sort_values('Weight', ascending = False)[ : int_select_top]\n",
    "    if (flo_select_share is not None):\n",
    "        df_group = df_group[df_group['Weight'] > flo_select_share]\n",
    "    flo_weighted = np.NaN\n",
    "    if (len(df_group.index) > 0):\n",
    "        flo_weighted = (df_group['Weight'] * df_group['Ret_USD']).sum() / df_group['Weight'].sum()\n",
    "    return flo_weighted\n",
    "### Global container:\n",
    "dict_all_weighted = {}\n",
    "### Number of datasets active:\n",
    "int_len = -1\n",
    "### Looping over returns dates:\n",
    "#for iter_date in sorted(ser_ret_ave.dropna().index.get_level_values('Date').unique()):\n",
    "#for iter_date in [pd.to_datetime('2020-03-31'), pd.to_datetime('2020-04-30')]:\n",
    "for iter_date in [pd.to_datetime('2020-03-18')]:\n",
    "    ### Dates defining:\n",
    "    date_bm_end = iter_date + pd.offsets.BMonthEnd(0)\n",
    "    if (date_bm_end > iter_date):\n",
    "        date_bm_end = date_bm_end - pd.offsets.BMonthEnd(1)\n",
    "    print(iter_date, '/', date_bm_end)\n",
    "    ### Daily returns extraction:\n",
    "    ser_iter_ret = ser_ret_ave.loc[iter_date]\n",
    "    ### Daily container:\n",
    "    dict_iter_weighted = {}\n",
    "#    for iter_dataset in dict_dataset:\n",
    "    for iter_dataset in ['imf_cpis']:\n",
    "        ### Loading raw dataset:\n",
    "        ser_iter_raw = dict_dataset[iter_dataset].loc[: (date_bm_end - pd.offsets.BMonthEnd(dict_lag[iter_dataset]))]\n",
    "        if (ser_iter_raw.count() > 0):\n",
    "            ### Defining last date by lagging original dataset:\n",
    "            date_last = ser_iter_raw.index.get_level_values('Date')[-1]\n",
    "            date_prev = date_last - pd.offsets.BMonthEnd(dict_ffill[iter_dataset])\n",
    "            print(iter_date, '/', date_bm_end, '/', date_prev, ':', date_last)\n",
    "            ### Perform lagging & forward filling on annualized dataset:\n",
    "            ser_iter_ann = dict_annual[iter_dataset].loc[date_prev : date_last].groupby(['Reporter', 'Partner']).ffill(limit = dict_ffill[iter_dataset])\n",
    "            ### Taking last date value:\n",
    "            ser_iter_last = ser_iter_ann[date_last]\n",
    "            ### Calculating of weighted average of returns:\n",
    "            df_to_weight = ser_iter_last.to_frame().join(ser_iter_ret)\n",
    "            df_to_weight.columns = ['Weight', 'Ret_USD']\n",
    "            df_to_weight['Weight'] = df_to_weight['Weight'].groupby('Reporter').apply(lambda ser_group: ser_group / ser_group.sum())\n",
    "            ### Weighted average with threshold by share:\n",
    "            dict_iter_weighted[iter_dataset] = df_to_weight.groupby('Reporter').apply(get_weighted_ret, None, flo_select_share)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "sorted(ser_ret_ave.dropna().index.get_level_values('Date').unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
