{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: BILATERAL EQUITY & DEBT INVESTMENT POSITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1) ### To display long strings\n",
    "import math\n",
    "import requests\n",
    "import json ### To correct JSON structure before unpacking\n",
    "import gc\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  0.25.3\n",
      "python version:  3.7.4\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: MAIN CONSTANTS\n",
    "\n",
    "### MultiIndex level slice constant:\n",
    "All = slice(None)\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### IMF CDIS datasets:\n",
    "str_path_imf_cdis_dataset = 'Data_Files/Source_Files/cdis_dataset.h5'\n",
    "str_key_do_total_imf_cdis_dataset = 'cdis_total_outward_dataset'\n",
    "str_key_di_total_imf_cdis_dataset = 'cdis_total_inward_dataset'\n",
    "str_key_do_equity_imf_cdis_dataset = 'cdis_equity_outward_dataset'\n",
    "str_key_di_equity_imf_cdis_dataset = 'cdis_equity_inward_dataset'\n",
    "str_path_imf_cdis_augmented = 'Data_Files/Source_Files/cdis_augmented.h5'\n",
    "str_key_do_total_imf_cdis_augmented = 'cdis_total_outward_augmented'\n",
    "str_key_do_equity_imf_cdis_augmented = 'cdis_equity_outward_augmented'\n",
    "str_path_imf_cdis_options = 'Data_Files/Source_Files/cdis_options.h5'\n",
    "str_key_total_imf_cdis_options = 'cdis_total_outward_options'\n",
    "str_key_equity_imf_cdis_options = 'cdis_equity_outward_options'\n",
    "### OECD FDI datasets:\n",
    "str_path_oecd_fdi_augmented = 'Data_Files/Source_Files/oecd_augmented.h5'\n",
    "str_key_do_total_oecd_fdi_augmented = 'fdi_total_outward_augmented'\n",
    "str_key_do_equity_oecd_fdi_augmented = 'fdi_equity_outward_augmented'\n",
    "str_path_oecd_fdi_options = 'Data_Files/Source_Files/oecd_options.h5'\n",
    "str_key_total_oecd_fdi_options = 'fdi_total_outward_options'\n",
    "str_key_equity_oecd_fdi_options = 'fdi_equity_outward_options'\n",
    "### Combined datasets:\n",
    "str_path_direct_total_augmented = 'Data_Files/Source_Files/direct_total_augmented.h5'\n",
    "str_path_direct_equity_augmented = 'Data_Files/Source_Files/direct_equity_augmented.h5'\n",
    "str_key_direct_augmented = 'direct_augmented'\n",
    "str_path_total_direct_options = 'Data_Files/Source_Files/direct_total_options.h5'\n",
    "str_key_total_direct_options = 'direct_total_options'\n",
    "str_path_equity_direct_options = 'Data_Files/Source_Files/direct_equity_options.h5'\n",
    "str_key_equity_direct_options = 'direct_equity_options'\n",
    "### Technical Constants:\n",
    "str_date_end = '2022-10-31'\n",
    "date_start = pd.Timestamp('1989-12-29')\n",
    "date_end = pd.Timestamp(str_date_end)\n",
    "date_ison = pd.Timestamp('1994-12-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING COUNTRY CODES EXTRACTOR\n",
    "\n",
    "def get_country_codes(use_local_copy = False):  \n",
    "    ### In case if URL is unavailable:\n",
    "    if (use_local_copy):\n",
    "        url_country_code = 'Data_Files/Source_Files/countrycode.html'\n",
    "    ### Online extraction:\n",
    "    else:\n",
    "        url_country_code = 'https://countrycode.org/'\n",
    "    df_full_codes = pd.read_html(url_country_code, index_col = 'COUNTRY')[0]\n",
    "    df_full_codes[['ISO SHORT', 'ISO LONG']] = df_full_codes['ISO CODES'].str.split(' / ', expand = True)\n",
    "    df_result = df_full_codes[['ISO SHORT', 'ISO LONG']].sort_index()    \n",
    "    df_result.index = df_result.index.str.upper()\n",
    "    ### Results output:\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(engine = 'openpyxl', io = str_path_universe, sheet_name = 'Switchers', header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: COMMON DATA EXTRACTION STEPS\n",
    "\n",
    "### World Country Codes:\n",
    "df_country_codes = get_country_codes()\n",
    "### ISON membership history:\n",
    "ser_ison_membership = ison_membership_converting(str_path_universe, pd.to_datetime(str_date_end))\n",
    "### ISON LONG IDs list:\n",
    "list_ison_long = list(df_country_codes.loc[df_country_codes['ISO SHORT'].isin(ser_ison_membership.index.get_level_values('Country').unique()), 'ISO LONG'].values)\n",
    "### ISON current status:\n",
    "ser_ison_status = ser_ison_membership.loc[str_date_end].droplevel('Date')\n",
    "### ISON stats:\n",
    "int_ison_number = len(list_ison_long)\n",
    "list_regions = ['DM', 'EM', 'FM']\n",
    "dict_ison_len = {}\n",
    "dict_ison_len['Full Universe'] = int_ison_number\n",
    "for iter_region in list_regions:\n",
    "    dict_ison_len[iter_region] = len(ser_ison_status[ser_ison_status == iter_region])\n",
    "ser_market_len = pd.Series(dict_ison_len)\n",
    "ser_market_len.index.names = ['Market']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "dict_request_headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'}\n",
    "str_imf_base_url = 'http://dataservices.imf.org/REST/SDMX_JSON.svc/'\n",
    "str_imf_dataflow_add = 'DataFlow'\n",
    "str_imf_datastructure_add = 'DataStructure/'\n",
    "str_imf_codelist_add = 'CodeList/'\n",
    "str_imf_dataset_add = 'CompactData/'\n",
    "int_seconds_to_sleep = 1\n",
    "int_imf_country_limit = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: REQUESTS SESSION INITIALIZING\n",
    "\n",
    "request_session = requests.Session()\n",
    "### For avoiding data request errors from IMF Data Service:\n",
    "request_session.headers.update(dict_request_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDIS\n"
     ]
    }
   ],
   "source": [
    "### IMF CDIS: DATAFLOW SEARCHING\n",
    "\n",
    "obj_imf_dataflow_list = request_session.get(str_imf_base_url + str_imf_dataflow_add).json()\n",
    "df_imf_dataflow = pd.DataFrame(obj_imf_dataflow_list['Structure']['Dataflows']['Dataflow'])\n",
    "df_imf_dataflow = df_imf_dataflow.assign(Description = df_imf_dataflow['Name'].apply(pd.Series)['#text'].values)[['@id', 'Description']]\n",
    "ser_imf_dataflow = df_imf_dataflow.set_index('@id', drop = True).squeeze()\n",
    "### Searching DataFlow code for further requests:\n",
    "str_imf_cdis_id = ser_imf_dataflow[ser_imf_dataflow.str.contains('CDIS')].index[0].replace('DS-', '')\n",
    "print(str_imf_cdis_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        @conceptRef          @codelist @isFrequencyDimension\n",
      "0  FREQ              CL_FREQ            true                \n",
      "1  REF_AREA          CL_AREA_CDIS       NaN                 \n",
      "2  INDICATOR         CL_INDICATOR_CDIS  NaN                 \n",
      "3  COUNTERPART_AREA  CL_AREA_CDIS       NaN                 \n"
     ]
    }
   ],
   "source": [
    "### IMF CDIS: DATASTRUCTURE SEARCHING\n",
    "\n",
    "obj_imf_cdis_structure = request_session.get(str_imf_base_url + str_imf_datastructure_add + str_imf_cdis_id).json()\n",
    "df_imf_cdis_params = pd.DataFrame(obj_imf_cdis_structure['Structure']['KeyFamilies']['KeyFamily']['Components']['Dimension'])\\\n",
    "                                [['@conceptRef', '@codelist', '@isFrequencyDimension']]\n",
    "### Receiving DataFlow parameters and code lists for each of them:\n",
    "print(df_imf_cdis_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: CODES DESCRIPTIONS LOADING\n",
    "\n",
    "for int_counter, str_param_code in enumerate(df_imf_cdis_params['@codelist']):\n",
    "    if (int_counter == 2):\n",
    "        time.sleep(int_seconds_to_sleep)    \n",
    "        obj_imf_cdis_param = request_session.get(str_imf_base_url + str_imf_codelist_add + str_param_code).json()\n",
    "        df_imf_cdis_param =  pd.DataFrame(obj_imf_cdis_param['Structure']['CodeLists']['CodeList']['Code'])\n",
    "        ### Receiving values for each code list:\n",
    "        df_imf_cdis_param = df_imf_cdis_param.assign(Text = df_imf_cdis_param['Description'].apply(pd.Series)['#text'].values)[['@value', 'Text']]\n",
    "        dict_indicator = dict(zip(df_imf_cdis_param['@value'], df_imf_cdis_param['Text']))\n",
    "        \n",
    "list_ison_countries = sorted(list(map(str, ser_ison_membership.index.get_level_values(1).unique())))\n",
    "str_cdis_freq = 'A' # 'B' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "dict_to_download = {iter_key: dict_indicator[iter_key] for iter_key in \\\n",
    "                    ('IOW_BP6_USD', 'IIW_BP6_USD', 'IOWE_BP6_USD', 'IIWE_BP6_USD')}\n",
    "pd.Series(dict_indicator).to_excel('Data_Files/Test_Files/IMF_CDIS_Indicators.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: REPORTED DIRECT INVESTMENT NET VOLUMES RETRIEVING\n",
    "\n",
    "gc.collect()\n",
    "### Extracting needed part of indicators:\n",
    "dict_to_download = {iter_key: dict_indicator[iter_key] for iter_key in ('IOW_BP6_USD', 'IIW_BP6_USD', 'IOWE_BP6_USD', 'IIWE_BP6_USD')}\n",
    "### List of bilateral dataframes for future concatenation:\n",
    "list_cdis_bilateral = [] \n",
    "### Beggining of request URL:\n",
    "str_cdis_const_url = str_imf_base_url + str_imf_dataset_add + str_imf_cdis_id + '/' \n",
    "### Looping over reporter:\n",
    "for iter_investor in list_ison_countries:\n",
    "#for iter_investor in ['CA']:  \n",
    "    ### Looping over indicator:\n",
    "    for iter_indicator in dict_to_download:        \n",
    "#    for iter_indicator in ['IOWE_BP6_USD', 'IIWE_BP6_USD']:        \n",
    "        if (iter_indicator[1] == 'O'):\n",
    "            str_cdis_full_url = str_cdis_const_url + '.'.join([str_cdis_freq, iter_investor, iter_indicator, ''])\n",
    "        else:\n",
    "            str_cdis_full_url = str_cdis_const_url + '.'.join([str_cdis_freq, '', iter_indicator, iter_investor])            \n",
    "        obj_cdis_set = request_session.get(str_cdis_full_url)\n",
    "        ### Data reading as JSON:\n",
    "        dict_cdis_set = json.loads(obj_cdis_set.text.replace('@OBS_STATUS', '@OBS_VALUE'))\n",
    "        ### Converting each bilateral dataset to dataframe and it's mungling:\n",
    "        if ('Series' in dict_cdis_set['CompactData']['DataSet']):\n",
    "            if isinstance(dict_cdis_set['CompactData']['DataSet']['Series'], list):\n",
    "                list_series = dict_cdis_set['CompactData']['DataSet']['Series']\n",
    "            else:\n",
    "                list_series = [dict_cdis_set['CompactData']['DataSet']['Series']]\n",
    "            for dict_cdis_pair in list_series:\n",
    "                if isinstance(dict_cdis_pair['Obs'], list):\n",
    "                    dict_bilateral = dict_cdis_pair['Obs']\n",
    "                else:\n",
    "                    dict_bilateral = [dict_cdis_pair['Obs']]\n",
    "                df_cdis_bilateral = pd.DataFrame(dict_bilateral)\n",
    "                if '@OBS_VALUE' in df_cdis_bilateral.columns:\n",
    "                    df_cdis_bilateral = df_cdis_bilateral[['@TIME_PERIOD', '@OBS_VALUE']]\n",
    "                    df_cdis_bilateral.columns = ['Date', 'Value']\n",
    "                    df_cdis_bilateral = df_cdis_bilateral.assign(Indicator = dict_cdis_pair['@INDICATOR'])\n",
    "                    df_cdis_bilateral = df_cdis_bilateral.assign(Reporter_ID = dict_cdis_pair['@REF_AREA'])\n",
    "                    df_cdis_bilateral = df_cdis_bilateral.assign(Partner_ID = dict_cdis_pair['@COUNTERPART_AREA'])\n",
    "                    list_cdis_bilateral.append(df_cdis_bilateral)  \n",
    "        else:\n",
    "            print('No data in response of the next request:\\n', str_cdis_full_url)\n",
    "        time.sleep(int_seconds_to_sleep)                    \n",
    "#        break            \n",
    "    print(iter_investor, ': loading completed')\n",
    "#    break\n",
    "### Bilateral datasets aggregating:\n",
    "df_cdis_raw = pd.concat(list_cdis_bilateral, axis = 0, ignore_index = True, sort = False)\n",
    "df_cdis_raw['Date'] = pd.to_datetime(df_cdis_raw['Date']) + pd.offsets.BYearEnd()\n",
    "df_cdis_raw.loc[df_cdis_raw['Value'] == 'C', 'Value'] = np.NaN\n",
    "df_cdis_raw.loc[df_cdis_raw['Value'] == '-', 'Value'] = np.NaN\n",
    "df_cdis_raw = df_cdis_raw[df_cdis_raw['Reporter_ID'] != df_cdis_raw['Partner_ID']]\n",
    "df_cdis_raw = df_cdis_raw[df_cdis_raw['Partner_ID'].isin(df_country_codes['ISO SHORT'].values)]\n",
    "print('Unique partners number:', len(df_cdis_raw['Partner_ID'].unique()))\n",
    "df_cdis_raw.rename({'Reporter_ID': 'Reporter', 'Partner_ID': 'Partner'}, axis = 1, inplace = True)\n",
    "df_cdis_raw = df_cdis_raw.astype({'Indicator': 'str', 'Reporter': 'str', 'Partner': 'str', \n",
    "                                  'Value': 'float32'})\n",
    "df_cdis_raw['Value'].clip(lower = 0.0, inplace = True)\n",
    "df_cdis_raw['Indicator'].replace(dict_to_download, inplace = True)\n",
    "df_cdis_raw['Direction'] = df_cdis_raw['Indicator'].str.partition(' ')[0]\n",
    "df_cdis_raw['Type'] = df_cdis_raw['Indicator'].str.partition(' ')[2].str.partition(' ')[0].replace({'Direct': 'Total'})\n",
    "### Data saving:\n",
    "ser_cdis_full = df_cdis_raw.set_index(['Type', 'Direction', 'Date', 'Reporter', 'Partner'])['Value'].sort_index()\n",
    "del df_cdis_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: POSITION DATASETS SAVING\n",
    "\n",
    "ser_cdis_full.loc['Total', 'Outward', :, ser_ison_status.index, df_country_codes['ISO SHORT'].values].droplevel(['Type', 'Direction'])\\\n",
    "    .to_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_do_total_imf_cdis_dataset, mode = 'w', format = 'fixed')\n",
    "ser_cdis_full.loc['Equity', 'Outward', :, ser_ison_status.index, df_country_codes['ISO SHORT'].values].droplevel(['Type', 'Direction'])\\\n",
    "    .to_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_do_equity_imf_cdis_dataset, mode = 'a', format = 'fixed')\n",
    "ser_total_di = ser_cdis_full.loc['Total', 'Inward', :, df_country_codes['ISO SHORT'].values, ser_ison_status.index].droplevel(['Type', 'Direction'])\n",
    "ser_total_di.index.names = ['Date', 'Partner', 'Reporter']\n",
    "ser_total_di.reorder_levels(['Date', 'Reporter', 'Partner']).sort_index()\\\n",
    "    .to_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_di_total_imf_cdis_dataset, mode = 'a', format = 'fixed')\n",
    "ser_equity_di = ser_cdis_full.loc['Equity', 'Inward', :, df_country_codes['ISO SHORT'].values, ser_ison_status.index].droplevel(['Type', 'Direction'])\n",
    "ser_equity_di.index.names = ['Date', 'Partner', 'Reporter']\n",
    "ser_equity_di.reorder_levels(['Date', 'Reporter', 'Partner']).sort_index()\\\n",
    "    .to_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_di_equity_imf_cdis_dataset, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: TOTAL DATA AGGREGATION: DATASETS LOADING\n",
    "\n",
    "gc.collect()\n",
    "ser_cdis_asset = pd.read_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_do_total_imf_cdis_dataset)\n",
    "ser_cdis_asset.name = 'Asset'\n",
    "ser_cdis_liability_inv = pd.read_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_di_total_imf_cdis_dataset)\n",
    "ser_cdis_liability_inv.name = 'Liability_Inverted'\n",
    "df_cdis_total = pd.concat([ser_cdis_asset, ser_cdis_liability_inv], axis = 1, names = 'Data Source').astype('float32').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: TOTAL DATA AGGREGATION: DATA QUALITY RATIOS\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "### Defining similarity for investors by date\n",
    "def get_investor_ratio(df_group):\n",
    "#    df_group['Asset'].fillna(0.0, inplace = True)\n",
    "    df_group.fillna(0.0, inplace = True)    \n",
    "    df_both = df_group.dropna()    \n",
    "    if (df_both['Asset'].sum() > 0.0):\n",
    "        flo_result = (df_both['Asset'] - df_both['Liability_Inverted']).abs().clip(upper = df_group['Asset'].max()).sum() / df_group['Asset'].sum() / len(df_group)    \n",
    "    else:\n",
    "        flo_result = np.NaN    \n",
    "    return flo_result\n",
    "### Defining similarity for borrowers by date\n",
    "def get_borrower_ratio(df_group):\n",
    "#    df_group['Liability_Inverted'].fillna(0.0, inplace = True)\n",
    "    df_group.fillna(0.0, inplace = True)     \n",
    "    df_both = df_group.dropna()\n",
    "    if (df_both['Liability_Inverted'].sum() > 0.0):\n",
    "        flo_result = (df_both['Asset'] - df_both['Liability_Inverted']).abs().clip(upper = df_group['Liability_Inverted'].max()).sum() \\\n",
    "                                                                        / df_group['Liability_Inverted'].sum() / len(df_group)\n",
    "    else:\n",
    "        flo_result = np.NaN    \n",
    "    return flo_result\n",
    "### Similarity values calculation:\n",
    "ser_investor_ratio = df_cdis_total.groupby(['Date', 'Reporter']).apply(get_investor_ratio)\n",
    "ser_investor_ratio.name = 'Investor_Ratio'\n",
    "ser_borrower_ratio = df_cdis_total.groupby(['Date', 'Partner']).apply(get_borrower_ratio)\n",
    "ser_borrower_ratio.name = 'Borrower_Ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0021 / (Timestamp('2019-12-31 00:00:00'), 'BR')\n",
      "0.1436 / (Timestamp('2014-12-31 00:00:00'), 'CW')\n"
     ]
    }
   ],
   "source": [
    "### IMF CDIS: TOTAL DATA AGGREGATION: SIMILARITY TEST\n",
    "\n",
    "print(round(ser_borrower_ratio.min(), 4), '/', ser_borrower_ratio.idxmin())\n",
    "print(round(ser_borrower_ratio.max(), 4), '/', ser_borrower_ratio.idxmax())\n",
    "\n",
    "#display(df_cdis_total.loc[('2019-12-31', All, 'BR'), :].dropna())\n",
    "#display(df_cdis_total.loc[('2014-12-31', All, 'CW'), :].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009 / (Timestamp('2015-12-31 00:00:00'), 'FR')\n",
      "0.1847 / (Timestamp('2016-12-30 00:00:00'), 'MT')\n"
     ]
    }
   ],
   "source": [
    "### IMF CDIS: TOTAL DATA AGGREGATION: SIMILARITY TEST\n",
    "\n",
    "print(round(ser_investor_ratio.min(), 4), '/', ser_investor_ratio.idxmin())\n",
    "print(round(ser_investor_ratio.max(), 4), '/', ser_investor_ratio.idxmax())\n",
    "\n",
    "#display(df_cdis_total.loc[('2014-12-31', 'SE', All), :].dropna())\n",
    "#display(df_cdis_total.loc[('2016-12-30', 'MT', All), :].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: TOTAL DATA AGGREGATION: ADDING RATIOS\n",
    "\n",
    "df_cdis_to_augment = df_cdis_total.join(ser_investor_ratio).join(ser_borrower_ratio)\n",
    "df_cdis_to_augment['Asset_Augmented'] = np.NaN # -999 # \n",
    "#df_cpis_augmented['Verified'] = False\n",
    "df_cdis_to_augment = df_cdis_to_augment.reorder_levels(['Date', 'Reporter', 'Partner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: TOTAL DATA AGGREGATION: CONDITIONAL REPLACING\n",
    "\n",
    "gc.collect()\n",
    "def augment_by_date(df_date, int_option = 0):\n",
    "    '''\n",
    "       -1 : Replace NaN Asset values unconditionally    \n",
    "        0 : Replace NaN Asset values when Investor's Ratio > Borrower's Ratio\n",
    "        1 : Replace NaN or zero Asset values when Investor's Ratio > Borrower's Ratio\n",
    "        2 : Replace any Asset values when Investor's Ratio > Borrower's Ratio\n",
    "    '''\n",
    "    if (int_option == -1):\n",
    "        ### Replacing zero Asset & Liability values with NaN:\n",
    "        df_date.loc[df_date['Asset'] == 0.0, 'Asset'] = np.NaN        \n",
    "        df_date['Asset_Augmented'] = df_date['Asset'].combine_first(df_date['Liability_Inverted'])\n",
    "    elif (int_option == 0):\n",
    "        ### Fill resulting column with not NaN Asset values:\n",
    "        df_date.loc[df_date['Asset'].notna(), 'Asset_Augmented'] = df_date[df_date['Asset'].notna()]['Asset'].values\n",
    "        ### Fill resulting column with Liability value if Asset value is NaN & Investor Ratio is NaN (and doesn't matter if Borrower Ratio is NaN):\n",
    "        df_date.loc[df_date['Asset'].isna() & df_date['Investor_Ratio'].isna(), 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Asset'].isna() & df_date['Investor_Ratio'].isna()]['Liability_Inverted'].values\n",
    "        ### Fill resulting column with Liability value if Asset value is NaN & Investor Ratio is bigger than Borrower Ratio:\n",
    "        df_date.loc[df_date['Asset'].isna() & df_date['Investor_Ratio'].notna() & df_date['Borrower_Ratio'].notna() & \\\n",
    "                    (df_date['Investor_Ratio'] > df_date['Borrower_Ratio']), 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Asset'].isna() & df_date['Investor_Ratio'].notna() & df_date['Borrower_Ratio'].notna() & \\\n",
    "                    (df_date['Investor_Ratio'] > df_date['Borrower_Ratio'])]['Liability_Inverted'].values\n",
    "    elif (int_option == 1):\n",
    "        ### Replacing zero Asset values with NaN:\n",
    "        df_date.loc[df_date['Asset'] == 0.0, 'Asset'] = np.NaN\n",
    "        ### Fill resulting column with not NaN Asset values:\n",
    "        df_date.loc[df_date['Asset'].notna(), 'Asset_Augmented'] = df_date.loc[df_date['Asset'].notna(), 'Asset'].values\n",
    "        ### Fill resulting column with Liability value if Asset value is NaN & Investor Ratio is NaN (and doesn't matter if Borrower Ratio is NaN):\n",
    "        df_date.loc[df_date['Asset'].isna() & df_date['Investor_Ratio'].isna(), 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Asset'].isna() & df_date['Investor_Ratio'].isna()]['Liability_Inverted'].values\n",
    "        ### Fill resulting column with Liability value if Asset value is NaN & Investor Ratio is bigger than Borrower Ratio:\n",
    "        df_date.loc[df_date['Asset'].isna() & df_date['Investor_Ratio'].notna() & df_date['Borrower_Ratio'].notna() & \\\n",
    "                    (df_date['Investor_Ratio'] > df_date['Borrower_Ratio']), 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Asset'].isna() & df_date['Investor_Ratio'].notna() & df_date['Borrower_Ratio'].notna() & \\\n",
    "                    (df_date['Investor_Ratio'] > df_date['Borrower_Ratio'])]['Liability_Inverted'].values\n",
    "    else:\n",
    "        ### Replacing zero Asset & Liability values with NaN:\n",
    "        df_date.loc[df_date['Asset'] == 0.0, 'Asset'] = np.NaN        \n",
    "        df_date.loc[df_date['Liability_Inverted'] == 0.0, 'Liability_Inverted'] = np.NaN\n",
    "        ### Ratios preparation:\n",
    "        df_date.loc[df_date['Investor_Ratio'].isna(), 'Investor_Ratio'] = 999.0\n",
    "        df_date.loc[df_date['Borrower_Ratio'].isna(), 'Borrower_Ratio'] = 1000.0\n",
    "        ### Ratios comparision:\n",
    "        df_date.loc[df_date['Investor_Ratio'] <= df_date['Borrower_Ratio'], 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Investor_Ratio'] <= df_date['Borrower_Ratio']]['Asset'].values\n",
    "        df_date.loc[df_date['Investor_Ratio'] > df_date['Borrower_Ratio'], 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Investor_Ratio'] > df_date['Borrower_Ratio']]['Liability_Inverted'].values                                       \n",
    "    return df_date\n",
    "\n",
    "dict_cdis_augmented = {}\n",
    "dict_cdis_augmented[-1] = df_cdis_to_augment.groupby('Date').apply(augment_by_date, -1)\n",
    "dict_cdis_augmented[0] = df_cdis_to_augment.groupby('Date').apply(augment_by_date, 0)\n",
    "dict_cdis_augmented[1] = df_cdis_to_augment.groupby('Date').apply(augment_by_date, 1)\n",
    "dict_cdis_augmented[2] = df_cdis_to_augment.groupby('Date').apply(augment_by_date, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: TOTAL DATA AGGREGATION: RESULTS TESTING\n",
    "\n",
    "ser_quantity = df_cdis_to_augment['Asset'].replace({0.0: np.NaN}).groupby(['Date','Reporter']).count()\n",
    "ser_quantity = ser_quantity[ser_quantity > 0].groupby('Date').count()\n",
    "ser_quantity.name = 'raw'\n",
    "ser_quantity.to_excel('Data_Files/Test_Files/Augmentation_Test_' + ser_quantity.name + '.xlsx', merge_cells = False)\n",
    "\n",
    "for iter_option in dict_cdis_augmented:\n",
    "    dt_option = dict_cdis_augmented[iter_option]\n",
    "    ser_quantity = dt_option['Asset_Augmented'].replace({0.0: np.NaN}).groupby(['Date','Reporter']).count()\n",
    "    ser_quantity = ser_quantity[ser_quantity > 0].groupby('Date').count()\n",
    "    ser_quantity.name = str(iter_option)\n",
    "    ser_quantity.to_excel('Data_Files/Test_Files/Augmentation_Test_' + str(iter_option) + '.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: TOTAL DATA AGGREGATION: RESULTS SAVING TO SERIES\n",
    "\n",
    "ser_total_augmented = dict_cdis_augmented[2]['Asset_Augmented']\n",
    "ser_total_augmented.name = 'Total'\n",
    "ser_total_augmented.replace({0.0: np.NaN})\\\n",
    "    .to_hdf(path_or_buf = str_path_imf_cdis_augmented, key = str_key_do_total_imf_cdis_augmented, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: TOTAL DATA AGGREGATION: RESULTS CONSOLIDATION TO DATAFRAME AND SAVING\n",
    "\n",
    "df_augmentation_way = pd.concat([df_cdis_to_augment['Asset'].replace({0.0: np.NaN}), \n",
    "                                 dict_cdis_augmented[-1]['Asset_Augmented'], \n",
    "                                 dict_cdis_augmented[2]['Asset_Augmented']], \n",
    "                                axis = 1, keys = ['Assets_Only', 'Unconditional', 'Option_2'], names = 'Augmentation_Way')\n",
    "df_augmentation_way.to_hdf(path_or_buf = str_path_imf_cdis_options, key = str_key_total_imf_cdis_options, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: EQUITY DATA AGGREGATION: DATASETS LOADING\n",
    "\n",
    "gc.collect()\n",
    "ser_cdis_asset = pd.read_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_do_equity_imf_cdis_dataset)\n",
    "ser_cdis_asset.name = 'Asset'\n",
    "ser_cdis_liability_inv = pd.read_hdf(path_or_buf = str_path_imf_cdis_dataset, key = str_key_di_equity_imf_cdis_dataset)\n",
    "ser_cdis_liability_inv.name = 'Liability_Inverted'\n",
    "df_cdis_equity = pd.concat([ser_cdis_asset, ser_cdis_liability_inv], axis = 1, names = 'Data Source').astype('float32').round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: EQUITY DATA AGGREGATION: DATA QUALITY RATIOS\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "### Defining similarity for investors by date\n",
    "def get_investor_ratio(df_group):\n",
    "#    df_group['Asset'].fillna(0.0, inplace = True)\n",
    "    df_group.fillna(0.0, inplace = True)    \n",
    "    df_both = df_group.dropna()    \n",
    "    if (df_both['Asset'].sum() > 0.0):\n",
    "        flo_result = (df_both['Asset'] - df_both['Liability_Inverted']).abs().clip(upper = df_group['Asset'].max()).sum() / df_group['Asset'].sum() / len(df_group)    \n",
    "    else:\n",
    "        flo_result = np.NaN    \n",
    "    return flo_result\n",
    "### Defining similarity for borrowers by date\n",
    "def get_borrower_ratio(df_group):\n",
    "#    df_group['Liability_Inverted'].fillna(0.0, inplace = True)\n",
    "    df_group.fillna(0.0, inplace = True)     \n",
    "    df_both = df_group.dropna()\n",
    "    if (df_both['Liability_Inverted'].sum() > 0.0):\n",
    "        flo_result = (df_both['Asset'] - df_both['Liability_Inverted']).abs().clip(upper = df_group['Liability_Inverted'].max()).sum() \\\n",
    "                                                                        / df_group['Liability_Inverted'].sum() / len(df_group)\n",
    "    else:\n",
    "        flo_result = np.NaN    \n",
    "    return flo_result\n",
    "### Similarity values calculation:\n",
    "ser_investor_ratio = df_cdis_equity.groupby(['Date', 'Reporter']).apply(get_investor_ratio)\n",
    "ser_investor_ratio.name = 'Investor_Ratio'\n",
    "ser_borrower_ratio = df_cdis_equity.groupby(['Date', 'Partner']).apply(get_borrower_ratio)\n",
    "ser_borrower_ratio.name = 'Borrower_Ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0016 / (Timestamp('2011-12-30 00:00:00'), 'BE')\n",
      "0.1314 / (Timestamp('2018-12-31 00:00:00'), 'CW')\n"
     ]
    }
   ],
   "source": [
    "### IMF CDIS: EQUITY DATA AGGREGATION: SIMILARITY TEST\n",
    "\n",
    "print(round(ser_borrower_ratio.min(), 4), '/', ser_borrower_ratio.idxmin())\n",
    "print(round(ser_borrower_ratio.max(), 4), '/', ser_borrower_ratio.idxmax())\n",
    "\n",
    "#display(df_cdis_equity.loc[('2011-12-30', All, 'BE'), :].dropna())\n",
    "#display(df_cdis_equity.loc[('2018-12-31', All, 'CW'), :].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007 / (Timestamp('2012-12-31 00:00:00'), 'FR')\n",
      "0.2911 / (Timestamp('2016-12-30 00:00:00'), 'MT')\n"
     ]
    }
   ],
   "source": [
    "### IMF CDIS: EQUITY DATA AGGREGATION: SIMILARITY TEST\n",
    "\n",
    "print(round(ser_investor_ratio.min(), 4), '/', ser_investor_ratio.idxmin())\n",
    "print(round(ser_investor_ratio.max(), 4), '/', ser_investor_ratio.idxmax())\n",
    "\n",
    "#display(df_cdis_equity.loc[('2014-12-31', 'SE', All), :].dropna())\n",
    "#display(df_cdis_equity.loc[('2016-12-30', 'MT', All), :].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: EQUITY DATA AGGREGATION: ADDING RATIOS\n",
    "\n",
    "df_cdis_to_augment = df_cdis_equity.join(ser_investor_ratio).join(ser_borrower_ratio)\n",
    "df_cdis_to_augment['Asset_Augmented'] = np.NaN # -999 # \n",
    "#df_cpis_augmented['Verified'] = False\n",
    "df_cdis_to_augment = df_cdis_to_augment.reorder_levels(['Date', 'Reporter', 'Partner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: EQUITY DATA AGGREGATION: CONDITIONAL REPLACING\n",
    "\n",
    "gc.collect()\n",
    "def augment_by_date(df_date, int_option = 0):\n",
    "    '''\n",
    "       -1 : Replace NaN Asset values unconditionally    \n",
    "        0 : Replace NaN Asset values when Investor's Ratio > Borrower's Ratio\n",
    "        1 : Replace NaN or zero Asset values when Investor's Ratio > Borrower's Ratio\n",
    "        2 : Replace any Asset values when Investor's Ratio > Borrower's Ratio\n",
    "    '''\n",
    "    if (int_option == -1):\n",
    "        ### Replacing zero Asset & Liability values with NaN:\n",
    "        df_date.loc[df_date['Asset'] == 0.0, 'Asset'] = np.NaN        \n",
    "        df_date['Asset_Augmented'] = df_date['Asset'].combine_first(df_date['Liability_Inverted'])\n",
    "    elif (int_option == 0):\n",
    "        ### Fill resulting column with not NaN Asset values:\n",
    "        df_date.loc[df_date['Asset'].notna(), 'Asset_Augmented'] = df_date[df_date['Asset'].notna()]['Asset'].values\n",
    "        ### Fill resulting column with Liability value if Asset value is NaN & Investor Ratio is NaN (and doesn't matter if Borrower Ratio is NaN):\n",
    "        df_date.loc[df_date['Asset'].isna() & df_date['Investor_Ratio'].isna(), 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Asset'].isna() & df_date['Investor_Ratio'].isna()]['Liability_Inverted'].values\n",
    "        ### Fill resulting column with Liability value if Asset value is NaN & Investor Ratio is bigger than Borrower Ratio:\n",
    "        df_date.loc[df_date['Asset'].isna() & df_date['Investor_Ratio'].notna() & df_date['Borrower_Ratio'].notna() & \\\n",
    "                    (df_date['Investor_Ratio'] > df_date['Borrower_Ratio']), 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Asset'].isna() & df_date['Investor_Ratio'].notna() & df_date['Borrower_Ratio'].notna() & \\\n",
    "                    (df_date['Investor_Ratio'] > df_date['Borrower_Ratio'])]['Liability_Inverted'].values\n",
    "    elif (int_option == 1):\n",
    "        ### Replacing zero Asset values with NaN:\n",
    "        df_date.loc[df_date['Asset'] == 0.0, 'Asset'] = np.NaN\n",
    "        ### Fill resulting column with not NaN Asset values:\n",
    "        df_date.loc[df_date['Asset'].notna(), 'Asset_Augmented'] = df_date.loc[df_date['Asset'].notna(), 'Asset'].values\n",
    "        ### Fill resulting column with Liability value if Asset value is NaN & Investor Ratio is NaN (and doesn't matter if Borrower Ratio is NaN):\n",
    "        df_date.loc[df_date['Asset'].isna() & df_date['Investor_Ratio'].isna(), 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Asset'].isna() & df_date['Investor_Ratio'].isna()]['Liability_Inverted'].values\n",
    "        ### Fill resulting column with Liability value if Asset value is NaN & Investor Ratio is bigger than Borrower Ratio:\n",
    "        df_date.loc[df_date['Asset'].isna() & df_date['Investor_Ratio'].notna() & df_date['Borrower_Ratio'].notna() & \\\n",
    "                    (df_date['Investor_Ratio'] > df_date['Borrower_Ratio']), 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Asset'].isna() & df_date['Investor_Ratio'].notna() & df_date['Borrower_Ratio'].notna() & \\\n",
    "                    (df_date['Investor_Ratio'] > df_date['Borrower_Ratio'])]['Liability_Inverted'].values\n",
    "    else:\n",
    "        ### Replacing zero Asset & Liability values with NaN:\n",
    "        df_date.loc[df_date['Asset'] == 0.0, 'Asset'] = np.NaN        \n",
    "        df_date.loc[df_date['Liability_Inverted'] == 0.0, 'Liability_Inverted'] = np.NaN\n",
    "        ### Ratios preparation:\n",
    "        df_date.loc[df_date['Investor_Ratio'].isna(), 'Investor_Ratio'] = 999.0\n",
    "        df_date.loc[df_date['Borrower_Ratio'].isna(), 'Borrower_Ratio'] = 1000.0\n",
    "        ### Ratios comparision:\n",
    "        df_date.loc[df_date['Investor_Ratio'] <= df_date['Borrower_Ratio'], 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Investor_Ratio'] <= df_date['Borrower_Ratio']]['Asset'].values\n",
    "        df_date.loc[df_date['Investor_Ratio'] > df_date['Borrower_Ratio'], 'Asset_Augmented'] = \\\n",
    "            df_date[df_date['Investor_Ratio'] > df_date['Borrower_Ratio']]['Liability_Inverted'].values                                       \n",
    "    return df_date\n",
    "\n",
    "dict_cdis_augmented = {}\n",
    "dict_cdis_augmented[-1] = df_cdis_to_augment.groupby('Date').apply(augment_by_date, -1)\n",
    "dict_cdis_augmented[0] = df_cdis_to_augment.groupby('Date').apply(augment_by_date, 0)\n",
    "dict_cdis_augmented[1] = df_cdis_to_augment.groupby('Date').apply(augment_by_date, 1)\n",
    "dict_cdis_augmented[2] = df_cdis_to_augment.groupby('Date').apply(augment_by_date, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: EQUITY DATA AGGREGATION: RESULTS TESTING\n",
    "\n",
    "ser_quantity = df_cdis_to_augment['Asset'].replace({0.0: np.NaN}).groupby(['Date','Reporter']).count()\n",
    "ser_quantity = ser_quantity[ser_quantity > 0].groupby('Date').count()\n",
    "ser_quantity.name = 'raw'\n",
    "ser_quantity.to_excel('Data_Files/Test_Files/Augmentation_Test_' + ser_quantity.name + '.xlsx', merge_cells = False)\n",
    "\n",
    "for iter_option in dict_cdis_augmented:\n",
    "    dt_option = dict_cdis_augmented[iter_option]\n",
    "    ser_quantity = dt_option['Asset_Augmented'].replace({0.0: np.NaN}).groupby(['Date','Reporter']).count()\n",
    "    ser_quantity = ser_quantity[ser_quantity > 0].groupby('Date').count()\n",
    "    ser_quantity.name = str(iter_option)\n",
    "    ser_quantity.to_excel('Data_Files/Test_Files/Augmentation_Test_' + str(iter_option) + '.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: EQUITY DATA AGGREGATION: RESULTS SAVING (SERIES)\n",
    "\n",
    "ser_equity_augmented = dict_cdis_augmented[2]['Asset_Augmented']\n",
    "ser_equity_augmented.name = 'Total'\n",
    "ser_equity_augmented.replace({0.0: np.NaN})\\\n",
    "    .to_hdf(path_or_buf = str_path_imf_cdis_augmented, key = str_key_do_equity_imf_cdis_augmented, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMF CDIS: EQUITY DATA AGGREGATION: RESULTS CONSOLIDATION TO DATAFRAME AND SAVING\n",
    "\n",
    "df_augmentation_way = pd.concat([df_cdis_to_augment['Asset'].replace({0.0: np.NaN}), \n",
    "                                 dict_cdis_augmented[-1]['Asset_Augmented'], \n",
    "                                 dict_cdis_augmented[2]['Asset_Augmented']], \n",
    "                                axis = 1, keys = ['Assets_Only', 'Unconditional', 'Option_2'], names = 'Augmentation_Way')\n",
    "df_augmentation_way.to_hdf(path_or_buf = str_path_imf_cdis_options, key = str_key_equity_imf_cdis_options, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DIRECT INVESTMENT DATA AGGREGATION: DATASETS LOADING (SERIES)\n",
    "\n",
    "ser_oecd_total = pd.read_hdf(path_or_buf = str_path_oecd_fdi_augmented, key = str_key_do_total_oecd_fdi_augmented)\n",
    "ser_oecd_equity = pd.read_hdf(path_or_buf = str_path_oecd_fdi_augmented, key = str_key_do_equity_oecd_fdi_augmented)\n",
    "ser_cdis_total = pd.read_hdf(path_or_buf = str_path_imf_cdis_augmented, key = str_key_do_total_imf_cdis_augmented)\n",
    "ser_cdis_equity = pd.read_hdf(path_or_buf = str_path_imf_cdis_augmented, key = str_key_do_equity_imf_cdis_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DIRECT INVESTMENT DATA AGGREGATION: DATASETS COMBINING (SERIES)\n",
    "\n",
    "ser_total_combined = ser_cdis_total.combine_first(ser_oecd_total)\n",
    "ser_total_combined.to_hdf(path_or_buf = str_path_direct_total_augmented, key = str_key_direct_augmented, mode = 'w', format = 'fixed')\n",
    "ser_equity_combined = ser_cdis_equity.combine_first(ser_oecd_equity)\n",
    "ser_equity_combined.to_hdf(path_or_buf = str_path_direct_equity_augmented, key = str_key_direct_augmented, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DIRECT INVESTMENT DATA AGGREGATION: DATASETS LOADING (DATAFRAMES)\n",
    "\n",
    "df_total_oecd_options = pd.read_hdf(path_or_buf = str_path_oecd_fdi_options, key = str_key_total_oecd_fdi_options)\n",
    "df_total_cdis_options = pd.read_hdf(path_or_buf = str_path_imf_cdis_options, key = str_key_total_imf_cdis_options)\n",
    "df_equity_oecd_options = pd.read_hdf(path_or_buf = str_path_oecd_fdi_options, key = str_key_equity_oecd_fdi_options)\n",
    "df_equity_cdis_options = pd.read_hdf(path_or_buf = str_path_imf_cdis_options, key = str_key_equity_imf_cdis_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DIRECT INVESTMENT DATA AGGREGATION: DATASAETS COMBINING (DATAFRAMES)\n",
    "\n",
    "df_direct_total_options = df_total_cdis_options.combine_first(df_total_oecd_options).replace({0.0: np.NaN})\n",
    "df_direct_total_options.to_hdf(path_or_buf = str_path_total_direct_options, key = str_key_total_direct_options, mode = 'w', format = 'fixed')\n",
    "df_direct_equity_options = df_equity_cdis_options.combine_first(df_equity_oecd_options).replace({0.0: np.NaN})\n",
    "df_direct_equity_options.to_hdf(path_or_buf = str_path_equity_direct_options, key = str_key_equity_direct_options, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
