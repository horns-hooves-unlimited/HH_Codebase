{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: FACTORS BASED ON BLOOMBERG ECONOMIC INDICES RELEASES HISTORY DATA (NO CHANGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION (REMOVE PARALLELIZATION MODULES) (CELL TO REPLACE)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import math\n",
    "from statsmodels.tsa.x13 import x13_arima_analysis\n",
    "from itertools import combinations_with_replacement\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "### Plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "### Profiling:\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: X13PATH=C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
      "env: X12PATH=C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n"
     ]
    }
   ],
   "source": [
    "### RUN ONLY WHEN ARIMA X13 SA LAUNCHING \n",
    "\n",
    "### Warnings hiding:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "### Seasonal adjustment module paths set up:\n",
    "%env X13PATH = C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
    "%env X12PATH = C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
    "#%env X13PATH = C:\\Users\\igharok\\Desktop\\Job\\ARIMA\n",
    "#%env X12PATH = C:\\Users\\igharok\\Desktop\\Job\\ARIMA\n",
    "#%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  2.1.4\n",
      "numpy version:  1.26.3\n",
      "python version:  3.11.5\n"
     ]
    }
   ],
   "source": [
    "### VERSION CONTROL (NO CHANGES)\n",
    "\n",
    "from platform import python_version\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('numpy version: ', np.__version__)\n",
    "print('python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: PARAMETERS & CONSTANTS (FILE NAMES OF OUTPUT HDF FILES MODIFIED)\n",
    "\n",
    "### GENERAL CONSTANTS:\n",
    "All = slice(None)\n",
    "### Business year length:\n",
    "int_bus_year = 260\n",
    "\n",
    "### EXCEL DATA EXTRACTION:\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable']\n",
    "### Raw data path and sheets:\n",
    "str_path_bb_idx_source = 'Data_Files/Source_Files/Bloomberg_Eco_Indices.xlsx'\n",
    "str_all_sheet = 'All Eco Const'\n",
    "### Flags data path and sheets:\n",
    "str_path_bb_idx_flags = 'Data_Files/Source_Files/Bloomberg_Eco_Flags_Extended.xlsx'\n",
    "str_flag_sheet = 'Bloomberg Description'\n",
    "### Source data constants:\n",
    "int_idx_cols = 12\n",
    "### TRANSFORMED BLOOMBERG DATA KEEPING:\n",
    "### HDF file with converted source data:\n",
    "str_path_bb_idx_hdf = 'Data_Files/Source_Files/2024/Bloomberg_Preparation_2024.h5' ### LINE TO MODIFY\n",
    "str_key_flags = 'flags_exported' ### Acadian flags list\n",
    "str_key_exported = 'all_idx_exported' ### Raw export with only replacing zero dates and after 2021-01-01 dates with np.NaN\n",
    "str_key_raw_filled = 'all_idx_raw_filled' ### Raw export with initial dates, dates gaps, absent date columns filled\n",
    "str_key_raw_history = 'raw_history' ### Export with all the corrections and fillings (restructured to [Index_Name -> Data_Date -> Observation_Date] | Value series)\n",
    "str_key_bday_history = 'bday_history' ### Raw history vector with observation dates moved to nearest future business dates\n",
    "str_key_types_info = 'types_info' ### Dataframe with 'Type_Prime' / 'Sub_Type' / 'Region' groups descriptions\n",
    "str_key_flags_typed = 'flags_typed' ### Dataframe with economic indices descriptions taking into account \n",
    "str_key_survey_history = 'survey_history' ### Release values are replaced with Survey Medians to normalize it ### LINE TO ADD\n",
    "str_key_norm_filled = 'all_idx_norm_filled' ### Normalized values with initial dates, dates gaps, absent date columns filled ### LINE TO ADD\n",
    "\n",
    "### DATA TRANSFORMATION:\n",
    "### Date range:\n",
    "datetime_start = datetime(1984, 12, 31) # Start date for efficacy measures\n",
    "date_start = datetime_start.date()\n",
    "datetime_end = datetime(2020, 8, 31) # End date for efficacy measures\n",
    "date_end = datetime_end.date()\n",
    "num_date_control = (datetime_end.year + 1) * 10000 ### LINE TO ADD\n",
    "idx_date_range = pd.date_range(date_start, date_end, freq = 'B')\n",
    "datetime_basis = datetime(1993, 12, 31) # End date for efficacy measures\n",
    "date_basis = datetime_basis.date()\n",
    "### Gaps filling options:\n",
    "int_revision_shift = 1\n",
    "int_final_shift = 2\n",
    "int_first_mean_length = 12\n",
    "dict_final_only_lag = {}\n",
    "dict_final_only_lag['Quarterly'] = 90 // 2\n",
    "dict_final_only_lag['Monthly'] = 30 // 2\n",
    "dict_final_only_lag['Other'] = 7 // 2\n",
    "\n",
    "### TRANSFORMATION TO MOM & MATRIX Z-SCORING:\n",
    "### Group tickers rebasing options:\n",
    "int_not_to_rebase_term = 7 ### Term in years for min ticker data date when we do not need to rebase it with basis group ticker\n",
    "int_not_to_rebase_diff = 2 ### Minimal difference in years between basis ticker and other group ticker min date when we need to rebase group ticker\n",
    "### CPU count to use during multiprocessing:\n",
    "int_cpu_count = 4\n",
    "### Cumprod shifts for monthly data frequency:\n",
    "dict_cumprod_step = {}\n",
    "dict_cumprod_step['MoM%'] = 1\n",
    "dict_cumprod_step['QoQ%'] = 3\n",
    "dict_cumprod_step['YoY%'] = 12\n",
    "### Stock-like series shifts for MoM transformation:\n",
    "dict_mom_shift = {}\n",
    "dict_mom_shift['Monthly'] = 1\n",
    "dict_mom_shift['Other'] = 4\n",
    "### Z-scoring options:\n",
    "int_winsorize_bound = 4\n",
    "flo_winsorize_tolerance = 0.0001\n",
    "int_winsorize_steps_limit = 5\n",
    "### Diagonal options:\n",
    "int_min_years_z_score = 3\n",
    "int_max_years_z_score = 10\n",
    "date_diag_start = datetime(1994, 1, 1)\n",
    "### HDF file with matrices:\n",
    "str_path_bb_matrix_mom_hdf = 'Data_Files/Source_Files/2024/Matrix_Eco_Indices_mom_2024.h5' ### LINE TO MODIFY\n",
    "str_key_matrix_z = 'matrix_cube_z_scored'\n",
    "\n",
    "### MATRICES AGGREGATION:\n",
    "### Data filling limit\n",
    "int_fill_limit = 20\n",
    "### Average region correlation matrix weight for daily correlation matrix shrinking:\n",
    "flo_reg_weight = 0.5\n",
    "### Regions weights:\n",
    "dict_region_weight = {}\n",
    "dict_region_weight['US'] = 0.50\n",
    "dict_region_weight['Europe'] = 0.25\n",
    "dict_region_weight['Japan'] = 0.15\n",
    "dict_region_weight['UK'] = 0.10\n",
    "### HDF file with group averages:\n",
    "str_path_group_matrix_mom_hdf = 'Data_Files/Source_Files/2024/Matrix_Groups_mom_2024.h5' ### LINE TO MODIFY\n",
    "str_key_group_matrix = 'matrix_cube_groups'\n",
    "### HDF file with overall event dates as series index:\n",
    "str_path_overall_dates_hdf = 'Data_Files/Source_Files/2024/Overall_Dates_2024.h5' ### LINE TO MODIFY\n",
    "str_key_event_dates = 'overall_event_dates'\n",
    "str_key_obs_dates = 'overall_obs_dates'\n",
    "str_key_triangle_dates = 'overall_triangle_dates'\n",
    "### HDF file with sub type averages:\n",
    "str_path_sub_matrix_mom_hdf = 'Data_Files/Source_Files/2024/Matrix_Sub_mom_2024.h5' ### LINE TO MODIFY\n",
    "str_key_sub_matrix = 'matrix_cube_subs'\n",
    "### Global indices files:\n",
    "str_path_ind_matrix_inf_mom_hdf = 'Data_Files/Source_Files/2024/Matrix_Global_mom_2024.h5' ### LINE TO MODIFY\n",
    "str_key_global_matrix = 'matrix_cube_globals'\n",
    "### Global indices compositions:\n",
    "dict_global_index_hdf = {}\n",
    "dict_global_index_hdf[('INF_mom')] = str_path_ind_matrix_inf_mom_hdf\n",
    "### Global indices names:\n",
    "dict_global_index_name = {}\n",
    "dict_global_index_name[('INF_mom')] = 'Inflation Index (MoM)'\n",
    "### Rolling correlation tail length:\n",
    "int_corr_tail = 5\n",
    "### A-la Newey-West adjustment maximum lag:\n",
    "int_n_w_lag = 4\n",
    "### Covariance subsamples number:\n",
    "int_cov_samples = 22\n",
    "### Minimal years to use column for PCA performing:\n",
    "int_min_years_pca = 7\n",
    "### CPU count to use during multiprocessing:\n",
    "int_cpu_count = 4\n",
    "### HDF file with weights collection:\n",
    "str_path_bb_weights_hdf = 'Data_Files/Source_Files/2024/FPC_Weights_2024_mom.h5' ### LINE TO MODIFY\n",
    "### HDF file with correlation matrices collection:\n",
    "str_path_bb_corrs_hdf = 'Data_Files/Source_Files/2024/FPC_Correlations_2024_mom.h5' ### LINE TO MODIFY\n",
    "### HDF file with percentiles:\n",
    "str_path_bb_percentiles_hdf = 'Data_Files/Source_Files/2024/Main_Percentiles_2024_mom.h5' ### LINE TO MODIFY\n",
    "### Percentils calculation options:\n",
    "int_ptile_months = 10 * 12\n",
    "int_ave_months = 5 * 12\n",
    "int_halflife_months = 1 * 12\n",
    "\n",
    "### DIAGONALS COLLECTION:\n",
    "### HDF file with diagonals:\n",
    "str_path_bb_diag_hdf = 'Data_Files/Source_Files/2024/Matrix_Diagonals_2024_mom.h5' ### LINE TO MODIFY\n",
    "str_key_diag_daily_mom = 'matrix_diagonal_mom'\n",
    "str_key_diag_group_mom = 'groups_diagonal_mom'\n",
    "str_key_diag_sub_mom = 'sub_types_diagonal_mom'\n",
    "str_key_diag_agg_z_lim = 'aggregated_diagonal_z_limited'\n",
    "### Global indices diagonals keys:\n",
    "dict_global_index_diag_key = {}\n",
    "dict_global_index_diag_key[('INF_mom')] = 'global_diagonal_inf_mom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING OBSERVATION DATE VECTOR EXTRACTION (NO CHANGES)\n",
    "\n",
    "def get_obs_date_vector(str_ticker, str_date, bool_exact_date = False, bool_drop_levels = False):\n",
    "    ### Vector for exact date:\n",
    "    if bool_exact_date:\n",
    "        ser_obs_date = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = '(Index_Name == str_ticker) & (Observation_Date == str_date)') ### LINE TO MODIFY\n",
    "    ### Vector for nearest date:        \n",
    "    else:\n",
    "        ### Loading full ticker series:        \n",
    "        ser_z_scored = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = '(Index_Name == str_ticker) & (Observation_Date <= str_date)') ### LINE TO MODIFY\n",
    "        ### Extracting data for max date less or equal to needed date:\n",
    "        ser_obs_date = ser_z_scored.loc[All, All, [ser_z_scored.index.levels[-1].max()]]\n",
    "    ### Dropping constant index levels if needed:\n",
    "    if bool_drop_levels:\n",
    "        return ser_obs_date.droplevel(['Index_Name', 'Observation_Date'])\n",
    "    else:\n",
    "        return ser_obs_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE FOR DATAFRAME COLUMNS (NO CHANGES)\n",
    "\n",
    "def columns_average(df_series, list_weights = False): \n",
    "    ### Single column check\n",
    "    if (len(df_series.columns) > 1):\n",
    "        ### Equal weights list creating:\n",
    "        if isinstance(list_weights, bool):\n",
    "            list_weights = [1] * len(df_series.columns)\n",
    "        ### Dataframe of weights initialising:\n",
    "        df_weights = pd.DataFrame(np.NaN, index = df_series.index, columns = df_series.columns)\n",
    "        for iter_num, iter_col in enumerate(df_weights.columns):\n",
    "            df_weights[iter_col] = list_weights[iter_num]\n",
    "        ### Zeroing weights for NaN values:\n",
    "        for iter_col in df_weights.columns:\n",
    "            df_weights.loc[df_series[iter_col].isna(), iter_col] = 0\n",
    "        ser_mean = (df_series.multiply(df_weights).sum(axis = 1)).div(df_weights.sum(axis = 1))    \n",
    "        ### Results output:\n",
    "        del df_series\n",
    "        del df_weights    \n",
    "        gc.collect()\n",
    "    else:\n",
    "        ser_mean = df_series.squeeze()\n",
    "        del df_series\n",
    "        gc.collect()        \n",
    "    return ser_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT (NO CHANGES)\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE (NO CHANGES)\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECCPEMUM Index : Reindexation\n",
      "ECCPEMUM Index : Transformation to stock-like series: Cumulative product\n",
      "ECCPEMUM Index : Seasonality adjustment\n",
      "SA success :  ECCPEMUM Index  :  1999-03-19\n",
      "SA success :  ECCPEMUM Index  :  1999-04-19\n",
      "SA success :  ECCPEMUM Index  :  1999-05-19\n",
      "SA success :  ECCPEMUM Index  :  1999-06-21\n",
      "SA success :  ECCPEMUM Index  :  1999-07-19\n",
      "SA success :  ECCPEMUM Index  :  1999-08-19\n",
      "SA success :  ECCPEMUM Index  :  1999-09-20\n",
      "SA success :  ECCPEMUM Index  :  1999-10-19\n",
      "SA success :  ECCPEMUM Index  :  1999-11-19\n",
      "SA success :  ECCPEMUM Index  :  1999-12-20\n",
      "SA success :  ECCPEMUM Index  :  2000-01-19\n",
      "SA success :  ECCPEMUM Index  :  2000-02-21\n",
      "SA success :  ECCPEMUM Index  :  2000-03-20\n",
      "SA success :  ECCPEMUM Index  :  2000-04-19\n",
      "SA success :  ECCPEMUM Index  :  2000-05-19\n",
      "SA success :  ECCPEMUM Index  :  2000-06-19\n",
      "SA success :  ECCPEMUM Index  :  2000-07-19\n",
      "SA success :  ECCPEMUM Index  :  2000-08-21\n",
      "SA success :  ECCPEMUM Index  :  2000-09-19\n",
      "SA success :  ECCPEMUM Index  :  2000-10-19\n",
      "SA success :  ECCPEMUM Index  :  2000-11-20\n",
      "SA success :  ECCPEMUM Index  :  2000-12-19\n",
      "SA success :  ECCPEMUM Index  :  2001-01-19\n",
      "SA success :  ECCPEMUM Index  :  2001-02-19\n",
      "SA success :  ECCPEMUM Index  :  2001-03-19\n",
      "SA success :  ECCPEMUM Index  :  2001-04-19\n",
      "SA success :  ECCPEMUM Index  :  2001-05-16\n",
      "SA success :  ECCPEMUM Index  :  2001-06-18\n",
      "SA success :  ECCPEMUM Index  :  2001-07-18\n",
      "SA success :  ECCPEMUM Index  :  2001-08-17\n",
      "SA success :  ECCPEMUM Index  :  2001-09-18\n",
      "SA success :  ECCPEMUM Index  :  2001-10-17\n",
      "SA success :  ECCPEMUM Index  :  2001-11-16\n",
      "SA success :  ECCPEMUM Index  :  2001-12-18\n",
      "SA success :  ECCPEMUM Index  :  2002-01-22\n",
      "SA success :  ECCPEMUM Index  :  2002-02-28\n",
      "SA success :  ECCPEMUM Index  :  2002-03-18\n",
      "SA success :  ECCPEMUM Index  :  2002-04-17\n",
      "SA success :  ECCPEMUM Index  :  2002-05-16\n",
      "SA success :  ECCPEMUM Index  :  2002-06-18\n",
      "SA success :  ECCPEMUM Index  :  2002-07-17\n",
      "SA success :  ECCPEMUM Index  :  2002-08-19\n",
      "SA success :  ECCPEMUM Index  :  2002-09-18\n",
      "SA success :  ECCPEMUM Index  :  2002-10-16\n",
      "SA success :  ECCPEMUM Index  :  2002-11-18\n",
      "SA success :  ECCPEMUM Index  :  2002-12-18\n",
      "SA success :  ECCPEMUM Index  :  2003-01-22\n",
      "SA success :  ECCPEMUM Index  :  2003-02-28\n",
      "SA success :  ECCPEMUM Index  :  2003-03-18\n",
      "SA success :  ECCPEMUM Index  :  2003-04-16\n",
      "SA success :  ECCPEMUM Index  :  2003-05-16\n",
      "SA success :  ECCPEMUM Index  :  2003-06-18\n",
      "SA success :  ECCPEMUM Index  :  2003-07-16\n",
      "SA success :  ECCPEMUM Index  :  2003-08-19\n",
      "SA success :  ECCPEMUM Index  :  2003-09-17\n",
      "SA success :  ECCPEMUM Index  :  2003-10-16\n",
      "SA success :  ECCPEMUM Index  :  2003-11-18\n",
      "SA success :  ECCPEMUM Index  :  2003-12-17\n",
      "SA success :  ECCPEMUM Index  :  2004-01-21\n",
      "SA success :  ECCPEMUM Index  :  2004-02-27\n",
      "SA success :  ECCPEMUM Index  :  2004-03-17\n",
      "SA success :  ECCPEMUM Index  :  2004-04-16\n",
      "SA success :  ECCPEMUM Index  :  2004-05-18\n",
      "SA success :  ECCPEMUM Index  :  2004-06-16\n",
      "SA success :  ECCPEMUM Index  :  2004-07-16\n",
      "SA success :  ECCPEMUM Index  :  2004-08-18\n",
      "SA success :  ECCPEMUM Index  :  2004-09-16\n",
      "SA success :  ECCPEMUM Index  :  2004-10-18\n",
      "SA success :  ECCPEMUM Index  :  2004-11-17\n",
      "SA success :  ECCPEMUM Index  :  2004-12-16\n",
      "SA success :  ECCPEMUM Index  :  2005-01-20\n",
      "SA success :  ECCPEMUM Index  :  2005-02-28\n",
      "SA success :  ECCPEMUM Index  :  2005-03-16\n",
      "SA success :  ECCPEMUM Index  :  2005-04-18\n",
      "SA success :  ECCPEMUM Index  :  2005-05-19\n",
      "SA success :  ECCPEMUM Index  :  2005-06-16\n",
      "SA success :  ECCPEMUM Index  :  2005-07-18\n",
      "SA success :  ECCPEMUM Index  :  2005-08-18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 253\u001b[0m\n\u001b[0;32m    250\u001b[0m int_max_name_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(ser_history_other\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mlevels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen())\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m### Data transforming:\u001b[39;00m\n\u001b[0;32m    252\u001b[0m ser_history_other\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndex_Name\u001b[39m\u001b[38;5;124m'\u001b[39m, group_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\\\n\u001b[1;32m--> 253\u001b[0m         \u001b[38;5;241m.\u001b[39mapply(complex_transform, idx_date_range, df_flags_typed, int_max_name_length, int_min_years_z_score, str_path_bb_matrix_mom_hdf, bool_perform_sa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:228\u001b[0m, in \u001b[0;36mSeriesGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[0;32m    223\u001b[0m     _apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m, examples\u001b[38;5;241m=\u001b[39m_apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries_examples\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m )\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1770\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1769\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1770\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1772\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[0;32m   1773\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1819\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1791\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1792\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1817\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1819\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1821\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:911\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    910\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 911\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    913\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1758\u001b[0m, in \u001b[0;36mGroupBy.apply.<locals>.f\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m   1756\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(g):\n\u001b[1;32m-> 1758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(g, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[10], line 184\u001b[0m, in \u001b[0;36mcomplex_transform\u001b[1;34m(ser_name, idx_date_range, df_flags, int_max_name_length, int_min_years, str_path_bb_matrix_hdf, bool_perform_sa)\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[38;5;66;03m### Filling empty values:            \u001b[39;00m\n\u001b[0;32m    183\u001b[0m             ser_stock \u001b[38;5;241m=\u001b[39m ser_stock\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObservation_Date\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mffill()\n\u001b[1;32m--> 184\u001b[0m             ser_stock \u001b[38;5;241m=\u001b[39m ser_stock\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObservation_Date\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(perform_x13_sa)\u001b[38;5;241m.\u001b[39mswaplevel()\u001b[38;5;241m.\u001b[39msort_index()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m#            ser_stock = transformParallel(ser_stock.groupby('Observation_Date'), perform_x13_sa).swaplevel().sort_index()       \u001b[39;00m\n\u001b[0;32m    186\u001b[0m             \u001b[38;5;66;03m### Dropping NA values:\u001b[39;00m\n\u001b[0;32m    187\u001b[0m             ser_stock\u001b[38;5;241m.\u001b[39mloc[idx_isna] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mNaN    \n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:228\u001b[0m, in \u001b[0;36mSeriesGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[0;32m    223\u001b[0m     _apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m, examples\u001b[38;5;241m=\u001b[39m_apply_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries_examples\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    225\u001b[0m     )\n\u001b[0;32m    226\u001b[0m )\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1770\u001b[0m, in \u001b[0;36mGroupBy.apply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1768\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1769\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1770\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selected_obj)\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   1772\u001b[0m         \u001b[38;5;66;03m# gh-20949\u001b[39;00m\n\u001b[0;32m   1773\u001b[0m         \u001b[38;5;66;03m# try again, with .apply acting as a filtering\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m         \u001b[38;5;66;03m# on a string grouper column\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_apply_general(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1819\u001b[0m, in \u001b[0;36mGroupBy._python_apply_general\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1784\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_python_apply_general\u001b[39m(\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1791\u001b[0m     is_agg: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1792\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;124;03m    Apply function f in python space\u001b[39;00m\n\u001b[0;32m   1795\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1817\u001b[0m \u001b[38;5;124;03m        data after applying f\u001b[39;00m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1819\u001b[0m     values, mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39mapply_groupwise(f, data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n\u001b[0;32m   1820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1821\u001b[0m         not_indexed_same \u001b[38;5;241m=\u001b[39m mutated\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:911\u001b[0m, in \u001b[0;36mBaseGrouper.apply_groupwise\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[0;32m    910\u001b[0m group_axes \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39maxes\n\u001b[1;32m--> 911\u001b[0m res \u001b[38;5;241m=\u001b[39m f(group)\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[0;32m    913\u001b[0m     mutated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m, in \u001b[0;36mcomplex_transform.<locals>.perform_x13_sa\u001b[1;34m(ser_date)\u001b[0m\n\u001b[0;32m     45\u001b[0m flo_positron \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(ser_result\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m### Performing seasonality adjustment:\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     ser_result \u001b[38;5;241m=\u001b[39m x13_arima_analysis(ser_result \u001b[38;5;241m+\u001b[39m flo_positron, outlier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, trading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mseasadj \u001b[38;5;241m-\u001b[39m flo_positron\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSA success : \u001b[39m\u001b[38;5;124m'\u001b[39m, str_index_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m : \u001b[39m\u001b[38;5;124m'\u001b[39m, ser_date\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_level_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObservation_Date\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdate())             \n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:213\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m    212\u001b[0m     kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\x13.py:452\u001b[0m, in \u001b[0;36mx13_arima_analysis\u001b[1;34m(endog, maxorder, maxdiff, diff, exog, log, outlier, trading, forecast_periods, retspec, speconly, start, freq, print_stdout, x12path, prefer_x13, tempdir)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# call x12 arima\u001b[39;00m\n\u001b[0;32m    451\u001b[0m p \u001b[38;5;241m=\u001b[39m run_spec(x12path, ftempin\u001b[38;5;241m.\u001b[39mname[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m], ftempout\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 452\u001b[0m p\u001b[38;5;241m.\u001b[39mwait()\n\u001b[0;32m    453\u001b[0m stdout \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_stdout:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\subprocess.py:1588\u001b[0m, in \u001b[0;36mPopen._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1585\u001b[0m     timeout_millis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1587\u001b[0m     \u001b[38;5;66;03m# API note: Returns immediately if timeout_millis == 0.\u001b[39;00m\n\u001b[1;32m-> 1588\u001b[0m     result \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForSingleObject(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[0;32m   1589\u001b[0m                                          timeout_millis)\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;241m==\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWAIT_TIMEOUT:\n\u001b[0;32m   1591\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### RUN TO RE-EXPORT DATA: HISTORY DATA TRANSFORMATION : MOM ONLY (CELL TO REPLACE)\n",
    "\n",
    "### Defining Economic Index series transformation:\n",
    "def complex_transform(ser_name, idx_date_range, df_flags, int_max_name_length, int_min_years, str_path_bb_matrix_hdf, bool_perform_sa = False):\n",
    "    ### Defining triangle extraction:\n",
    "    def triangle_filter(ser_date):\n",
    "        ### Extracting particular Data Date:\n",
    "        date_diag = ser_date.index.get_level_values('Data_Date')[0]\n",
    "        ### Dropping constant level:\n",
    "        ser_result = ser_date.droplevel('Data_Date')\n",
    "        ### Filtering over-diagonal values:\n",
    "        ser_result = ser_result[ser_result.index >= date_diag] \n",
    "        ### Results output:\n",
    "        return ser_result\n",
    "    ### Period-over-period-percent ticker values transforming to stock-like series:\n",
    "    def pop_to_level(ser_date, int_step):\n",
    "        ### Dropping constant level:\n",
    "        ser_result = ser_date.droplevel('Observation_Date')\n",
    "        ### Basis initiating:\n",
    "        flo_basement = 1.0\n",
    "        ### Factor initiating: \n",
    "        flo_next_brick  = 1.0\n",
    "        ### Looping over month numbers:\n",
    "        for iter_period in range(min(int_step, len(ser_result.index))):         \n",
    "            ### Basement building up:\n",
    "            flo_basement = flo_basement * flo_next_brick\n",
    "            ### Next basement brick producing:\n",
    "            flo_next_brick = ((flo_next_brick ** (iter_period)) * (ser_result.iloc[iter_period] ** (1 / int_step))) ** (1 / (iter_period + 1)) \n",
    "            ### Jumping cumulative product performing:\n",
    "            idx_iter_data = ser_result.index[iter_period :: int_step]\n",
    "            ser_result.loc[idx_iter_data] = ser_result.loc[idx_iter_data].cumprod() * flo_basement       \n",
    "        ### Results output:            \n",
    "        return ser_result    \n",
    "    ### X13 ARIMA Seasonality adjustment model:\n",
    "    def perform_x13_sa(ser_date):\n",
    "        ### Dropping constant level:        \n",
    "        ser_result = ser_date.droplevel('Observation_Date')\n",
    "        ### Check for not empty vector:\n",
    "        if (ser_result.count() > 0):\n",
    "            ### Check for minimal quantity of observations to perform seasonality adjustment:\n",
    "            if (ser_result.last_valid_index() - ser_result.first_valid_index()).days >= (int_min_years * 365):   \n",
    "                ### Naming series for x13 performing:\n",
    "                ser_result.name = 'Ticker'\n",
    "                ### Calculating shift value to make all series positive:\n",
    "                flo_positron = abs(ser_result.min()) * 2\n",
    "                try:\n",
    "                    ### Performing seasonality adjustment:\n",
    "                    ser_result = x13_arima_analysis(ser_result + flo_positron, outlier = True, trading = True).seasadj - flo_positron\n",
    "#                    print('SA success : ', str_index_name, ' : ', ser_date.index.get_level_values('Observation_Date')[0].date())             \n",
    "                except Exception as error:\n",
    "                    print('SA error : ', str_index_name, ' : ', ser_date.index.get_level_values('Observation_Date')[0].date(), ' : ', type(error).__name__)\n",
    "                    pass\n",
    "        ### Results output:                \n",
    "        return ser_result \n",
    "#        return pd.concat([ser_result], keys = [ser_date.index.get_level_values('Observation_Date')[0]], names = ['Observation_Date'])    \n",
    "    ### Extracting Observation Date column for ticker:\n",
    "    def get_obs_date_vector(str_ticker, str_path_bb_matrix_hdf, str_date, bool_exact_date = False, bool_drop_levels = True):\n",
    "        ### Vector for exact date:\n",
    "        if bool_exact_date:\n",
    "            ser_obs_date = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == str_ticker & Observation_Date == str_date')\n",
    "        ### Vector for nearest date:        \n",
    "        else:\n",
    "            ### Loading full ticker series:        \n",
    "            ser_z_scored = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == str_ticker & Observation_Date <= str_date')\n",
    "            ### Extracting data for max date less or equal to needed date:\n",
    "            ser_obs_date = ser_z_scored.loc[All, All, [ser_z_scored.index.levels[-1].max()]]\n",
    "        ### Dropping constant index levels if needed:\n",
    "        if bool_drop_levels:\n",
    "            return ser_obs_date.droplevel(['Index_Name', 'Observation_Date'])\n",
    "        else:\n",
    "            return ser_obs_date    \n",
    "    ### Defining time-vector z-scoring procedure:    \n",
    "    def by_date_z_score(ser_date, int_winsorize_bound, flo_tolerance, int_winsorize_steps_limit, int_min_years_adj, \n",
    "                        str_path_bb_matrix_hdf, str_basis_index, bool_rebase_flag, list_continue_rebase):\n",
    "        ### Check for empty vector (doing nothing):\n",
    "        if ser_date.count():\n",
    "            ### Check for non-constant vector:\n",
    "            if (ser_date.std() > flo_tolerance):\n",
    "                ### Check for minimal quantity of observations to z-score:\n",
    "                if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years_adj * 365):   \n",
    "                    ### Calculating of z scores:\n",
    "                    ser_date = (ser_date - ser_date.mean()) / ser_date.std()        \n",
    "                    bool_to_winsor = True   \n",
    "                    int_iter = 1\n",
    "                    while (bool_to_winsor): \n",
    "                        int_iter += 1                \n",
    "                        ### Value based winsorization:                \n",
    "                        ser_date.clip(lower = -int_winsorize_bound, upper = int_winsorize_bound, inplace = True)\n",
    "                        ### Recalculating of z scores:\n",
    "                        ser_date = (ser_date - ser_date.mean()) / ser_date.std()\n",
    "                        ### Checking for boundaries and steps:\n",
    "                        if((ser_date.loc[ser_date.abs() >= (int_winsorize_bound + flo_tolerance)].count() == 0) | (int_iter > int_winsorize_steps_limit)):\n",
    "                            bool_to_winsor = False\n",
    "                    ### Checking if rebasing needed:\n",
    "                    if (bool(str_basis_index) & bool_rebase_flag & list_continue_rebase[0]):\n",
    "                        ### Extracting column from z-scored basis ticker series:\n",
    "                        str_obs_date = ser_date.index[0][1].strftime('%Y-%m-%d')\n",
    "                        ser_basis_date = get_obs_date_vector(str_basis_index, str_path_bb_matrix_hdf, str_obs_date, bool_exact_date = False, bool_drop_levels = True)\n",
    "                        ### Selecting only intersected time interval:\n",
    "                        ser_basis_part = ser_basis_date.loc[ser_date.first_valid_index()[0]: ]\n",
    "                        ### Rebasing ticker:\n",
    "                        ser_date = ser_date * ser_basis_part.std() + ser_basis_part.mean()\n",
    "                        ### Checking if future rebasing needed:\n",
    "                        if ((abs(ser_basis_part.std() - 1) < flo_tolerance) & (abs(ser_basis_part.mean()) < flo_tolerance)):\n",
    "                            list_continue_rebase[0] = False\n",
    "                else:\n",
    "                    ### Killing values that we can't z-score\n",
    "                    ser_date.loc[All] = np.NaN\n",
    "            else:\n",
    "                ### Check for minimal quantity of observations to z-score:\n",
    "                if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years_adj * 365):             \n",
    "                    ### Constant values demeaning:\n",
    "                    ser_date = ser_date - ser_date.mean()\n",
    "                    ### Checking if rebasing needed:\n",
    "                    if (bool(str_basis_index) & bool_rebase_flag & list_continue_rebase[0]):\n",
    "                        ### Extracting column from z-scored basis ticker series:\n",
    "                        str_obs_date = ser_date.index[0][1].strftime('%Y-%m-%d')                    \n",
    "                        ser_basis_date = get_obs_date_vector(str_basis_index, str_path_bb_matrix_hdf, str_obs_date, bool_exact_date = False, bool_drop_levels = True)\n",
    "                        ### Selecting only intersected time interval:\n",
    "                        ser_basis_part = ser_basis_date.loc[ser_date.first_valid_index()[0]: ]\n",
    "                        ### Rebasing ticker:\n",
    "                        ser_date = ser_date * ser_basis_part.std() + ser_basis_part.mean()\n",
    "                        ### Checking if future rebasing needed:\n",
    "                        if ((abs(ser_basis_part.std() - 1) < flo_tolerance) & (abs(ser_basis_part.mean()) < flo_tolerance)):\n",
    "                            list_continue_rebase[0] = False\n",
    "                else:\n",
    "                    ### Killing values that we can't z-score\n",
    "                    ser_date.loc[All] = np.NaN\n",
    "        ### Memory optimization:\n",
    "        ser_date = ser_date.astype('float32')\n",
    "        return ser_date    \n",
    "    ### EI name extracting:\n",
    "    str_index_name = ser_name.index.get_level_values(0)[0]\n",
    "    ### Observation dates reindexation:    \n",
    "    print(ser_name.index.get_level_values(0)[0], ': Reindexation')    \n",
    "    idx_observation_range = ser_name.index.get_level_values('Observation_Date').unique().intersection(idx_date_range).sort_values()\n",
    "    ser_full = ser_name.droplevel('Index_Name').unstack('Data_Date').reindex(idx_observation_range).stack('Data_Date', dropna = False).squeeze()\n",
    "    ser_full = ser_full.swaplevel()\n",
    "    ser_full.index.rename('Observation_Date', level = -1, inplace = True)    \n",
    "    ### Forward filling for each data date:\n",
    "    ser_full = ser_full.groupby('Data_Date').ffill()   \n",
    "    ### Diagonalization:\n",
    "    ser_triangle = ser_full.groupby('Data_Date').apply(triangle_filter).sort_index()\n",
    "    ### Flags extracting:\n",
    "    ser_flags = df_flags.loc[str_index_name, All].squeeze() \n",
    "    ### 'TAR' type checking:\n",
    "    if (ser_flags['Type_Prime'] == 'TAR'):\n",
    "        print(str_index_name, ': TAR Primary Type ignoring')        \n",
    "        pass\n",
    "    ### Flags-based transforming:\n",
    "#    else:\n",
    "    elif (ser_flags['Type_Prime'] == 'INF'):        \n",
    "        ### Indices of NA values collecting:\n",
    "        idx_isna = ser_triangle.loc[ser_triangle.isna()].index\n",
    "        ### Transforming to stock-like series:\n",
    "        if (ser_flags['Processing'] in ['Index', 'Level', 'Level%']):\n",
    "            ser_stock = ser_triangle\n",
    "        elif (ser_flags['Processing'] == 'Flow'):\n",
    "            print(str_index_name, ': Transformation to stock-like series: Cumulative sum')\n",
    "            ### Filling empty values:\n",
    "            ser_triangle = ser_triangle.fillna(0)\n",
    "            ### Cumulative sum for each observation date calculating:\n",
    "            ser_stock = ser_triangle.groupby('Observation_Date').cumsum()\n",
    "            ### Dropping NA values:\n",
    "            ser_stock.loc[idx_isna] = np.NaN\n",
    "        else:\n",
    "            print(str_index_name, ': Transformation to stock-like series: Cumulative product')\n",
    "            ### Filling empty values:\n",
    "            ser_triangle = ser_triangle.fillna(0)\n",
    "            ### Percents to multipliers converting:\n",
    "            ser_stock = 1 + ser_triangle / 100\n",
    "            ### Calculating with needed periodicity:\n",
    "            if (ser_flags['Frequency'] == 'Monthly'):\n",
    "                int_step = dict_cumprod_step[ser_flags['Processing']]\n",
    "                ### Period-by-period cumprod with rebasing:\n",
    "                ser_stock = ser_stock.groupby('Observation_Date').apply(pop_to_level, int_step).swaplevel().sort_index()\n",
    "            ### Dropping NA values:\n",
    "            ser_stock.loc[idx_isna] = np.NaN\n",
    "        ### Seasonality adjustment:\n",
    "        if (bool_perform_sa & (ser_flags['SA_Status'].strip(' ') != 'SA')):\n",
    "            print(str_index_name, ': Seasonality adjustment')            \n",
    "            ### Filling empty values:            \n",
    "            ser_stock = ser_stock.groupby('Observation_Date').ffill()\n",
    "            ser_stock = ser_stock.groupby('Observation_Date').apply(perform_x13_sa).swaplevel().sort_index()\n",
    "#            ser_stock = transformParallel(ser_stock.groupby('Observation_Date'), perform_x13_sa).swaplevel().sort_index()       \n",
    "            ### Dropping NA values:\n",
    "            ser_stock.loc[idx_isna] = np.NaN    \n",
    "        ### Transforming to PoP series:\n",
    "        if (ser_flags['Processing'] == 'Index'):\n",
    "            ### Debasing only:\n",
    "            print(str_index_name, ': Transformation to MoM series: Debasing')            \n",
    "            ser_mom = ser_stock - ser_flags['Base']           \n",
    "        elif (ser_flags['Processing'] in ['Flow', 'Level']):    \n",
    "            ### Simple difference:\n",
    "            print(str_index_name, ': Transformation to MoM series: Simple difference')\n",
    "            ### Shifting lag defining:\n",
    "            if (ser_flags['Frequency'] in dict_mom_shift.keys()):\n",
    "                int_mom_shift = dict_mom_shift[ser_flags['Frequency']]\n",
    "            else:\n",
    "                int_mom_shift = dict_mom_shift['Other']            \n",
    "            ### Stock-like series differing:\n",
    "            ser_mom = ser_stock.groupby('Observation_Date', group_keys = False).apply(lambda ser_obs_date: ser_obs_date - ser_obs_date.shift(int_mom_shift))\n",
    "        else:      \n",
    "            ### Difference with dividing:\n",
    "            print(str_index_name, ': Transformation to MoM series: Difference with dividing')\n",
    "            ### Shifting lag defining:\n",
    "            if (ser_flags['Frequency'] in dict_mom_shift.keys()):\n",
    "                int_mom_shift = dict_mom_shift[ser_flags['Frequency']]\n",
    "            else:\n",
    "                int_mom_shift = dict_mom_shift['Other']\n",
    "            ### Stock-like series differing:\n",
    "            ser_mom = ser_stock.groupby('Observation_Date', group_keys = False)\\\n",
    "                               .apply(lambda ser_obs_date: (ser_obs_date / ser_obs_date.shift(int_mom_shift) - 1))  \n",
    "        ser_mom.name = 'MoM'\n",
    "        ### Negative flag check:\n",
    "        if (ser_flags['Negative'] == 1):\n",
    "            ser_mom = -ser_mom\n",
    "        ### Z-scoring across the observation dates:\n",
    "        print(ser_name.index.get_level_values(0)[0], ': Z-scoring across the observation dates')\n",
    "        ### To stop rebasing when basic ticker (std, mean) are close to (1, 0):\n",
    "        list_continue_rebase = [True] \n",
    "        ### Adjusting Z-score period limit for some groups:\n",
    "        if (ser_flags['Data_Source'] == 'Markit'):\n",
    "            int_min_years_adj = int_min_years - 1\n",
    "        else:\n",
    "            int_min_years_adj = int_min_years    \n",
    "        ### Z-score tranformation:\n",
    "        ser_mom_z = ser_mom.groupby('Observation_Date')\\\n",
    "                           .transform(by_date_z_score, int_winsorize_bound, flo_winsorize_tolerance, int_winsorize_steps_limit, int_min_years_adj,\n",
    "                                      str_path_bb_matrix_hdf, ser_flags['Basic_Ticker'], ser_flags['Rebase_Flag'], list_continue_rebase)       \n",
    "#        ser_mom_z = transformParallel(ser_mom.groupby('Observation_Date'), by_date_z_score_to_parallel).sort_index() ### Parallelization attempt\n",
    "        ### Adding results to matrix cube:\n",
    "        pd.concat([ser_mom_z], keys = [str_index_name], names = ['Index_Name']).to_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, format = 'table',\n",
    "                                                                                       complevel = 9, append = True, mode = 'a',\n",
    "                                                                                       min_itemsize = {'Index_Name': int_max_name_length})    \n",
    "#    ### Results output:\n",
    "#    return pd.concat([ser_mom_z], keys = [str_index_name], names = ['Index_Name'])\n",
    "\n",
    "gc.collect()\n",
    "### Flags loading:\n",
    "df_flags_typed = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags_typed)\n",
    "df_flags_other = df_flags_typed[(df_flags_typed['Type_Prime'] != 'ANT') & (df_flags_typed['Processing'] != 'Index')]\n",
    "### Economic Indices vector loading:\n",
    "ser_history_bday = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_bday_history)\n",
    "ser_history_other = ser_history_bday.reindex(df_flags_other.index, level = 'Index_Name')#.loc[['GRFRIAMM Index', 'ITVHYOY Index', 'NERS20Y Index'], All, All]\n",
    "### Previous HDF file deleting:\n",
    "if os.path.isfile(str_path_bb_matrix_mom_hdf):\n",
    "    os.remove(str_path_bb_matrix_mom_hdf)\n",
    "### Maximum length calculating (for HDF manipulations):\n",
    "int_max_name_length = max(ser_history_other.index.levels[0].str.len())\n",
    "### Data transforming:\n",
    "ser_history_other.groupby('Index_Name', group_keys = False, sort = False)\\\n",
    "        .apply(complex_transform, idx_date_range, df_flags_typed, int_max_name_length, int_min_years_z_score, str_path_bb_matrix_mom_hdf, bool_perform_sa = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: Z_SCORED EI DIAGONAL CONSTRUCTING (2 LINES TO MODIFY)\n",
    "\n",
    "### Economic Indices vector loading:\n",
    "ser_history_bday = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_bday_history)\n",
    "### Tickers list preparing:\n",
    "idx_ticker_list = ser_history_bday.index.levels[0]\n",
    "### Internal links between matrix files and diagonal keys:\n",
    "dict_diag_link = {}\n",
    "dict_diag_link[str_key_diag_daily_mom] = str_path_bb_matrix_mom_hdf\n",
    "### Looping over transformation ways:\n",
    "for iter_way in dict_diag_link:\n",
    "    ### File linking:\n",
    "    str_path_bb_matrix_hdf = dict_diag_link[iter_way]\n",
    "    ### Creating container for tickers diagonals:\n",
    "    gc.collect()\n",
    "    dict_ei_diag = {}\n",
    "    ### Looping over tickers:\n",
    "    for iter_ticker in idx_ticker_list:\n",
    "        ### Loading matrix for each ticker:\n",
    "        ser_iter_matrix = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == iter_ticker')\n",
    "        ### Excluding TAR tickers:\n",
    "        if (len(ser_iter_matrix) > 0):\n",
    "            print(iter_way, ':', iter_ticker, 'performing')\n",
    "            ser_iter_matrix = ser_iter_matrix.droplevel('Index_Name')\n",
    "            ### Extracting unique observation dates for each ticker later than diagonal start date:\n",
    "            idx_date_list = ser_iter_matrix.loc[All, date_diag_start : ].index.dropna().get_level_values('Observation_Date').unique()\n",
    "            ### Creating future diagonal vector:\n",
    "            ser_iter_diag = pd.Series(np.NaN, idx_date_list)    \n",
    "            ### Determining first valid date and first date to place z-scoring results on diagonal:\n",
    "            ### Looping over unique dates:\n",
    "            for iter_date in idx_date_list:\n",
    "                ### Trying to get the latest data date observation:\n",
    "                try:\n",
    "                    ser_iter_diag[iter_date] = ser_iter_matrix.loc[All, iter_date].dropna().iloc[-1] ### LINE TO MODIFY\n",
    "                except:\n",
    "                    pass\n",
    "            ### Checking for data earlier than diagonal start date:\n",
    "            if ((ser_iter_matrix.index.get_level_values('Observation_Date').unique().min() < date_diag_start) & pd.notna(ser_iter_diag.values[0])):\n",
    "                ### Turning diagonal start date column to diagonal with data dates as index dates:\n",
    "                ser_iter_start_col = ser_iter_matrix.loc[ : idx_date_list[0], [idx_date_list[0]]]\n",
    "                ### Ticker values on the date to turn:\n",
    "                list_iter_values_to_turn = ser_iter_start_col.values\n",
    "                ### Implementing lag before the announcement for the first valid observation date (3 years of data dates with equal first valid index):\n",
    "                int_release_lag = (ser_iter_start_col.dropna().index[-1][-1] - ser_iter_start_col.dropna().index[-1][0]).days\n",
    "                ### Taking announcement dates as an index for first column values:\n",
    "                list_iter_index_to_turn = ser_iter_start_col.index.get_level_values('Data_Date') + pd.offsets.Day(int_release_lag)\n",
    "                ### Modified column to turn:\n",
    "                ser_iter_to_turn = pd.Series(list_iter_values_to_turn, index = list_iter_index_to_turn)\n",
    "                ### Dropping repeated dates and empty dates:\n",
    "                ser_iter_to_turn = ser_iter_to_turn.groupby(level = 0).apply(lambda ser_date: ser_date.iloc[-1]).dropna() ### LINE TO MODIFY\n",
    "                ### Cutting series not to intersect with diagonal index:\n",
    "                ser_iter_to_turn = ser_iter_to_turn.loc[ : idx_date_list[0] - pd.offsets.Day(1)]\n",
    "                ### Joining series:\n",
    "                ser_iter_diag = pd.concat([ser_iter_to_turn, ser_iter_diag], axis = 0).sort_index() \n",
    "            ### Reindexation to business daily vector and forward filling:\n",
    "            ser_iter_diag = ser_iter_diag.ffill().reindex(idx_date_range).ffill()\n",
    "            ### Saving ticker diagonal to the container:\n",
    "            dict_ei_diag[iter_ticker] = ser_iter_diag\n",
    "        else:\n",
    "            print(iter_way, ':', iter_ticker, 'is absent for this way')\n",
    "    ### Aggregating ticker diagonals:\n",
    "    ser_diagonal_z = pd.concat(dict_ei_diag, axis = 0)\n",
    "    ser_diagonal_z.index.names = ['Index_Name', 'Date']\n",
    "    ser_diagonal_z.name = 'EI_diagonal'\n",
    "    ### Saving results to hdf file:\n",
    "    ser_diagonal_z.to_hdf(str_path_bb_diag_hdf, key = iter_way, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO AGGREGATE: GROUP DATA CONSOLIDATION (CELL TO REPLACE)\n",
    "\n",
    "### Defining group aggregation function:\n",
    "def group_aggregate(ser_group_list, str_path_group_matrix_hdf):\n",
    "    ### Defining triangle extraction:\n",
    "    def triangle_filter(ser_date):\n",
    "        ### Extracting particular Data Date:\n",
    "        date_diag = ser_date.index.get_level_values('Data_Date')[0]\n",
    "        ### Dropping constant level:\n",
    "        ser_result = ser_date.droplevel(['Index_Name', 'Data_Date'])\n",
    "        ### Filtering over-diagonal values:\n",
    "        ser_result = ser_result[ser_result.index >= date_diag] \n",
    "        ### Results output:\n",
    "        return ser_result \n",
    "    def conditional_fill(ser_ticker, int_limit = int_fill_limit):\n",
    "        ### Category loading:\n",
    "        str_category = df_flags_typed.loc[ser_ticker.index[0][0], 'Category']\n",
    "        if (str_category == 'Leading'):\n",
    "            ser_filled = ser_ticker.groupby('Observation_Date').ffill(limit = int_limit)\n",
    "        else:\n",
    "            ser_filled = ser_ticker.groupby('Observation_Date').bfill(limit = int_limit).groupby('Observation_Date').ffill(limit = int_limit)\n",
    "        ### Results output:\n",
    "        return ser_filled   \n",
    "    ### Defining time-vector z-scoring procedure:    \n",
    "    def by_date_z_score(ser_date, int_winsorize_bound, flo_tolerance, int_winsorize_steps_limit, int_min_years_adj):\n",
    "        ### Check for empty vector (doing nothing):\n",
    "        if ser_date.count():\n",
    "            ### Check for non-constant vector:\n",
    "            if (ser_date.std() > flo_tolerance):\n",
    "                ### Check for minimal quantity of observations to z-score:\n",
    "                if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years_adj * 365):   \n",
    "                    ### Calculating of z scores:\n",
    "                    ser_date = (ser_date - ser_date.mean()) / ser_date.std()        \n",
    "                    bool_to_winsor = True   \n",
    "                    int_iter = 1\n",
    "                    while (bool_to_winsor): \n",
    "                        int_iter += 1                \n",
    "                        ### Value based winsorization:                \n",
    "                        ser_date.clip(lower = -int_winsorize_bound, upper = int_winsorize_bound, inplace = True)\n",
    "                        ### Recalculating of z scores:\n",
    "                        ser_date = (ser_date - ser_date.mean()) / ser_date.std()\n",
    "                        ### Checking for boundaries and steps:\n",
    "                        if((ser_date.loc[ser_date.abs() >= (int_winsorize_bound + flo_tolerance)].count() == 0) | (int_iter > int_winsorize_steps_limit)):\n",
    "                            bool_to_winsor = False\n",
    "                else:\n",
    "                    ### Killing values that we can't z-score\n",
    "                    ser_date.loc[All] = np.NaN\n",
    "            else:\n",
    "                ### Check for minimal quantity of observations to z-score:\n",
    "                if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years_adj * 365):             \n",
    "                    ### Constant values demeaning:\n",
    "                    ser_date = ser_date - ser_date.mean()\n",
    "                else:\n",
    "                    ### Killing values that we can't z-score\n",
    "                    ser_date.loc[All] = np.NaN\n",
    "        ### Memory optimization:\n",
    "        ser_date = ser_date.astype('float32')\n",
    "        return ser_date  \n",
    "    ### Group aggregation announce:\n",
    "    print(str_path_group_matrix_hdf, ':', ser_group_list.index[0], 'group average matrix construction started')    \n",
    "    ### Extracting group ticker names:\n",
    "    list_group_members = ser_group_list.to_list()\n",
    "    ### Creating ticker data container:\n",
    "    list_group_matrix = []\n",
    "    ### Looping over tickers to collect group data:\n",
    "    for iter_ticker in list_group_members:\n",
    "        list_group_matrix.append(pd.read_hdf(dict_group_link[str_path_group_matrix_hdf], key = str_key_matrix_z, where = 'Index_Name == iter_ticker'))\n",
    "    ### Group data aggregating:\n",
    "    ser_group_matrix = pd.concat(list_group_matrix)\n",
    "    ### Group have more than one member:\n",
    "    if (len(list_group_members) > 1):    \n",
    "        ### Union of observation dates defining:\n",
    "        idx_observation_range = ser_group_matrix.index.get_level_values('Observation_Date').unique().sort_values()       \n",
    "#        idx_observation_bm = pd.date_range(start = idx_observation_range[0], end = idx_observation_range[-1], freq = 'BM')\n",
    "#        idx_observation_range = idx_observation_range.union(idx_observation_bm).unique().sort_values()        \n",
    "        idx_observation_range.name = 'Observation_Date'\n",
    "        ### Reindexation of observation dates:\n",
    "        ser_obs_range = ser_group_matrix.groupby('Index_Name')\\\n",
    "                                        .apply(lambda ser_name: ser_name.droplevel('Index_Name').unstack('Data_Date').reindex(idx_observation_range)\\\n",
    "                                                                        .stack('Data_Date', dropna = False).squeeze().swaplevel().sort_index())\n",
    "        gc.collect\n",
    "        ### Filling for each data date:\n",
    "        ser_obs_range = ser_obs_range.groupby(['Index_Name', 'Data_Date']).ffill()\n",
    "        ### Union of event dates defining:\n",
    "        idx_event_range = ser_obs_range.index.get_level_values('Data_Date').unique().sort_values()\n",
    "        ### Reindexation of observation dates:\n",
    "        ser_event_range = ser_obs_range.groupby('Index_Name')\\\n",
    "                                       .apply(lambda ser_name: ser_name.droplevel('Index_Name').unstack('Observation_Date').reindex(idx_event_range)\\\n",
    "                                                                       .stack('Observation_Date', dropna = False).squeeze().sort_index())\n",
    "        del ser_obs_range\n",
    "        gc.collect        \n",
    "        ### Filling for each data date:\n",
    "        ser_event_range = ser_event_range.groupby(['Index_Name'], group_keys = False).apply(conditional_fill, int_fill_limit) \n",
    "        ### Cutting by the diagonal:\n",
    "        ser_triangle = ser_event_range.groupby(['Index_Name', 'Data_Date'], observed = True).apply(triangle_filter).sort_index() \n",
    "        del ser_event_range\n",
    "        gc.collect          \n",
    "        ### Weights list initialising:\n",
    "        list_ext_weights = [False]        \n",
    "        ### Group average taking:\n",
    "        ser_average = ser_triangle.unstack('Index_Name').mean(axis = 1)\n",
    "        ser_average.name = 'Average'\n",
    "        del ser_triangle\n",
    "        gc.collect         \n",
    "        ### Z-scoring for each observation date:\n",
    "        ser_average_z = ser_average.groupby('Observation_Date')\\\n",
    "                                   .transform(by_date_z_score, int_winsorize_bound, flo_winsorize_tolerance, int_winsorize_steps_limit, int_min_years_z_score) \n",
    "        del ser_average\n",
    "        gc.collect    \n",
    "        ### Adding group data to hdf file:\n",
    "        pd.concat([ser_average_z], keys = [ser_group_list.index[0]], names = ['Type_Prime', 'Sub_Type', 'Region'])\\\n",
    "                                .to_hdf(str_path_group_matrix_hdf, key = str_key_group_matrix, format = 'table', complevel = 9, append = True, mode = 'a',\n",
    "                                        min_itemsize = {'Type_Prime': int_max_type_prime_len, 'Sub_Type': int_max_sub_type_len, 'Region': int_max_region_len})\n",
    "#        ### Results output:\n",
    "#        return ser_triangle        \n",
    "    else:\n",
    "        ser_group_matrix.name = 'Average'\n",
    "        ### Adding group data to hdf file:\n",
    "        pd.concat([ser_group_matrix.droplevel('Index_Name')], keys = [ser_group_list.index[0]], names = ['Type_Prime', 'Sub_Type', 'Region'])\\\n",
    "                                .to_hdf(str_path_group_matrix_hdf, key = str_key_group_matrix, format = 'table', complevel = 9, append = True, mode = 'a',\n",
    "                                        min_itemsize = {'Type_Prime': int_max_type_prime_len, 'Sub_Type': int_max_sub_type_len, 'Region': int_max_region_len})\n",
    "#        ### Results output:\n",
    "#        return ser_group_matrix.droplevel('Index_Name')        \n",
    "    ### Success message:\n",
    "    print(str_path_group_matrix_hdf, ':', ser_group_list.index[0], 'group average matrix successfully added to file')\n",
    "    \n",
    "### Flags loading & removing tickers with seasonality adjustment failed:\n",
    "list_SA_failed = ['JNPIY Index', 'SLPRYOYA Index']\n",
    "df_flags_typed = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags_typed).drop(list_SA_failed, axis = 0)\n",
    "### Primary type excluding ANT list:\n",
    "list_pop_types = list(df_flags_typed['Type_Prime'].unique())\n",
    "list_pop_types.remove('ANT')\n",
    "list_pop_types.remove('TAR')\n",
    "### Length limits for levels determination:\n",
    "int_max_type_prime_len = df_flags_typed['Type_Prime'].str.len().max(axis = 0)\n",
    "int_max_sub_type_len = df_flags_typed['Sub_Type'].str.len().max(axis = 0)\n",
    "int_max_region_len = df_flags_typed['Region'].str.len().max(axis = 0)\n",
    "### Flags converting to group register:\n",
    "ser_group_register = df_flags_typed[['Type_Prime', 'Sub_Type', 'Region']].reset_index().set_index(['Type_Prime', 'Sub_Type', 'Region']).squeeze()\n",
    "### Internal links between matrix files and group files:\n",
    "dict_group_link = {}\n",
    "dict_group_link[str_path_group_matrix_mom_hdf] = str_path_bb_matrix_mom_hdf\n",
    "for str_path_group_matrix_hdf in dict_group_link: # [str_path_group_matrix_mom_hdf]: # \n",
    "    gc.collect()\n",
    "    ### Previous HDF file deleting:\n",
    "    if os.path.isfile(str_path_group_matrix_hdf):\n",
    "        os.remove(str_path_group_matrix_hdf)\n",
    "    if (str_path_group_matrix_hdf == str_path_group_matrix_ant_hdf):\n",
    "        ser_iter_register = ser_group_register.loc[['ANT'], All, All]\n",
    "    else: \n",
    "        ser_iter_register = ser_group_register.loc[list_pop_types, All, All]        \n",
    "    ### Region average matrix aggregating:\n",
    "    ser_iter_register.groupby(['Type_Prime', 'Sub_Type', 'Region'], group_keys = True, sort = False).apply(group_aggregate, str_path_group_matrix_hdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: REGIONS DIAGONAL CONSTRUCTING (1 LINE TO MODIFY)\n",
    "\n",
    "### Flags loading:\n",
    "df_flags_typed = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags_typed)\n",
    "### Internal links between matrix files and diagonal keys:\n",
    "dict_diag_link = {}\n",
    "dict_diag_link[str_key_diag_group_mom] = str_path_group_matrix_mom_hdf\n",
    "\n",
    "for iter_way in dict_diag_link:\n",
    "    ### File linking:\n",
    "    str_path_group_matrix_hdf = dict_diag_link[iter_way]\n",
    "    ### Creating container for groups diagonals:\n",
    "    list_groups_diag = []\n",
    "    ### Looping over groups:\n",
    "    for iter_group in df_flags_typed[['Type_Prime', 'Sub_Type', 'Region']].drop_duplicates().sort_values(['Type_Prime', 'Sub_Type', 'Region']).values:\n",
    "        ser_iter_matrix = pd.read_hdf(str_path_group_matrix_hdf, key = str_key_group_matrix, \n",
    "                                      where = '(Type_Prime == iter_group[0]) & (Sub_Type == iter_group[1]) & (Region == iter_group[2])')\\\n",
    "                            .droplevel(['Type_Prime', 'Sub_Type', 'Region'])\n",
    "        ### Excluding TAR tickers:\n",
    "        if (len(ser_iter_matrix) > 0):\n",
    "            print(iter_way, ':', iter_group, 'performing') \n",
    "            ### Extracting unique observation dates for each ticker later than diagonal start date:\n",
    "            idx_date_list = ser_iter_matrix.loc[All, date_diag_start : ].index.dropna().get_level_values('Observation_Date').unique()\n",
    "            ### Creating future diagonal vector:\n",
    "            ser_iter_diag = pd.Series(np.NaN, idx_date_list)    \n",
    "            ### Determining first valid date and first date to place z-scoring results on diagonal:\n",
    "            ### Looping over unique dates:\n",
    "            for iter_date in idx_date_list:\n",
    "                ### Trying to get the latest data date observation:\n",
    "                try:\n",
    "                    ser_iter_diag[iter_date] = ser_iter_matrix.loc[All, iter_date].dropna().iloc[-1] ### LINE TO MODIFY\n",
    "                except:\n",
    "                    pass\n",
    "            ### Checking for data earlier than diagonal start date:\n",
    "            if ((ser_iter_matrix.index.get_level_values('Observation_Date').unique().min() < date_diag_start) & pd.notna(ser_iter_diag.values[0])):\n",
    "                ### Selecting column to be turned:\n",
    "                ser_iter_to_turn = ser_iter_matrix.loc[ : idx_date_list[0] - pd.offsets.Day(1), [idx_date_list[0]]].droplevel('Observation_Date')\n",
    "                ### Joining series:\n",
    "                ser_iter_diag = pd.concat([ser_iter_to_turn, ser_iter_diag], axis = 0).sort_index() \n",
    "            ### Reindexation to business daily vector and forward filling:\n",
    "            ser_iter_diag = ser_iter_diag.ffill().reindex(idx_date_range).ffill()\n",
    "            ### Saving ticker diagonal to the container:\n",
    "            list_groups_diag.append(pd.concat([ser_iter_diag], keys = [tuple(iter_group)], names = ['Type_Prime', 'Sub_Type', 'Region']))\n",
    "            ser_iter_diag.name = '/'.join(iter_group)\n",
    "#            ser_iter_diag.plot(figsize = (15, 5))\n",
    "#            plt.show()\n",
    "#        else:\n",
    "#            print(iter_way, ':', iter_group, 'is not for this way')\n",
    "    ### Container converting and adding to HDF:\n",
    "    pd.concat(list_groups_diag, axis = 0).to_hdf(str_path_bb_diag_hdf, key = iter_way, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: REGIONAL MATRICES AGGREGATION TO SUB TYPE MATRICES (CELL TO REPLACE)\n",
    "\n",
    "### Defining triangle extraction:\n",
    "def triangle_filter(ser_date):\n",
    "    ### Extracting particular Data Date:\n",
    "    date_diag = ser_date.index.get_level_values('Data_Date')[0]\n",
    "    ### Dropping constant level:\n",
    "    ser_result = ser_date.droplevel('Data_Date')\n",
    "    ### Filtering over-diagonal values:\n",
    "    ser_result = ser_result[ser_result.index >= date_diag] \n",
    "    ### Results output:\n",
    "    return ser_result\n",
    "### Defining time-vector z-scoring procedure:    \n",
    "def by_date_z_score(ser_date, int_winsorize_bound, flo_tolerance, int_winsorize_steps_limit, int_min_years_adj):\n",
    "    ### Check for empty vector (doing nothing):\n",
    "    if ser_date.count():\n",
    "        ### Check for non-constant vector:\n",
    "        if (ser_date.std() > flo_tolerance):\n",
    "            ### Check for minimal quantity of observations to z-score:\n",
    "            if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years_adj * 365):   \n",
    "                ### Calculating of z scores:\n",
    "                ser_date = (ser_date - ser_date.mean()) / ser_date.std()        \n",
    "                bool_to_winsor = True   \n",
    "                int_iter = 1\n",
    "                while (bool_to_winsor): \n",
    "                    int_iter += 1                \n",
    "                    ### Value based winsorization:                \n",
    "                    ser_date.clip(lower = -int_winsorize_bound, upper = int_winsorize_bound, inplace = True)\n",
    "                    ### Recalculating of z scores:\n",
    "                    ser_date = (ser_date - ser_date.mean()) / ser_date.std()\n",
    "                    ### Checking for boundaries and steps:\n",
    "                    if((ser_date.loc[ser_date.abs() >= (int_winsorize_bound + flo_tolerance)].count() == 0) | (int_iter > int_winsorize_steps_limit)):\n",
    "                        bool_to_winsor = False\n",
    "            else:\n",
    "                ### Killing values that we can't z-score\n",
    "                ser_date.loc[All] = np.NaN\n",
    "        else:\n",
    "            ### Check for minimal quantity of observations to z-score:\n",
    "            if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years_adj * 365):             \n",
    "                ### Constant values demeaning:\n",
    "                ser_date = ser_date - ser_date.mean()\n",
    "            else:\n",
    "                ### Killing values that we can't z-score\n",
    "                ser_date.loc[All] = np.NaN\n",
    "    ### Memory optimization:\n",
    "    ser_date = ser_date.astype('float32')\n",
    "    return ser_date                \n",
    "### Defining region averaging:\n",
    "def sub_type_aggregate(ser_iter_sub, str_path_sub_matrix_hdf):\n",
    "    gc.collect()\n",
    "    print(str_path_sub_matrix_hdf, ':', ser_iter_sub.index[0][: 2], 'aggregation procedure started')\n",
    "    list_iter_weights = ser_iter_sub.values    ### Looping over regions:\n",
    "    for iter_num, iter_index in enumerate(ser_iter_sub.index):\n",
    "        print(str_path_sub_matrix_hdf, ':', iter_index, ser_iter_sub[iter_index])\n",
    "        ### Loading group matrix:\n",
    "        ser_iter_group = pd.read_hdf(dict_group_link[str_path_sub_matrix_hdf], key = str_key_group_matrix, \n",
    "                                where = '(Type_Prime == iter_index[0]) & (Sub_Type == iter_index[1]) & (Region == iter_index[2])')\\\n",
    "                            .droplevel(['Type_Prime', 'Sub_Type', 'Region'])\n",
    "        print(str_path_sub_matrix_hdf, ':', iter_index, 'matrix loaded')\n",
    "        ### Observation dates reindexation and forward filling for every event date:\n",
    "        gc.collect()\n",
    "        ser_iter_group = ser_iter_group.unstack('Data_Date').reindex(idx_obs_range).ffill(axis = 0).stack('Data_Date', dropna = False).swaplevel().sort_index()\n",
    "        ser_iter_group.index.names = ['Data_Date', 'Observation_Date']\n",
    "        print(str_path_sub_matrix_hdf, ':', iter_index, 'observation dates reindexed')        \n",
    "        ### Event dates reindexation and forward filling for every observation date:\n",
    "        gc.collect()\n",
    "        ser_iter_group = ser_iter_group.unstack('Observation_Date').reindex(idx_event_range).ffill(axis = 0).stack('Observation_Date', dropna = False).sort_index()\n",
    "        ser_iter_group.index.names = ['Data_Date', 'Observation_Date']\n",
    "        print(str_path_sub_matrix_hdf, ':', iter_index, 'event dates reindexed')        \n",
    "        ### Triangle filtering:\n",
    "        gc.collect()\n",
    "        ser_iter_group = ser_iter_group.astype('float16').groupby('Data_Date').apply(triangle_filter)\n",
    "        ### Creating dataframe for the future averaging:\n",
    "        if (iter_num == 0):\n",
    "            ser_iter_group.name = iter_index[2]\n",
    "            df_iter_sub = ser_iter_group.to_frame()\n",
    "        ### Adding column to existing dataframe:    \n",
    "        else:\n",
    "            df_iter_sub[iter_index[2]] = ser_iter_group.values\n",
    "        del ser_iter_group\n",
    "        gc.collect()\n",
    "        print(str_path_sub_matrix_hdf, ':', iter_index, 'matrix added to table')\n",
    "    ### Sub type average calculating:\n",
    "    ser_mean = columns_average(df_iter_sub, list_iter_weights)\n",
    "    del df_iter_sub\n",
    "    gc.collect()\n",
    "    print(str_path_sub_matrix_hdf, ':', iter_index[: 2], 'mean calculated')\n",
    "    ### Sub type average z-scoring:\n",
    "    ser_mean_z = ser_mean.groupby('Observation_Date').transform(by_date_z_score, int_winsorize_bound, flo_winsorize_tolerance, int_winsorize_steps_limit, \n",
    "                                                              int_min_years_z_score)\n",
    "    del ser_mean\n",
    "    gc.collect()    \n",
    "    print(str_path_sub_matrix_hdf, ':', iter_index[: 2], 'mean z-scored')\n",
    "    ### Sub type average adding to the HDF file:\n",
    "    pd.concat([ser_mean_z], keys = [iter_index[: 2]], names = ['Type_Prime', 'Sub_Type'])\\\n",
    "                                    .to_hdf(str_path_sub_matrix_hdf, key = str_key_sub_matrix, format = 'table', complevel = 9, append = True, mode = 'a',\n",
    "                                            min_itemsize = {'Type_Prime': int_max_type_prime_len, 'Sub_Type': int_max_sub_type_len}) \n",
    "    print(str_path_sub_matrix_hdf, ':', iter_index[: 2], 'z-scored matrix saved')    \n",
    "#    ### Results output:    \n",
    "#    return ser_mean_z\n",
    "\n",
    "### Garbage collecting:\n",
    "gc.collect()\n",
    "### Flags loading:\n",
    "list_SA_failed = ['JNPIY Index', 'SLPRYOYA Index']\n",
    "df_flags_typed = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags_typed).drop(list_SA_failed, axis = 0)\n",
    "### Primary type excluding ANT list:\n",
    "list_pop_types = list(df_flags_typed['Type_Prime'].unique())\n",
    "list_pop_types.remove('ANT')\n",
    "list_pop_types.remove('TAR')\n",
    "### Region weights adopting:\n",
    "ser_region_weight = pd.Series(dict_region_weight)\n",
    "ser_region_weight.name = 'Region'\n",
    "### Extracting group indices to groupby:\n",
    "ser_sub_type = df_flags_typed[['Type_Prime', 'Sub_Type', 'Region']].drop_duplicates().sort_values(['Type_Prime', 'Sub_Type'])\\\n",
    "                                                    .reset_index().set_index(['Type_Prime', 'Sub_Type']).drop('Index_Name', axis = 1).squeeze()\n",
    "### GDP Dropping:\n",
    "ser_sub_type = ser_sub_type.drop('TAR', level = 'Type_Prime')\n",
    "### Adding region weights to group info:\n",
    "ser_sub_type = ser_sub_type.to_frame().set_index('Region', append = True).join(ser_region_weight, on = 'Region').squeeze().sort_index()\n",
    "ser_sub_type.name = 'Weight'\n",
    "### Loading dates indices:\n",
    "idx_event_range = pd.read_hdf(str_path_overall_dates_hdf, key = str_key_event_dates).index\n",
    "idx_obs_range = pd.read_hdf(str_path_overall_dates_hdf, key = str_key_obs_dates).index\n",
    "### Length limits for levels determination:\n",
    "int_max_type_prime_len = ser_sub_type.index.levels[0].str.len().max()\n",
    "int_max_sub_type_len = ser_sub_type.index.levels[1].str.len().max()\n",
    "### Internal links between region files and sub type files:\n",
    "dict_group_link = {}\n",
    "dict_group_link[str_path_sub_matrix_mom_hdf] = str_path_group_matrix_mom_hdf\n",
    "\n",
    "for str_path_sub_matrix_hdf in dict_group_link:\n",
    "    ### Previous HDF file deleting:\n",
    "    if os.path.isfile(str_path_sub_matrix_hdf):\n",
    "        os.remove(str_path_sub_matrix_hdf)\n",
    "    if (str_path_sub_matrix_hdf == str_path_sub_matrix_ant_hdf):\n",
    "        ser_iter_sub_type = ser_sub_type.loc[['ANT'], All, All]\n",
    "    else:\n",
    "        ser_iter_sub_type = ser_sub_type.loc[sorted(list_pop_types), All] \n",
    "    ### Aggregation performing:\n",
    "    ser_iter_sub_type.groupby(['Type_Prime', 'Sub_Type']).apply(sub_type_aggregate, str_path_sub_matrix_hdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: Z_SCORED SUB TYPES DIAGONAL CONSTRUCTING (1 LINE TO MODIFY)\n",
    "\n",
    "### Flags loading:\n",
    "df_flags_typed = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags_typed)\n",
    "### Chunk size defining:\n",
    "int_chunksize = 10 ** 6\n",
    "### Internal links between matrix files and diagonal keys:\n",
    "dict_diag_link = {}\n",
    "dict_diag_link[str_key_diag_sub_mom] = str_path_sub_matrix_mom_hdf\n",
    "\n",
    "for iter_way in dict_diag_link:\n",
    "    ### File linking:\n",
    "    str_path_sub_matrix_hdf = dict_diag_link[iter_way]\n",
    "    ### Creating container for groups diagonals:\n",
    "    list_groups_diag = []\n",
    "    ### Looping over sub types:\n",
    "    for iter_group in df_flags_typed[['Type_Prime', 'Sub_Type']].drop_duplicates().sort_values(['Type_Prime', 'Sub_Type']).values:\n",
    "        ### Check for not GDP:\n",
    "        if (iter_group[0] != 'TAR'):\n",
    "            ### Iteration container preparing:\n",
    "            gc.collect()\n",
    "            list_iter_container = []\n",
    "            ### Sub type matrix loading:\n",
    "            print(iter_way, ':', iter_group, 'loading') \n",
    "            for iter_chunk in pd.read_hdf(str_path_sub_matrix_hdf, key = str_key_sub_matrix, chunksize = int_chunksize, \n",
    "                                          where = '(Type_Prime == iter_group[0]) & (Sub_Type == iter_group[1]) & (Observation_Date >= date_diag_start)'):\n",
    "                list_iter_container.append(iter_chunk)\n",
    "                del iter_chunk\n",
    "                gc.collect()    \n",
    "            ### Excluding TAR tickers:\n",
    "            if (len(list_iter_container) > 0):                 \n",
    "                ### Sub type matrix constructing:\n",
    "                ser_iter_matrix = pd.concat(list_iter_container, axis = 0).droplevel(['Type_Prime', 'Sub_Type']).sort_index()\n",
    "                del list_iter_container\n",
    "                gc.collect()           \n",
    "                print(iter_way, ':', iter_group, 'main diagonal part constructing') \n",
    "                ### Extracting unique observation dates for each ticker later than diagonal start date:\n",
    "                idx_date_list = ser_iter_matrix.index.get_level_values('Observation_Date').unique()\n",
    "                ### Creating future diagonal vector:\n",
    "                ser_iter_diag = ser_iter_matrix.groupby('Observation_Date')\\\n",
    "                                               .apply(lambda ser_obs_date: ser_obs_date.dropna().iloc[-1] if (ser_obs_date.count() > 0) else np.NaN) ### LINE TO CHANGE\n",
    "                ### Checking for data earlier than diagonal start date:\n",
    "                if (ser_iter_matrix.dropna().loc[All, idx_date_list[0]].index[0] < date_diag_start):\n",
    "                    ### Selecting column to be turned:\n",
    "                    print(iter_way, ':', iter_group, 'auxiliary diagonal part constructing') \n",
    "                    ser_iter_to_turn = ser_iter_matrix.loc[ : idx_date_list[0] - pd.offsets.Day(1), [idx_date_list[0]]].droplevel('Observation_Date')\n",
    "                    ### Joining series:\n",
    "                    ser_iter_diag = pd.concat([ser_iter_to_turn, ser_iter_diag], axis = 0).sort_index() \n",
    "                ### Reindexation to business daily vector and forward filling:\n",
    "                ser_iter_diag = ser_iter_diag.ffill().reindex(idx_date_range).ffill()\n",
    "                ser_iter_diag = ser_iter_diag.astype('float32')\n",
    "                ### Saving ticker diagonal to the container:\n",
    "                list_groups_diag.append(pd.concat([ser_iter_diag], keys = [tuple(iter_group)], names = ['Type_Prime', 'Sub_Type']))\n",
    "                print(iter_way, ':', iter_group, 'diagonal added to container')\n",
    "    ### Container converting and adding to HDF:\n",
    "    ser_sub_type_diag = pd.concat(list_groups_diag, axis = 0)\n",
    "    ser_sub_type_diag.index.set_names(names = 'Data_Date', level = 2, inplace = True)\n",
    "    ser_sub_type_diag.to_hdf(str_path_bb_diag_hdf, key = iter_way, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: SUB TYPES MATRICES AGGREGATION TO GROUP MATRICES BY PCA FPC (COVARIANCE MATRIX NEWEY-WEST ADJUSTMENT + LIMITED Z-SCORING) (2 LINES TO MODIFY)\n",
    "\n",
    "### Defining dataframe columns PCA performing:\n",
    "def single_date_pca(df_iter_date, int_min_years_pca, list_list_weights, bool_do_nw_adj = True, list_dict_weights = False, list_dict_corrs = False):\n",
    "    ### Dropping constant level:\n",
    "    date_iter_obs = df_iter_date.index[0][-1]\n",
    "    df_iter_date = df_iter_date.droplevel('Observation_Date')\n",
    "    ### Weights vector initialising:\n",
    "    if (isinstance(list_dict_weights, bool) == True):\n",
    "        bool_collect_weights = False\n",
    "    else:\n",
    "        bool_collect_weights = True\n",
    "        ser_iter_weights = pd.Series(np.NaN, index = df_iter_date.columns)    \n",
    "    ### Correlation matrix collection flag initialising:\n",
    "    if (isinstance(list_dict_corrs, bool) == True):\n",
    "        bool_collect_corrs = False\n",
    "    else:\n",
    "        bool_collect_corrs = True          \n",
    "    ### Check for not empty observation date vector:\n",
    "    if (len(df_iter_date.dropna(how = 'all').index) > 0):\n",
    "        ### Dropping columns that does not have enough data length:\n",
    "        for iter_col in df_iter_date.columns:\n",
    "            if (df_iter_date[iter_col].count() == 0):\n",
    "                df_iter_date.drop(iter_col, axis = 1, inplace = True)\n",
    "            elif (df_iter_date[iter_col].dropna().last_valid_index() - df_iter_date[iter_col].dropna().first_valid_index()).days < (int_min_years_pca * 365):\n",
    "                df_iter_date.drop(iter_col, axis = 1, inplace = True)\n",
    "        ### No columns (all sub types dropped) check:\n",
    "        if (len(df_iter_date.columns) == 0):\n",
    "            ser_iter_res = pd.Series(np.NaN, index = df_iter_date.index)\n",
    "            if (date_iter_obs == (date_iter_obs + pd.offsets.BMonthEnd(0))):\n",
    "                print(date_iter_obs.strftime('%Y-%m-%d'), ': All sub types are too short')\n",
    "        ### Single column (single sub type) check:            \n",
    "        elif (len(df_iter_date.columns) == 1):\n",
    "            ser_iter_res = df_iter_date.squeeze()\n",
    "            ### Weights vector filling:\n",
    "            if bool_collect_weights:\n",
    "                ser_iter_weights.loc[df_iter_date.columns[0]] = 1.0\n",
    "        ### Two columns (sinple mean) check:            \n",
    "        elif (len(df_iter_date.columns) == 2):\n",
    "            ser_iter_res = df_iter_date.mean(axis = 1)\n",
    "            ### Weights vector filling:    \n",
    "            if bool_collect_weights:\n",
    "                ser_iter_weights.loc[df_iter_date.columns] = [0.5, 0.5]\n",
    "            ### Weights vector filling:    \n",
    "            if bool_collect_corrs:\n",
    "                df_iter_corr = df_iter_date.resample('B').ffill()[- int_corr_tail * int_bus_year :].corr()\n",
    "                df_iter_corr[df_iter_corr < 0.0] = 0.0  \n",
    "                list_dict_corrs[0][date_iter_obs] = df_iter_corr.stack(dropna = False)   \n",
    "        ### More than one vectors to aggregate:\n",
    "        else:\n",
    "            ### Check if we need to calculate new weights (if observation date is BusinessMonthEnd):\n",
    "            if ((date_iter_obs == (date_iter_obs + pd.offsets.BMonthEnd(0))) | (isinstance(list_list_weights[0], bool) == True)):\n",
    "                if bool_do_nw_adj:\n",
    "                    ### Samples correlation matrices collection:\n",
    "                    dict_sample_corr = {}\n",
    "                    ### Perform covariance adjustment for subsamples:\n",
    "                    df_iter_daily = df_iter_date.resample('B').ffill()[- int_corr_tail * int_bus_year : ]\n",
    "                    for iter_sample in range(int_cov_samples):\n",
    "                        df_iter_sample = df_iter_daily[iter_sample :: int_cov_samples]\n",
    "                        ### Looping over lags:\n",
    "                        for iter_lag in range(int_n_w_lag + 1):\n",
    "                            df_iter_cov_lagged = pd.DataFrame(np.NaN, index = df_iter_sample.columns, columns = df_iter_sample.columns)\n",
    "                            ### Looping over sub tupe pairs to calculate lagged covariance\n",
    "                            for iter_pair in combinations_with_replacement(df_iter_sample.columns, r = 2):\n",
    "                                ### Lagged covariance calculation:\n",
    "                                if (iter_lag == 0):\n",
    "                                    flo_cov_lagged = df_iter_sample[iter_pair[0]].cov(df_iter_sample[iter_pair[1]])\n",
    "                                else:\n",
    "                                    flo_cov_lagged = df_iter_sample[iter_pair[0]].shift(iter_lag).cov(df_iter_sample[iter_pair[1]]) + \\\n",
    "                                                     df_iter_sample[iter_pair[1]].shift(iter_lag).cov(df_iter_sample[iter_pair[0]])\n",
    "                                ### Weight adding:\n",
    "                                flo_cov_lagged = (1 - iter_lag / (int_n_w_lag + 1)) * flo_cov_lagged\n",
    "                                ### Adding results to the dataframe:\n",
    "                                df_iter_cov_lagged.loc[iter_pair[0], iter_pair[1]] = flo_cov_lagged\n",
    "                                df_iter_cov_lagged.loc[iter_pair[1], iter_pair[0]] = flo_cov_lagged\n",
    "                                ### Covariance NaN check:\n",
    "                                if np.isnan(flo_cov_lagged):\n",
    "                                    print(iter_sample, '/', iter_lag, '/', date_iter_obs.strftime('%Y-%m-%d'), '/', iter_pair[0], '/', iter_pair[1], \n",
    "                                          ': NaN covariance')\n",
    "                            ### Covariance matrix summation:\n",
    "                            if (iter_lag == 0):\n",
    "                                df_iter_n_w_cov = df_iter_cov_lagged\n",
    "                            else:\n",
    "                                df_iter_n_w_cov = df_iter_n_w_cov + df_iter_cov_lagged\n",
    "                        ### Standard deviation extracting:\n",
    "                        ser_iter_n_w_std = pd.Series(np.NaN, index = df_iter_n_w_cov.columns)\n",
    "                        for iter_col in df_iter_n_w_cov.columns:\n",
    "                            ser_iter_n_w_std.loc[iter_col] = (df_iter_n_w_cov.loc[iter_col, iter_col]) ** (1/2)\n",
    "                        ### Correlation matrix calculation:\n",
    "                        df_iter_n_w_std = ser_iter_n_w_std.to_frame().dot(ser_iter_n_w_std.to_frame().T)\n",
    "                        ### Samples correlcation matrices summation:\n",
    "                        dict_sample_corr[iter_sample] = df_iter_n_w_cov / df_iter_n_w_std\n",
    "                    ### Samples correlation matrices averaging:\n",
    "#                    df_iter_corr = pd.concat(dict_sample_corr).mean(axis = 0, level = 1)\n",
    "                    df_iter_corr = pd.concat(dict_sample_corr).groupby(level = 1).mean() ### LINE TO MODIFY\n",
    "                else:    \n",
    "#                    ### Simple correlation matrix:\n",
    "#                    df_iter_corr = df_iter_date.corr()\n",
    "                    df_iter_corr = df_iter_date.resample('B').ffill()[- int_corr_tail * int_bus_year :].corr()\n",
    "                ### Negative coefficients correction:\n",
    "                df_iter_corr[df_iter_corr < 0.0] = 0.0\n",
    "                ### Adding correlation matrix to collection:\n",
    "                if bool_collect_corrs:            \n",
    "                    list_dict_corrs[0][date_iter_obs] = df_iter_corr.stack(dropna = False)                 \n",
    "                ### PCA weights calculating:\n",
    "                list_evals, list_evecs = np.linalg.eigh(df_iter_corr)\n",
    "                df_iter_evecs = pd.DataFrame(data = list_evecs).round(4)\n",
    "                ### First Principal Component based weighted average calculating:\n",
    "                list_iter_weights = df_iter_evecs.iloc[:, -1].values\n",
    "                ### Save calulated weights:\n",
    "                list_list_weights[0] = list_iter_weights\n",
    "                print(date_iter_obs.strftime('%Y-%m-%d'), '(', list_iter_weights, ') : New weights saved')\n",
    "            ### New PCA participant:\n",
    "            elif (len(df_iter_date.columns) > len(list_list_weights[0])):\n",
    "                if bool_do_nw_adj:                \n",
    "                    ### Samples correlation matrices collection:\n",
    "                    dict_sample_corr = {}\n",
    "                    ### Perform covariance adjustment for subsamples:\n",
    "                    df_iter_daily = df_iter_date.resample('B').ffill()[- int_corr_tail * int_bus_year : ]\n",
    "                    for iter_sample in range(int_cov_samples):\n",
    "                        df_iter_sample = df_iter_daily[iter_sample :: int_cov_samples]\n",
    "                        ### Looping over lags:\n",
    "                        for iter_lag in range(int_n_w_lag + 1):\n",
    "                            df_iter_cov_lagged = pd.DataFrame(np.NaN, index = df_iter_sample.columns, columns = df_iter_sample.columns)\n",
    "                            ### Looping over sub tupe pairs to calculate lagged covariance\n",
    "                            for iter_pair in combinations_with_replacement(df_iter_sample.columns, r = 2):\n",
    "                                ### Lagged covariance calculation:\n",
    "                                if (iter_lag == 0):\n",
    "                                    flo_cov_lagged = df_iter_sample[iter_pair[0]].cov(df_iter_sample[iter_pair[1]])\n",
    "                                else:\n",
    "                                    flo_cov_lagged = df_iter_sample[iter_pair[0]].shift(iter_lag).cov(df_iter_sample[iter_pair[1]]) + \\\n",
    "                                                     df_iter_sample[iter_pair[1]].shift(iter_lag).cov(df_iter_sample[iter_pair[0]])\n",
    "                                ### Weight adding:\n",
    "                                flo_cov_lagged = (1 - iter_lag / (int_n_w_lag + 1)) * flo_cov_lagged\n",
    "                                ### Adding results to the dataframe:\n",
    "                                df_iter_cov_lagged.loc[iter_pair[0], iter_pair[1]] = flo_cov_lagged\n",
    "                                df_iter_cov_lagged.loc[iter_pair[1], iter_pair[0]] = flo_cov_lagged\n",
    "                                ### Covariance NaN check:\n",
    "                                if np.isnan(flo_cov_lagged):\n",
    "                                    print(date_iter_obs.strftime('%Y-%m-%d'), '/', iter_pair[0], '/', iter_pair[1], ': NaN covariance')\n",
    "                            ### Covariance matrix summation:\n",
    "                            if (iter_lag == 0):\n",
    "                                df_iter_n_w_cov = df_iter_cov_lagged\n",
    "                            else:\n",
    "                                df_iter_n_w_cov = df_iter_n_w_cov + df_iter_cov_lagged\n",
    "                        ### Standard deviation extracting:\n",
    "                        ser_iter_n_w_std = pd.Series(np.NaN, index = df_iter_n_w_cov.columns)\n",
    "                        for iter_col in df_iter_n_w_cov.columns:\n",
    "                            ser_iter_n_w_std.loc[iter_col] = (df_iter_n_w_cov.loc[iter_col, iter_col]) ** (1/2)\n",
    "                        ### Correlation matrix calculation:\n",
    "                        df_iter_n_w_std = ser_iter_n_w_std.to_frame().dot(ser_iter_n_w_std.to_frame().T)\n",
    "                        ### Samples correlcation matrices summation:\n",
    "                        dict_sample_corr[iter_sample] = df_iter_n_w_cov / df_iter_n_w_std\n",
    "                    ### Samples correlation matrices averaging:\n",
    "#                    df_iter_corr = pd.concat(dict_sample_corr).mean(axis = 0, level = 1)\n",
    "                    df_iter_corr = pd.concat(dict_sample_corr).groupby(level = 1).mean() ### LINE TO MODIFY             \n",
    "                else:\n",
    "#                    ### Simple correlation matrix:\n",
    "#                    df_iter_corr = df_iter_date.corr()\n",
    "                    ### Das experiment:\n",
    "                    df_iter_corr = df_iter_date.resample('B').ffill()[- int_corr_tail * int_bus_year : ].corr()\n",
    "                ### Negative coefficients correction:\n",
    "                df_iter_corr[df_iter_corr < 0.0] = 0.0\n",
    "                ### Adding correlation matrix to collection:\n",
    "                if bool_collect_corrs:            \n",
    "                    list_dict_corrs[0][date_iter_obs] = df_iter_corr.stack(dropna = False)  \n",
    "                ### PCA weights calculating:\n",
    "                list_evals, list_evecs = np.linalg.eigh(df_iter_corr)\n",
    "                df_iter_evecs = pd.DataFrame(data = list_evecs).round(4)\n",
    "                ### First Principal Component based weighted average calculating:\n",
    "                list_iter_weights = df_iter_evecs.iloc[:, -1].values\n",
    "                ### Save calulated weights:\n",
    "                list_list_weights[0] = list_iter_weights\n",
    "                print(date_iter_obs.strftime('%Y-%m-%d'), '(', list_iter_weights, ') : New weights saved')              \n",
    "            else:\n",
    "                ### Get weights calculated for the last BusinessMonthEnd:\n",
    "                list_iter_weights = list_list_weights[0]            \n",
    "            ### Weighted average calculating:\n",
    "            ser_iter_res = columns_average(df_iter_date, list_iter_weights)\n",
    "            ### Different signs of weights check:\n",
    "            if (min(list_iter_weights) < 0 < max(list_iter_weights)):\n",
    "                print(date_iter_obs.strftime('%Y-%m-%d'), ': Some weights have different signs')\n",
    "            ### Weights vector filling:\n",
    "            ### Sign flipping (if needed):            \n",
    "            if (sum(list_iter_weights) < 0):\n",
    "                list_iter_weights = list(map(lambda iter_weight: -1 * iter_weight, list_iter_weights))\n",
    "            ### Weights normalizing:\n",
    "            list_iter_weights = list(map(lambda iter_weight: iter_weight / sum(list_iter_weights), list_iter_weights))\n",
    "            ### Weights collecting:\n",
    "            if bool_collect_weights:            \n",
    "                ser_iter_weights.loc[df_iter_date.columns] = list_iter_weights                \n",
    "    ### If observation vector is empty:\n",
    "    else:\n",
    "        ser_iter_res = df_iter_date.iloc[All, 0]\n",
    "    ### Weights collecting:    \n",
    "    if bool_collect_weights:\n",
    "        ser_iter_weights.name = 'FPC'       \n",
    "        ser_iter_weights = ser_iter_weights.astype('float32')\n",
    "        list_dict_weights[0][date_iter_obs] = ser_iter_weights\n",
    "    ### Results output:\n",
    "    ser_iter_res.name = 'PCA'\n",
    "    return ser_iter_res.astype('float32')\n",
    "### Defining time-vector z-scoring procedure:    \n",
    "def by_date_z_score(ser_date, int_winsorize_bound, flo_tolerance, int_winsorize_steps_limit, int_min_years_adj, int_max_years_adj = 100):\n",
    "    ### Cutting old values:\n",
    "    ser_date.loc[ : ser_date.index[-1][0] - pd.DateOffset(months = int_max_years_adj * 12)] = np.NaN\n",
    "    ### Check for empty vector (doing nothing):\n",
    "    if ser_date.count():\n",
    "        ### Check for non-constant vector:\n",
    "        if (ser_date.std() > flo_tolerance):\n",
    "            ### Check for minimal quantity of observations to z-score:\n",
    "            if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years_adj * 365):   \n",
    "                ### Calculating of z scores:\n",
    "                ser_date = (ser_date - ser_date.mean()) / ser_date.std()        \n",
    "                bool_to_winsor = True   \n",
    "                int_iter = 1\n",
    "                while (bool_to_winsor): \n",
    "                    int_iter += 1                \n",
    "                    ### Value based winsorization:                \n",
    "                    ser_date.clip(lower = -int_winsorize_bound, upper = int_winsorize_bound, inplace = True)\n",
    "                    ### Recalculating of z scores:\n",
    "                    ser_date = (ser_date - ser_date.mean()) / ser_date.std()\n",
    "                    ### Checking for boundaries and steps:\n",
    "                    if((ser_date.loc[ser_date.abs() >= (int_winsorize_bound + flo_tolerance)].count() == 0) | (int_iter > int_winsorize_steps_limit)):\n",
    "                        bool_to_winsor = False\n",
    "            else:\n",
    "                ### Killing values that we can't z-score\n",
    "                ser_date.loc[All] = np.NaN\n",
    "        else:\n",
    "            ### Check for minimal quantity of observations to z-score:\n",
    "            if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years_adj * 365):             \n",
    "                ### Constant values demeaning:\n",
    "                ser_date = ser_date - ser_date.mean()\n",
    "            else:\n",
    "                ### Killing values that we can't z-score:\n",
    "                ser_date.loc[All] = np.NaN\n",
    "    ### Memory optimization:\n",
    "    ser_date = ser_date.astype('float32')\n",
    "    return ser_date        \n",
    "\n",
    "### Flags loading:\n",
    "df_flags_typed = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags_typed)\n",
    "### Extracting sub types:\n",
    "ser_type_prime = df_flags_typed[['Type_Prime', 'Sub_Type']].drop_duplicates().sort_values(['Type_Prime', 'Sub_Type'])\\\n",
    "                                                           .reset_index().set_index(['Type_Prime']).drop('Index_Name', axis = 1).squeeze()\n",
    "### Chunk size defining:\n",
    "int_chunksize = 10 ** 6 \n",
    "### Index name limitation:\n",
    "int_max_global_name = max(map(len, dict_global_index_name.values()))\n",
    "### Previous HDF weights collection file deleting:\n",
    "if os.path.isfile(str_path_bb_weights_hdf):\n",
    "    os.remove(str_path_bb_weights_hdf)\n",
    "### Previous HDF correlation matrices collection file deleting:\n",
    "if os.path.isfile(str_path_bb_corrs_hdf):\n",
    "    os.remove(str_path_bb_corrs_hdf)    \n",
    "### Looping over global indices combinations:\n",
    "for iter_combo in dict_global_index_name:   \n",
    "    gc.collect()\n",
    "    ### Resulting matrix defining:\n",
    "    str_path_global_matrix_z_hdf = dict_global_index_hdf[iter_combo]\n",
    "    ### Global index name defining:\n",
    "    str_global_name = dict_global_index_name[iter_combo]\n",
    "    print(str_global_name, ': Calculation started')\n",
    "    if (isinstance(iter_combo, str)):\n",
    "        iter_list = [iter_combo]\n",
    "    else:\n",
    "        iter_list = list(iter_combo)\n",
    "#    print(iter_list, '/', str_global_name, '/', str_path_global_matrix_z_hdf)\n",
    "    ### Previous HDF file deleting:\n",
    "    if os.path.isfile(str_path_global_matrix_z_hdf):\n",
    "        os.remove(str_path_global_matrix_z_hdf)\n",
    "    ### External actual weights list to change:\n",
    "    list_ext_weights = [False] \n",
    "    ### External full history weights list to change:\n",
    "    list_dict_weights = [{}]\n",
    "    ### External full history correlation matrices list to change:\n",
    "    list_dict_corrs = [{}]    \n",
    "    ### Sub type list for global index initiating:\n",
    "    dict_sub_types = {}\n",
    "    ### Looping over global index list elements to prepare info for data extraction loop:\n",
    "    for iter_member in iter_list:\n",
    "        ### Source files for element defining:\n",
    "        if (iter_member == 'ANT'):\n",
    "            str_path_sub_matrix_hdf = str_path_sub_matrix_ant_hdf\n",
    "        elif iter_member.endswith('_mom'):\n",
    "            str_path_sub_matrix_hdf = str_path_sub_matrix_mom_hdf\n",
    "        ### Element sub types defining\n",
    "        iter_prime_type = iter_member.split('_')[0]\n",
    "        list_iter_sub_types = ser_type_prime[[iter_member.split('_')[0]]].values\n",
    "        for iter_sub_type in list_iter_sub_types:\n",
    "            dict_sub_types[(iter_prime_type, iter_sub_type)] = str_path_sub_matrix_hdf\n",
    "    ### First element flag for dataframe initialization by first series:\n",
    "    bool_first_element = True            \n",
    "    ### Data extraction loop:\n",
    "    for iter_type in dict_sub_types:\n",
    "        ### Iteration container preparing:\n",
    "        list_iter_container = []\n",
    "        str_path_sub_matrix_hdf = dict_sub_types[iter_type]\n",
    "        ### Sub type matrix loading by looping over chunks:\n",
    "        for iter_chunk in pd.read_hdf(str_path_sub_matrix_hdf, key = str_key_sub_matrix, chunksize = int_chunksize, \n",
    "                                      where = '(Type_Prime == iter_type[0]) & (Sub_Type == iter_type[1])'):\n",
    "#        for iter_chunk in pd.read_hdf(str_path_sub_matrix_hdf, key = str_key_sub_matrix, chunksize = int_chunksize, \n",
    "#                                      where = '(Type_Prime == iter_type[0]) & (Sub_Type == iter_type[1]) & (Observation_Date > idx_obs_range[-174])'):\n",
    "#        for iter_chunk in pd.read_hdf(str_path_sub_matrix_hdf, key = str_key_sub_matrix, chunksize = int_chunksize, \n",
    "#                                      where = '(Type_Prime == iter_type[0]) & (Sub_Type == iter_type[1]) & (Observation_Date < idx_obs_range[1900])'):             \n",
    "            list_iter_container.append(iter_chunk)\n",
    "            del iter_chunk\n",
    "            gc.collect()\n",
    "        print(str_global_name, ':', iter_type, ': Chunks loaded')\n",
    "        ser_iter_matrix = pd.concat(list_iter_container, axis = 0).droplevel(['Type_Prime', 'Sub_Type']).sort_index()\n",
    "        del list_iter_container\n",
    "        gc.collect()        \n",
    "        ser_iter_matrix.name = '/'.join([iter_type[0], iter_type[1]])\n",
    "        print(str_global_name, ':', iter_type, ': Chunks aggregated')\n",
    "        if bool_first_element:\n",
    "            df_iter_matrix = ser_iter_matrix.to_frame()\n",
    "            bool_first_element = False\n",
    "        else:\n",
    "            df_iter_matrix[ser_iter_matrix.name] = ser_iter_matrix.values\n",
    "        del ser_iter_matrix\n",
    "        gc.collect()                         \n",
    "        print(str_global_name, ':', iter_type, ': Vector added to dataframe')\n",
    "#    ### PCA first component extracting without correlation matrix Newey-West adjustment procedure:\n",
    "#    ser_iter_pca = df_iter_matrix.groupby('Observation_Date').apply(single_date_pca, int_min_years_pca, list_ext_weights, False, list_dict_weights, list_dict_corrs)\\\n",
    "#                                                             .swaplevel().sort_index()\n",
    "    ### PCA first component extracting with correlation matrix Newey-West adjustment procedure:\n",
    "    ser_iter_pca = df_iter_matrix.groupby('Observation_Date').apply(single_date_pca, int_min_years_pca, list_ext_weights, True, list_dict_weights, list_dict_corrs)\\\n",
    "                                                             .swaplevel().sort_index()    \n",
    "    del df_iter_matrix\n",
    "    gc.collect()    \n",
    "    print(str_global_name, ': PCA weighting performed')\n",
    "    ### Saving weights collection to HDF:\n",
    "    str_global_diag_key = dict_global_index_diag_key[iter_combo]\n",
    "    pd.DataFrame(list_dict_weights[0]).transpose().to_hdf(str_path_bb_weights_hdf, key = str_global_diag_key, mode = 'a')   \n",
    "    del list_dict_weights[0]\n",
    "    gc.collect()    \n",
    "    print(str_global_name, ': Weights collection added to the file')  \n",
    "    ### Saving correlation matrices collection to HDF:\n",
    "    str_global_diag_key = dict_global_index_diag_key[iter_combo]\n",
    "    pd.DataFrame(list_dict_corrs[0]).transpose().stack().to_hdf(str_path_bb_corrs_hdf, key = str_global_diag_key, mode = 'a')   \n",
    "    del list_dict_corrs[0]\n",
    "    gc.collect()    \n",
    "    print(str_global_name, ': Pairwise correlations collection added to the file')    \n",
    "    ### Observation date vectors z-scoring:\n",
    "    ser_iter_pca_z = ser_iter_pca.groupby('Observation_Date')\\\n",
    "                                 .transform(by_date_z_score, int_winsorize_bound, flo_winsorize_tolerance, int_winsorize_steps_limit, \n",
    "                                            int_min_years_z_score, int_max_years_z_score)\n",
    "    ser_iter_pca_z.name = str_global_name\n",
    "    ser_iter_pca_z.name = 'PCA_FPC_Z_Scored'    \n",
    "    del ser_iter_pca\n",
    "    gc.collect() \n",
    "    print(str_global_name, ': Index matrix z-scored')\n",
    "    ### Adding aggregated data to the file:  \n",
    "#    pd.concat([ser_iter_pca_z], keys = [str_global_name], names = ['Global_Index'])\\\n",
    "#                                    .to_hdf(str_path_global_matrix_z_hdf, key = str_key_global_matrix, format = 'table', complevel = 9, append = True, mode = 'a',\n",
    "#                                            min_itemsize = {'Global_Index': int_max_global_name}) \n",
    "    ser_iter_pca_z.to_hdf(str_path_global_matrix_z_hdf, key = str_key_global_matrix, format = 'table', complevel = 9, append = True, mode = 'a')    \n",
    "    del ser_iter_pca_z\n",
    "    gc.collect()  \n",
    "    print(str_global_name, ': Index matrix added to the file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflation Index (YoY) : Diagonal extraction started\n",
      "Inflation Index (YoY) : Global index matrix loaded\n",
      "Inflation Index (YoY) : Main diagonal part constructing\n",
      "Inflation Index (YoY) : Auxiliary diagonal part constructing\n",
      "Inflation Index (YoY) : Diagonal saved\n",
      "Growth Index (YoY) : Diagonal extraction started\n",
      "Growth Index (YoY) : Global index matrix loaded\n",
      "Growth Index (YoY) : Main diagonal part constructing\n",
      "Growth Index (YoY) : Auxiliary diagonal part constructing\n",
      "Growth Index (YoY) : Diagonal saved\n"
     ]
    }
   ],
   "source": [
    "### RUN TO RE-EXPORT DATA: Z_SCORED PCA FPC DIAGONAL CONSTRUCTING (1 LINE TO MODIFY)\n",
    "\n",
    "### Chunk size defining:\n",
    "int_chunksize = 10 ** 6\n",
    "\n",
    "for iter_combo in dict_global_index_name:\n",
    "    gc.collect()\n",
    "    ### Resulting matrix defining:\n",
    "    str_path_global_matrix_z_hdf = dict_global_index_hdf[iter_combo]\n",
    "    ### Global index name defining:\n",
    "    str_global_name = dict_global_index_name[iter_combo]\n",
    "    ### Global index diagonal key defining:\n",
    "    str_global_diag_key = dict_global_index_diag_key[iter_combo]\n",
    "    ### Iteration container preparing:\n",
    "    list_iter_container = []\n",
    "    ### Sub type matrix loading:\n",
    "    print(str_global_name, ': Diagonal extraction started')\n",
    "    for iter_chunk in pd.read_hdf(str_path_global_matrix_z_hdf, key = str_key_global_matrix, chunksize = int_chunksize, \n",
    "                                  where = 'Observation_Date >= date_diag_start'):\n",
    "        list_iter_container.append(iter_chunk)\n",
    "        del iter_chunk\n",
    "        gc.collect()    \n",
    "    ### Sub type matrix constructing:\n",
    "    ser_iter_matrix = pd.concat(list_iter_container, axis = 0).sort_index()\n",
    "    del list_iter_container\n",
    "    if ('Global_Index' in ser_iter_matrix.index.names):\n",
    "        ser_iter_matrix = ser_iter_matrix.droplevel('Global_Index')\n",
    "    gc.collect()            \n",
    "    print(str_global_name, ': Global index matrix loaded')   \n",
    "    print(str_global_name, ': Main diagonal part constructing') \n",
    "    ### Extracting unique observation dates for each ticker later than diagonal start date:\n",
    "    idx_date_list = ser_iter_matrix.index.get_level_values('Observation_Date').unique()\n",
    "    ### Creating future diagonal vector:\n",
    "    ser_iter_diag = ser_iter_matrix.groupby('Observation_Date')\\\n",
    "                                   .apply(lambda ser_obs_date: ser_obs_date.dropna().iloc[-1] if (ser_obs_date.count() > 0) else np.NaN)  ### LINE TO MODIFY\n",
    "    ### Checking for data earlier than diagonal start date:\n",
    "    if (ser_iter_matrix.dropna().loc[All, idx_date_list[0]].index[0] < date_diag_start):\n",
    "        ### Selecting column to be turned:\n",
    "        print(str_global_name, ': Auxiliary diagonal part constructing') \n",
    "        ser_iter_to_turn = ser_iter_matrix.loc[ : idx_date_list[0] - pd.offsets.Day(1), [idx_date_list[0]]].droplevel('Observation_Date')\n",
    "        ### Joining series:\n",
    "        ser_iter_diag = pd.concat([ser_iter_to_turn, ser_iter_diag], axis = 0).sort_index() \n",
    "    ### Reindexation to business daily vector and forward filling:\n",
    "    ser_iter_diag = ser_iter_diag.ffill().reindex(idx_date_range).ffill()\n",
    "    ser_iter_diag = ser_iter_diag.astype('float32')\n",
    "    ser_iter_diag.name = str_global_name\n",
    "    ser_iter_diag.index.names = ['Date']\n",
    "    ### Saving resulting series to HDF:\n",
    "    ser_iter_diag.to_hdf(str_path_bb_diag_hdf, key = str_global_diag_key, mode = 'a')\n",
    "    print(str_global_name, ': Diagonal saved')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN FOR TESTING: GLOBAL INDICES AND SUB TYPE DIAGONALS SAVING (NO CHANGES)\n",
    "\n",
    "dict_global_diag = {}\n",
    "for iter_combo in dict_global_index_name:   \n",
    "    gc.collect()\n",
    "    ### Global index name defining:\n",
    "    str_global_name = dict_global_index_name[iter_combo]\n",
    "    ### Global index elements extracting:\n",
    "    if (isinstance(iter_combo, str)):\n",
    "        iter_list = [iter_combo]\n",
    "    else:\n",
    "        iter_list = list(iter_combo)\n",
    "    ### Global index diagonal key defining:\n",
    "    str_global_diag_key = dict_global_index_diag_key[iter_combo]\n",
    "    ### Adding diagonal to collection:\n",
    "    dict_global_diag[str_global_name] = pd.read_hdf(str_path_bb_diag_hdf, key = str_global_diag_key)\n",
    "### Diagonals collection saving:\n",
    "pd.DataFrame(dict_global_diag).to_excel('Data_Files/Test_Files/2024/Global_Indices.xlsx', merge_cells = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN FOR TESTING: GLOBAL INDICES AND SUB TYPE DIAGONALS SAVING (NO CHANGES)\n",
    "\n",
    "### All sub type diagonal loading:\n",
    "list_sub_diag = []\n",
    "for iter_str_key_diag_sub in [str_key_diag_sub_mom]:\n",
    "    list_sub_diag.append(pd.concat([pd.read_hdf(str_path_bb_diag_hdf, key = iter_str_key_diag_sub)], keys = [iter_str_key_diag_sub[-3 : ]]))\n",
    "### Diagonals data merging:\n",
    "ser_sub_type_diag = pd.concat(list_sub_diag, axis = 0)\n",
    "ser_sub_type_diag.index.set_names(names = 'Frequency', level = 0, inplace = True)\n",
    "df_sub_type_diag = ser_sub_type_diag.unstack(['Type_Prime', 'Sub_Type', 'Frequency'])\n",
    "df_sub_type_diag.columns = ['/'.join(iter_column) for iter_column in df_sub_type_diag.columns]\n",
    "### Diagonals collection saving:\n",
    "df_sub_type_diag.to_excel('Data_Files/Test_Files/2024/Sub_Type_Diagonals.xlsx', merge_cells = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN FOR TESTING: WEIGHTS SAVING (NO CHANGES)\n",
    "\n",
    "dict_global_weights = {}\n",
    "for iter_combo in dict_global_index_name:   \n",
    "    gc.collect()\n",
    "    ### Global index name defining:\n",
    "    str_global_name = dict_global_index_name[iter_combo]\n",
    "    ### Global index elements extracting:\n",
    "    if (isinstance(iter_combo, str)):\n",
    "        iter_list = [iter_combo]\n",
    "    else:\n",
    "        iter_list = list(iter_combo)\n",
    "    ### Global index diagonal key defining:\n",
    "    str_global_diag_key = dict_global_index_diag_key[iter_combo]\n",
    "    ### Adding diagonal to collection:\n",
    "    dict_global_weights[str_global_name] = pd.read_hdf(str_path_bb_weights_hdf, key = str_global_diag_key).resample('BM').asfreq()\n",
    "### Weights collection saving:\n",
    "with pd.ExcelWriter('Data_Files/Test_Files/2024/Global_Weights.xlsx') as xls_writer:\n",
    "    for iter_name in dict_global_weights:\n",
    "        str_sheet_name = iter_name.replace('Anticipated', 'Ant').replace('Employment', 'Emp').replace('Inflation', 'Inf')\n",
    "        dict_global_weights[iter_name].to_excel(xls_writer, sheet_name = str_sheet_name, merge_cells = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN FOR TESTING: CORRELATION MATRICES SAVING (NO CHANGES)\n",
    "\n",
    "dict_global_corrs = {}\n",
    "for iter_combo in dict_global_index_name:   \n",
    "    gc.collect()\n",
    "    ### Global index name defining:\n",
    "    str_global_name = dict_global_index_name[iter_combo]\n",
    "    ### Global index elements extracting:\n",
    "    if (isinstance(iter_combo, str)):\n",
    "        iter_list = [iter_combo]\n",
    "    else:\n",
    "        iter_list = list(iter_combo)\n",
    "    ### Global index diagonal key defining:\n",
    "    str_global_diag_key = dict_global_index_diag_key[iter_combo]\n",
    "    ### Adding diagonal to collection:\n",
    "    dict_global_corrs[str_global_name] = pd.read_hdf(str_path_bb_corrs_hdf, key = str_global_diag_key)\\\n",
    "                                           .unstack(1).resample('BM').asfreq().stack(dropna = False).sort_index()\n",
    "### Weights collection saving:\n",
    "with pd.ExcelWriter('Data_Files/Test_Files/2024/Global_Corr_Matrices.xlsx') as xls_writer:\n",
    "    for iter_name in dict_global_corrs:\n",
    "        str_sheet_name = iter_name.replace('Anticipated', 'Ant').replace('Employment', 'Emp').replace('Inflation', 'Inf')\n",
    "        dict_global_corrs[iter_name].to_excel(xls_writer, sheet_name = str_sheet_name, merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflation Index (YoY) : Percentiles calculation started\n",
      "Inflation Index (YoY) : Global index matrix loaded\n",
      "Inflation Index (YoY) : Percentiles calculated\n",
      "Inflation Index (YoY) : Percentiles saved\n",
      "Growth Index (YoY) : Percentiles calculation started\n",
      "Growth Index (YoY) : Global index matrix loaded\n",
      "Growth Index (YoY) : Percentiles calculated\n",
      "Growth Index (YoY) : Percentiles saved\n"
     ]
    }
   ],
   "source": [
    "### RUN TO RE-EXPORT DATA: LEVEL & MOMENTUM PERCENTILE CONSTRUCTING (NO CHANGES)\n",
    "\n",
    "### Defining percentiles generator:\n",
    "def take_ptiles(ser_date, int_ptile_months, int_ave_months, list_weight, int_lag):\n",
    "    ### Cutting old values:\n",
    "    date_obs = ser_date.index[0][1]\n",
    "    ser_date = ser_date.droplevel('Observation_Date')\n",
    "    ser_long = ser_date[ser_date.index[-1] - pd.DateOffset(months = int_ptile_months) : ]\n",
    "    ser_short = ser_date[ser_date.index[-1] - pd.DateOffset(months = int_ave_months) : ]    \n",
    "    ### Check for empty vector (doing nothing):\n",
    "    if (ser_long.count() > 0):\n",
    "        ### Level percentile calculation:\n",
    "        flo_level_ptile = ser_long.rank(method = 'average', na_option = 'keep', ascending = True, pct = True).iloc[-1]\n",
    "        ### Momentum percentile calculation start: change series\n",
    "        ser_long = ser_long.resample('B').ffill()\n",
    "        ser_long_change = (ser_long - ser_long.shift(int_lag)).replace(0.0, np.NaN)\n",
    "        ### Short series of changes calculation:\n",
    "        ser_short = ser_short.resample('B').ffill()\n",
    "        ser_short_change = (ser_short - ser_short.shift()).replace(0.0, np.NaN)\n",
    "        ### Weighted mean of changes calculation:\n",
    "        ser_weight = pd.Series(list_weight[-len(ser_short_change.index) : ], ser_short_change.index)\n",
    "        flo_ave_change = weighted_average(ser_short_change, ser_weight)\n",
    "        ser_weight_dropped = ser_weight[ser_short_change.dropna().index]\n",
    "        ser_weight_dropped = ser_weight_dropped / ser_weight_dropped.sum()\n",
    "        flo_ave_change = flo_ave_change * math.sqrt(int_lag / (ser_weight_dropped ** 2).sum())\n",
    "        ### Weighted mean rank defining:\n",
    "        ser_long_change[-1] = flo_ave_change\n",
    "        flo_mom_ptile = ser_long_change.rank(method = 'average', na_option = 'keep', ascending = True, pct = True).iloc[-1]\n",
    "        ### Results output:\n",
    "        return pd.DataFrame([[flo_level_ptile, flo_mom_ptile]], index = [date_obs])\n",
    "    else:\n",
    "        ### Results output:        \n",
    "        return pd.DataFrame([[np.NaN, np.NaN]], index = [date_obs])\n",
    "\n",
    "### Lagging step:\n",
    "int_lag = 22 # 5 # 1 # \n",
    "### Weights array creating:\n",
    "list_weight = list(map(lambda iter_num: exp_weight_single(int_halflife_months * 22, iter_num), range(int_ave_months * 22)))[::-1]\n",
    "### Chunk size defining:\n",
    "int_chunksize = 10 ** 6\n",
    "### Previous HDF file deleting:\n",
    "if os.path.isfile(str_path_bb_percentiles_hdf):\n",
    "    os.remove(str_path_bb_percentiles_hdf)\n",
    "### Looping over indices:\n",
    "for iter_combo in dict_global_index_name:\n",
    "    gc.collect()\n",
    "    ### Resulting matrix defining:\n",
    "    str_path_global_matrix_z_hdf = dict_global_index_hdf[iter_combo]\n",
    "    ### Global index name defining:\n",
    "    str_global_name = dict_global_index_name[iter_combo]\n",
    "    ### Global index diagonal key defining:\n",
    "    str_global_diag_key = dict_global_index_diag_key[iter_combo]\n",
    "    ### Iteration container preparing:\n",
    "    list_iter_container = []\n",
    "    ### Sub type matrix loading:\n",
    "    print(str_global_name, ': Percentiles calculation started')\n",
    "    for iter_chunk in pd.read_hdf(str_path_global_matrix_z_hdf, key = str_key_global_matrix, chunksize = int_chunksize, \n",
    "                                  where = 'Observation_Date >= date_diag_start'):\n",
    "        list_iter_container.append(iter_chunk)\n",
    "        del iter_chunk\n",
    "        gc.collect()    \n",
    "    ### Sub type matrix constructing:\n",
    "    ser_iter_matrix = pd.concat(list_iter_container, axis = 0).sort_index()\n",
    "    del list_iter_container\n",
    "    if ('Global_Index' in ser_iter_matrix.index.names):\n",
    "        ser_iter_matrix = ser_iter_matrix.droplevel('Global_Index')\n",
    "    gc.collect()            \n",
    "    print(str_global_name, ': Global index matrix loaded')   \n",
    "    ### Percentiles calculation procedure:\n",
    "    df_iter_ptile = ser_iter_matrix.groupby('Observation_Date', group_keys = False).apply(take_ptiles, int_ptile_months, int_ave_months, list_weight, int_lag)\n",
    "    df_iter_ptile.columns = ['Level_Percentile', 'Momentum_Percentile']\n",
    "    print(str_global_name, ': Percentiles calculated')      \n",
    "    ### Reindexation to business daily vector and forward filling:\n",
    "    df_iter_ptile = df_iter_ptile.reindex(idx_date_range).ffill()\n",
    "    df_iter_ptile = df_iter_ptile.astype('float32')\n",
    "    df_iter_ptile.name = str_global_name\n",
    "    df_iter_ptile.index.names = ['Date']\n",
    "    ### Saving resulting series to HDF:\n",
    "    df_iter_ptile.to_hdf(str_path_bb_percentiles_hdf, key = str_global_diag_key, mode = 'a')\n",
    "    print(str_global_name, ': Percentiles saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: LEVEL & MOMENTUM PERCENTILE SAVING (NO CHANGES)\n",
    "\n",
    "dict_ptiles = {}\n",
    "for iter_combo in dict_global_index_name:   \n",
    "    gc.collect()\n",
    "    ### Global index name defining:\n",
    "    str_global_name = dict_global_index_name[iter_combo]\n",
    "    ### Global index elements extracting:\n",
    "    if (isinstance(iter_combo, str)):\n",
    "        iter_list = [iter_combo]\n",
    "    else:\n",
    "        iter_list = list(iter_combo)\n",
    "    ### Global index diagonal key defining:\n",
    "    str_global_diag_key = dict_global_index_diag_key[iter_combo]\n",
    "    ### Adding diagonal to collection:\n",
    "    dict_ptiles[str_global_name] = pd.read_hdf(str_path_bb_percentiles_hdf, key = str_global_diag_key)\n",
    "### Factors collection saving:\n",
    "pd.concat(dict_ptiles, axis = 1).to_excel('Data_Files/Test_Files/2024/Global_Percentiles.xlsx', merge_cells = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inflation Index (YoY) / Level_Percentile : Sign factors calculation started\n",
      "Inflation Index (YoY) / Level_Percentile : Sign factors calculation finished\n",
      "Inflation Index (YoY) / Momentum_Percentile : Sign factors calculation started\n",
      "Inflation Index (YoY) / Momentum_Percentile : Sign factors calculation finished\n",
      "Growth Index (YoY) / Level_Percentile : Sign factors calculation started\n",
      "Growth Index (YoY) / Level_Percentile : Sign factors calculation finished\n",
      "Growth Index (YoY) / Momentum_Percentile : Sign factors calculation started\n",
      "Growth Index (YoY) / Momentum_Percentile : Sign factors calculation finished\n"
     ]
    }
   ],
   "source": [
    "### RUN TO RE-EXPORT DATA: LEVEL & MOMENTUM PERCENTILE SIGN FACTORS GENERATING: RESEARCH VERSION (NO CHANGES)\n",
    "\n",
    "### Constants defining:\n",
    "tumbler_to_minus = 0.40\n",
    "tumbler_to_plus = 0.60\n",
    "### Percentiles initialization:\n",
    "dict_ptiles = {}\n",
    "\n",
    "### Looping over global indices:\n",
    "for iter_combo in dict_global_index_name:   \n",
    "    gc.collect()\n",
    "    ### Global index name defining:\n",
    "    str_global_name = dict_global_index_name[iter_combo]\n",
    "    ### Global index elements extracting:\n",
    "    if (isinstance(iter_combo, str)):\n",
    "        iter_list = [iter_combo]\n",
    "    else:\n",
    "        iter_list = list(iter_combo)\n",
    "    ### Global index diagonal key defining:\n",
    "    str_global_diag_key = dict_global_index_diag_key[iter_combo]\n",
    "    ### Percentiles table reading:\n",
    "    df_iter_combo = pd.read_hdf(str_path_bb_percentiles_hdf, key = str_global_diag_key)\n",
    "    ### Looping over percentile types inside each global index:\n",
    "    for str_ptile_type in df_iter_combo.columns:\n",
    "        print(str_global_name, '/', str_ptile_type, ': Sign factors calculation started')\n",
    "        ### Container initialization:\n",
    "        dict_signs = {}\n",
    "        ### Percentile extraction:\n",
    "        ser_iter_ptile = df_iter_combo[str_ptile_type]\n",
    "        ### Looping over dateline:\n",
    "        for sign_date in ser_iter_ptile.dropna().index[-1 :]:\n",
    "#            print(str_global_name, '/', str_ptile_type, '/', sign_date, ': Sign defining')\n",
    "            ### Initial sign defining:\n",
    "            ser_iter_signs = pd.Series(np.NaN, index = ser_iter_ptile.dropna()[ : sign_date].index)\n",
    "            ser_iter_signs.name = 'Sign'\n",
    "            ser_iter_signs.iloc[0] = 1         \n",
    "            ### Looping over part of the timeseries till the sign date to perform sign flipping:\n",
    "            if (len(ser_iter_signs) > 1) :\n",
    "                for iter_date in ser_iter_signs.index[1 : ]:\n",
    "                    if (ser_iter_signs.loc[iter_date - pd.offsets.BusinessDay()] == 1):\n",
    "                        if (ser_iter_ptile[iter_date] < tumbler_to_minus):\n",
    "                            ser_iter_signs.loc[iter_date] = -1\n",
    "                        else:\n",
    "                            ser_iter_signs.loc[iter_date] = 1\n",
    "                    else:\n",
    "                        if (ser_iter_ptile[iter_date] > tumbler_to_plus):\n",
    "                            ser_iter_signs.loc[iter_date] = 1\n",
    "                        else:\n",
    "                            ser_iter_signs.loc[iter_date] = -1\n",
    "        print(str_global_name, '/', str_ptile_type, ': Sign factors calculation finished')           \n",
    "        ### Signs vector collecting:\n",
    "        dict_ptiles[str_global_name + ' / ' + str_ptile_type] = ser_iter_signs        \n",
    "### Factors collection saving:\n",
    "#pd.concat(dict_ptiles, axis = 1).to_hdf('Data_Files/Source_Files/Percentile_Signs_Research.hdf', key = 'percentile_signs', mode = 'a')\n",
    "pd.concat(dict_ptiles, axis = 1).to_excel('Data_Files/Test_Files/2024/Percentile_Signs_Research.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
