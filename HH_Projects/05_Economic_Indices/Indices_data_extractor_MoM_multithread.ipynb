{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: ECONOMIC INDICES RELEASES HISTORY EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "from math import floor, ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.x13 import x13_arima_analysis\n",
    "from itertools import combinations_with_replacement\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import gc\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: X13PATH=C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
      "env: X12PATH=C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n"
     ]
    }
   ],
   "source": [
    "### RUN ONLY WHEN ARIMA X13 SA LAUNCHING \n",
    "\n",
    "### Warnings hiding:\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "### Seasonal adjustment module paths set up:\n",
    "%env X13PATH = C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
    "%env X12PATH = C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
    "#%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  0.25.3\n",
      "python version:  3.7.4\n"
     ]
    }
   ],
   "source": [
    "## VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable']\n",
    "### Raw data path and sheets:\n",
    "str_path_bb_idx_source = 'Data_Files/Source_Files/Bloomberg_Eco_Indices.xlsx'\n",
    "str_us_sheet = 'US Eco Const'\n",
    "str_all_sheet = 'All Eco Const'\n",
    "### Flags data path and sheets:\n",
    "str_path_bb_idx_flags = 'Data_Files/Source_Files/Bloomberg_Eco_Flags_Extended.xlsx'\n",
    "str_flag_sheet = 'Bloomberg Description'\n",
    "### Source data constants:\n",
    "int_idx_cols = 12\n",
    "### HDF file with converted source data:\n",
    "str_path_bb_idx_hdf = 'Data_Files/Source_Files/Bloomberg_Eco_Indices.h5'\n",
    "str_key_flags = 'flags_exported' ### Acadian flags list\n",
    "str_key_exported = 'all_idx_exported' ### Raw export with only replacing zero dates and after 2021-01-01 dates with np.NaN\n",
    "str_key_raw_filled = 'all_idx_raw_filled' ### Raw export with initial dates, dates gaps, absent date coluns filled\n",
    "str_key_raw_history = 'raw_history' ### Export with all the corrections and fillings (restructured to [Index_Name -> Data_Date -> Observation_Date] | Index_Value series)\n",
    "str_key_bday_history = 'bday_history' ### Raw history vector with observation dates moved to nearest future business dates\n",
    "str_key_num_history = 'num_history' ### Bday history vector with observation dates changed to their date numbers (for future matrix cube saving as hdf file)\n",
    "str_key_from_date = 'idx_from_date' ### Series to get date numbers from dates\n",
    "str_key_to_date = 'idx_to_date' ### Series to get dates from date numbers\n",
    "str_key_types_info = 'types_info' ### Dataframe with 'Type_Prime' / 'Sub_Type' / 'Region' groups descriptions\n",
    "str_key_flags_typed = 'flags_typed' ### Dataframe with economic indices descriptions taking into account \n",
    "### HDF file with matrices:\n",
    "str_path_bb_matrix_hdf = 'Data_Files/Source_Files/Matrix_Eco_Indices.h5'\n",
    "str_key_matrix_z = 'matrix_cube_z_scored'\n",
    "### HDF file with diagonals:\n",
    "str_path_bb_diag_hdf = 'Data_Files/Source_Files/Diag_Eco_Indices.h5'\n",
    "str_key_diag_daily_raw = 'matrix_diagonal_raw'\n",
    "str_key_diag_daily_z = 'matrix_diagonal_z'\n",
    "str_key_diag_group_z = 'groups_diagonal_z'\n",
    "str_key_diag_sub_z = 'sub_types_diagonal_z'\n",
    "str_key_diag_agg_z = 'aggregated_diagonal_z'\n",
    "str_key_diag_mean_z = 'mean_diagonal_z'\n",
    "### HDF file with group averages:\n",
    "str_path_group_matrix_hdf = 'Data_Files/Source_Files/Matrix_Groups.h5'\n",
    "str_key_group_matrix = 'matrix_cube_groups'\n",
    "### HDF file with overall event dates as series index:\n",
    "str_path_overall_dates_hdf = 'Data_Files/Source_Files/Overall_Dates.h5'\n",
    "str_key_event_dates = 'overall_event_dates'\n",
    "str_key_obs_dates = 'overall_obs_dates'\n",
    "str_key_triangle_dates = 'overall_triangle_dates'\n",
    "### HDF file with sub type averages:\n",
    "str_path_sub_matrix_hdf = 'Data_Files/Source_Files/Matrix_Sub.h5'\n",
    "str_key_sub_matrix = 'matrix_cube_subs'\n",
    "### HDF file with global indices PCA FPC:\n",
    "str_path_global_matrix_hdf = 'Data_Files/Source_Files/Matrix_Global.h5'\n",
    "str_key_global_matrix = 'matrix_cube_globals'\n",
    "### HDF file with global indices simple averages:\n",
    "str_path_mean_matrix_hdf = 'Data_Files/Source_Files/Matrix_Mean.h5'\n",
    "str_key_mean_matrix = 'matrix_cube_means'\n",
    "### Observation axis range:\n",
    "datetime_start = datetime(1984, 12, 31) # Start date for efficacy measures\n",
    "date_start = datetime_start.date()\n",
    "datetime_end = datetime(2020, 8, 31) # End date for efficacy measures\n",
    "date_end = datetime_end.date()\n",
    "idx_date_range = pd.date_range(date_start, date_end, freq = 'B')\n",
    "datetime_basis = datetime(1993, 12, 31) # End date for efficacy measures\n",
    "date_basis = datetime_basis.date()\n",
    "### Gaps filling options:\n",
    "int_revision_shift = 1\n",
    "int_final_shift = 2\n",
    "int_first_mean_length = 12\n",
    "dict_final_only_lag = {}\n",
    "dict_final_only_lag['Quarterly'] = 90 // 2\n",
    "dict_final_only_lag['Monthly'] = 30 // 2\n",
    "dict_final_only_lag['Other'] = 7 // 2\n",
    "### Cumprod shifts for monthly data frequency:\n",
    "dict_cumprod_step = {}\n",
    "dict_cumprod_step['MoM%'] = 1\n",
    "dict_cumprod_step['QoQ%'] = 3\n",
    "dict_cumprod_step['YoY%'] = 12\n",
    "### Stock-like series shifts for YoY transformation:\n",
    "dict_yoy_shift = {}\n",
    "dict_yoy_shift['Monthly'] = 12\n",
    "dict_yoy_shift['Quarterly'] = 4\n",
    "dict_yoy_shift['Other'] = 52\n",
    "### Stock-like series shifts for MoM transformation:\n",
    "dict_mom_shift = {}\n",
    "dict_mom_shift['Monthly'] = 1\n",
    "dict_mom_shift['Other'] = 4\n",
    "### Group tickers rebasing options:\n",
    "int_not_to_rebase_term = 7 ### Term in years for min ticker data date when we do not need to rebase it with basis group ticker\n",
    "int_not_to_rebase_diff = 2 ### Minimal difference in years between basis ticker and other group ticker min date when we need to rebase group ticker\n",
    "### Z-scoring options:\n",
    "int_winsorize_bound = 4\n",
    "flo_winsorize_tolerance = 0.0001\n",
    "int_winsorize_steps_limit = 5\n",
    "### Diagonal options:\n",
    "int_min_years_z_score = 3\n",
    "date_diag_start = datetime(1994, 1, 1)\n",
    "### Data filling limit\n",
    "int_fill_limit = 20\n",
    "### Regions weights:\n",
    "dict_region_weight = {}\n",
    "dict_region_weight['US'] = 0.50\n",
    "dict_region_weight['Europe'] = 0.25\n",
    "dict_region_weight['Japan'] = 0.15\n",
    "dict_region_weight['UK'] = 0.10\n",
    "### Groups aggregation order:\n",
    "list_agg_groups = [['EMP'], ['INF'], ['ANT', 'CON', 'OUT']]\n",
    "dict_agg_groups = {}\n",
    "dict_agg_groups['Employment'] = ['EMP']\n",
    "dict_agg_groups['Inflation'] = ['INF']\n",
    "dict_agg_groups['Economy'] = ['ANT', 'CON', 'OUT']\n",
    "### A-la Newey-West adjustment maximum lag:\n",
    "int_n_w_lag = 4\n",
    "### Covariance subsamples number:\n",
    "int_cov_samples = 22\n",
    "### Minimal years to use column for PCA performing\n",
    "int_min_years_pca = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING OBSERVATION DATE VECTOR EXTRACTION\n",
    "\n",
    "def get_obs_date_vector(str_ticker, str_date, bool_exact_date = False, bool_drop_levels = False):\n",
    "    ### Vector for exact date:\n",
    "    if bool_exact_date:\n",
    "        ser_obs_date = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == str_ticker & Observation_Date == str_date')\n",
    "    ### Vector for nearest date:        \n",
    "    else:\n",
    "        ### Loading full ticker series:        \n",
    "        ser_z_scored = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == str_ticker & Observation_Date <= str_date')\n",
    "        ### Extracting data for max date less or equal to needed date:\n",
    "        ser_obs_date = ser_z_scored.loc[All, All, [ser_z_scored.index.levels[-1].max()]]\n",
    "    ### Dropping constant index levels if needed:\n",
    "    if bool_drop_levels:\n",
    "        return ser_obs_date.droplevel(['Index_Name', 'Observation_Date'])\n",
    "    else:\n",
    "        return ser_obs_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE FOR DATAFRAME COLUMNS (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def columns_average(df_series, list_weights = False): \n",
    "    ### Single column check\n",
    "    if (len(df_series.columns) > 1):\n",
    "        ### Equal weights list creating:\n",
    "        if isinstance(list_weights, bool):\n",
    "            list_weights = [1] * len(df_series.columns)\n",
    "        ### Dataframe of weights initialising:\n",
    "        df_weights = pd.DataFrame(np.NaN, index = df_series.index, columns = df_series.columns)\n",
    "        for iter_num, iter_col in enumerate(df_weights.columns):\n",
    "            df_weights[iter_col] = list_weights[iter_num]\n",
    "        ### Zeroing weights for NaN values:\n",
    "        for iter_col in df_weights.columns:\n",
    "            df_weights.loc[df_series[iter_col].isna(), iter_col] = 0\n",
    "        ser_mean = (df_series.multiply(df_weights).sum(axis = 1)).div(df_weights.sum(axis = 1))    \n",
    "        ### Results output:\n",
    "        del df_series\n",
    "        del df_weights    \n",
    "        gc.collect()\n",
    "    else:\n",
    "        ser_mean = df_series.squeeze()\n",
    "        del df_series\n",
    "        gc.collect()        \n",
    "    return ser_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAPMPMI Index : Reindexation\n",
      "CHPMINDX Index : Reindexation\n",
      "DFEDGBA Index : Reindexation\n",
      "EMPRGBCI Index : Reindexation\n",
      "MAPMINDX Index : Reindexation\n",
      "NAPMNMI Index : Reindexation\n",
      "OUTFGAF Index : Reindexation\n",
      "RCHSINDX Index : Reindexation\n"
     ]
    }
   ],
   "source": [
    "### RUN TO TEST: INDICES DATA CONVERTING TO STOCK-LIKE TO FURTHER SA TEST\n",
    "\n",
    "### Defining Economic Index series transformation:\n",
    "def complex_transform(ser_name, idx_date_range, df_flags, int_max_name_length, int_min_years, bool_perform_sa = False):\n",
    "    ### Defining triangle extraction:\n",
    "    def triangle_filter(ser_date):\n",
    "        ### Extracting particular Data Date:\n",
    "        date_diag = ser_date.index.get_level_values('Data_Date')[0]\n",
    "        ### Dropping constant level:\n",
    "        ser_result = ser_date.droplevel('Data_Date')\n",
    "        ### Filtering over-diagonal values:\n",
    "        ser_result = ser_result[ser_result.index >= date_diag] \n",
    "        ### Results output:\n",
    "        return ser_result\n",
    "    ### Year-over-year-percent ticker values transforming to stock-like series:\n",
    "    def yoy_to_level(ser_date, int_step):\n",
    "        ### Dropping constant level:\n",
    "        ser_result = ser_date.droplevel('Observation_Date')\n",
    "        ### Basis initiating:\n",
    "        flo_basement = 1.0\n",
    "        ### Factor initiating: \n",
    "        flo_next_brick  = 1.0\n",
    "        ### Looping over month numbers:\n",
    "        for iter_period in range(min(int_step, len(ser_result.index))):         \n",
    "            ### Basement building up:\n",
    "            flo_basement = flo_basement * flo_next_brick\n",
    "            ### Next basement brick producing:\n",
    "            flo_next_brick = ((flo_next_brick ** (iter_period)) * (ser_result.iloc[iter_period] ** (1 / int_step))) ** (1 / (iter_period + 1)) \n",
    "            ### Jumping cumulative product performing:\n",
    "            idx_iter_data = ser_result.index[iter_period :: int_step]\n",
    "            ser_result.loc[idx_iter_data] = ser_result.loc[idx_iter_data].cumprod() * flo_basement       \n",
    "        ### Results output:            \n",
    "        return ser_result    \n",
    "    ### EI name extracting:\n",
    "    str_index_name = ser_name.index.get_level_values(0)[0]\n",
    "    ### Observation dates reindexation:    \n",
    "    print(ser_name.index.get_level_values(0)[0], ': Reindexation')    \n",
    "    idx_observation_range = ser_name.index.get_level_values('Observation_Date').unique().intersection(idx_date_range).sort_values()\n",
    "    ser_full = ser_name.droplevel('Index_Name').unstack('Data_Date').reindex(idx_observation_range).stack('Data_Date', dropna = False).squeeze()\n",
    "    ser_full = ser_full.swaplevel()\n",
    "    ser_full.index.rename('Observation_Date', level = -1, inplace = True)    \n",
    "    ### Forward filling for each data date:\n",
    "    ser_full = ser_full.groupby('Data_Date').ffill()   \n",
    "    ### Diagonalization:\n",
    "    ser_triangle = ser_full.groupby('Data_Date').apply(triangle_filter).sort_index()\n",
    "    ### Flags extracting:\n",
    "    ser_flags = df_flags.loc[str_index_name, All].squeeze() \n",
    "    ### 'TAR' type checking:\n",
    "    if (ser_flags['Type_Prime'] == 'TAR'):\n",
    "        print(str_index_name, ': TAR Primary Type ignoring')        \n",
    "        pass\n",
    "    ### Flags-based transforming:\n",
    "    else:\n",
    "        ### Indices of NA values collecting:\n",
    "        idx_isna = ser_triangle.loc[ser_triangle.isna()].index\n",
    "        ### Transforming to stock-like series:\n",
    "        if (ser_flags['Processing'] in ['Index', 'Level', 'Level%']):\n",
    "            ser_stock = ser_triangle\n",
    "        elif (ser_flags['Processing'] == 'Flow'):\n",
    "            print(str_index_name, ': Transformation to stock-like series: Cumulative sum')\n",
    "            ### Filling empty values:\n",
    "            ser_triangle = ser_triangle.fillna(0)\n",
    "            ### Cumulative sum for each observation date calculating:\n",
    "            ser_stock = ser_triangle.groupby('Observation_Date').cumsum()\n",
    "            ### Dropping NA values:\n",
    "            ser_stock.loc[idx_isna] = np.NaN\n",
    "        else:\n",
    "            print(str_index_name, ': Transformation to stock-like series: Cumulative product')\n",
    "            ### Filling empty values:\n",
    "            ser_triangle = ser_triangle.fillna(0)\n",
    "            ### Percents to multipliers converting:\n",
    "            ser_stock = 1 + ser_triangle / 100\n",
    "            ### Calculating with needed periodicity:\n",
    "            if (ser_flags['Frequency'] == 'Monthly'):\n",
    "                int_step = dict_cumprod_step[ser_flags['Processing']]\n",
    "                ### Year-by-year cumprod with rebasing:\n",
    "                ser_stock = ser_stock.groupby('Observation_Date').apply(yoy_to_level, int_step).swaplevel().sort_index()\n",
    "            ### Dropping NA values:\n",
    "            ser_stock.loc[idx_isna] = np.NaN\n",
    "    ### Results output:\n",
    "    return pd.concat([ser_stock], keys = [str_index_name], names = ['Index_Name'])\n",
    "\n",
    "### Flags loading:\n",
    "df_flags_typed = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags_typed)\n",
    "### Economic Indices vector loading:\n",
    "ser_history_bday = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_bday_history)\n",
    "ser_history_bday = ser_history_bday.reindex(df_flags_typed.index, level = 'Index_Name')\n",
    "### Testing:\n",
    "import tables\n",
    "tables.file._open_files.close_all()\n",
    "#list_test_ticker = ['JCOMHCF Index']\n",
    "list_test_ticker = ['NAPMPMI Index', 'CHPMINDX Index', 'DFEDGBA Index', 'EMPRGBCI Index', 'MAPMINDX Index', 'NAPMNMI Index', 'OUTFGAF Index', 'RCHSINDX Index']\n",
    "ser_history_bday_test = ser_history_bday.loc[list_test_ticker, All, All]\n",
    "ser_test_stock = ser_history_bday_test.groupby('Index_Name', group_keys = False)\\\n",
    "                                     .apply(complex_transform, idx_date_range, df_flags_typed, 0, int_min_years_z_score, bool_perform_sa = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFEDGBA Index : Seasonality adjustment\n",
      "DFEDGBA Index : Seasonality adjustment\n",
      "DFEDGBA Index : Seasonality adjustment\n",
      "DFEDGBA Index : Seasonality adjustment\n",
      "DFEDGBA Index : Seasonality adjustment\n",
      "DFEDGBA Index : Seasonality adjustment\n",
      "DFEDGBA Index : Seasonality adjustment\n",
      "DFEDGBA Index : Seasonality adjustment\n",
      "1.8 s ± 21.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "### RUN TO TEST: PERFORMING X13-SA WITH STOCK-LIKE CONVERTED DATA\n",
    "\n",
    "### Defining Economic Index series transformation:\n",
    "def complex_transform(ser_stock, idx_date_range, df_flags, int_max_name_length, int_min_years, bool_perform_sa = False):\n",
    "    ### X13 ARIMA Seasonality adjustment model:\n",
    "    def perform_x13_sa(ser_date):\n",
    "        ### Dropping constant level:        \n",
    "        ser_result = ser_date.droplevel('Observation_Date')\n",
    "        ### Check for nom empty vector:\n",
    "        if (ser_result.count() > 0):\n",
    "            ### Check for minimal quantity of observations to perform seasonality adjustment:\n",
    "            if (ser_result.last_valid_index() - ser_result.first_valid_index()).days >= (int_min_years * 365):   \n",
    "                ### Naming series for x13 performing:\n",
    "                ser_result.name = 'Ticker'\n",
    "                ### Calculating shift value to make all series positive:\n",
    "                flo_positron = abs(ser_result.min()) * 2\n",
    "                try:\n",
    "                    ### Performing seasonality adjustment:\n",
    "                    ser_result = x13_arima_analysis(ser_result + flo_positron, outlier = True, trading = True).seasadj - flo_positron\n",
    "    #                print('SA success : ', ser_date.index.get_level_values('Observation_Date')[0])                 \n",
    "                except:\n",
    "                    print('SA error : ', ser_date.index.get_level_values('Observation_Date')[0])\n",
    "                    pass\n",
    "        ### Results output:                \n",
    "        return ser_result \n",
    "    ### EI name extracting:\n",
    "    str_index_name = ser_stock.index.get_level_values(0)[0]\n",
    "    ser_stock = ser_stock.droplevel('Index_Name')\n",
    "    ### Flags extracting:\n",
    "    ser_flags = df_flags.loc[str_index_name, All].squeeze() \n",
    "    ### 'TAR' type checking:\n",
    "    if (ser_flags['Type_Prime'] == 'TAR'):\n",
    "        print(str_index_name, ': TAR Primary Type ignoring')        \n",
    "        pass\n",
    "    ### Flags-based transforming:\n",
    "    else:\n",
    "        ### Indices of NA values collecting:\n",
    "        idx_isna = ser_stock.loc[ser_stock.isna()].index\n",
    "        ### Seasonality adjustment testing:\n",
    "        if (bool_perform_sa & (ser_flags['SA_Status'].strip(' ') != 'SA')):\n",
    "            print(str_index_name, ': Seasonality adjustment')            \n",
    "            ### Filling empty values:            \n",
    "            ser_stock = ser_stock.groupby('Observation_Date').ffill()\n",
    "            ser_stock = ser_stock.groupby('Observation_Date').transform(perform_x13_sa).swaplevel().sort_index()\n",
    "            ### Dropping NA values:\n",
    "            ser_stock.loc[idx_isna] = np.NaN    \n",
    "    return pd.concat([ser_stock], keys = [str_index_name], names = ['Index_Name'])\n",
    "### Flags loading:\n",
    "df_flags_typed = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags_typed)\n",
    "### Testing:\n",
    "import tables\n",
    "tables.file._open_files.close_all()\n",
    "#ser_test_x13_sa = ser_test_stock.groupby('Index_Name', group_keys = False)\\\n",
    "#                                     .apply(complex_transform, idx_date_range, df_flags_typed, 0, int_min_years_z_score, bool_perform_sa = True)\n",
    "#%timeit ser_test_stock.loc[['DFEDGBA Index'], All, ['2020-07-31','2020-08-31']].groupby('Index_Name', group_keys = False)\\\n",
    "#                                                          .transform(complex_transform, idx_date_range, df_flags_typed, 0, int_min_years_z_score, bool_perform_sa = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3241"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data_Date   Observation_Date\n",
       "2004-06-30  2020-03-31          47.8\n",
       "            2020-04-27          47.8\n",
       "            2020-04-30          47.8\n",
       "            2020-05-26          47.8\n",
       "            2020-06-01          47.8\n",
       "                                ... \n",
       "2020-06-30  2020-07-27          -6.1\n",
       "            2020-07-31          -6.1\n",
       "            2020-08-31          -6.1\n",
       "2020-07-31  2020-07-31          -3.0\n",
       "            2020-08-31          -3.0\n",
       "Name: Value, Length: 1920, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "idx_obs_range = ser_test_stock.loc[['DFEDGBA Index'], All, All].index.get_level_values('Observation_Date').unique()\n",
    "ser_test_single = ser_test_stock.loc[['DFEDGBA Index'], All, idx_obs_range[-10 : ]].droplevel('Index_Name')\n",
    "#ser_test_single = ser_test_stock.loc[['DFEDGBA Index'], All, All].droplevel('Index_Name')\n",
    "ser_test_single.name = 'Value'\n",
    "ser_test_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "def perform_x13_sa(ser_date):\n",
    "    ### Dropping constant level:        \n",
    "    ser_result = ser_date.droplevel('Observation_Date')\n",
    "    ### Check for nom empty vector:\n",
    "    if (ser_result.count() > 0):\n",
    "        ### Check for minimal quantity of observations to perform seasonality adjustment:\n",
    "        if (ser_result.last_valid_index() - ser_result.first_valid_index()).days >= (int_min_years * 365):   \n",
    "            ### Naming series for x13 performing:\n",
    "            ser_result.name = 'Ticker'\n",
    "            ### Calculating shift value to make all series positive:\n",
    "            flo_positron = abs(ser_result.min()) * 2\n",
    "            try:\n",
    "                ### Performing seasonality adjustment:\n",
    "                ser_result = x13_arima_analysis(ser_result + flo_positron, outlier = True, trading = True).seasadj - flo_positron\n",
    "#                print('SA success : ', ser_date.index.get_level_values('Observation_Date')[0])                 \n",
    "            except Exception as e:\n",
    "                print('SA error : ', ser_date.index.get_level_values('Observation_Date')[0], '/', e.__class__)\n",
    "                pass\n",
    "    ### Results output:                \n",
    "    return pd.concat([ser_result], keys = [ser_date.index.get_level_values('Observation_Date')[0]], names = ['Observation_Date']) \n",
    "\n",
    "def transformParallel(serGrouped, func):\n",
    "    retLst = Parallel(n_jobs = 4)(delayed(func)(group) for name, group in serGrouped)    \n",
    "    return pd.concat(retLst)\n",
    "\n",
    "int_min_years = 7\n",
    "\n",
    "#print('regular version: ')\n",
    "#%timeit ser_test_single.groupby('Observation_Date').transform(perform_x13_sa).sort_index(level = ['Observation_Date', 'Data_Date'])\n",
    "\n",
    "#print('parallel version: ')\n",
    "#%timeit transformParallel(ser_test_single.groupby('Observation_Date'), perform_x13_sa).sort_index(level = ['Observation_Date', 'Data_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_test_single.groupby('Observation_Date').transform(perform_x13_sa).sort_index(level = ['Observation_Date', 'Data_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data_Date   Observation_Date\n",
       "2004-06-30  2020-03-31          47.358938\n",
       "2004-07-31  2020-03-31          39.502098\n",
       "2004-08-31  2020-03-31          36.851518\n",
       "2004-09-30  2020-03-31          30.178766\n",
       "2004-10-31  2020-03-31          28.379187\n",
       "                                  ...    \n",
       "2020-03-31  2020-08-31         -67.926482\n",
       "2020-04-30  2020-08-31         -67.407643\n",
       "2020-05-31  2020-08-31         -49.261507\n",
       "2020-06-30  2020-08-31          -7.672658\n",
       "2020-07-31  2020-08-31          -4.089245\n",
       "Name: seasadj, Length: 1920, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "transformParallel(ser_test_single.groupby('Observation_Date'), perform_x13_sa).sort_index(level = ['Observation_Date', 'Data_Date']).swaplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data_Date   Observation_Date\n",
       "2004-06-30  2020-07-31          47.8\n",
       "            2020-08-31          47.8\n",
       "2004-07-31  2020-07-31          39.2\n",
       "            2020-08-31          39.2\n",
       "2004-08-31  2020-07-31          37.3\n",
       "                                ... \n",
       "2020-05-31  2020-08-31         -49.2\n",
       "2020-06-30  2020-07-31          -6.1\n",
       "            2020-08-31          -6.1\n",
       "2020-07-31  2020-07-31          -3.0\n",
       "            2020-08-31          -3.0\n",
       "Name: Value, Length: 388, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_test_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
