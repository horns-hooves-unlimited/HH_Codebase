{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: ECONOMIC INDICES RELEASES HISTORY EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "from math import floor, ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.x13 import x13_arima_analysis\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable']\n",
    "### Raw data path and sheets:\n",
    "str_path_bb_idx_source = 'Data_Files/Source_Files/Bloomberg_Eco_Indices.xlsx'\n",
    "str_us_sheet = 'US Eco Const'\n",
    "str_all_sheet = 'All Eco Const'\n",
    "str_description_sheet = 'Description Const'\n",
    "### Flags data path and sheets:\n",
    "str_path_bb_idx_flags = 'Data_Files/Source_Files/Bloomberg_Eco_Flags.xlsx'\n",
    "str_flag_sheet = 'All to check'\n",
    "### Source data constants:\n",
    "int_idx_cols = 12\n",
    "### HDF file with converted source data:\n",
    "str_path_bb_idx_hdf = 'Data_Files/Source_Files/Bloomberg_Eco_Indices.h5'\n",
    "str_key_description = 'descriptions_exported' ### Bloomberg tickers description list\n",
    "str_key_flags = 'flags_exported' ### Acadian flags list\n",
    "str_key_exported = 'all_idx_exported' ### Raw export with only replacing zero dates and after 2021-01-01 dates with np.NaN\n",
    "str_key_raw_filled = 'all_idx_raw_filled' ### Raw export with initial dates, dates gaps, absent date coluns filled\n",
    "str_key_raw_history = 'raw_history' ### Export with all the corrections and fillings (restructured to [Index_Name -> Data_Date -> Observation_Date] | Index_Value series)\n",
    "str_key_bday_history = 'bday_history' ### Raw history vector with observation dates moved to nearest future business dates\n",
    "str_key_num_history = 'num_history' ### Bday history vector with observation dates changed to their date numbers (for future matrix cube saving as hdf file)\n",
    "str_key_from_date = 'idx_from_date' ### Series to get date numbers from dates\n",
    "str_key_to_date = 'idx_to_date' ### Series to get dates from date numbers\n",
    "### HDF file with matrices:\n",
    "str_path_bb_matrix_hdf = 'Data_Files/Source_Files/Matrix_Eco_Indices.h5'\n",
    "str_key_matrix_flagged = 'matrix_cube_flagged'\n",
    "str_key_matrix_winsorized = 'matrix_cube_winsorized'\n",
    "### Observation axis range:\n",
    "datetime_start = datetime(1984, 12, 31) # Start date for efficacy measures\n",
    "datetime_end = datetime(2020, 8, 31) # End date for efficacy measures\n",
    "date_start = datetime_start.date()\n",
    "date_end = datetime_end.date()\n",
    "idx_date_range = pd.date_range(date_start, date_end, freq = 'B')\n",
    "### Gaps filling options\n",
    "int_revision_shift = 1\n",
    "int_final_shift = 2\n",
    "int_first_mean_length = 12\n",
    "dict_final_only_lag = {}\n",
    "dict_final_only_lag['Quarterly'] = 90 // 2\n",
    "dict_final_only_lag['Monthly'] = 30 // 2\n",
    "dict_final_only_lag['Other'] = 7 // 2\n",
    "### Winsorization options:\n",
    "int_winsorize_bound = 5\n",
    "flo_winsorize_tolerance = 0.01\n",
    "int_winsorize_steps_limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: SOURCE FILE EXPORT\n",
    "\n",
    "### Reading excel file:\n",
    "df_all_idx_raw = pd.read_excel(io = str_path_bb_idx_source, sheet_name = str_all_sheet, skiprows = [0], index_col = None, header = None, parse_dates = True,\n",
    "                               na_values = list_na_excel_values, keep_default_na = False)\n",
    "### List of dataframes for each eco index initializing:\n",
    "list_idx_raw = []\n",
    "### Extracting and converting each eco index data block seperately to proper form:\n",
    "for int_iter_idx_col in range(len(df_all_idx_raw.columns) // int_idx_cols):\n",
    "    ### Extracting raw data:\n",
    "    df_iter_idx_raw = df_all_idx_raw.iloc[All, (int_iter_idx_col * int_idx_cols) : ((int_iter_idx_col + 1) * int_idx_cols) - 1]\n",
    "    ### Assigning colum names:\n",
    "    df_iter_idx_raw.columns = df_iter_idx_raw.iloc[1]\n",
    "    df_iter_idx_raw.columns.name = ''    \n",
    "    ### Dropping empty rows:\n",
    "    df_iter_idx_raw.dropna(how = 'all', inplace = True)\n",
    "    ### Extracting eco index name:\n",
    "    df_iter_idx_raw['Index_Name'] = df_iter_idx_raw.iloc[0, 0]\n",
    "    ### Dropping identification rows (no longer needed):\n",
    "    df_iter_idx_raw.drop([0, 1], axis = 0, inplace = True)\n",
    "    ### Setting multiindex:\n",
    "    df_iter_idx_raw.set_index(['Date', 'Index_Name'], drop = True, append = False, inplace = True)\n",
    "    ### ADding dataframe to list:\n",
    "    list_idx_raw.append(df_iter_idx_raw)\n",
    "### Exported data aggregating:\n",
    "df_all_idx_exported = pd.concat(list_idx_raw, axis = 0).swaplevel()\n",
    "### Exported data saving:\n",
    "df_all_idx_exported.index.names = ['Index_Name', 'Data_Date']\n",
    "### Types converting for numerical data:\n",
    "df_all_idx_exported = df_all_idx_exported.astype({'PX_LAST': 'float',\n",
    "                                                  'ACTUAL_RELEASE': 'float',\n",
    "                                                  'FIRST_REVISION': 'float',\n",
    "                                                  'BN_SURVEY_NUMBER_OBSERVATIONS': 'float',\n",
    "                                                  'BN_SURVEY_MEDIAN': 'float',\n",
    "                                                  'BN_SURVEY_AVERAGE': 'float',\n",
    "                                                  'FORECAST_STANDARD_DEVIATION': 'float',\n",
    "                                                 })\n",
    "### Zero dates dropping:\n",
    "df_all_idx_exported.loc[df_all_idx_exported['ECO_RELEASE_DT'] == 0, ['ECO_RELEASE_DT']] = np.NaN\n",
    "df_all_idx_exported.loc[df_all_idx_exported['FIRST_REVISION_DATE'] == 0, ['FIRST_REVISION_DATE']] = np.NaN\n",
    "df_all_idx_exported.loc[df_all_idx_exported['ECO_FUTURE_RELEASE_DATE'] == 0, ['ECO_FUTURE_RELEASE_DATE']] = np.NaN\n",
    "### Incorrect dates droppping:\n",
    "df_all_idx_exported.loc[df_all_idx_exported['ECO_RELEASE_DT'] > 20210000, 'ECO_RELEASE_DT'] = np.NaN\n",
    "df_all_idx_exported.loc[df_all_idx_exported['FIRST_REVISION_DATE'] > 20210000, 'FIRST_REVISION_DATE'] = np.NaN\n",
    "df_all_idx_exported.loc[df_all_idx_exported['ECO_FUTURE_RELEASE_DATE'] > 20210000, 'ECO_FUTURE_RELEASE_DATE'] = np.NaN\n",
    "### Dates formatting:\n",
    "df_all_idx_exported['ECO_RELEASE_DT'] = pd.to_datetime(df_all_idx_exported['ECO_RELEASE_DT'].astype(str), format = '%Y%m%d')\n",
    "df_all_idx_exported['FIRST_REVISION_DATE'] = pd.to_datetime(df_all_idx_exported['FIRST_REVISION_DATE'].astype(str), format = '%Y%m%d')\n",
    "df_all_idx_exported['ECO_FUTURE_RELEASE_DATE'] = pd.to_datetime(df_all_idx_exported['ECO_FUTURE_RELEASE_DATE'].astype(str), format = '%Y%m%d')\n",
    "### Exported dataframe saving:\n",
    "df_all_idx_exported.to_hdf(str_path_bb_idx_hdf, key = str_key_exported, mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: DESCRIPTION FILE EXPORT\n",
    "\n",
    "### Readind excel file:\n",
    "df_description = pd.read_excel(io = str_path_bb_idx_source, sheet_name = str_description_sheet, skiprows = [0], usecols = list(range(10)), index_col = 0, header = 0, \n",
    "                               parse_dates = True, na_values = list_na_excel_values, keep_default_na = False)\n",
    "### Dataframe transforming:\n",
    "df_description.dropna(how = 'all', inplace = True)\n",
    "df_description.index.name = 'Index_Name'\n",
    "### Types converting:\n",
    "df_description = df_description.astype({'ID_BB0': int,\n",
    "                                        'SECURITY_DES': str,\n",
    "                                        'SHORT_NAME': str,\n",
    "                                        'FEED_CODE': str,\n",
    "                                        'INDX_SOURCE': str,\n",
    "                                        'COUNTRY': str,\n",
    "                                        'INDX_HIST_START_DT_MONTHLY': str,\n",
    "                                        'DES_NOTES': str,\n",
    "                                        'INDX_FREQ': str,                       \n",
    "                                      })\n",
    "### Exported dataframe saving:\n",
    "df_description.to_hdf(str_path_bb_idx_hdf, key = str_key_description, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: FLAGS FILE EXPORT\n",
    "\n",
    "### Reading excel file:\n",
    "df_flags = pd.read_excel(io = str_path_bb_idx_flags, sheet_name = str_flag_sheet, usecols = list(range(16)), index_col = 4, header = 0, \n",
    "                               parse_dates = True, na_values = list_na_excel_values, keep_default_na = False)\n",
    "### Dataframe transforming:\n",
    "df_flags.dropna(how = 'all', inplace = True)\n",
    "df_flags.index.name = 'Index_Name'\n",
    "### Exported dataframe saving:\n",
    "df_flags.to_hdf(str_path_bb_idx_hdf, key = str_key_flags, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: RELEASES AND REVISIONS DATA EXTRACTING\n",
    "\n",
    "### Revisions data reading (99,180 rows):\n",
    "df_all_idx_revisions = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_exported)[\n",
    "                                   ['PX_LAST', 'ECO_RELEASE_DT', 'ACTUAL_RELEASE', 'FIRST_REVISION_DATE', 'FIRST_REVISION', 'ECO_FUTURE_RELEASE_DATE']]\n",
    "### Dropping rows with all Index Values missed (96,369 rows left):\n",
    "#df_all_idx_revisions = \\\n",
    "#        df_all_idx_revisions.loc[df_all_idx_revisions['PX_LAST'].notna() | df_all_idx_revisions['ACTUAL_RELEASE'].notna() | df_all_idx_revisions['FIRST_REVISION'].notna()]\n",
    "### Date filtering:\n",
    "df_all_idx_revisions = df_all_idx_revisions.reset_index('Data_Date')\n",
    "df_all_idx_revisions = df_all_idx_revisions.loc[df_all_idx_revisions['Data_Date'] >= datetime_start].set_index('Data_Date', append = True)\n",
    "### Columns renaming:\n",
    "df_all_idx_revisions.columns = ['Final_Value', 'Release_Date', 'Release_Value', 'Revision_Date', 'Revision_Value', 'Future_Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: DATA REPAIRING\n",
    "\n",
    "### 1 : MPMIITMA Index Revision_Value mistake correction:\n",
    "df_all_idx_revisions.loc[('MPMIITMA Index', All), 'Revision_Value'] = np.NaN\n",
    "### 2 : JNCPT Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNCPT Index', '1999-07-31'), 'Release_Date'] = pd.to_datetime('1999-07-30')\n",
    "df_all_idx_revisions.loc[('JNCPT Index', '2001-01-31'), 'Release_Date'] = pd.to_datetime('2001-01-26')\n",
    "df_all_idx_revisions.loc[('JNCPT Index', '2001-02-28'), 'Release_Date'] = pd.to_datetime('2001-03-02')\n",
    "df_all_idx_revisions.loc[('JNCPT Index', '2001-06-30'), 'Release_Date'] = pd.to_datetime('2001-06-29')\n",
    "### 3 : JNCSTOTY Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNCSTOTY Index', '2018-12-31'), 'Release_Date'] = pd.to_datetime('2019-01-31')\n",
    "### 4: JNPIY Index Revision_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNPIY Index', '2015-12-31'), 'Revision_Date'] = pd.to_datetime('2016-02-23')\n",
    "### 5: JNPIY Index Revision_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNTIAIAM Index', '2017-11-30'), 'Revision_Date'] = pd.to_datetime('2018-02-20')\n",
    "### 6: BECPCHNG Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('BECPCHNG Index', '1997-01-31'), 'Release_Date'] = pd.to_datetime('1997-01-30')\n",
    "df_all_idx_revisions.loc[('BECPCHNG Index', '2001-10-31'), 'Release_Date'] = pd.to_datetime('2001-10-28')\n",
    "df_all_idx_revisions.loc[('BECPCHNG Index', '2001-11-30'), 'Release_Date'] = pd.to_datetime('2001-11-29')\n",
    "df_all_idx_revisions.loc[('BECPCHNG Index', '2001-12-31'), 'Release_Date'] = pd.to_datetime('2001-12-23')\n",
    "### 7 : BEUER Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('BEUER Index', '1997-01-31'), 'Release_Date'] = pd.to_datetime('1997-02-06')\n",
    "### 8 : FRCPEECM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('FRCPEECM Index', '1997-07-31'), 'Release_Date'] = pd.to_datetime('1997-08-14')\n",
    "### 9 : FRCPIMOM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('FRCPIMOM Index', '1999-01-31'), 'Release_Date'] = pd.to_datetime('1999-02-24')\n",
    "df_all_idx_revisions.loc[('FRCPIMOM Index', '1999-02-28'), 'Release_Date'] = pd.to_datetime('1999-03-24')\n",
    "df_all_idx_revisions.loc[('FRCPIMOM Index', '2006-01-31'), 'Release_Date'] = pd.to_datetime('2006-02-21')\n",
    "### 10 : GKCPNEWY Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-03-31'), 'Release_Date'] = pd.to_datetime('2001-04-09')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-04-30'), 'Release_Date'] = pd.to_datetime('2001-05-10')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-05-31'), 'Release_Date'] = pd.to_datetime('2001-06-10')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-06-30'), 'Release_Date'] = pd.to_datetime('2001-07-05')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-07-31'), 'Release_Date'] = pd.to_datetime('2001-08-09')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-08-31'), 'Release_Date'] = pd.to_datetime('2001-09-09')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-09-30'), 'Release_Date'] = pd.to_datetime('2001-10-08')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-10-31'), 'Release_Date'] = pd.to_datetime('2001-11-08')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-11-30'), 'Release_Date'] = pd.to_datetime('2001-12-09')\n",
    "### 11 : GKUERATE Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('GKUERATE Index', '2007-01-31'), 'Release_Date'] = np.NaN\n",
    "### 12 : GRCP2NRM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('GRCP2NRM Index', '2006-01-31'), 'Release_Date'] = pd.to_datetime('2006-01-30')\n",
    "### 13 : GRCP2SAM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('GRCP2SAM Index', '2006-01-31'), 'Release_Date'] = pd.to_datetime('2006-01-30')\n",
    "### 14 : ITVHYOY Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('ITVHYOY Index', '1999-01-31'), 'Release_Date'] = pd.to_datetime('1999-02-03')\n",
    "### 15 : JNLSUCTL Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNLSUCTL Index', '2003-01-31'), 'Release_Date'] = pd.to_datetime('2003-03-01')\n",
    "### 16 : PTCIEC Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('PTCIEC Index', '2007-12-31'), 'Release_Date'] = pd.to_datetime('2008-01-08')\n",
    "### 17 : SLCPLHMM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('SLCPLHMM Index', '2004-04-30'), 'Release_Date'] = pd.to_datetime('2004-05-20')\n",
    "df_all_idx_revisions.loc[('SLCPLHMM Index', '2005-01-31'), 'Release_Date'] = pd.to_datetime('2005-02-27')\n",
    "df_all_idx_revisions.loc[('SLCPLHMM Index', '2005-02-28'), 'Release_Date'] = pd.to_datetime('2005-03-16')\n",
    "### 18 : SPCPEUMM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-01-31'), 'Release_Date'] = pd.to_datetime('2000-02-13')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-02-29'), 'Release_Date'] = pd.to_datetime('2000-03-14')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-03-31'), 'Release_Date'] = pd.to_datetime('2000-04-11')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-04-30'), 'Release_Date'] = pd.to_datetime('2000-05-11')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-05-31'), 'Release_Date'] = pd.to_datetime('2000-06-13')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-06-30'), 'Release_Date'] = pd.to_datetime('2000-07-13')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-07-31'), 'Release_Date'] = pd.to_datetime('2000-08-14')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-08-31'), 'Release_Date'] = pd.to_datetime('2000-09-13')\n",
    "### 19 : SPROCHNG Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('SPROCHNG Index', '2008-01-31'), 'Release_Date'] = pd.to_datetime('2008-02-25')\n",
    "### 20 : SVAWRYOY Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('SVAWRYOY Index', '2006-08-31'), 'Release_Date'] = pd.to_datetime('2006-10-15')\n",
    "df_all_idx_revisions.loc[('SVAWRYOY Index', '2006-09-30'), 'Release_Date'] = pd.to_datetime('2006-11-15')\n",
    "df_all_idx_revisions.loc[('SVAWRYOY Index', '2006-10-31'), 'Release_Date'] = pd.to_datetime('2006-12-17')\n",
    "### 21 : INJCJC Index Data Date shifts correction:\n",
    "df_all_idx_revisions.loc[('INJCJC Index', '2020-08-28'), 'Release_Date'] = np.NaN\n",
    "str_ei_to_correct = 'INJCJC Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_to_correct = df_ei_to_correct.drop(df_ei_to_correct.loc[df_ei_to_correct.isna().all(axis = 1)].index)\n",
    "df_ei_to_correct_shifted = df_ei_to_correct.shift(-1)\n",
    "df_ei_to_correct_shifted.columns = [str_column_name + '_shifted' for str_column_name in df_ei_to_correct_shifted.columns] \n",
    "df_ei_corrected = pd.concat([df_ei_to_correct, df_ei_to_correct_shifted], axis = 1)\n",
    "df_ei_corrected.loc[df_ei_corrected['Final_Value_shifted'].isna() & df_ei_corrected['Revision_Value_shifted'].notna(), 'Revision_Date'] = \\\n",
    "                df_ei_corrected['Revision_Date_shifted']\n",
    "df_ei_corrected.loc[df_ei_corrected['Final_Value_shifted'].isna() & df_ei_corrected['Revision_Value_shifted'].notna(), 'Revision_Value'] = \\\n",
    "                df_ei_corrected['Revision_Value_shifted']\n",
    "df_ei_corrected = df_ei_corrected[df_ei_to_correct.columns]\n",
    "df_ei_corrected = df_ei_corrected.drop(df_ei_corrected[df_ei_corrected['Final_Value'].isna()].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 22 : INJCSP Index Data Date shifts correction:\n",
    "str_ei_to_correct = 'INJCSP Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct['Final_Value'].isna()].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 23 : ITSR1B Index Data Date shifts correction: \n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(index = ('ITSR1B Index', '1992-12-31'))\n",
    "### 24 : ATIPIMM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'ATIPIMM Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2004-04-30')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 25 : ATIPIMM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'GKIPIYOY Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2001-01-31')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 26 : IEIPIMOM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'IEIPIMOM Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2001-02-28')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 27 : ITNHMOM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'ITNHMOM Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2003-03-31')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 28 : MPMIGBXA Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'MPMIGBXA Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2005-07-31')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 29 : PCE CHNC Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'PCE CHNC Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('1999-02-28')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 30 : PTPPMOM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'PTPPMOM Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('1999-02-28')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 31 : SNTEEUGX Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'SNTEEUGX Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2003-01-31')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 32 : UKISCTMM Index Data Date shifts correction: \n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(index = ('UKISCTMM Index', '1992-12-31'))\n",
    "### 33 : IERSVMOM Index adding missed rows:\n",
    "str_ei_to_correct = 'IERSVMOM Index'\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2001-01-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2001-02-28')), All] = np.NaN\n",
    "### 34 : ITNHMOM Index adding missed rows:\n",
    "str_ei_to_correct = 'ITNHMOM Index'\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2020-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2020-08-31')), All] = np.NaN\n",
    "### 35 : NEISIYOY Index adding missed rows:\n",
    "str_ei_to_correct = 'NEISIYOY Index'\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('1999-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('1999-08-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2000-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2000-11-30')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2001-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2002-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2003-07-31')), All] = np.NaN\n",
    "### 36 : NEISIYOY Index adding missed rows:\n",
    "str_ei_to_correct = 'MAPMINDX Index'\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-05-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-06-30')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-08-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-09-30')), All] = np.NaN\n",
    "### 37 : COMFCOMF Index values correcting:\n",
    "str_ei_to_correct = 'COMFCOMF Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.copy().loc[(str_ei_to_correct, All), All]\n",
    "df_ei_to_correct.loc[df_ei_to_correct['Release_Value'] < 10, 'Release_Value'] = (1 + df_ei_to_correct['Release_Value'] / 100) * 50\n",
    "df_ei_to_correct.loc[df_ei_to_correct['Revision_Value'] < 10, 'Revision_Value'] = (1 + df_ei_to_correct['Revision_Value'] / 100) * 50\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_to_correct], axis = 0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: RELEASE DATE < DATA DATE\n",
    "\n",
    "df_less_data_date = df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] < df_all_idx_revisions.index.get_level_values(1) - pd.offsets.BDay(15)]\n",
    "print(df_less_data_date.index.get_level_values('Index_Name').unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: REVISION DATE < RELEASE DATE\n",
    "\n",
    "df_less_release_date = df_all_idx_revisions.loc[df_all_idx_revisions['Revision_Date'] < df_all_idx_revisions['Release_Date']]\n",
    "print(df_less_release_date.index.get_level_values('Index_Name').unique())\n",
    "print(df_less_release_date.loc[(All, All), All])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: RELEASE DATE >> DATA DATE\n",
    "\n",
    "df_less_data_date = df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] > df_all_idx_revisions.index.get_level_values(1) + pd.offsets.BDay(150)]\n",
    "print(df_less_data_date.index.get_level_values('Index_Name').unique())\n",
    "print(df_less_data_date.loc[('ASPPIMOM Index', All), All])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: RELEASE DATE > NEXT RELEASE DATE\n",
    "\n",
    "df_released_date = df_all_idx_revisions['Release_Date'].to_frame()\n",
    "df_released_date['Release_Date_next'] = df_released_date['Release_Date'].groupby('Index_Name').transform(lambda ser_eco_ind: ser_eco_ind.shift(-1))\n",
    "df_less_release_date = df_released_date.loc[df_released_date['Release_Date_next'] < df_released_date['Release_Date']]\n",
    "print(df_less_release_date.index.get_level_values('Index_Name').unique())\n",
    "print(df_less_release_date.loc[(All, All), All])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: EXTRA SHORT INTERVALS\n",
    "\n",
    "### Frequency check:\n",
    "ser_data_date = df_all_idx_revisions.reset_index('Data_Date')['Data_Date'].squeeze()\n",
    "df_eco_ind_agg = ser_data_date.groupby('Index_Name').apply(lambda ser_eco_ind: (ser_eco_ind - ser_eco_ind.shift()).dt.days)\\\n",
    "                               .groupby('Index_Name').agg(['min', 'max', 'median'])\n",
    "print('Frequency control: Extra minimum:\\n', df_eco_ind_agg.loc[df_eco_ind_agg['median'] / 1.2 > df_eco_ind_agg['min']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: EXTRA LONG INTERVALS\n",
    "\n",
    "### Frequency check:\n",
    "ser_data_date = df_all_idx_revisions.reset_index('Data_Date')['Data_Date'].squeeze()\n",
    "df_eco_ind_agg = ser_data_date.groupby('Index_Name').apply(lambda ser_eco_ind: (ser_eco_ind - ser_eco_ind.shift()).dt.days)\\\n",
    "                               .groupby('Index_Name').agg(['min', 'max', 'median'])\n",
    "print('Frequency control: Extra maximum:\\n', df_eco_ind_agg.loc[df_eco_ind_agg['median'] * 3 < df_eco_ind_agg['max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: DATA QUALITY AND COMPLETENESS\n",
    "\n",
    "### Release Date <= Event Date:\n",
    "print('Overall observations number:', len(df_all_idx_revisions.index))\n",
    "df_less_data_date = df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] < df_all_idx_revisions.index.get_level_values(1)]\n",
    "print('Observations with Release Date < Data Date number:', len(df_less_data_date.index))\n",
    "print('Unique Eco Indices with Release Date < Data Date cases number:', len(df_less_data_date.index.get_level_values(0).unique()))\n",
    "### Release Date vs First Revision Date:\n",
    "print('Release Date > First Revision Date:\\n', \n",
    "      df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] > df_all_idx_revisions['Revision_Date'], ['Release_Date', 'Revision_Date']])\n",
    "### Release Date vs next Release Date:\n",
    "df_released_date = df_all_idx_revisions['Release_Date'].to_frame()\n",
    "df_released_date['Release_Date_shifted'] = df_released_date['Release_Date'].groupby('Index_Name').transform(lambda ser_eco_ind: ser_eco_ind.shift(-1))\n",
    "print('Release Date > next Release Date:\\n', df_released_date.loc[df_released_date['Release_Date'] > df_released_date['Release_Date_shifted']])\n",
    "### Frequency check:\n",
    "ser_data_date = df_all_idx_revisions.reset_index('Data_Date')['Data_Date'].squeeze()\n",
    "df_eco_ind_agg = ser_data_date.groupby('Index_Name').apply(lambda ser_eco_ind: (ser_eco_ind - ser_eco_ind.shift()).dt.days)\\\n",
    "                               .groupby('Index_Name').agg(['min', 'max', 'median'])\n",
    "print('Frequency control: Extra short intervals:\\n', df_eco_ind_agg.loc[df_eco_ind_agg['median'] / 1.2 > df_eco_ind_agg['min']])\n",
    "print('Frequency control: Extra long intervals:\\n', df_eco_ind_agg.loc[df_eco_ind_agg['median'] * 2 < df_eco_ind_agg['max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: EXTRACTING Release and first revision date/value pairs:\n",
    "\n",
    "### Release data extracting:\n",
    "df_release_based = df_all_idx_revisions[['Release_Date', 'Release_Value']]\n",
    "df_release_based.columns = ['Observation_Date', 'Index_Value']\n",
    "### All empty release date vector Eco Indices:\n",
    "ser_empty_release_date = df_release_based['Observation_Date'].groupby('Index_Name').count()\n",
    "ser_empty_release_date = ser_empty_release_date[ser_empty_release_date == 0]\n",
    "### All empty release value vector Eco Indices:\n",
    "ser_empty_release_value = df_release_based['Index_Value'].groupby('Index_Name').count()\n",
    "ser_empty_release_value = ser_empty_release_value[ser_empty_release_value == 0]\n",
    "### All empty release date or all empty release value indices list:\n",
    "list_empty_release = sorted(list(set(ser_empty_release_date.index).union(set(ser_empty_release_value.index))))\n",
    "### Revision data extracting:\n",
    "df_revision_based = df_all_idx_revisions[['Revision_Date', 'Revision_Value']]\n",
    "df_revision_based.columns = ['Observation_Date', 'Index_Value']\n",
    "### All empty first revision date vector Eco Indices:\n",
    "ser_empty_revision_date = df_revision_based['Observation_Date'].groupby('Index_Name').count()\n",
    "ser_empty_revision_date = ser_empty_revision_date[ser_empty_revision_date == 0]\n",
    "### All empty first revision value vector Eco Indices:\n",
    "ser_empty_revision_value = df_revision_based['Index_Value'].groupby('Index_Name').count()\n",
    "ser_empty_revision_value = ser_empty_revision_value[ser_empty_revision_value == 0]\n",
    "### All empty first revision date or all empty first revision value indices list:\n",
    "list_empty_revision = sorted(list(set(ser_empty_revision_date.index).union(set(ser_empty_revision_value.index))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: MAIN STATS\n",
    "\n",
    "### General stats:\n",
    "print('Rows with at least one value filled:', len(df_all_idx_revisions.index))\n",
    "print('Eco Indices number:', len(df_all_idx_revisions.index.get_level_values(0).unique()))\n",
    "print('Average data dates number:', int(df_all_idx_revisions.groupby('Index_Name').apply(lambda df_group: len(df_group.index)).mean()))\n",
    "### Release data analyzing (96,369 rows):\n",
    "### Data quantity:\n",
    "print('Release Info: filled date & value:', len(df_release_based.loc[df_release_based['Observation_Date'].notna() & df_release_based['Index_Value'].notna()]), 'rows')\n",
    "print('Release Info: empty date & value:', len(df_release_based.loc[df_release_based['Observation_Date'].isna() & df_release_based['Index_Value'].isna()]), 'rows')\n",
    "print('Release Info: empty date & filled value:', len(df_release_based.loc[df_release_based['Observation_Date'].isna() & df_release_based['Index_Value'].notna()]), 'rows')\n",
    "print('Release Info: filled date & empty value:', len(df_release_based.loc[df_release_based['Observation_Date'].notna() & df_release_based['Index_Value'].isna()]), 'rows')\n",
    "### First revision data analyzing (96,369 rows):\n",
    "### Data quantity:\n",
    "print('First revision Info: filled date & value:', \n",
    "      len(df_revision_based.loc[df_revision_based['Observation_Date'].notna() & df_revision_based['Index_Value'].notna()]), 'rows')\n",
    "print('First revision Info: empty date & value:', \n",
    "      len(df_revision_based.loc[df_revision_based['Observation_Date'].isna() & df_revision_based['Index_Value'].isna()]), 'rows')\n",
    "print('First revision Info: empty date & filled value:', \n",
    "      len(df_revision_based.loc[df_revision_based['Observation_Date'].isna() & df_revision_based['Index_Value'].notna()]), 'rows')\n",
    "print('First revision Info: filled date & empty value:', \n",
    "      len(df_revision_based.loc[df_revision_based['Observation_Date'].notna() & df_revision_based['Index_Value'].isna()]), 'rows') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: DETAILED STATS: RELEASE DATE/VALUE PAIR\n",
    "\n",
    "print('Eco Indices without any release date number:', len(ser_empty_release_date.index))\n",
    "print('Eco Indices without any release value number:', len(ser_empty_release_value.index))\n",
    "### Empty Eco Indices relese info lists comparision:\n",
    "print('Symmetric difference of empty release info Eco Indices lists:', list(set(ser_empty_release_date.index).symmetric_difference(set(ser_empty_release_value.index))))\n",
    "print('Union of empty release info Eco Indices lists:\\n', sorted(list(set(ser_empty_release_date.index).union(set(ser_empty_release_value.index)))))\n",
    "for iter_eco_index in sorted(list(set(ser_empty_release_date.index).symmetric_difference(set(ser_empty_release_value.index)))):\n",
    "    print('Symmetric difference for', iter_eco_index, ':\\n', \n",
    "          df_all_idx_revisions.loc[(iter_eco_index, All), All].dropna(subset = ['Release_Date', 'Release_Value'], how = 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: DESCRIPTION OF PX_LAST ONLY ECO INDICES\n",
    "\n",
    "#df_description = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_description)\n",
    "#df_description.loc[list_empty_release]#.to_excel('Data_Files/Test_Files/PX_LAST_only_indices.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: DETAILED STATS: FIRST REVISION DATE/VALUE PAIR\n",
    "\n",
    "print('Eco Indices without any revision date number:', len(ser_empty_revision_date.index))\n",
    "print('Eco Indices without any revision value number:', len(ser_empty_revision_value.index))\n",
    "### Empty Eco Indices relese info lists comparision:\n",
    "print('Symmetric difference of empty revision info Eco Indices lists:', \n",
    "      sorted(list(set(ser_empty_revision_date.index).symmetric_difference(set(ser_empty_revision_value.index)))))\n",
    "print('Union of empty revision info Eco Indices lists:\\n', sorted(list(set(ser_empty_revision_date.index).union(set(ser_empty_revision_value.index)))))\n",
    "for iter_eco_index in sorted(list(set(ser_empty_revision_date.index).symmetric_difference(set(ser_empty_revision_value.index)))):\n",
    "    print('Symmetric difference for', iter_eco_index, ':\\n', \n",
    "          df_all_idx_revisions.loc[(iter_eco_index, All), All].dropna(subset = ['Revision_Date', 'Revision_Value'], how = 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CANCELLED: RUN TO RE-EXPORT DATA: REPAIRING SOURCE FILE\n",
    "\n",
    "#### Move early Release Date to Data Date:\n",
    "#df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] < df_all_idx_revisions.index.get_level_values(-1), 'Release_Date'] = \\\n",
    "#    df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] < df_all_idx_revisions.index.get_level_values(-1)].index.get_level_values(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CANCELLED: RUN TO TESTING: RELEASE DATE MOVING CONTROL:\n",
    "\n",
    "#df_all_idx_revisions.loc[('BEBCI Index', All), All].iloc[ -10 : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: ALL DATES FILLING PROCEDURE\n",
    "\n",
    "df_description = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_description)\n",
    "\n",
    "#### Dates filling:\n",
    "def all_dates_filler(df_eco_ind):\n",
    "    ### Eco Indice options saving:\n",
    "    str_index_name = df_eco_ind.index.get_level_values(0)[0]\n",
    "    str_eco_ind_freq = df_description.loc[str_index_name, 'INDX_FREQ']\n",
    "    if (str_eco_ind_freq in dict_final_only_lag.keys()):\n",
    "        int_final_only_lag = dict_final_only_lag[str_eco_ind_freq]\n",
    "    else:\n",
    "        int_final_only_lag = dict_final_only_lag['Other']\n",
    "    ### Index_Name level dropping:\n",
    "    df_eco_ind = df_eco_ind.droplevel(0)\n",
    "    ### Index duplicating to column:\n",
    "    df_eco_ind['Event_Date'] = df_eco_ind.index\n",
    "    df_eco_ind['Release_Lag'] = np.NaN\n",
    "    ### No release checking:\n",
    "    if (str_index_name in list_empty_release): \n",
    "        df_eco_ind['Final_Date'] = df_eco_ind['Event_Date'] + pd.Timedelta(int_final_only_lag, 'D')\n",
    "    elif (str_index_name in list_empty_revision):    \n",
    "        ### Backfilling release dates lag before the first known released date: \n",
    "        idx_first_valid_release = df_eco_ind['Release_Date'].first_valid_index()\n",
    "        df_first_released = df_eco_ind.dropna(subset = ['Release_Date']).iloc[ : int_first_mean_length]        \n",
    "        int_first_release_mean = (df_first_released['Release_Date'] - df_first_released['Event_Date']).mean().days + 1\n",
    "        df_eco_ind.loc[ : idx_first_valid_release, 'Release_Lag'] = int_first_release_mean\n",
    "        ### Interpolating missed release dates lag after the first known released date:\n",
    "        df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'] = (df_eco_ind['Release_Date'] - df_eco_ind['Event_Date']).dt.days\n",
    "        df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'] = df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'].interpolate(method = 'linear').round()\n",
    "        ### Filling empty release date by lag adding:\n",
    "        df_eco_ind['Release_Lag'] = pd.to_timedelta(df_eco_ind['Release_Lag'], 'D')    \n",
    "        df_eco_ind.loc[df_eco_ind['Release_Date'].isna(), 'Release_Date'] = df_eco_ind['Event_Date'] + df_eco_ind['Release_Lag']\n",
    "        ### Final date filling as one event shifted release date\n",
    "        df_eco_ind['Final_Date'] = df_eco_ind['Release_Date'].shift(-int_revision_shift)   \n",
    "    else:\n",
    "        ### Backfilling release dates lag before the first known released date: \n",
    "        idx_first_valid_release = df_eco_ind['Release_Date'].first_valid_index()\n",
    "        df_first_released = df_eco_ind.dropna(subset = ['Release_Date']).iloc[ : int_first_mean_length]     \n",
    "        int_first_release_mean = (df_first_released['Release_Date'] - df_first_released['Event_Date']).mean().days + 1        \n",
    "        df_eco_ind.loc[ : idx_first_valid_release, 'Release_Lag'] = int_first_release_mean\n",
    "        ### Interpolating missed release dates lag after the first known released date:\n",
    "        df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'] = (df_eco_ind['Release_Date'] - df_eco_ind['Event_Date']).dt.days\n",
    "        df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'] = df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'].interpolate(method = 'linear').round()\n",
    "        ### Filling empty release date with lag adding:\n",
    "        df_eco_ind['Release_Lag'] = pd.to_timedelta(df_eco_ind['Release_Lag'], 'D')    \n",
    "        df_eco_ind.loc[df_eco_ind['Release_Date'].isna(), 'Release_Date'] = df_eco_ind['Event_Date'] + df_eco_ind['Release_Lag']\n",
    "        ### Filling empty revision dates: \n",
    "        df_eco_ind.loc[df_eco_ind['Revision_Date'].isna(), 'Revision_Date'] = df_eco_ind['Release_Date'].shift(-int_revision_shift)\n",
    "        ### Final date filling as one event shifted release date\n",
    "        df_eco_ind['Final_Date'] = df_eco_ind['Release_Date'].shift(-int_final_shift)\n",
    "    ### Last values having revision date with no revision value managing:\n",
    "    idx_last_values = df_eco_ind['Revision_Date'].notna() & df_eco_ind['Revision_Value'].isna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Revision_Value'] = df_eco_ind['Final_Value']\n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Value'] = np.NaN\n",
    "    ### Last values having release date with no release value managing:\n",
    "    idx_last_values = df_eco_ind['Release_Date'].notna() & df_eco_ind['Release_Value'].isna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Release_Value'] = df_eco_ind['Final_Value']\n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Value'] = np.NaN\n",
    "    ### Last values having final value with no final date and future date managing (no news detected):\n",
    "    idx_last_values = df_eco_ind['Future_Date'].isna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Value'] = np.NaN\n",
    "    ### Last values having final value with no final date and having revision pair managing (no news detected):\n",
    "    idx_last_values = df_eco_ind['Revision_Date'].notna() & df_eco_ind['Revision_Value'].notna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Value'] = np.NaN\n",
    "    ### Last values having final value with no final date and no revision pair managing:\n",
    "    idx_last_values = df_eco_ind['Revision_Date'].isna() & df_eco_ind['Revision_Value'].isna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Date'] = df_eco_ind['Future_Date']    \n",
    "    ### Results ouput:\n",
    "    return df_eco_ind[['Release_Date', 'Release_Value', 'Revision_Date', 'Revision_Value', 'Final_Date', 'Final_Value']]\n",
    "\n",
    "### Date filling for each eco index performing:\n",
    "df_dates_filled = df_all_idx_revisions.groupby('Index_Name').apply(all_dates_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: DATA REPAIRING AND SAVING\n",
    "\n",
    "### 38 : GKCPIUHY Index Final Date shifts correction: \n",
    "df_dates_filled.loc[('GKCPIUHY Index', '2003-10-31'), 'Final_Date'] = pd.to_datetime('2004-02-17')\n",
    "df_dates_filled.loc[('GKCPIUHY Index', '2003-11-30'), 'Final_Date'] = pd.to_datetime('2004-02-17')\n",
    "### 39 : GKCPNEWY Index Final Date shifts correction: \n",
    "df_dates_filled.loc[('GKCPNEWY Index', '2003-10-31'), 'Final_Date'] = pd.to_datetime('2004-02-10')\n",
    "df_dates_filled.loc[('GKCPNEWY Index', '2003-11-30'), 'Final_Date'] = pd.to_datetime('2004-02-10')\n",
    "### 40 : EUCPTSAM Index Final Date shifts correction: \n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2004-12-31'), 'Final_Date'] = pd.to_datetime('2005-09-12')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2005-03-31'), 'Final_Date'] = pd.to_datetime('2005-12-09')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2005-06-30'), 'Final_Date'] = pd.to_datetime('2006-03-10')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2005-09-30'), 'Final_Date'] = pd.to_datetime('2006-06-09')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2005-12-31'), 'Final_Date'] = pd.to_datetime('2006-09-06')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2006-03-31'), 'Final_Date'] = pd.to_datetime('2006-12-06')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2006-06-30'), 'Final_Date'] = pd.to_datetime('2006-12-06')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2006-09-30'), 'Final_Date'] = pd.to_datetime('2007-01-18')\n",
    "### 41 : SVUER Index Revision Date shifts correction: \n",
    "df_dates_filled.loc[('SVUER Index', '2005-06-30'), 'Revision_Date'] = pd.to_datetime('2005-09-18')\n",
    "### 42 : UKMPIMOM Index Revision Date shifts correction: \n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-01-31'), 'Revision_Date'] = pd.to_datetime('2001-04-09')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-02-28'), 'Revision_Date'] = pd.to_datetime('2001-05-08')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-03-31'), 'Revision_Date'] = pd.to_datetime('2001-06-11')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-04-30'), 'Revision_Date'] = pd.to_datetime('2001-07-10')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-05-31'), 'Revision_Date'] = pd.to_datetime('2001-08-05')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-08-31'), 'Revision_Date'] = pd.to_datetime('2001-11-05')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-10-31'), 'Revision_Date'] = pd.to_datetime('2002-01-14')\n",
    "### Empty Dates dropping:\n",
    "df_dates_filled.loc[df_dates_filled['Release_Date'].notna() & df_dates_filled['Release_Value'].isna(), 'Release_Date'] = np.NaN\n",
    "df_dates_filled.loc[df_dates_filled['Revision_Date'].notna() & df_dates_filled['Revision_Value'].isna(), 'Revision_Date'] = np.NaN\n",
    "df_dates_filled.loc[df_dates_filled['Final_Date'].notna() & df_dates_filled['Final_Value'].isna(), 'Final_Date'] = np.NaN\n",
    "### Dataframe types control:\n",
    "print(df_dates_filled.dtypes)\n",
    "### Filled dates saving:\n",
    "df_dates_filled.to_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR FINAL VALUE WITHOUT FINAL DATE:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "df_dates_filled.loc[df_dates_filled['Final_Value'].notna() & df_dates_filled['Final_Date'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR REVISION DATE TO BE AFTER RELEASE DATE:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "df_dates_filled.loc[df_dates_filled['Revision_Date'] < df_dates_filled['Release_Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR FINAL DATE TO BE AFTER RELEASE DATE:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "df_dates_filled.loc[df_dates_filled['Final_Date'] < df_dates_filled['Release_Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR FINAL DATE TO BE AFTER REVISION DATE:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "df_dates_filled.loc[df_dates_filled['Final_Date'] < df_dates_filled['Revision_Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR RELEASE PAIR COMPLETENESS:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "print(df_dates_filled.loc[df_dates_filled['Release_Date'].isna() & df_dates_filled['Release_Value'].notna()])\n",
    "print(df_dates_filled.loc[df_dates_filled['Release_Date'].notna() & df_dates_filled['Release_Value'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR REVISION PAIR COMPLETENESS:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "print(df_dates_filled.loc[df_dates_filled['Revision_Date'].isna() & df_dates_filled['Revision_Value'].notna()])\n",
    "print(df_dates_filled.loc[df_dates_filled['Revision_Date'].notna() & df_dates_filled['Revision_Value'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR FINAL PAIR COMPLETENESS:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "print(df_dates_filled.loc[df_dates_filled['Final_Date'].isna() & df_dates_filled['Final_Value'].notna()])\n",
    "print(df_dates_filled.loc[df_dates_filled['Final_Date'].notna() & df_dates_filled['Final_Value'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: FILLED ECO INDICES DATA PLOTTING TO VISUAL CONTROL\n",
    "\n",
    "### Index choosing:\n",
    "str_test_index = 'ITCPEM Index'\n",
    "### Data loading:\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)[['Release_Value', 'Revision_Value', 'Final_Value']]\n",
    "df_flags = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags)\n",
    "### Flags printing:\n",
    "print(df_flags.loc[str_test_index, ['Country', 'Type', 'Category', 'Name', 'Units', 'Frequency']])\n",
    "### Series creating:\n",
    "df_eco_ind = df_dates_filled.loc[(str_test_index, All), All].droplevel(0)\n",
    "date_xlim_start = df_eco_ind.index.min()\n",
    "date_xlim_finish = df_eco_ind.index.max()\n",
    "#date_xlim_start = datetime(1984, 12, 31)\n",
    "#date_xlim_finish = datetime(2020, 1, 31)\n",
    "flo_ylim_min = df_eco_ind.loc[date_xlim_start : date_xlim_finish, All].min().min()\n",
    "flo_ylim_max = df_eco_ind.loc[date_xlim_start : date_xlim_finish, All].max().max()\n",
    "tup_ylim = (flo_ylim_min - abs(flo_ylim_min) / 10, flo_ylim_max + abs(flo_ylim_max) / 10)\n",
    "ax_eco_ind = df_eco_ind.plot(figsize = (20, 7.5), title = str_test_index + ' revisions', \n",
    "                style = ['c+', 'mx', 'k-'], markersize = 7.5, markeredgewidth = 1.5, \n",
    "                xlim = (date_xlim_start, date_xlim_finish), ylim = tup_ylim, x_compat = True)\n",
    "ax_eco_ind.xaxis.set_major_locator(mdates.YearLocator(base = 1))                             \n",
    "ax_eco_ind.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))                \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: VALUES STACKING\n",
    "\n",
    "### Dataframe loading:\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "### Release pair stacking:\n",
    "df_release_pair = df_dates_filled[['Release_Date', 'Release_Value']]\n",
    "df_release_pair.columns = ['Observation_Date', 'Index_Value']\n",
    "df_release_pair = df_release_pair.dropna()\n",
    "ser_release_based = df_release_pair.set_index('Observation_Date', append = True).squeeze()\n",
    "#df_release_pair['Stage'] = 'Release'\n",
    "#ser_release_based = df_release_pair.set_index(['Observation_Date', 'Stage'], append = True).squeeze()\n",
    "### First revision pair stacking:\n",
    "df_revision_pair = df_dates_filled[['Revision_Date', 'Revision_Value']]\n",
    "df_revision_pair.columns = ['Observation_Date', 'Index_Value']\n",
    "df_revision_pair = df_revision_pair.dropna()\n",
    "ser_revision_based = df_revision_pair.set_index('Observation_Date', append = True).squeeze()\n",
    "#df_revision_pair['Stage'] = 'First Revision'\n",
    "#ser_revision_based = df_revision_pair.set_index(['Observation_Date', 'Stage'], append = True).squeeze()\n",
    "### Final revision pair stacking:\n",
    "df_final_pair = df_dates_filled[['Final_Date', 'Final_Value']]\n",
    "df_final_pair.columns = ['Observation_Date', 'Index_Value']\n",
    "df_final_pair = df_final_pair.dropna()\n",
    "ser_final_based = df_final_pair.set_index('Observation_Date', append = True).squeeze()\n",
    "#df_final_pair['Stage'] = 'Final Revision'\n",
    "#ser_final_based = df_final_pair.set_index(['Observation_Date', 'Stage'], append = True).squeeze()\n",
    "### Vectors aggregating:\n",
    "ser_history_raw = ser_final_based.combine_first(ser_revision_based).combine_first(ser_release_based).dropna().sort_index()\n",
    "### Results saving:\n",
    "ser_history_raw.to_hdf(str_path_bb_idx_hdf, key = str_key_raw_history, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: HISTORY REINDEXATION TO OBSERVATION DATE BUSINESS DAILY MATRIX STRUCTURE (PRELIMINARY ACTIONS)\n",
    "\n",
    "### Data loading:\n",
    "ser_history_raw = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_history)\n",
    "### Moving all dates to tne nearest Business day:\n",
    "df_history_raw = ser_history_raw.to_frame().reset_index('Observation_Date')\n",
    "df_history_raw['Observation_Date'] = df_history_raw['Observation_Date'] + 0 * pd.offsets.BDay()\n",
    "ser_history_bday = df_history_raw.set_index('Observation_Date', append = True).squeeze()\n",
    "ser_history_bday = ser_history_bday.loc[All, All, idx_date_range]\n",
    "### Replacing dates with numbers:\n",
    "idx_observation_range = idx_date_range.copy()\n",
    "idx_observation_range.freq = None\n",
    "list_all_dates = sorted(list(set(ser_history_bday.index.get_level_values(1).unique()) | set(idx_observation_range)))\n",
    "dict_from_date = dict(zip(list_all_dates, range(len(list_all_dates))))\n",
    "dict_to_date = dict(zip(range(len(list_all_dates)), list_all_dates))\n",
    "ser_history_number = ser_history_bday.reset_index(['Data_Date', 'Observation_Date'])\\\n",
    "                                     .replace({'Data_Date': dict_from_date, 'Observation_Date': dict_from_date})\n",
    "ser_history_number[['Data_Date', 'Observation_Date']] = ser_history_number[['Data_Date', 'Observation_Date']].apply(pd.to_numeric, downcast = 'signed')\n",
    "ser_history_number = ser_history_number.set_index(['Data_Date', 'Observation_Date'], append = True).squeeze()\n",
    "### Data adding to hdf collection:\n",
    "ser_history_bday.to_hdf(str_path_bb_idx_hdf, key = str_key_bday_history, mode = 'a') ### BDay moved observation dates\n",
    "ser_history_number.to_hdf(str_path_bb_idx_hdf, key = str_key_num_history, mode = 'a') ### Data dates and BDate observation dates replaced with their numbers\n",
    "pd.Series(dict_from_date).to_hdf(str_path_bb_idx_hdf, key = str_key_from_date, mode = 'a') ### Date to number convertation dictionary\n",
    "pd.Series(dict_to_date).to_hdf(str_path_bb_idx_hdf, key = str_key_to_date, mode = 'a') ### Number to date convertation dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING TICKER SEASONALITY ADJUSTMENT FUNCTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def seasonality_adjustment(ser_vector, str_way = 'month_scale'):\n",
    "    if (str_way == 'month_scale'):\n",
    "        ### Export adjustment parameters (for factor source tuning):\n",
    "        int_season_adj_ma = 12 ### Moving average length for seasonal adjustment, months\n",
    "        int_season_adj_shift = ceil(int_season_adj_ma / 2) ### Moving average shift for seasonal adjustment, months\n",
    "        ### Input vector adjusting:\n",
    "        ser_vector.name = 'Input'        \n",
    "        ### Year length moving average calculating:\n",
    "        ser_ma_centered = ser_vector.rolling(window = int_season_adj_ma, min_periods = int_season_adj_shift).mean()\n",
    "        ser_ma_test = pd.Series(ser_ma_centered.values, ser_ma_centered.index)\n",
    "        ### Year length moving average shifting:\n",
    "        ser_ma_centered = ser_ma_centered.shift(-int_season_adj_shift)\n",
    "        ser_ma_centered.name = 'MA Centered'\n",
    "        ### Vectors concatenation:\n",
    "        df_season_adjustment = pd.concat([ser_vector, ser_ma_centered], axis = 1)\n",
    "        ### Input / MA Ratio calculating:\n",
    "        df_season_adjustment['Ratio'] = df_season_adjustment['Input'] / df_season_adjustment['MA Centered']\n",
    "        ### Adding month number to index:\n",
    "        df_season_adjustment['Month'] = df_season_adjustment.index.month\n",
    "        df_season_adjustment.set_index(['Month'], append = True, inplace = True)\n",
    "        df_season_test = pd.DataFrame(df_season_adjustment.values, df_season_adjustment.index, df_season_adjustment.columns)\n",
    "        ### Month number Ratio median calculating as Scale:\n",
    "        ser_ratio_median = df_season_adjustment['Ratio'].groupby('Month').median()\n",
    "        ser_ratio_median.name = 'Scale'\n",
    "        ### Scale modification:\n",
    "        ser_ratio_median = ser_ratio_median / ser_ratio_median.mean()\n",
    "        ### Adding Scale to data table:\n",
    "        df_season_adjustment = df_season_adjustment.join(ser_ratio_median, on = ['Month'], how = 'left', sort = True).reset_index('Month', drop = True)\n",
    "        ### Seasonally adjusted Export calculating:\n",
    "        ser_vector_adjusted = (df_season_adjustment['Input'] / df_season_adjustment['Scale']).sort_index()    \n",
    "        ser_vector_adjusted = ser_vector_adjusted\n",
    "    ### Results output:\n",
    "    return ser_vector_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "u = [-1, 2, 3, -2, 0, 1, 2];\n",
    "v = [2, 4, -1, 1];\n",
    "print(np.convolve(u, v, 'full'))\n",
    "np.convolve(u, v, 'full')[floor(len(v) / 2) : floor(len(v) / 2) + len(u)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNDER CONSTRUCTION: SEASONALITY ADJUSTMENT RESEARCH\n",
    "\n",
    "ser_history_bday = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_bday_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNDER CONSTRUCTION: SEASONALITY ADJUSTMENT RESEARCH\n",
    "\n",
    "str_date_test = '2020-08-31' # '2014-12-31' # \n",
    "str_ticker_test = 'UKRPMOM Index' # 'ITCPEM Index'\n",
    "ser_test_full = ser_history_bday.loc[str_ticker_test, All].droplevel('Index_Name')\n",
    "ser_test_full = ser_test_full.unstack('Data_Date').reindex(idx_date_range).stack('Data_Date', dropna = False).squeeze()\n",
    "ser_test_full = ser_test_full.swaplevel()\n",
    "ser_test_full.index.rename('Observation_Date', level = -1, inplace = True)    \n",
    "ser_test_full = ser_test_full.groupby('Data_Date').ffill().sort_index()\n",
    "\n",
    "ser_test_obs = ser_test_full.loc[: str_date_test, str_date_test].droplevel('Observation_Date')\n",
    "\n",
    "int_correction = 100\n",
    "\n",
    "#ser_test_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\x13.py:187: X13Warning: WARNING: At least one visually significant seasonal peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\statsmodels\\tsa\\x13.py:187: X13Warning: WARNING: At least one visually significant seasonal peak has been found\n",
      "          in the estimated spectrum of the regARIMA residuals.\n",
      "  \n",
      " WARNING: At least one visually significant seasonal peak has been\n",
      "          found in one or more of the estimated spectra.\n",
      "  warn(errors, X13Warning)\n"
     ]
    }
   ],
   "source": [
    "### UNDER CONSTRUCTION: SEASONALITY ADJUSTMENT RESEARCH\n",
    "\n",
    "def sa_time_test(ser_test_obs, algo = 'x13', int_correction = 100):\n",
    "    date_obs_date = ser_test_obs.index.get_level_values('Observation_Date')[0]\n",
    "    ser_test_obs = ser_test_obs.droplevel('Observation_Date').loc[: date_obs_date].dropna()\n",
    "    if ((len(ser_test_obs.index) > 40) & (date_obs_date == date_obs_date + pd.tseries.offsets.BMonthEnd(0))):\n",
    "        ser_test_obs.name = 'Test'\n",
    "        if (algo == 'x13'):\n",
    "            ser_test_sa = x13_arima_analysis(ser_test_obs + int_correction, outlier = False).seasadj\n",
    "        else:\n",
    "            ser_test_sa = seasonality_adjustment(ser_test_obs + int_correction)\n",
    "        return ser_test_sa\n",
    "#sa_time_test(ser_test_full.loc[: str_date_test, str_date_test], algo = 'x13')\n",
    "ser_test_res = ser_test_full.groupby('Observation_Date').apply(sa_time_test, algo = 'x13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Observation_Date\n",
       "1984-12-31                                                 None\n",
       "1985-01-01                                                 None\n",
       "1985-01-02                                                 None\n",
       "1985-01-03                                                 None\n",
       "1985-01-04                                                 None\n",
       "                                    ...                        \n",
       "2020-08-25                                                 None\n",
       "2020-08-26                                                 None\n",
       "2020-08-27                                                 None\n",
       "2020-08-28                                                 None\n",
       "2020-08-31    Data_Date\n",
       "1984-12-31    100.145584\n",
       "1985-01-31 ...\n",
       "Freq: B, Length: 9306, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### UNDER CONSTRUCTION: SEASONALITY ADJUSTMENT RESEARCH\n",
    "\n",
    "ser_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNDER CONSTRUCTION: SEASONALITY ADJUSTMENT RESEARCH\n",
    "\n",
    "#%env\n",
    "#%env X13PATH = C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
    "#%env X12PATH = C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNDER CONSTRUCTION: SEASONALITY ADJUSTMENT RESEARCH\n",
    "\n",
    "#'[' + '; '.join(map(str, ser_test_obs.values + int_correction)) + ']'\n",
    "'{' + '; '.join(map(lambda iter_date: '\\'' + iter_date + '\\'', map(str, ser_test_obs.index.date))) + '}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNDER CONSTRUCTION: SEASONALITY ADJUSTMENT RESEARCH\n",
    "\n",
    "### Implemented internal adjustment method:\n",
    "ser_test_sa = seasonality_adjustment(ser_test_obs + int_correction)\n",
    "pd.concat([ser_test_obs + int_correction, ser_test_sa], axis = 1).to_excel('Data_Files/Test_Files/Seasonality_adjustment_test.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNDER CONSTRUCTION: SEASONALITY ADJUSTMENT RESEARCH\n",
    "\n",
    "### X13 ARIMA SEATS (CENSUS)\n",
    "ser_test_sa = x13_arima_analysis(ser_test_obs.dropna() + int_correction, prefer_x13 = False, outlier = True).seasadj\n",
    "pd.concat([ser_test_obs + int_correction, ser_test_sa], axis = 1).to_excel('Data_Files/Test_Files/Seasonality_adjustment_test.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: HISTORY REINDEXATION TO OBSERVATION DATE BUSINESS DAILY MATRIX STRUCTURE (REINDEXATION, FLAGS IMPLEMENTATION, WINSORIZATION) - FINAL\n",
    "\n",
    "### Reindexation and diagonalization function:\n",
    "def complex_transform(ser_name, idx_number_range, df_flags, int_max_name_length):\n",
    "    ### Local constants definition:\n",
    "    dict_interval = {}\n",
    "    dict_interval['Q'] = 4\n",
    "    dict_interval['M'] = 12\n",
    "    dict_interval['W'] = 52    \n",
    "    ### Diagonal clearing subfunction:\n",
    "    def diagonal_filter(ser_date):\n",
    "        num_date_diag = ser_date.index.get_level_values(0)[0]\n",
    "        ser_result = ser_date.droplevel(0)\n",
    "        ser_result = ser_result[ser_result.index >= num_date_diag]\n",
    "        return ser_result\n",
    "    def by_date_winsorization(ser_date, int_winsorize_bound, flo_winsorize_tolerance, int_winsorize_steps_limit):\n",
    "        if (ser_date.std() > flo_winsorize_tolerance):\n",
    "            ### Calculating of z scores:\n",
    "            ser_date = (ser_date - ser_date.mean()) / ser_date.std()        \n",
    "            bool_to_winsor = True   \n",
    "            int_iter = 1\n",
    "            while (bool_to_winsor): \n",
    "                int_iter += 1                \n",
    "                ### Value based winsorization:                \n",
    "                ser_date.clip(lower = -int_winsorize_bound, upper = int_winsorize_bound, inplace = True)\n",
    "                ### Recalculating of z scores:\n",
    "                ser_date = (ser_date - ser_date.mean()) / ser_date.std()\n",
    "                ### Checking for boundaries and steps:\n",
    "                if((ser_date.loc[ser_date.abs() >= (int_winsorize_bound + flo_winsorize_tolerance)].count() == 0) | (int_iter > int_winsorize_steps_limit)):\n",
    "                    bool_to_winsor = False\n",
    "        else:\n",
    "            ### Constant values demeaning:\n",
    "            ser_date = ser_date - ser_date.mean()\n",
    "        ### Memory optimization:\n",
    "        ser_date = ser_date.astype('float32')\n",
    "        return ser_date\n",
    "    print(ser_name.index.get_level_values(0)[0], ': Reindexation')\n",
    "    ### Reindexation:\n",
    "    str_index_name = ser_name.index.get_level_values(0)[0]\n",
    "    ser_full = ser_name.droplevel(0).unstack('Data_Date').reindex(idx_number_range).stack('Data_Date', dropna = False).squeeze()\n",
    "    ser_full = ser_full.swaplevel()\n",
    "    ser_full.index.rename('Observation_Date', level = -1, inplace = True)    \n",
    "    ### Forward filling for each data date:\n",
    "    ser_full = ser_full.groupby('Data_Date').ffill()    \n",
    "    ### Diagonalization:\n",
    "    ser_triangle = ser_full.groupby('Data_Date').apply(diagonal_filter).sort_index()\n",
    "    ### Flags extracting:\n",
    "    ser_flags = df_flags.loc[str_index_name, All].squeeze()\n",
    "    ### 'TAR' type checking:\n",
    "    if (ser_flags['Type'] == 'TAR'):\n",
    "        print(ser_name.index.get_level_values(0)[0], ': TAR Type ignoring')        \n",
    "        pass\n",
    "    ### Flags-based transforming:\n",
    "    else:\n",
    "        ### Eco Index demeaning:\n",
    "        if ser_flags['Demean']:\n",
    "            print(ser_name.index.get_level_values(0)[0], ': Demean flag implementing')\n",
    "            ser_triangle = ser_triangle - 50\n",
    "        ### Eco Index rebasing for Index-based Eco Indices:\n",
    "        if ser_flags['Index']:\n",
    "            print(ser_name.index.get_level_values(0)[0], ': Index flag implementing')            \n",
    "            ### For relative index:            \n",
    "            if ser_flags['YoY']:\n",
    "                int_interval = dict_interval[ser_flags['Frequency']]\n",
    "                for int_shift in range(int_interval):\n",
    "                    idx_interval = ser_triangle.index.levels[0][int_shift :: int_interval]\n",
    "                    date_interval_start = ser_triangle.index.levels[0][int_shift]\n",
    "                    ser_triangle.loc[idx_interval, All] = ser_triangle.loc[idx_interval, All] / 100.0 + 1\n",
    "                    ser_triangle.loc[date_interval_start, All] = ser_triangle.loc[date_interval_start, All] * 100.0\n",
    "                    idx_isna = ser_triangle.loc[idx_interval, All].loc[ser_triangle.loc[idx_interval, All].isna() == True].index\n",
    "                    ser_triangle.loc[idx_isna] = 1\n",
    "                    ser_triangle.loc[idx_interval, All] = ser_diag.loc[idx_interval, All].groupby('Observation_Date').cumprod()\n",
    "                    ser_triangle.loc[idx_isna] = np.NaN                    \n",
    "            ### For through index:    \n",
    "            else:\n",
    "                ser_triangle = ser_triangle / 100.0 + 1\n",
    "                ser_triangle.loc[ser_triangle.index.levels[0][0], All] = ser_triangle.loc[ser_triangle.index.levels[0][0], All] * 100.0\n",
    "                idx_isna = ser_triangle.isna()\n",
    "                ser_triangle.loc[idx_isna] = 1\n",
    "                ser_triangle = ser_triangle.groupby('Observation_Date').cumprod()\n",
    "                ser_triangle.loc[idx_isna] = np.NaN\n",
    "        ### Eco Index rebasing for Sum-based Eco Indices:\n",
    "        if ser_flags['Sum']:\n",
    "            print(ser_name.index.get_level_values(0)[0], ': Sum flag implementing')             \n",
    "            ### For relative index:            \n",
    "            if ser_flags['YoY']:\n",
    "                int_interval = dict_interval[ser_flags['Frequency']]\n",
    "                for int_shift in range(int_interval):\n",
    "                    idx_interval = ser_triangle.index.levels[0][int_shift :: int_interval]\n",
    "                    idx_isna = ser_triangle.loc[idx_interval, All].loc[ser_triangle.loc[idx_interval, All].isna() == True].index\n",
    "                    ser_triangle.loc[idx_isna] = 0\n",
    "                    ser_triangle.loc[idx_interval, All] = ser_triangle.loc[idx_interval, All].groupby('Observation_Date').cumsum()\n",
    "                    ser_triangle.loc[idx_isna] = np.NaN                   \n",
    "            ### For through index:    \n",
    "            else:\n",
    "                idx_isna = ser_triangle.isna()\n",
    "                ser_triangle.loc[idx_isna] = 0\n",
    "                ser_triangle = ser_triangle.groupby('Observation_Date').cumsum()\n",
    "                ser_triangle.loc[idx_isna] = np.NaN\n",
    "        ### Eco Index rebasing for Adjustment-based Eco Indices:\n",
    "        if ser_flags['Adjustment']:    \n",
    "            print(ser_name.index.get_level_values(0)[0], ': Adjustment flag implementing')    \n",
    "            int_adjustment = ser_flags['Adjustment'] // 3\n",
    "            ser_triangle = ser_triangle.groupby('Observation_Date').diff(int_adjustment)       \n",
    "        ### Eco Index rebasing for log-based Eco Indices:\n",
    "        if ser_flags['Log']:        \n",
    "            print(ser_name.index.get_level_values(0)[0], ': Log flag implementing')\n",
    "            ### For relative index:            \n",
    "            if ser_flags['YoY']:            \n",
    "                pass\n",
    "            ### For through index:    \n",
    "            else:\n",
    "                if (len(ser_triangle[ser_triangle <= 0].index) > 0):\n",
    "                    ser_triangle = ser_triangle + 2 * abs(ser_triangle.min())\n",
    "                ser_triangle = np.log(ser_triangle)\n",
    "        ### Eco Index rebasing for Negative-based Eco Indices:\n",
    "        if ser_flags['Negative']:       \n",
    "            print(ser_name.index.get_level_values(0)[0], ': Negative flag implementing')            \n",
    "            ser_triangle = -ser_triangle\n",
    "#    ### Back filling across observation date vectors:\n",
    "#    print(ser_name.index.get_level_values(0)[0], ': Back filling across the observation dates')    \n",
    "#    ser_triangle = ser_triangle.groupby('Observation_Date').bfill()        \n",
    "#    ### Forward filling across observation date vectors:\n",
    "#    print(ser_name.index.get_level_values(0)[0], ': Forward filling across the observation dates')    \n",
    "#    ser_triangle = ser_triangle.groupby('Observation_Date').ffill()\n",
    "    ### Adding to flagged matrix cube:\n",
    "    pd.concat([ser_triangle], keys = [str_index_name], names = ['Index_Name']).to_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_flagged, format = 'table', \n",
    "                                                                                      complevel = 9, append = True, mode = 'a', \n",
    "                                                                                      min_itemsize = {'Index_Name': int_max_name_length})\n",
    "    ### Winsorizing across observation date vectors:\n",
    "    print(ser_name.index.get_level_values(0)[0], ': Winsorization across the observation dates')\n",
    "    ser_triangle = ser_triangle.groupby('Observation_Date').apply(by_date_winsorization, int_winsorize_bound, flo_winsorize_tolerance, int_winsorize_steps_limit)\n",
    "    ### Adding to winsorized matrix cube:\n",
    "    pd.concat([ser_triangle], keys = [str_index_name], names = ['Index_Name']).to_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_winsorized, format = 'table',\n",
    "                                                                                      complevel = 9, append = True, mode = 'a',\n",
    "                                                                                      min_itemsize = {'Index_Name': int_max_name_length})\n",
    "#    return ser_triangle\n",
    "\n",
    "### Flags loading:\n",
    "ser_history_number = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_num_history)\n",
    "df_flags = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags)\n",
    "ser_from_date = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_from_date)\n",
    "### Converting Data Date to Data Date number:\n",
    "idx_number_range = list(pd.Series(idx_date_range).replace(ser_from_date).values)\n",
    "### Maximum length calculating (for HDF manipulations):\n",
    "int_max_name_length = max(ser_history_number.index.levels[0].str.len())\n",
    "### Reindexation implementing:\n",
    "ser_history_number.groupby('Index_Name').apply(complex_transform, idx_number_range, df_flags, int_max_name_length)\n",
    "#list_flag_test = ['BAKETOT Index', 'BRCPALLY Index', 'CHPMINDX Index', 'CPI CHNG Index', 'GDP CQOQ Index', 'UKEGESTG Index', 'USURTOT Index']\n",
    "#ser_history_number.loc[ser_history_number.index.levels[0][50 * 4 : 50 * 6], All, All].groupby('Index_Name')\\\n",
    "#                                                                                     .apply(complex_transform, idx_number_range, df_flags, int_max_name_length)\n",
    "#ser_test_flagged = ser_history_number.loc[list_flag_test, All, All].groupby('Index_Name').apply(complex_transform, idx_number_range, df_flags, int_max_name_length)\n",
    "#ser_history_number.loc[list_flag_test, All, All].groupby('Index_Name').apply(complex_transform, idx_number_range, df_flags, int_max_name_length)\n",
    "#ser_test_flagged = ser_history_number.loc[['USURTOT Index'], All, All].groupby('Index_Name').apply(complex_transform, idx_number_range, df_flags, int_max_name_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "### Mappings extracting:\n",
    "ser_from_date = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_from_date, mode = 'a')\n",
    "ser_to_date = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_to_date, mode = 'a')\n",
    "### Cube filtering:\n",
    "str_test_key = 'USURTOT Index'\n",
    "ser_test_flagged_full = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_flagged, where = '(Index_Name == str_test_key)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_test_flagged.loc[ser_test_flagged.index.get_level_values('Data_Date') == ser_test_flagged.index.get_level_values('Observation_Date')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNDER CONSTRUCTION: DATA EXTRACTING\n",
    "\n",
    "### Mappings extracting:\n",
    "ser_from_date = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_from_date, mode = 'a')\n",
    "ser_to_date = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_to_date, mode = 'a')\n",
    "### Cube filtering:\n",
    "str_test_key = 'USURTOT Index'\n",
    "str_test_date = '2020-06-10'\n",
    "str_test_idx = ser_from_date[str_test_date]\n",
    "ser_test_flagged = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_winsorized, where = '(Index_Name == str_test_key) & (Observation_Date == str_test_idx)')\n",
    "### Index levels mapping to dates:\n",
    "ser_test_recovered = ser_test_flagged.copy()\n",
    "ser_test_recovered.index.set_levels(pd.to_datetime(ser_test_recovered.index.levels[1].map(ser_to_date)), level = 1, inplace = True)\n",
    "ser_test_recovered.index.set_levels(pd.to_datetime(ser_test_recovered.index.levels[2].map(ser_to_date)), level = 2, inplace = True)\n",
    "### Example:\n",
    "#ser_test_recovered.droplevel(0).unstack('Observation_Date').to_excel('Data_Files/Test_Files/Flags_implemented.xlsx', merge_cells = False)\n",
    "ser_test_recovered#.droplevel(0)#.loc[All, All]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: EXTERNAL HDF GENERATING\n",
    "\n",
    "### Local parameters:\n",
    "str_path_bb_idx_export = 'Data_Files/Source_Files/Bloomberg_Eco_Indices_for_Acadian.h5'\n",
    "str_matrix_export_key = 'matrix_export'\n",
    "str_flags_export_key = 'flags_export'\n",
    "\n",
    "ser_history_raw = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_raw_history)\n",
    "df_flags_short = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags).drop(['Adjustment', 'Demean', 'Sum', 'Negative', 'Index', 'YoY', 'Log',], axis = 1)\n",
    "\n",
    "ser_history_raw.to_hdf(str_path_bb_idx_export, key = str_matrix_export_key, mode = 'w')\n",
    "df_flags_short.to_hdf(str_path_bb_idx_export, key = str_flags_export_key, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
