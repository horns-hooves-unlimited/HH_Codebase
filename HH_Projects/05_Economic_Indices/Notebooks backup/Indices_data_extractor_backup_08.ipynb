{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: ECONOMIC INDICES RELEASES HISTORY EXTRACTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "from math import floor, ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.x13 import x13_arima_analysis\n",
    "import os\n",
    "\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: X13PATH=C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
      "env: X12PATH=C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n"
     ]
    }
   ],
   "source": [
    "### RUN EVERY TIME: SEASONAL ADJUSTMENT MODULE PATHS SET UP\n",
    "\n",
    "%env X13PATH = C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
    "%env X12PATH = C:\\Users\\ighar\\AppData\\Roaming\\jupyter\\x13as\n",
    "#%env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  0.25.3\n",
      "python version:  3.7.4\n"
     ]
    }
   ],
   "source": [
    "## VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN EVERY TIME: GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable']\n",
    "### Raw data path and sheets:\n",
    "str_path_bb_idx_source = 'Data_Files/Source_Files/Bloomberg_Eco_Indices.xlsx'\n",
    "str_us_sheet = 'US Eco Const'\n",
    "str_all_sheet = 'All Eco Const'\n",
    "### Flags data path and sheets:\n",
    "str_path_bb_idx_flags = 'Data_Files/Source_Files/Bloomberg_Eco_Flags_Extended.xlsx'\n",
    "str_flag_sheet = 'Bloomberg Description'\n",
    "### Source data constants:\n",
    "int_idx_cols = 12\n",
    "### HDF file with converted source data:\n",
    "str_path_bb_idx_hdf = 'Data_Files/Source_Files/Bloomberg_Eco_Indices.h5'\n",
    "str_key_flags = 'flags_exported' ### Acadian flags list\n",
    "str_key_exported = 'all_idx_exported' ### Raw export with only replacing zero dates and after 2021-01-01 dates with np.NaN\n",
    "str_key_raw_filled = 'all_idx_raw_filled' ### Raw export with initial dates, dates gaps, absent date coluns filled\n",
    "str_key_raw_history = 'raw_history' ### Export with all the corrections and fillings (restructured to [Index_Name -> Data_Date -> Observation_Date] | Index_Value series)\n",
    "str_key_bday_history = 'bday_history' ### Raw history vector with observation dates moved to nearest future business dates\n",
    "str_key_num_history = 'num_history' ### Bday history vector with observation dates changed to their date numbers (for future matrix cube saving as hdf file)\n",
    "str_key_from_date = 'idx_from_date' ### Series to get date numbers from dates\n",
    "str_key_to_date = 'idx_to_date' ### Series to get dates from date numbers\n",
    "### HDF file with matrices:\n",
    "str_path_bb_matrix_hdf = 'Data_Files/Source_Files/Matrix_Eco_Indices.h5'\n",
    "str_key_matrix_z = 'matrix_cube_z_scored'\n",
    "### HDF file with diagonals:\n",
    "str_path_bb_diag_hdf = 'Data_Files/Source_Files/Diag_Eco_Indices.h5'\n",
    "str_key_diag_daily_raw = 'matrix_diagonal_raw'\n",
    "str_key_diag_daily_z = 'matrix_diagonal_z'\n",
    "### Observation axis range:\n",
    "datetime_start = datetime(1984, 12, 31) # Start date for efficacy measures\n",
    "date_start = datetime_start.date()\n",
    "datetime_end = datetime(2020, 8, 31) # End date for efficacy measures\n",
    "date_end = datetime_end.date()\n",
    "idx_date_range = pd.date_range(date_start, date_end, freq = 'B')\n",
    "datetime_basis = datetime(1993, 12, 31) # End date for efficacy measures\n",
    "date_basis = datetime_basis.date()\n",
    "### Gaps filling options:\n",
    "int_revision_shift = 1\n",
    "int_final_shift = 2\n",
    "int_first_mean_length = 12\n",
    "dict_final_only_lag = {}\n",
    "dict_final_only_lag['Quarterly'] = 90 // 2\n",
    "dict_final_only_lag['Monthly'] = 30 // 2\n",
    "dict_final_only_lag['Other'] = 7 // 2\n",
    "### Cumprod shifts for monthly data frequency:\n",
    "dict_cumprod_step = {}\n",
    "dict_cumprod_step['MoM%'] = 1\n",
    "dict_cumprod_step['QoQ%'] = 3\n",
    "dict_cumprod_step['YoY%'] = 12\n",
    "### Stock-like series shifts for YoY transformation:\n",
    "dict_yoy_shift = {}\n",
    "dict_yoy_shift['Monthly'] = 12\n",
    "dict_yoy_shift['Quarterly'] = 4\n",
    "dict_yoy_shift['Other'] = 52\n",
    "### Group tickers rebasing options:\n",
    "int_not_to_rebase_term = 7 ### Term in years for min ticker data date when we do not need to rebase it with basis group ticker\n",
    "int_not_to_rebase_diff = 2 ### Minimal difference in years between basis ticker and other group ticker min date when we need to rebase group ticker\n",
    "### Z-scoring options:\n",
    "int_winsorize_bound = 4\n",
    "flo_winsorize_tolerance = 0.0001\n",
    "int_winsorize_steps_limit = 5\n",
    "### Diagonal options:\n",
    "int_min_years = 3\n",
    "date_diag_start = datetime(1994, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: SOURCE FILE EXPORT\n",
    "\n",
    "### Reading excel file:\n",
    "df_all_idx_raw = pd.read_excel(io = str_path_bb_idx_source, sheet_name = str_all_sheet, skiprows = [0], index_col = None, header = None, parse_dates = True,\n",
    "                               na_values = list_na_excel_values, keep_default_na = False)\n",
    "### List of dataframes for each eco index initializing:\n",
    "list_idx_raw = []\n",
    "### Extracting and converting each eco index data block seperately to proper form:\n",
    "for int_iter_idx_col in range(len(df_all_idx_raw.columns) // int_idx_cols):\n",
    "    ### Extracting raw data:\n",
    "    df_iter_idx_raw = df_all_idx_raw.iloc[All, (int_iter_idx_col * int_idx_cols) : ((int_iter_idx_col + 1) * int_idx_cols) - 1]\n",
    "    ### Assigning colum names:\n",
    "    df_iter_idx_raw.columns = df_iter_idx_raw.iloc[1]\n",
    "    df_iter_idx_raw.columns.name = ''    \n",
    "    ### Dropping empty rows:\n",
    "    df_iter_idx_raw.dropna(how = 'all', inplace = True)\n",
    "    ### Extracting eco index name:\n",
    "    df_iter_idx_raw['Index_Name'] = df_iter_idx_raw.iloc[0, 0]\n",
    "    ### Dropping identification rows (no longer needed):\n",
    "    df_iter_idx_raw.drop([0, 1], axis = 0, inplace = True)\n",
    "    ### Setting multiindex:\n",
    "    df_iter_idx_raw.set_index(['Date', 'Index_Name'], drop = True, append = False, inplace = True)\n",
    "    ### ADding dataframe to list:\n",
    "    list_idx_raw.append(df_iter_idx_raw)\n",
    "### Exported data aggregating:\n",
    "df_all_idx_exported = pd.concat(list_idx_raw, axis = 0).swaplevel()\n",
    "### Exported data saving:\n",
    "df_all_idx_exported.index.names = ['Index_Name', 'Data_Date']\n",
    "### Types converting for numerical data:\n",
    "df_all_idx_exported = df_all_idx_exported.astype({'PX_LAST': 'float',\n",
    "                                                  'ACTUAL_RELEASE': 'float',\n",
    "                                                  'FIRST_REVISION': 'float',\n",
    "                                                  'BN_SURVEY_NUMBER_OBSERVATIONS': 'float',\n",
    "                                                  'BN_SURVEY_MEDIAN': 'float',\n",
    "                                                  'BN_SURVEY_AVERAGE': 'float',\n",
    "                                                  'FORECAST_STANDARD_DEVIATION': 'float',\n",
    "                                                 })\n",
    "### Zero dates dropping:\n",
    "df_all_idx_exported.loc[df_all_idx_exported['ECO_RELEASE_DT'] == 0, ['ECO_RELEASE_DT']] = np.NaN\n",
    "df_all_idx_exported.loc[df_all_idx_exported['FIRST_REVISION_DATE'] == 0, ['FIRST_REVISION_DATE']] = np.NaN\n",
    "df_all_idx_exported.loc[df_all_idx_exported['ECO_FUTURE_RELEASE_DATE'] == 0, ['ECO_FUTURE_RELEASE_DATE']] = np.NaN\n",
    "### Incorrect dates droppping:\n",
    "df_all_idx_exported.loc[df_all_idx_exported['ECO_RELEASE_DT'] > 20210000, 'ECO_RELEASE_DT'] = np.NaN\n",
    "df_all_idx_exported.loc[df_all_idx_exported['FIRST_REVISION_DATE'] > 20210000, 'FIRST_REVISION_DATE'] = np.NaN\n",
    "df_all_idx_exported.loc[df_all_idx_exported['ECO_FUTURE_RELEASE_DATE'] > 20210000, 'ECO_FUTURE_RELEASE_DATE'] = np.NaN\n",
    "### Dates formatting:\n",
    "df_all_idx_exported['ECO_RELEASE_DT'] = pd.to_datetime(df_all_idx_exported['ECO_RELEASE_DT'].astype(str), format = '%Y%m%d')\n",
    "df_all_idx_exported['FIRST_REVISION_DATE'] = pd.to_datetime(df_all_idx_exported['FIRST_REVISION_DATE'].astype(str), format = '%Y%m%d')\n",
    "df_all_idx_exported['ECO_FUTURE_RELEASE_DATE'] = pd.to_datetime(df_all_idx_exported['ECO_FUTURE_RELEASE_DATE'].astype(str), format = '%Y%m%d')\n",
    "### Exported dataframe saving:\n",
    "df_all_idx_exported.to_hdf(str_path_bb_idx_hdf, key = str_key_exported, mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: FLAGS FILE EXPORT\n",
    "\n",
    "### Reading excel file:\n",
    "df_flags = pd.read_excel(io = str_path_bb_idx_flags, sheet_name = str_flag_sheet, usecols = list(range(38))[1: ], index_col = 0, skiprows = list(range(6)), header = 0,\n",
    "                               parse_dates = True, na_values = list_na_excel_values, keep_default_na = False)\n",
    "### Dataframe transforming:\n",
    "df_flags.dropna(how = 'all', inplace = True)\n",
    "df_flags.index.name = 'Index_Name'\n",
    "### Choosing columns to go on with:\n",
    "df_flags = \\\n",
    "        df_flags[['CTRY', 'Type', 'Category', 'Negative', 'Security Description', 'SA/NSA', 'Processing', 'Index: Base', 'Type 2', 'INDX_SOURCE', 'DES_NOTES', 'INDX_FREQ']]\n",
    "### Columns renaming:\n",
    "df_flags.columns = ['Region', 'Type_Prime', 'Category', 'Negative', 'Description', 'SA_Status', 'Processing', 'Base', 'Type_Second', 'Data_Source', 'Notes', 'Frequency']\n",
    "### Seasonality Adjustment Flag filling:\n",
    "df_flags['SA_Status'] = df_flags['SA_Status'].fillna('Unclear')\n",
    "### Processing Index separating:\n",
    "df_flags['Processing'] = df_flags['Processing'].str.partition(': ')[0]\n",
    "### Base filling:\n",
    "df_flags['Base'] = df_flags['Base'].fillna(-1)\n",
    "### Euro countries region renaming:\n",
    "df_flags.loc[~df_flags['Region'].isin(['Japan', 'UK', 'US']), 'Region'] = 'Europe'\n",
    "### Columns reordering:\n",
    "df_flags = df_flags[['Type_Prime', 'Type_Second', 'Category', 'Region', 'Processing', 'Base', 'SA_Status', 'Frequency', 'Negative', 'Description', 'Data_Source', 'Notes']]\n",
    "### Types converting:\n",
    "df_flags = df_flags.astype({'Type_Prime': 'str',\n",
    "                            'Type_Second': 'str',\n",
    "                            'Category': 'str',\n",
    "                            'Region': 'str',\n",
    "                            'Processing': 'str',\n",
    "                            'Base': 'int',\n",
    "                            'SA_Status': 'str',\n",
    "                            'Frequency': 'str',\n",
    "                            'Negative': 'int',\n",
    "                            'Description': 'str',\n",
    "                            'Data_Source': 'str',\n",
    "                            'Notes': 'str'})\n",
    "### Exported dataframe saving:\n",
    "df_flags.to_hdf(str_path_bb_idx_hdf, key = str_key_flags, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: RELEASES AND REVISIONS DATA EXTRACTING\n",
    "\n",
    "### Revisions data reading (99,180 rows):\n",
    "df_all_idx_revisions = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_exported)[\n",
    "                                   ['PX_LAST', 'ECO_RELEASE_DT', 'ACTUAL_RELEASE', 'FIRST_REVISION_DATE', 'FIRST_REVISION', 'ECO_FUTURE_RELEASE_DATE']]\n",
    "### Dropping rows with all Index Values missed (96,369 rows left):\n",
    "#df_all_idx_revisions = \\\n",
    "#        df_all_idx_revisions.loc[df_all_idx_revisions['PX_LAST'].notna() | df_all_idx_revisions['ACTUAL_RELEASE'].notna() | df_all_idx_revisions['FIRST_REVISION'].notna()]\n",
    "### Date filtering:\n",
    "df_all_idx_revisions = df_all_idx_revisions.reset_index('Data_Date')\n",
    "df_all_idx_revisions = df_all_idx_revisions.loc[df_all_idx_revisions['Data_Date'] >= datetime_start].set_index('Data_Date', append = True)\n",
    "### Columns renaming:\n",
    "df_all_idx_revisions.columns = ['Final_Value', 'Release_Date', 'Release_Value', 'Revision_Date', 'Revision_Value', 'Future_Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: DATA REPAIRING\n",
    "\n",
    "### 1 : MPMIITMA Index Revision_Value mistake correction:\n",
    "df_all_idx_revisions.loc[('MPMIITMA Index', All), 'Revision_Value'] = np.NaN\n",
    "### 2 : JNCPT Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNCPT Index', '1999-07-31'), 'Release_Date'] = pd.to_datetime('1999-07-30')\n",
    "df_all_idx_revisions.loc[('JNCPT Index', '2001-01-31'), 'Release_Date'] = pd.to_datetime('2001-01-26')\n",
    "df_all_idx_revisions.loc[('JNCPT Index', '2001-02-28'), 'Release_Date'] = pd.to_datetime('2001-03-02')\n",
    "df_all_idx_revisions.loc[('JNCPT Index', '2001-06-30'), 'Release_Date'] = pd.to_datetime('2001-06-29')\n",
    "### 3 : JNCSTOTY Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNCSTOTY Index', '2018-12-31'), 'Release_Date'] = pd.to_datetime('2019-01-31')\n",
    "### 4: JNPIY Index Revision_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNPIY Index', '2015-12-31'), 'Revision_Date'] = pd.to_datetime('2016-02-23')\n",
    "### 5: JNPIY Index Revision_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNTIAIAM Index', '2017-11-30'), 'Revision_Date'] = pd.to_datetime('2018-02-20')\n",
    "### 6: BECPCHNG Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('BECPCHNG Index', '1997-01-31'), 'Release_Date'] = pd.to_datetime('1997-01-30')\n",
    "df_all_idx_revisions.loc[('BECPCHNG Index', '2001-10-31'), 'Release_Date'] = pd.to_datetime('2001-10-28')\n",
    "df_all_idx_revisions.loc[('BECPCHNG Index', '2001-11-30'), 'Release_Date'] = pd.to_datetime('2001-11-29')\n",
    "df_all_idx_revisions.loc[('BECPCHNG Index', '2001-12-31'), 'Release_Date'] = pd.to_datetime('2001-12-23')\n",
    "### 7 : BEUER Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('BEUER Index', '1997-01-31'), 'Release_Date'] = pd.to_datetime('1997-02-06')\n",
    "### 8 : FRCPEECM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('FRCPEECM Index', '1997-07-31'), 'Release_Date'] = pd.to_datetime('1997-08-14')\n",
    "### 9 : FRCPIMOM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('FRCPIMOM Index', '1999-01-31'), 'Release_Date'] = pd.to_datetime('1999-02-24')\n",
    "df_all_idx_revisions.loc[('FRCPIMOM Index', '1999-02-28'), 'Release_Date'] = pd.to_datetime('1999-03-24')\n",
    "df_all_idx_revisions.loc[('FRCPIMOM Index', '2006-01-31'), 'Release_Date'] = pd.to_datetime('2006-02-21')\n",
    "### 10 : GKCPNEWY Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-03-31'), 'Release_Date'] = pd.to_datetime('2001-04-09')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-04-30'), 'Release_Date'] = pd.to_datetime('2001-05-10')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-05-31'), 'Release_Date'] = pd.to_datetime('2001-06-10')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-06-30'), 'Release_Date'] = pd.to_datetime('2001-07-05')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-07-31'), 'Release_Date'] = pd.to_datetime('2001-08-09')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-08-31'), 'Release_Date'] = pd.to_datetime('2001-09-09')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-09-30'), 'Release_Date'] = pd.to_datetime('2001-10-08')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-10-31'), 'Release_Date'] = pd.to_datetime('2001-11-08')\n",
    "df_all_idx_revisions.loc[('GKCPNEWY Index', '2001-11-30'), 'Release_Date'] = pd.to_datetime('2001-12-09')\n",
    "### 11 : GKUERATE Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('GKUERATE Index', '2007-01-31'), 'Release_Date'] = np.NaN\n",
    "### 12 : GRCP2NRM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('GRCP2NRM Index', '2006-01-31'), 'Release_Date'] = pd.to_datetime('2006-01-30')\n",
    "### 13 : GRCP2SAM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('GRCP2SAM Index', '2006-01-31'), 'Release_Date'] = pd.to_datetime('2006-01-30')\n",
    "### 14 : ITVHYOY Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('ITVHYOY Index', '1999-01-31'), 'Release_Date'] = pd.to_datetime('1999-02-03')\n",
    "### 15 : JNLSUCTL Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('JNLSUCTL Index', '2003-01-31'), 'Release_Date'] = pd.to_datetime('2003-03-01')\n",
    "### 16 : PTCIEC Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('PTCIEC Index', '2007-12-31'), 'Release_Date'] = pd.to_datetime('2008-01-08')\n",
    "### 17 : SLCPLHMM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('SLCPLHMM Index', '2004-04-30'), 'Release_Date'] = pd.to_datetime('2004-05-20')\n",
    "df_all_idx_revisions.loc[('SLCPLHMM Index', '2005-01-31'), 'Release_Date'] = pd.to_datetime('2005-02-27')\n",
    "df_all_idx_revisions.loc[('SLCPLHMM Index', '2005-02-28'), 'Release_Date'] = pd.to_datetime('2005-03-16')\n",
    "### 18 : SPCPEUMM Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-01-31'), 'Release_Date'] = pd.to_datetime('2000-02-13')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-02-29'), 'Release_Date'] = pd.to_datetime('2000-03-14')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-03-31'), 'Release_Date'] = pd.to_datetime('2000-04-11')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-04-30'), 'Release_Date'] = pd.to_datetime('2000-05-11')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-05-31'), 'Release_Date'] = pd.to_datetime('2000-06-13')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-06-30'), 'Release_Date'] = pd.to_datetime('2000-07-13')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-07-31'), 'Release_Date'] = pd.to_datetime('2000-08-14')\n",
    "df_all_idx_revisions.loc[('SPCPEUMM Index', '2000-08-31'), 'Release_Date'] = pd.to_datetime('2000-09-13')\n",
    "### 19 : SPROCHNG Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('SPROCHNG Index', '2008-01-31'), 'Release_Date'] = pd.to_datetime('2008-02-25')\n",
    "### 20 : SVAWRYOY Index Release_Date mistakes correction:\n",
    "df_all_idx_revisions.loc[('SVAWRYOY Index', '2006-08-31'), 'Release_Date'] = pd.to_datetime('2006-10-15')\n",
    "df_all_idx_revisions.loc[('SVAWRYOY Index', '2006-09-30'), 'Release_Date'] = pd.to_datetime('2006-11-15')\n",
    "df_all_idx_revisions.loc[('SVAWRYOY Index', '2006-10-31'), 'Release_Date'] = pd.to_datetime('2006-12-17')\n",
    "### 21 : INJCJC Index Data Date shifts correction:\n",
    "df_all_idx_revisions.loc[('INJCJC Index', '2020-08-28'), 'Release_Date'] = np.NaN\n",
    "str_ei_to_correct = 'INJCJC Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_to_correct = df_ei_to_correct.drop(df_ei_to_correct.loc[df_ei_to_correct.isna().all(axis = 1)].index)\n",
    "df_ei_to_correct_shifted = df_ei_to_correct.shift(-1)\n",
    "df_ei_to_correct_shifted.columns = [str_column_name + '_shifted' for str_column_name in df_ei_to_correct_shifted.columns] \n",
    "df_ei_corrected = pd.concat([df_ei_to_correct, df_ei_to_correct_shifted], axis = 1)\n",
    "df_ei_corrected.loc[df_ei_corrected['Final_Value_shifted'].isna() & df_ei_corrected['Revision_Value_shifted'].notna(), 'Revision_Date'] = \\\n",
    "                df_ei_corrected['Revision_Date_shifted']\n",
    "df_ei_corrected.loc[df_ei_corrected['Final_Value_shifted'].isna() & df_ei_corrected['Revision_Value_shifted'].notna(), 'Revision_Value'] = \\\n",
    "                df_ei_corrected['Revision_Value_shifted']\n",
    "df_ei_corrected = df_ei_corrected[df_ei_to_correct.columns]\n",
    "df_ei_corrected = df_ei_corrected.drop(df_ei_corrected[df_ei_corrected['Final_Value'].isna()].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 22 : INJCSP Index Data Date shifts correction:\n",
    "str_ei_to_correct = 'INJCSP Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct['Final_Value'].isna()].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 23 : ITSR1B Index Data Date shifts correction: \n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(index = ('ITSR1B Index', '1992-12-31'))\n",
    "### 24 : ATIPIMM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'ATIPIMM Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2004-04-30')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 25 : ATIPIMM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'GKIPIYOY Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2001-01-31')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 26 : IEIPIMOM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'IEIPIMOM Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2001-02-28')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 27 : ITNHMOM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'ITNHMOM Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2003-03-31')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 28 : MPMIGBXA Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'MPMIGBXA Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2005-07-31')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 29 : PCE CHNC Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'PCE CHNC Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('1999-02-28')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 30 : PTPPMOM Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'PTPPMOM Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('1999-02-28')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 31 : SNTEEUGX Index partly dropping due to the gap:\n",
    "str_ei_to_correct = 'SNTEEUGX Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.loc[([str_ei_to_correct], All), All]\n",
    "df_ei_corrected = df_ei_to_correct.drop(df_ei_to_correct[df_ei_to_correct.index.get_level_values('Data_Date') < pd.to_datetime('2003-01-31')].index)\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_corrected], axis = 0).sort_index()\n",
    "### 32 : UKISCTMM Index Data Date shifts correction: \n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(index = ('UKISCTMM Index', '1992-12-31'))\n",
    "### 33 : IERSVMOM Index adding missed rows:\n",
    "str_ei_to_correct = 'IERSVMOM Index'\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2001-01-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2001-02-28')), All] = np.NaN\n",
    "### 34 : ITNHMOM Index adding missed rows:\n",
    "str_ei_to_correct = 'ITNHMOM Index'\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2020-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2020-08-31')), All] = np.NaN\n",
    "### 35 : NEISIYOY Index adding missed rows:\n",
    "str_ei_to_correct = 'NEISIYOY Index'\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('1999-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('1999-08-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2000-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2000-11-30')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2001-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2002-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2003-07-31')), All] = np.NaN\n",
    "### 36 : NEISIYOY Index adding missed rows:\n",
    "str_ei_to_correct = 'MAPMINDX Index'\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-05-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-06-30')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-07-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-08-31')), All] = np.NaN\n",
    "df_all_idx_revisions.loc[(str_ei_to_correct, pd.to_datetime('2016-09-30')), All] = np.NaN\n",
    "### 37 : COMFCOMF Index values correcting:\n",
    "str_ei_to_correct = 'COMFCOMF Index'\n",
    "df_ei_to_correct = df_all_idx_revisions.copy().loc[(str_ei_to_correct, All), All]\n",
    "df_ei_to_correct.loc[df_ei_to_correct['Release_Value'] < 10, 'Release_Value'] = (1 + df_ei_to_correct['Release_Value'] / 100) * 50\n",
    "df_ei_to_correct.loc[df_ei_to_correct['Revision_Value'] < 10, 'Revision_Value'] = (1 + df_ei_to_correct['Revision_Value'] / 100) * 50\n",
    "df_all_idx_revisions = df_all_idx_revisions.drop(str_ei_to_correct, level = 'Index_Name')\n",
    "df_all_idx_revisions = pd.concat([df_all_idx_revisions, df_ei_to_correct], axis = 0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: RELEASE DATE < DATA DATE\n",
    "\n",
    "df_less_data_date = df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] < df_all_idx_revisions.index.get_level_values(1) - pd.offsets.BDay(15)]\n",
    "print(df_less_data_date.index.get_level_values('Index_Name').unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: REVISION DATE < RELEASE DATE\n",
    "\n",
    "df_less_release_date = df_all_idx_revisions.loc[df_all_idx_revisions['Revision_Date'] < df_all_idx_revisions['Release_Date']]\n",
    "print(df_less_release_date.index.get_level_values('Index_Name').unique())\n",
    "print(df_less_release_date.loc[(All, All), All])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: RELEASE DATE >> DATA DATE\n",
    "\n",
    "df_less_data_date = df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] > df_all_idx_revisions.index.get_level_values(1) + pd.offsets.BDay(150)]\n",
    "print(df_less_data_date.index.get_level_values('Index_Name').unique())\n",
    "print(df_less_data_date.loc[('ASPPIMOM Index', All), All])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: RELEASE DATE > NEXT RELEASE DATE\n",
    "\n",
    "df_released_date = df_all_idx_revisions['Release_Date'].to_frame()\n",
    "df_released_date['Release_Date_next'] = df_released_date['Release_Date'].groupby('Index_Name').transform(lambda ser_eco_ind: ser_eco_ind.shift(-1))\n",
    "df_less_release_date = df_released_date.loc[df_released_date['Release_Date_next'] < df_released_date['Release_Date']]\n",
    "print(df_less_release_date.index.get_level_values('Index_Name').unique())\n",
    "print(df_less_release_date.loc[(All, All), All])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: EXTRA SHORT INTERVALS\n",
    "\n",
    "### Frequency check:\n",
    "ser_data_date = df_all_idx_revisions.reset_index('Data_Date')['Data_Date'].squeeze()\n",
    "df_eco_ind_agg = ser_data_date.groupby('Index_Name').apply(lambda ser_eco_ind: (ser_eco_ind - ser_eco_ind.shift()).dt.days)\\\n",
    "                               .groupby('Index_Name').agg(['min', 'max', 'median'])\n",
    "print('Frequency control: Extra minimum:\\n', df_eco_ind_agg.loc[df_eco_ind_agg['median'] / 1.2 > df_eco_ind_agg['min']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: EXTRA LONG INTERVALS\n",
    "\n",
    "### Frequency check:\n",
    "ser_data_date = df_all_idx_revisions.reset_index('Data_Date')['Data_Date'].squeeze()\n",
    "df_eco_ind_agg = ser_data_date.groupby('Index_Name').apply(lambda ser_eco_ind: (ser_eco_ind - ser_eco_ind.shift()).dt.days)\\\n",
    "                               .groupby('Index_Name').agg(['min', 'max', 'median'])\n",
    "print('Frequency control: Extra maximum:\\n', df_eco_ind_agg.loc[df_eco_ind_agg['median'] * 3 < df_eco_ind_agg['max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: DATA QUALITY AND COMPLETENESS\n",
    "\n",
    "### Release Date <= Event Date:\n",
    "print('Overall observations number:', len(df_all_idx_revisions.index))\n",
    "df_less_data_date = df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] < df_all_idx_revisions.index.get_level_values(1)]\n",
    "print('Observations with Release Date < Data Date number:', len(df_less_data_date.index))\n",
    "print('Unique Eco Indices with Release Date < Data Date cases number:', len(df_less_data_date.index.get_level_values(0).unique()))\n",
    "### Release Date vs First Revision Date:\n",
    "print('Release Date > First Revision Date:\\n', \n",
    "      df_all_idx_revisions.loc[df_all_idx_revisions['Release_Date'] > df_all_idx_revisions['Revision_Date'], ['Release_Date', 'Revision_Date']])\n",
    "### Release Date vs next Release Date:\n",
    "df_released_date = df_all_idx_revisions['Release_Date'].to_frame()\n",
    "df_released_date['Release_Date_shifted'] = df_released_date['Release_Date'].groupby('Index_Name').transform(lambda ser_eco_ind: ser_eco_ind.shift(-1))\n",
    "print('Release Date > next Release Date:\\n', df_released_date.loc[df_released_date['Release_Date'] > df_released_date['Release_Date_shifted']])\n",
    "### Frequency check:\n",
    "ser_data_date = df_all_idx_revisions.reset_index('Data_Date')['Data_Date'].squeeze()\n",
    "df_eco_ind_agg = ser_data_date.groupby('Index_Name').apply(lambda ser_eco_ind: (ser_eco_ind - ser_eco_ind.shift()).dt.days)\\\n",
    "                               .groupby('Index_Name').agg(['min', 'max', 'median'])\n",
    "print('Frequency control: Extra short intervals:\\n', df_eco_ind_agg.loc[df_eco_ind_agg['median'] / 1.2 > df_eco_ind_agg['min']])\n",
    "print('Frequency control: Extra long intervals:\\n', df_eco_ind_agg.loc[df_eco_ind_agg['median'] * 2 < df_eco_ind_agg['max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: EXTRACTING Release and first revision date/value pairs:\n",
    "\n",
    "### Release data extracting:\n",
    "df_release_based = df_all_idx_revisions[['Release_Date', 'Release_Value']]\n",
    "df_release_based.columns = ['Observation_Date', 'Index_Value']\n",
    "### All empty release date vector Eco Indices:\n",
    "ser_empty_release_date = df_release_based['Observation_Date'].groupby('Index_Name').count()\n",
    "ser_empty_release_date = ser_empty_release_date[ser_empty_release_date == 0]\n",
    "### All empty release value vector Eco Indices:\n",
    "ser_empty_release_value = df_release_based['Index_Value'].groupby('Index_Name').count()\n",
    "ser_empty_release_value = ser_empty_release_value[ser_empty_release_value == 0]\n",
    "### All empty release date or all empty release value indices list:\n",
    "list_empty_release = sorted(list(set(ser_empty_release_date.index).union(set(ser_empty_release_value.index))))\n",
    "### Revision data extracting:\n",
    "df_revision_based = df_all_idx_revisions[['Revision_Date', 'Revision_Value']]\n",
    "df_revision_based.columns = ['Observation_Date', 'Index_Value']\n",
    "### All empty first revision date vector Eco Indices:\n",
    "ser_empty_revision_date = df_revision_based['Observation_Date'].groupby('Index_Name').count()\n",
    "ser_empty_revision_date = ser_empty_revision_date[ser_empty_revision_date == 0]\n",
    "### All empty first revision value vector Eco Indices:\n",
    "ser_empty_revision_value = df_revision_based['Index_Value'].groupby('Index_Name').count()\n",
    "ser_empty_revision_value = ser_empty_revision_value[ser_empty_revision_value == 0]\n",
    "### All empty first revision date or all empty first revision value indices list:\n",
    "list_empty_revision = sorted(list(set(ser_empty_revision_date.index).union(set(ser_empty_revision_value.index))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: MAIN STATS\n",
    "\n",
    "### General stats:\n",
    "print('Rows with at least one value filled:', len(df_all_idx_revisions.index))\n",
    "print('Eco Indices number:', len(df_all_idx_revisions.index.get_level_values(0).unique()))\n",
    "print('Average data dates number:', int(df_all_idx_revisions.groupby('Index_Name').apply(lambda df_group: len(df_group.index)).mean()))\n",
    "### Release data analyzing (96,369 rows):\n",
    "### Data quantity:\n",
    "print('Release Info: filled date & value:', len(df_release_based.loc[df_release_based['Observation_Date'].notna() & df_release_based['Index_Value'].notna()]), 'rows')\n",
    "print('Release Info: empty date & value:', len(df_release_based.loc[df_release_based['Observation_Date'].isna() & df_release_based['Index_Value'].isna()]), 'rows')\n",
    "print('Release Info: empty date & filled value:', len(df_release_based.loc[df_release_based['Observation_Date'].isna() & df_release_based['Index_Value'].notna()]), 'rows')\n",
    "print('Release Info: filled date & empty value:', len(df_release_based.loc[df_release_based['Observation_Date'].notna() & df_release_based['Index_Value'].isna()]), 'rows')\n",
    "### First revision data analyzing (96,369 rows):\n",
    "### Data quantity:\n",
    "print('First revision Info: filled date & value:', \n",
    "      len(df_revision_based.loc[df_revision_based['Observation_Date'].notna() & df_revision_based['Index_Value'].notna()]), 'rows')\n",
    "print('First revision Info: empty date & value:', \n",
    "      len(df_revision_based.loc[df_revision_based['Observation_Date'].isna() & df_revision_based['Index_Value'].isna()]), 'rows')\n",
    "print('First revision Info: empty date & filled value:', \n",
    "      len(df_revision_based.loc[df_revision_based['Observation_Date'].isna() & df_revision_based['Index_Value'].notna()]), 'rows')\n",
    "print('First revision Info: filled date & empty value:', \n",
    "      len(df_revision_based.loc[df_revision_based['Observation_Date'].notna() & df_revision_based['Index_Value'].isna()]), 'rows') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: DETAILED STATS: RELEASE DATE/VALUE PAIR\n",
    "\n",
    "print('Eco Indices without any release date number:', len(ser_empty_release_date.index))\n",
    "print('Eco Indices without any release value number:', len(ser_empty_release_value.index))\n",
    "### Empty Eco Indices relese info lists comparision:\n",
    "print('Symmetric difference of empty release info Eco Indices lists:', list(set(ser_empty_release_date.index).symmetric_difference(set(ser_empty_release_value.index))))\n",
    "print('Union of empty release info Eco Indices lists:\\n', sorted(list(set(ser_empty_release_date.index).union(set(ser_empty_release_value.index)))))\n",
    "for iter_eco_index in sorted(list(set(ser_empty_release_date.index).symmetric_difference(set(ser_empty_release_value.index)))):\n",
    "    print('Symmetric difference for', iter_eco_index, ':\\n', \n",
    "          df_all_idx_revisions.loc[(iter_eco_index, All), All].dropna(subset = ['Release_Date', 'Release_Value'], how = 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: DETAILED STATS: FIRST REVISION DATE/VALUE PAIR\n",
    "\n",
    "print('Eco Indices without any revision date number:', len(ser_empty_revision_date.index))\n",
    "print('Eco Indices without any revision value number:', len(ser_empty_revision_value.index))\n",
    "### Empty Eco Indices relese info lists comparision:\n",
    "print('Symmetric difference of empty revision info Eco Indices lists:', \n",
    "      sorted(list(set(ser_empty_revision_date.index).symmetric_difference(set(ser_empty_revision_value.index)))))\n",
    "print('Union of empty revision info Eco Indices lists:\\n', sorted(list(set(ser_empty_revision_date.index).union(set(ser_empty_revision_value.index)))))\n",
    "for iter_eco_index in sorted(list(set(ser_empty_revision_date.index).symmetric_difference(set(ser_empty_revision_value.index)))):\n",
    "    print('Symmetric difference for', iter_eco_index, ':\\n', \n",
    "          df_all_idx_revisions.loc[(iter_eco_index, All), All].dropna(subset = ['Revision_Date', 'Revision_Value'], how = 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: ALL DATES FILLING PROCEDURE\n",
    "\n",
    "df_flags = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags)\n",
    "\n",
    "#### Dates filling:\n",
    "def all_dates_filler(df_eco_ind):\n",
    "    ### Eco Indice options saving:\n",
    "    str_index_name = df_eco_ind.index.get_level_values(0)[0]\n",
    "    str_eco_ind_freq = df_flags.loc[str_index_name, 'Frequency']\n",
    "    if (str_eco_ind_freq in dict_final_only_lag.keys()):\n",
    "        int_final_only_lag = dict_final_only_lag[str_eco_ind_freq]\n",
    "    else:\n",
    "        int_final_only_lag = dict_final_only_lag['Other']\n",
    "    ### Index_Name level dropping:\n",
    "    df_eco_ind = df_eco_ind.droplevel(0)\n",
    "    ### Index duplicating to column:\n",
    "    df_eco_ind['Event_Date'] = df_eco_ind.index\n",
    "    df_eco_ind['Release_Lag'] = np.NaN\n",
    "    ### No release checking:\n",
    "    if (str_index_name in list_empty_release): \n",
    "        df_eco_ind['Final_Date'] = df_eco_ind['Event_Date'] + pd.Timedelta(int_final_only_lag, 'D')\n",
    "    elif (str_index_name in list_empty_revision):    \n",
    "        ### Backfilling release dates lag before the first known released date: \n",
    "        idx_first_valid_release = df_eco_ind['Release_Date'].first_valid_index()\n",
    "        df_first_released = df_eco_ind.dropna(subset = ['Release_Date']).iloc[ : int_first_mean_length]        \n",
    "        int_first_release_mean = (df_first_released['Release_Date'] - df_first_released['Event_Date']).mean().days + 1\n",
    "        df_eco_ind.loc[ : idx_first_valid_release, 'Release_Lag'] = int_first_release_mean\n",
    "        ### Interpolating missed release dates lag after the first known released date:\n",
    "        df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'] = (df_eco_ind['Release_Date'] - df_eco_ind['Event_Date']).dt.days\n",
    "        df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'] = df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'].interpolate(method = 'linear').round()\n",
    "        ### Filling empty release date by lag adding:\n",
    "        df_eco_ind['Release_Lag'] = pd.to_timedelta(df_eco_ind['Release_Lag'], 'D')    \n",
    "        df_eco_ind.loc[df_eco_ind['Release_Date'].isna(), 'Release_Date'] = df_eco_ind['Event_Date'] + df_eco_ind['Release_Lag']\n",
    "        ### Final date filling as one event shifted release date\n",
    "        df_eco_ind['Final_Date'] = df_eco_ind['Release_Date'].shift(-int_revision_shift)   \n",
    "    else:\n",
    "        ### Backfilling release dates lag before the first known released date: \n",
    "        idx_first_valid_release = df_eco_ind['Release_Date'].first_valid_index()\n",
    "        df_first_released = df_eco_ind.dropna(subset = ['Release_Date']).iloc[ : int_first_mean_length]     \n",
    "        int_first_release_mean = (df_first_released['Release_Date'] - df_first_released['Event_Date']).mean().days + 1        \n",
    "        df_eco_ind.loc[ : idx_first_valid_release, 'Release_Lag'] = int_first_release_mean\n",
    "        ### Interpolating missed release dates lag after the first known released date:\n",
    "        df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'] = (df_eco_ind['Release_Date'] - df_eco_ind['Event_Date']).dt.days\n",
    "        df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'] = df_eco_ind.loc[idx_first_valid_release : , 'Release_Lag'].interpolate(method = 'linear').round()\n",
    "        ### Filling empty release date with lag adding:\n",
    "        df_eco_ind['Release_Lag'] = pd.to_timedelta(df_eco_ind['Release_Lag'], 'D')    \n",
    "        df_eco_ind.loc[df_eco_ind['Release_Date'].isna(), 'Release_Date'] = df_eco_ind['Event_Date'] + df_eco_ind['Release_Lag']\n",
    "        ### Filling empty revision dates: \n",
    "        df_eco_ind.loc[df_eco_ind['Revision_Date'].isna(), 'Revision_Date'] = df_eco_ind['Release_Date'].shift(-int_revision_shift)\n",
    "        ### Final date filling as one event shifted release date\n",
    "        df_eco_ind['Final_Date'] = df_eco_ind['Release_Date'].shift(-int_final_shift)\n",
    "    ### Last values having revision date with no revision value managing:\n",
    "    idx_last_values = df_eco_ind['Revision_Date'].notna() & df_eco_ind['Revision_Value'].isna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Revision_Value'] = df_eco_ind['Final_Value']\n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Value'] = np.NaN\n",
    "    ### Last values having release date with no release value managing:\n",
    "    idx_last_values = df_eco_ind['Release_Date'].notna() & df_eco_ind['Release_Value'].isna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Release_Value'] = df_eco_ind['Final_Value']\n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Value'] = np.NaN\n",
    "    ### Last values having final value with no final date and future date managing (no news detected):\n",
    "    idx_last_values = df_eco_ind['Future_Date'].isna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Value'] = np.NaN\n",
    "    ### Last values having final value with no final date and having revision pair managing (no news detected):\n",
    "    idx_last_values = df_eco_ind['Revision_Date'].notna() & df_eco_ind['Revision_Value'].notna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Value'] = np.NaN\n",
    "    ### Last values having final value with no final date and no revision pair managing:\n",
    "    idx_last_values = df_eco_ind['Revision_Date'].isna() & df_eco_ind['Revision_Value'].isna() & df_eco_ind['Final_Date'].isna() & df_eco_ind['Final_Value'].notna() \n",
    "    df_eco_ind.loc[idx_last_values, 'Final_Date'] = df_eco_ind['Future_Date']    \n",
    "    ### Results ouput:\n",
    "    return df_eco_ind[['Release_Date', 'Release_Value', 'Revision_Date', 'Revision_Value', 'Final_Date', 'Final_Value']]\n",
    "\n",
    "### Date filling for each eco index performing:\n",
    "df_dates_filled = df_all_idx_revisions.groupby('Index_Name').apply(all_dates_filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: DATA REPAIRING AND SAVING\n",
    "\n",
    "### 38 : GKCPIUHY Index Final Date shifts correction: \n",
    "df_dates_filled.loc[('GKCPIUHY Index', '2003-10-31'), 'Final_Date'] = pd.to_datetime('2004-02-17')\n",
    "df_dates_filled.loc[('GKCPIUHY Index', '2003-11-30'), 'Final_Date'] = pd.to_datetime('2004-02-17')\n",
    "### 39 : GKCPNEWY Index Final Date shifts correction: \n",
    "df_dates_filled.loc[('GKCPNEWY Index', '2003-10-31'), 'Final_Date'] = pd.to_datetime('2004-02-10')\n",
    "df_dates_filled.loc[('GKCPNEWY Index', '2003-11-30'), 'Final_Date'] = pd.to_datetime('2004-02-10')\n",
    "### 40 : EUCPTSAM Index Final Date shifts correction: \n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2004-12-31'), 'Final_Date'] = pd.to_datetime('2005-09-12')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2005-03-31'), 'Final_Date'] = pd.to_datetime('2005-12-09')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2005-06-30'), 'Final_Date'] = pd.to_datetime('2006-03-10')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2005-09-30'), 'Final_Date'] = pd.to_datetime('2006-06-09')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2005-12-31'), 'Final_Date'] = pd.to_datetime('2006-09-06')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2006-03-31'), 'Final_Date'] = pd.to_datetime('2006-12-06')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2006-06-30'), 'Final_Date'] = pd.to_datetime('2006-12-06')\n",
    "df_dates_filled.loc[('EUCPTSAM Index', '2006-09-30'), 'Final_Date'] = pd.to_datetime('2007-01-18')\n",
    "### 41 : SVUER Index Revision Date shifts correction: \n",
    "df_dates_filled.loc[('SVUER Index', '2005-06-30'), 'Revision_Date'] = pd.to_datetime('2005-09-18')\n",
    "### 42 : UKMPIMOM Index Revision Date shifts correction: \n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-01-31'), 'Revision_Date'] = pd.to_datetime('2001-04-09')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-02-28'), 'Revision_Date'] = pd.to_datetime('2001-05-08')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-03-31'), 'Revision_Date'] = pd.to_datetime('2001-06-11')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-04-30'), 'Revision_Date'] = pd.to_datetime('2001-07-10')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-05-31'), 'Revision_Date'] = pd.to_datetime('2001-08-05')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-08-31'), 'Revision_Date'] = pd.to_datetime('2001-11-05')\n",
    "df_dates_filled.loc[('UKMPIMOM Index', '2001-10-31'), 'Revision_Date'] = pd.to_datetime('2002-01-14')\n",
    "### 43 : ESCPLMOM Index early values clearing: \n",
    "df_dates_filled.loc[('ESCPLMOM Index', df_dates_filled.index.get_level_values('Data_Date') < datetime(1994, 1, 1)), All] = np.NaN\n",
    "### 44 : ESCPLMOM Index early values clearing: \n",
    "df_dates_filled.loc[('ESPPIMOM Index', df_dates_filled.index.get_level_values('Data_Date') < datetime(2003, 1, 1)), All] = np.NaN\n",
    "### 45 : ESCPLMOM Index revisions clearing:\n",
    "df_dates_filled.loc[('ESPPIMOM Index', All), 'Revision_Value'] = np.NaN\n",
    "### 46 : IECPIMOM Index early values clearing:\n",
    "df_dates_filled.loc[('IECPIMOM Index', df_dates_filled.index.get_level_values('Data_Date') < datetime(1998, 1, 1)), All] = np.NaN\n",
    "### 47 : JBTARATE Index early values clearing:\n",
    "df_dates_filled.loc[('JBTARATE Index', df_dates_filled.index.get_level_values('Data_Date') < datetime(1990, 1, 1)), All] = np.NaN\n",
    "### 48 : SVBTSI Index early values clearing:\n",
    "df_dates_filled.loc[('SVBTSI Index', df_dates_filled.index.get_level_values('Data_Date') < datetime(2005, 1, 1)), All] = np.NaN\n",
    "### 49 : SVIPTMOM Index early values clearing:\n",
    "df_dates_filled.loc[('SVIPTMOM Index', df_dates_filled.index.get_level_values('Data_Date') < datetime(2008, 1, 1)), All] = np.NaN\n",
    "### 50 : UKPPIINC Index early values clearing:\n",
    "df_dates_filled.loc[('UKPPIINC Index', df_dates_filled.index.get_level_values('Data_Date') < datetime(1997, 1, 1)), All] = np.NaN\n",
    "### 51 : UKPPXFBM Index early values clearing:\n",
    "df_dates_filled.loc[('UKPPXFBM Index', df_dates_filled.index.get_level_values('Data_Date') < datetime(1997, 1, 1)), All] = np.NaN\n",
    "### 52 : GRCP2BRM Index outlier values clipping:\n",
    "df_dates_filled.loc[('GRCP2BRM Index', df_dates_filled['Final_Value'] < -2), 'Final_Value'] = -2.0\n",
    "### 53 : GRCP2SAM Index outlier values clipping:\n",
    "df_dates_filled.loc[('GRCP2SAM Index', df_dates_filled['Final_Value'] < -5), 'Final_Value'] = -5.0\n",
    "df_dates_filled.loc[('GRCP2SAM Index', df_dates_filled['Final_Value'] > 5), 'Final_Value'] = 5.0\n",
    "### 54 : PITLCHNG Index outlier values clipping:\n",
    "df_dates_filled.loc[('PITLCHNG Index', df_dates_filled['Release_Value'] < -2), 'Release_Value'] = -2.0\n",
    "df_dates_filled.loc[('PITLCHNG Index', df_dates_filled['Release_Value'] > 2), 'Release_Value'] = 2.0\n",
    "df_dates_filled.loc[('PITLCHNG Index', df_dates_filled['Revision_Value'] < -2), 'Revision_Value'] = -2.0\n",
    "df_dates_filled.loc[('PITLCHNG Index', df_dates_filled['Revision_Value'] > 2), 'Revision_Value'] = 2.0\n",
    "df_dates_filled.loc[('PITLCHNG Index', df_dates_filled['Final_Value'] < -2), 'Final_Value'] = -2.0\n",
    "df_dates_filled.loc[('PITLCHNG Index', df_dates_filled['Final_Value'] > 2), 'Final_Value'] = 2.0\n",
    "### 55 : ITNSSTN Index outlier values clipping:\n",
    "df_dates_filled.loc[('ITNSSTN Index', df_dates_filled['Release_Value'] < -10), 'Release_Value'] = -10.0\n",
    "df_dates_filled.loc[('ITNSSTN Index', df_dates_filled['Release_Value'] > 10), 'Release_Value'] = 10.0\n",
    "df_dates_filled.loc[('ITNSSTN Index', df_dates_filled['Revision_Value'] < -10), 'Revision_Value'] = -10.0\n",
    "df_dates_filled.loc[('ITNSSTN Index', df_dates_filled['Revision_Value'] > 10), 'Revision_Value'] = 10.0\n",
    "df_dates_filled.loc[('ITNSSTN Index', df_dates_filled['Final_Value'] < -10), 'Final_Value'] = -10.0\n",
    "df_dates_filled.loc[('ITNSSTN Index', df_dates_filled['Final_Value'] > 10), 'Final_Value'] = 10.0\n",
    "### 56 : EUNOEUM Index dropping:\n",
    "df_dates_filled.drop(labels = 'ESRSREHY Index', level = 'Index_Name', inplace = True)\n",
    "### 57 : EUNOEUM Index dropping:\n",
    "df_dates_filled.drop(labels = 'EUNOEUM Index', level = 'Index_Name', inplace = True)\n",
    "### 58 : NEUETOTR Index dropping:\n",
    "df_dates_filled.drop(labels = 'NEUETOTR Index', level = 'Index_Name', inplace = True)\n",
    "### 59 : RSSA25M Index dropping:\n",
    "df_dates_filled.drop(labels = 'RSSA25M Index', level = 'Index_Name', inplace = True)\n",
    "### 60 : UKISCTMM Index dropping:\n",
    "df_dates_filled.drop(labels = 'UKISCTMM Index', level = 'Index_Name', inplace = True)\n",
    "### 61 : UKMSVTVJ Index dropping:\n",
    "df_dates_filled.drop(labels = 'UKMSVTVJ Index', level = 'Index_Name', inplace = True)\n",
    "### 62 : UKNCCCIS Index dropping:\n",
    "df_dates_filled.drop(labels = 'UKNCCCIS Index', level = 'Index_Name', inplace = True)\n",
    "### 63 : SVUER Index revisions clearing:\n",
    "df_dates_filled.loc[('SVUER Index', All), 'Revision_Value'] = np.NaN\n",
    "### 64 : CPTICHNG Index finals clearing:\n",
    "df_dates_filled.loc[('CPTICHNG Index', All), 'Final_Value'] = np.NaN\n",
    "### 65 : ECO1GFKC Index finals clearing:\n",
    "df_dates_filled.loc[('ECO1GFKC Index', All), 'Final_Value'] = np.NaN\n",
    "### 66 : JNPIY Index finals clearing:\n",
    "df_dates_filled.loc[('JNPIY Index', All), 'Final_Value'] = np.NaN\n",
    "### 67 : PTCCI Index finals clearing:\n",
    "df_dates_filled.loc[('PTCCI Index', All), 'Final_Value'] = np.NaN\n",
    "### 68 : PTPPMOM Index finals clearing:\n",
    "df_dates_filled.loc[('PTPPMOM Index', All), 'Final_Value'] = np.NaN\n",
    "### 69 : UKUER Index finals clearing:\n",
    "df_dates_filled.loc[('UKUER Index', All), 'Final_Value'] = np.NaN\n",
    "### 70 : UMRTEMU Index finals clearing:\n",
    "df_dates_filled.loc[('UMRTEMU Index', All), 'Final_Value'] = np.NaN\n",
    "### 71 : GRIFPBUS Index finals partly clearing and releases/revisions partly shifting:\n",
    "date_break = datetime(2018, 3, 31)\n",
    "flo_shift = (df_dates_filled.loc[('GRIFPBUS Index', [date_break]), 'Release_Value'] - df_dates_filled.loc[('GRIFPBUS Index', [date_break]), 'Revision_Value']).values[0]\n",
    "df_dates_filled.loc[('GRIFPBUS Index', df_dates_filled.index.get_level_values('Data_Date') < date_break), 'Final_Value'] = np.NaN\n",
    "df_dates_filled.loc[('GRIFPBUS Index', df_dates_filled.index.get_level_values('Data_Date') <= date_break), 'Release_Value'] = \\\n",
    "        df_dates_filled.loc[('GRIFPBUS Index', df_dates_filled.index.get_level_values('Data_Date') <= date_break), 'Release_Value'] - flo_shift\n",
    "df_dates_filled.loc[('GRIFPBUS Index', df_dates_filled.index.get_level_values('Data_Date') < date_break), 'Revision_Value'] = \\\n",
    "        df_dates_filled.loc[('GRIFPBUS Index', df_dates_filled.index.get_level_values('Data_Date') < date_break), 'Revision_Value'] - flo_shift\n",
    "### 72 : GRIFPCA Index finals partly clearing and releases/revisions partly shifting:\n",
    "date_break = datetime(2018, 3, 31)\n",
    "flo_shift = (df_dates_filled.loc[('GRIFPCA Index', [date_break]), 'Release_Value'] - df_dates_filled.loc[('GRIFPCA Index', [date_break]), 'Revision_Value']).values[0]\n",
    "df_dates_filled.loc[('GRIFPCA Index', df_dates_filled.index.get_level_values('Data_Date') < date_break), 'Final_Value'] = np.NaN\n",
    "df_dates_filled.loc[('GRIFPCA Index', df_dates_filled.index.get_level_values('Data_Date') <= date_break), 'Release_Value'] = \\\n",
    "        df_dates_filled.loc[('GRIFPCA Index', df_dates_filled.index.get_level_values('Data_Date') <= date_break), 'Release_Value'] - flo_shift\n",
    "df_dates_filled.loc[('GRIFPCA Index', df_dates_filled.index.get_level_values('Data_Date') < date_break), 'Revision_Value'] = \\\n",
    "        df_dates_filled.loc[('GRIFPCA Index', df_dates_filled.index.get_level_values('Data_Date') < date_break), 'Revision_Value'] - flo_shift\n",
    "### 73 : FRJSTCHG Index releases/revisions clearing:\n",
    "df_dates_filled.loc[('FRJSTCHG Index', All), ['Release_Value', 'Revision_Value']] = np.NaN\n",
    "### 74 : JNCAP Index releases/revisions clearing:\n",
    "df_dates_filled.loc[('JNCAP Index', All), ['Release_Value', 'Revision_Value']] = np.NaN\n",
    "### 75 : IEUERT Index releases/revisions clearing:\n",
    "df_dates_filled.loc[('IEUERT Index', df_dates_filled.index.get_level_values('Data_Date') <= '2000-01-01'), ['Release_Value', 'Revision_Value']] = np.NaN\n",
    "### 76 : JNCICCOI Index releases/revisions clearing:\n",
    "df_dates_filled.loc[('JNCICCOI Index', df_dates_filled.index.get_level_values('Data_Date') <= '2020-01-01'), ['Release_Value', 'Revision_Value']] = np.NaN\n",
    "### Empty Dates dropping:\n",
    "df_dates_filled.loc[df_dates_filled['Release_Date'].notna() & df_dates_filled['Release_Value'].isna(), 'Release_Date'] = np.NaN\n",
    "df_dates_filled.loc[df_dates_filled['Revision_Date'].notna() & df_dates_filled['Revision_Value'].isna(), 'Revision_Date'] = np.NaN\n",
    "df_dates_filled.loc[df_dates_filled['Final_Date'].notna() & df_dates_filled['Final_Value'].isna(), 'Final_Date'] = np.NaN\n",
    "### Moving early release dates to data dates:\n",
    "df_dates_filled.loc[df_dates_filled.index.get_level_values('Data_Date') > df_dates_filled['Release_Date'], 'Release_Date'] = \\\n",
    "    df_dates_filled.loc[df_dates_filled.index.get_level_values('Data_Date') > df_dates_filled['Release_Date']].index.get_level_values('Data_Date')\n",
    "### Moving early revision dates to data dates:\n",
    "df_dates_filled.loc[df_dates_filled['Revision_Date'] < df_dates_filled['Release_Date'], 'Revision_Date'] = \\\n",
    "    df_dates_filled.loc[df_dates_filled['Revision_Date'] < df_dates_filled['Release_Date'], 'Release_Date']\n",
    "### Dataframe types control:\n",
    "print(df_dates_filled.dtypes)\n",
    "### Filled dates saving:\n",
    "df_dates_filled.to_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR FINAL VALUE WITHOUT FINAL DATE:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "df_dates_filled.loc[df_dates_filled['Final_Value'].notna() & df_dates_filled['Final_Date'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR REVISION DATE TO BE AFTER RELEASE DATE:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "df_dates_filled.loc[df_dates_filled['Revision_Date'] < df_dates_filled['Release_Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR FINAL DATE TO BE AFTER RELEASE DATE:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "df_dates_filled.loc[df_dates_filled['Final_Date'] < df_dates_filled['Release_Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR FINAL DATE TO BE AFTER REVISION DATE:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "df_dates_filled.loc[df_dates_filled['Final_Date'] < df_dates_filled['Revision_Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR RELEASE PAIR COMPLETENESS:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "print(df_dates_filled.loc[df_dates_filled['Release_Date'].isna() & df_dates_filled['Release_Value'].notna()])\n",
    "print(df_dates_filled.loc[df_dates_filled['Release_Date'].notna() & df_dates_filled['Release_Value'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR REVISION PAIR COMPLETENESS:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "print(df_dates_filled.loc[df_dates_filled['Revision_Date'].isna() & df_dates_filled['Revision_Value'].notna()])\n",
    "print(df_dates_filled.loc[df_dates_filled['Revision_Date'].notna() & df_dates_filled['Revision_Value'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: CHECKING FOR FINAL PAIR COMPLETENESS:\n",
    "\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "print(df_dates_filled.loc[df_dates_filled['Final_Date'].isna() & df_dates_filled['Final_Value'].notna()])\n",
    "print(df_dates_filled.loc[df_dates_filled['Final_Date'].notna() & df_dates_filled['Final_Value'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: FILLED ECO INDICES DATA PLOTTING TO VISUAL CONTROL\n",
    "\n",
    "### Index choosing:\n",
    "str_test_index = 'ADP CHNG Index'\n",
    "### Data loading:\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "df_flags = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags)\n",
    "### Flags printing:\n",
    "print(df_flags.loc[str_test_index, All])\n",
    "### Series creating:\n",
    "df_eco_ind = df_dates_filled.loc[(str_test_index, All), All].droplevel(0)[['Release_Value', 'Revision_Value', 'Final_Value']]\n",
    "date_xlim_start = df_eco_ind.index.min()\n",
    "date_xlim_finish = df_eco_ind.index.max()\n",
    "#date_xlim_start = datetime(1984, 12, 31)\n",
    "date_xlim_finish = datetime(2020, 1, 31)\n",
    "flo_ylim_min = df_eco_ind.loc[date_xlim_start : date_xlim_finish, All].min().min()\n",
    "flo_ylim_max = df_eco_ind.loc[date_xlim_start : date_xlim_finish, All].max().max()\n",
    "tup_ylim = (flo_ylim_min - abs(flo_ylim_min) / 10, flo_ylim_max + abs(flo_ylim_max) / 10)\n",
    "ax_eco_ind = df_eco_ind.plot(figsize = (20, 7.5), title = str_test_index + ' revisions', \n",
    "                style = ['c+', 'mx', 'k-'], markersize = 7.5, markeredgewidth = 1.5, \n",
    "                xlim = (date_xlim_start, date_xlim_finish), ylim = tup_ylim, x_compat = True)\n",
    "ax_eco_ind.xaxis.set_major_locator(mdates.YearLocator(base = 1))                             \n",
    "ax_eco_ind.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))                \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: VALUES STACKING\n",
    "\n",
    "### Dataframe loading:\n",
    "df_dates_filled = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_filled)\n",
    "### Release pair stacking:\n",
    "df_release_pair = df_dates_filled[['Release_Date', 'Release_Value']]\n",
    "df_release_pair.columns = ['Observation_Date', 'Index_Value']\n",
    "df_release_pair = df_release_pair.dropna()\n",
    "ser_release_based = df_release_pair.set_index('Observation_Date', append = True).squeeze()\n",
    "#df_release_pair['Stage'] = 'Release'\n",
    "#ser_release_based = df_release_pair.set_index(['Observation_Date', 'Stage'], append = True).squeeze()\n",
    "### First revision pair stacking:\n",
    "df_revision_pair = df_dates_filled[['Revision_Date', 'Revision_Value']]\n",
    "df_revision_pair.columns = ['Observation_Date', 'Index_Value']\n",
    "df_revision_pair = df_revision_pair.dropna()\n",
    "ser_revision_based = df_revision_pair.set_index('Observation_Date', append = True).squeeze()\n",
    "#df_revision_pair['Stage'] = 'First Revision'\n",
    "#ser_revision_based = df_revision_pair.set_index(['Observation_Date', 'Stage'], append = True).squeeze()\n",
    "### Final revision pair stacking:\n",
    "df_final_pair = df_dates_filled[['Final_Date', 'Final_Value']]\n",
    "df_final_pair.columns = ['Observation_Date', 'Index_Value']\n",
    "df_final_pair = df_final_pair.dropna()\n",
    "ser_final_based = df_final_pair.set_index('Observation_Date', append = True).squeeze()\n",
    "#df_final_pair['Stage'] = 'Final Revision'\n",
    "#ser_final_based = df_final_pair.set_index(['Observation_Date', 'Stage'], append = True).squeeze()\n",
    "### Vectors aggregating:\n",
    "ser_history_raw = ser_final_based.combine_first(ser_revision_based).combine_first(ser_release_based).dropna().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: FREQUENCY RECREATING\n",
    "\n",
    "### 77 : BAKETOT Index frequency rerceating:\n",
    "ser_history_raw.loc['BAKETOT Index', pd.to_datetime('1994-12-23'), pd.to_datetime('1994-12-23')] = np.NaN\n",
    "ser_history_raw.loc['BAKETOT Index', pd.to_datetime('1995-12-22'), pd.to_datetime('1995-12-22')] = np.NaN\n",
    "### 78 : IEIPIMOM Index frequency rerceating:\n",
    "ser_history_raw.loc['IEIPIMOM Index', pd.to_datetime('2004-08-31'), pd.to_datetime('2004-08-31')] = np.NaN\n",
    "### 79 : IERSVMOM Index frequency rerceating:\n",
    "ser_history_raw.loc['IERSVMOM Index', pd.to_datetime('2001-01-31'), pd.to_datetime('2001-01-31')] = np.NaN\n",
    "ser_history_raw.loc['IERSVMOM Index', pd.to_datetime('2001-02-28'), pd.to_datetime('2001-02-28')] = np.NaN\n",
    "### 80 : ITBCI Index frequency rerceating:\n",
    "ser_history_raw.loc['ITBCI Index', pd.to_datetime('2020-04-30'), pd.to_datetime('2020-04-30')] = np.NaN\n",
    "### 81 : ITPSSA Index frequency rerceating:\n",
    "ser_history_raw.loc['ITPSSA Index', pd.to_datetime('2020-04-30'), pd.to_datetime('2020-04-30')] = np.NaN\n",
    "### 82 : JCOMHCF Index frequency rerceating:\n",
    "idx_ticker_date = ser_history_raw.loc[['JCOMHCF Index'], All, All].index.get_level_values('Data_Date').unique()\n",
    "idx_ticker_date = pd.date_range(start = '1984-12-31', end = '2004-03-31', freq = 'M').difference(idx_ticker_date)\n",
    "idx_ticker_reindex = pd.MultiIndex.from_tuples(tuple(zip(['JCOMHCF Index'] * len(idx_ticker_date), idx_ticker_date, idx_ticker_date)))\n",
    "ser_ei_to_add = pd.Series(np.NaN, index = idx_ticker_reindex)\n",
    "ser_ei_to_add.index.names = ser_history_raw.index.names\n",
    "ser_history_raw = pd.concat([ser_history_raw, ser_ei_to_add], axis = 0)\n",
    "### 83 : JNPIY Index frequency rerceating:\n",
    "ser_history_raw.loc['JNPIY Index', pd.to_datetime('2001-02-28'), pd.to_datetime('2001-02-28')] = np.NaN\n",
    "### 84 : MAPMINDX Index frequency rerceating:\n",
    "ser_history_raw.loc['MAPMINDX Index', pd.to_datetime('2016-05-31'), pd.to_datetime('2016-05-31')] = np.NaN\n",
    "ser_history_raw.loc['MAPMINDX Index', pd.to_datetime('2016-06-30'), pd.to_datetime('2016-06-30')] = np.NaN\n",
    "ser_history_raw.loc['MAPMINDX Index', pd.to_datetime('2016-07-31'), pd.to_datetime('2016-07-31')] = np.NaN\n",
    "ser_history_raw.loc['MAPMINDX Index', pd.to_datetime('2016-08-31'), pd.to_datetime('2016-08-31')] = np.NaN\n",
    "ser_history_raw.loc['MAPMINDX Index', pd.to_datetime('2016-09-30'), pd.to_datetime('2016-09-30')] = np.NaN\n",
    "### 85 : MPMIGBMA Index frequency rerceating:\n",
    "idx_ticker_date = pd.date_range(start = '2014-01-31', end = '2015-11-30', freq = 'M')\n",
    "idx_ticker_reindex = pd.MultiIndex.from_tuples(tuple(zip(['MPMIGBMA Index'] * len(idx_ticker_date), idx_ticker_date, idx_ticker_date)))\n",
    "ser_ei_to_add = pd.Series(np.NaN, index = idx_ticker_reindex)\n",
    "ser_ei_to_add.index.names = ser_history_raw.index.names\n",
    "ser_history_raw = pd.concat([ser_history_raw, ser_ei_to_add], axis = 0)\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2012-05-31'), pd.to_datetime('2012-05-31')] = np.NaN\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2013-02-28'), pd.to_datetime('2013-02-28')] = np.NaN\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2016-01-31'), pd.to_datetime('2016-01-31')] = np.NaN\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2016-02-29'), pd.to_datetime('2016-02-29')] = np.NaN\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2016-06-30'), pd.to_datetime('2016-06-30')] = np.NaN\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2016-07-31'), pd.to_datetime('2016-07-31')] = np.NaN\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2016-08-31'), pd.to_datetime('2016-08-31')] = np.NaN\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2016-12-31'), pd.to_datetime('2016-12-31')] = np.NaN\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2017-03-31'), pd.to_datetime('2017-03-31')] = np.NaN\n",
    "ser_history_raw.loc['MPMIGBMA Index', pd.to_datetime('2017-04-30'), pd.to_datetime('2017-04-30')] = np.NaN\n",
    "### 86 : NEISIYOY Index frequency rerceating:\n",
    "ser_history_raw.loc['NEISIYOY Index', pd.to_datetime('1999-07-31'), pd.to_datetime('1999-07-31')] = np.NaN\n",
    "ser_history_raw.loc['NEISIYOY Index', pd.to_datetime('1999-08-31'), pd.to_datetime('1999-08-31')] = np.NaN\n",
    "ser_history_raw.loc['NEISIYOY Index', pd.to_datetime('2000-06-30'), pd.to_datetime('2000-06-30')] = np.NaN\n",
    "ser_history_raw.loc['NEISIYOY Index', pd.to_datetime('2000-07-31'), pd.to_datetime('2000-07-31')] = np.NaN\n",
    "ser_history_raw.loc['NEISIYOY Index', pd.to_datetime('2000-09-30'), pd.to_datetime('2000-09-30')] = np.NaN\n",
    "ser_history_raw.loc['NEISIYOY Index', pd.to_datetime('2000-11-30'), pd.to_datetime('2000-11-30')] = np.NaN\n",
    "ser_history_raw.loc['NEISIYOY Index', pd.to_datetime('2001-07-31'), pd.to_datetime('2001-07-31')] = np.NaN\n",
    "ser_history_raw.loc['NEISIYOY Index', pd.to_datetime('2002-07-31'), pd.to_datetime('2002-07-31')] = np.NaN\n",
    "ser_history_raw.loc['NEISIYOY Index', pd.to_datetime('2003-07-31'), pd.to_datetime('2003-07-31')] = np.NaN\n",
    "### 87 : NERS20Y Index frequency rerceating:\n",
    "ser_history_raw.loc['NERS20Y Index', pd.to_datetime('2001-07-31'), pd.to_datetime('2001-07-31')] = np.NaN\n",
    "### 88 : PTCCI Index frequency rerceating:\n",
    "ser_history_raw.loc['PTCCI Index', pd.to_datetime('2004-05-31'), pd.to_datetime('2004-05-31')] = np.NaN\n",
    "### 89 : PTCPHAMM Index frequency rerceating:\n",
    "ser_history_raw.loc['PTCPHAMM Index', pd.to_datetime('2019-01-31'), pd.to_datetime('2019-01-31')] = np.NaN\n",
    "### 90 : UKCCI Index frequency rerceating:\n",
    "ser_history_raw.loc['UKCCI Index', pd.to_datetime('1995-05-31'), pd.to_datetime('1995-05-31')] = np.NaN\n",
    "### Index sorting:\n",
    "ser_history_raw = ser_history_raw.sort_index()\n",
    "### Results saving:\n",
    "ser_history_raw.to_hdf(str_path_bb_idx_hdf, key = str_key_raw_history, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: FREQUENCY CONTROL\n",
    "\n",
    "ser_history_raw = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_history)\n",
    "ser_idx_test = ser_history_raw.to_frame().reset_index('Data_Date')['Data_Date'].squeeze().reset_index('Observation_Date', drop = True)\n",
    "ser_idx_test = ser_idx_test.groupby('Index_Name', group_keys = False).apply(lambda ser_ticker: ser_ticker.drop_duplicates())\n",
    "df_idx_test = ser_idx_test.groupby('Index_Name').apply(lambda ser_ticker: (ser_ticker - ser_ticker.shift()).dt.days).groupby('Index_Name').agg(['min', 'max', 'median'])\n",
    "df_idx_test['delta'] = df_idx_test['max'] - df_idx_test['min']\n",
    "df_idx_test.sort_values('delta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: HISTORY REINDEXATION TO OBSERVATION DATE BUSINESS DAILY MATRIX STRUCTURE (PRELIMINARY ACTIONS)\n",
    "\n",
    "### Data loading:\n",
    "ser_history_raw = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_raw_history)\n",
    "### Moving all dates to tne nearest Business day:\n",
    "df_history_raw = ser_history_raw.to_frame().reset_index('Observation_Date')\n",
    "df_history_raw['Observation_Date'] = df_history_raw['Observation_Date'] + 0 * pd.offsets.BDay()\n",
    "ser_history_bday = df_history_raw.set_index('Observation_Date', append = True).squeeze()\n",
    "ser_history_bday = ser_history_bday.loc[All, All, idx_date_range]\n",
    "### Data adding to hdf collection:\n",
    "ser_history_bday.to_hdf(str_path_bb_idx_hdf, key = str_key_bday_history, mode = 'a') ### BDay moved observation dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: HISTORY DATA DIAGONAL EXTRACTION\n",
    "\n",
    "def get_diagonal(ser_name, idx_date_range):\n",
    "    ### Defining triangle extraction:\n",
    "    def triangle_filter(ser_date):\n",
    "        date_diag = ser_date.index.get_level_values('Data_Date')[0]\n",
    "        ser_result = ser_date.droplevel('Data_Date')\n",
    "        ser_result = ser_result[ser_result.index >= date_diag] \n",
    "        return ser_result    \n",
    "    ### EI name extracting:\n",
    "    str_index_name = ser_name.index.get_level_values(0)[0]\n",
    "    ### Observation dates reindexation:    \n",
    "    print(ser_name.index.get_level_values(0)[0], ': Reindexation')    \n",
    "    idx_observation_range = ser_name.index.get_level_values('Observation_Date').dropna().unique().intersection(idx_date_range)\n",
    "    ser_full = ser_name.droplevel('Index_Name').unstack('Data_Date').reindex(idx_observation_range).stack('Data_Date', dropna = False).squeeze()\n",
    "    ser_full = ser_full.swaplevel()\n",
    "    ser_full.index.rename('Observation_Date', level = -1, inplace = True)    \n",
    "    ### Forward filling for each data date:\n",
    "    ser_full = ser_full.groupby('Data_Date').ffill()   \n",
    "    ### Diagonalization:\n",
    "    ser_triangle = ser_full.groupby('Data_Date').apply(triangle_filter).sort_index()\n",
    "    ### Creating future diagonal vector:\n",
    "    ser_iter_diag = pd.Series(np.NaN, idx_observation_range)\n",
    "    ### Looping over unique dates:\n",
    "    for iter_date in idx_observation_range:\n",
    "        ### Trying to get the latest data date observation:\n",
    "        try:\n",
    "            ser_iter_diag[iter_date] = ser_name.droplevel('Index_Name').loc[All, iter_date].dropna()[-1]\n",
    "        except:\n",
    "            pass\n",
    "    ser_iter_diag = ser_iter_diag.reindex(idx_date_range).ffill()\n",
    "    ### Results output:\n",
    "    return ser_iter_diag\n",
    "### Economic Indices vector loading:\n",
    "ser_history_bday = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_bday_history)\n",
    "### Data transforming:\n",
    "ser_diagonal_raw = ser_history_bday.groupby('Index_Name').apply(get_diagonal, idx_date_range)    \n",
    "### Saving results to hdf file:\n",
    "ser_diagonal_raw.to_hdf(str_path_bb_diag_hdf, key = str_key_diag_daily_raw, mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: GROUPING RESEARCH TO SELECT BASIS TICKERS\n",
    "\n",
    "### Defining minimum Data Date for each ticker:\n",
    "ser_history_bday = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_bday_history)\n",
    "ser_min_date = ser_history_bday.to_frame().reset_index('Data_Date').drop(0, axis = 1).reset_index('Observation_Date', drop = True).squeeze().groupby('Index_Name').agg(min)\n",
    "ser_min_date.name = 'Min_Date'\n",
    "### Creating description table for all types combinations:\n",
    "df_flags = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_flags)\n",
    "### Defining description table generator:\n",
    "def type_analyzer(df_type):\n",
    "    ### Type tickers distribution by region calculating:\n",
    "    ser_region_distribution = df_type.groupby('Region').apply(lambda df_region: len(df_region.index))\n",
    "    ser_region_distribution['All'] = len(df_type.index)\n",
    "    ser_region_distribution = pd.concat([ser_region_distribution], keys = ['Regional_Presence'], names = ['Info'])\n",
    "    ### Adding information about basis ticker for each type:\n",
    "    ser_region_distribution['Oldest_Ticker_Info', 'Oldest_Name'] = df_type['Min_Date'].idxmin()\n",
    "    ser_region_distribution['Oldest_Ticker_Info', 'Region'] = df_type.loc[df_type['Min_Date'].idxmin(), 'Region']\n",
    "    ser_region_distribution['Oldest_Ticker_Info', 'Min_Date'] = df_type.loc[df_type['Min_Date'].idxmin(), 'Min_Date']    \n",
    "    ser_region_distribution['Oldest_Ticker_Info', 'Description'] = df_type.loc[df_type['Min_Date'].idxmin(), 'Description']     \n",
    "    ### Results output:\n",
    "    return ser_region_distribution\n",
    "### Extending flags table and dropping tickers, that were cleared earlier:\n",
    "df_flags_typed = pd.concat([df_flags, ser_min_date], axis = 1, sort = False)\n",
    "df_flags_typed = df_flags_typed.loc[df_flags_typed['Min_Date'].notna()]\n",
    "### Description table generating:\n",
    "df_type_info = df_flags_typed.groupby(['Type_Prime', 'Type_Second']).apply(type_analyzer)\n",
    "df_type_info = df_type_info.unstack(['Info', 'Region'], fill_value = 0)\n",
    "df_type_info.columns.names = ['', '']\n",
    "### Binding each ticker with it's basis:\n",
    "df_oldest_ticker = df_type_info[[('Oldest_Ticker_Info', 'Oldest_Name'), ('Oldest_Ticker_Info', 'Min_Date')]]\n",
    "df_oldest_ticker.columns = ['Basic_Ticker', 'Basic_Min_Date']\n",
    "df_flags_typed = df_flags_typed.join(df_oldest_ticker, how = 'left', on = ['Type_Prime', 'Type_Second'])\n",
    "### Clearing basis ticker for basis tickers and resort values to guarantee basis tickers priority:\n",
    "df_flags_typed.loc[df_flags_typed.index == df_flags_typed['Basic_Ticker'], 'Basic_Ticker'] = ''\n",
    "df_flags_typed = df_flags_typed.groupby('Basic_Ticker', group_keys = False).apply(lambda df_group: df_group.sort_index())\n",
    "### Adding flag to define if we need to rebase ticker:\n",
    "df_flags_typed['Rebase_Flag'] = True\n",
    "df_flags_typed.loc[df_flags_typed['Basic_Ticker'] == '', 'Rebase_Flag'] = False\n",
    "df_flags_typed.loc[df_flags_typed['Min_Date'] < (datetime_basis - pd.DateOffset(years = int_not_to_rebase_term)), 'Rebase_Flag'] = False\n",
    "df_flags_typed.loc[df_flags_typed['Min_Date'] < (df_flags_typed['Basic_Min_Date'] - pd.DateOffset(years = int_not_to_rebase_diff)), 'Rebase_Flag'] = False\n",
    "df_flags_typed.loc[df_flags_typed['Basic_Min_Date'] > datetime_basis, 'Rebase_Flag'] = False\n",
    "#df_type_info.to_excel('Data_Files/Test_Files/Ticker_Info.xlsx', merge_cells = True)\n",
    "df_flags_typed.to_excel('Data_Files/Test_Files/Ticker_Info.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEBCI Index : Reindexation\n",
      "BEBCI Index : Transformation to YoY series: Debasing\n",
      "BEBCI Index : Z-scoring across the observation dates\n",
      "EUSCEMU Index : Reindexation\n",
      "EUSCEMU Index : Transformation to YoY series: Debasing\n",
      "EUSCEMU Index : Z-scoring across the observation dates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index_Name\n",
       "BEBCI Index      None\n",
       "EUSCEMU Index    None\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RUN TO RE-EXPORT DATA: HISTORY DATA TRANSFORMATION (REINDEXATION, FLAGS IMPLEMENTATION, Z-SCORING)\n",
    "\n",
    "### Defining Economic Index series transformation:\n",
    "def complex_transform(ser_name, idx_date_range, df_flags, int_max_name_length, int_min_years, bool_perform_sa = False):\n",
    "    ### Local constants definition:\n",
    "    dict_interval = {}\n",
    "    dict_interval['Q'] = 4\n",
    "    dict_interval['M'] = 12\n",
    "    dict_interval['W'] = 52      \n",
    "    ### Defining triangle extraction:\n",
    "    def triangle_filter(ser_date):\n",
    "        ### Extracting particular Data Date:\n",
    "        date_diag = ser_date.index.get_level_values('Data_Date')[0]\n",
    "        ### Dropping constant level:\n",
    "        ser_result = ser_date.droplevel('Data_Date')\n",
    "        ### Filtering over-diagonal values:\n",
    "        ser_result = ser_result[ser_result.index >= date_diag] \n",
    "        ### Results output:\n",
    "        return ser_result\n",
    "    ### Year-over-year-percent ticker values transforming to stock-like series:\n",
    "    def yoy_to_level(ser_date, int_step):\n",
    "        ### Dropping constant level:\n",
    "        ser_result = ser_date.droplevel('Observation_Date')\n",
    "        ### Basis initiating:\n",
    "        flo_basement = 1.0\n",
    "        ### Factor initiating: \n",
    "        flo_next_brick  = 1.0\n",
    "        ### Looping over month numbers:\n",
    "        for iter_period in range(min(int_step, len(ser_result.index))):         \n",
    "            ### Basement building up:\n",
    "            flo_basement = flo_basement * flo_next_brick\n",
    "            ### Next basement brick producing:\n",
    "            flo_next_brick = ((flo_next_brick ** (iter_period)) * (ser_result.iloc[iter_period] ** (1 / int_step))) ** (1 / (iter_period + 1)) \n",
    "            ### Jumping cumulative product performing:\n",
    "            idx_iter_data = ser_result.index[iter_period :: int_step]\n",
    "            ser_result.loc[idx_iter_data] = ser_result.loc[idx_iter_data].cumprod() * flo_basement       \n",
    "        ### Results output:            \n",
    "        return ser_result    \n",
    "    ### X13 ARIMA Seasonality adjustment model:\n",
    "    def perform_x13_sa(ser_date):\n",
    "        ### Dropping constant level:        \n",
    "        ser_result = ser_date.droplevel('Observation_Date')\n",
    "        ### Check for minimal quantity of observations to z-score:\n",
    "        if (ser_result.last_valid_index() - ser_result.first_valid_index()).days >= (int_min_years * 365):   \n",
    "            ### Naming series for x13 performing:\n",
    "            ser_result.name = 'Ticker'\n",
    "            ### Calculating shift value to make all series positive:\n",
    "            flo_positron = abs(ser_result.min()) * 2\n",
    "            try:\n",
    "                ### Performing seasonality adjustment:\n",
    "                ser_result = x13_arima_analysis(ser_result + flo_positron, outlier = True, trading = True).seasadj - flo_positron\n",
    "                print('SA success : ', ser_date.index.get_level_values('Observation_Date')[0])                 \n",
    "            except:\n",
    "                print('SA error : ', ser_date.index.get_level_values('Observation_Date')[0]) \n",
    "        ### Results output:                \n",
    "        return ser_result \n",
    "    ### Extracting Observation Date column for ticker:\n",
    "    def get_obs_date_vector(str_ticker, str_date, bool_exact_date = False, bool_drop_levels = False):\n",
    "        ### Vector for exact date:\n",
    "        if bool_exact_date:\n",
    "            ser_obs_date = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == str_ticker & Observation_Date == str_date')\n",
    "        ### Vector for nearest date:        \n",
    "        else:\n",
    "            ### Loading full ticker series:        \n",
    "            ser_z_scored = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == str_ticker & Observation_Date <= str_date')\n",
    "            ### Extracting data for max date less or equal to needed date:\n",
    "            ser_obs_date = ser_z_scored.loc[All, All, [ser_z_scored.index.levels[-1].max()]]\n",
    "        ### Dropping constant index levels if needed:\n",
    "        if bool_drop_levels:\n",
    "            return ser_obs_date.droplevel(['Index_Name', 'Observation_Date'])\n",
    "        else:\n",
    "            return ser_obs_date    \n",
    "    ### Defining time-vector z-scoring procedure:    \n",
    "    def by_date_z_score(ser_date, int_winsorize_bound, flo_winsorize_tolerance, int_winsorize_steps_limit, int_min_years, str_basis_index):\n",
    "        ### Check for empty vector (doing nothing):\n",
    "        if ser_date.count():\n",
    "            ### Check for non-constant vector:\n",
    "            if (ser_date.std() > flo_winsorize_tolerance):\n",
    "                ### Check for minimal quantity of observations to z-score:\n",
    "                if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years * 365):   \n",
    "                    ### Calculating of z scores:\n",
    "                    ser_date = (ser_date - ser_date.mean()) / ser_date.std()        \n",
    "                    bool_to_winsor = True   \n",
    "                    int_iter = 1\n",
    "                    while (bool_to_winsor): \n",
    "                        int_iter += 1                \n",
    "                        ### Value based winsorization:                \n",
    "                        ser_date.clip(lower = -int_winsorize_bound, upper = int_winsorize_bound, inplace = True)\n",
    "                        ### Recalculating of z scores:\n",
    "                        ser_date = (ser_date - ser_date.mean()) / ser_date.std()\n",
    "                        ### Checking for boundaries and steps:\n",
    "                        if((ser_date.loc[ser_date.abs() >= (int_winsorize_bound + flo_winsorize_tolerance)].count() == 0) | (int_iter > int_winsorize_steps_limit)):\n",
    "                            bool_to_winsor = False\n",
    "                    ### Checking for basis index:\n",
    "                    if str_basis_index:\n",
    "                        ### Extracting column from z-scored basis ticker series:\n",
    "                        str_obs_date = ser_date.index[0][1].strftime('%Y-%m-%d')\n",
    "                        ser_basis_date = get_obs_date_vector(str_basis_index, str_obs_date, bool_exact_date = False, bool_drop_levels = True)\n",
    "                        ser_date = ser_date * ser_basis_date.std() + ser_basis_date.mean()\n",
    "                else:\n",
    "                    ### Killing values that we can't z-score\n",
    "                    ser_date.loc[All] = np.NaN\n",
    "            else:\n",
    "                ### Check for minimal quantity of observations to z-score:\n",
    "                if (ser_date.last_valid_index()[0] - ser_date.first_valid_index()[0]).days >= (int_min_years * 365):             \n",
    "                    ### Constant values demeaning:\n",
    "                    ser_date = ser_date - ser_date.mean()\n",
    "                    ### Checking for basis index:\n",
    "                    if str_basis_index:\n",
    "                        ### Extacting column from z-scored basis ticker series:\n",
    "                        str_obs_date = ser_date.index[0][1].strftime('%Y-%m-%d')\n",
    "                        ser_basis_date = get_obs_date_vector(str_basis_index, str_obs_date, bool_exact_date = False, bool_drop_levels = True)\n",
    "                        ser_date = ser_date * ser_basis_date.std() + ser_basis_date.mean()                    \n",
    "                else:\n",
    "                    ### Killing values that we can't z-score\n",
    "                    ser_date.loc[All] = np.NaN\n",
    "        ### Memory optimization:\n",
    "        ser_date = ser_date.astype('float32')\n",
    "        return ser_date    \n",
    "    ### EI name extracting:\n",
    "    str_index_name = ser_name.index.get_level_values(0)[0]\n",
    "    ### Observation dates reindexation:    \n",
    "    print(ser_name.index.get_level_values(0)[0], ': Reindexation')    \n",
    "    idx_observation_range = ser_name.index.get_level_values('Observation_Date').unique().intersection(idx_date_range).sort_values()\n",
    "    ser_full = ser_name.droplevel('Index_Name').unstack('Data_Date').reindex(idx_observation_range).stack('Data_Date', dropna = False).squeeze()\n",
    "    ser_full = ser_full.swaplevel()\n",
    "    ser_full.index.rename('Observation_Date', level = -1, inplace = True)    \n",
    "    ### Forward filling for each data date:\n",
    "    ser_full = ser_full.groupby('Data_Date').ffill()   \n",
    "    ### Diagonalization:\n",
    "    ser_triangle = ser_full.groupby('Data_Date').apply(triangle_filter).sort_index()\n",
    "    ### Flags extracting:\n",
    "    ser_flags = df_flags.loc[str_index_name, All].squeeze() \n",
    "    ### 'TAR' type checking:\n",
    "    if (ser_flags['Type_Prime'] == 'TAR'):\n",
    "        print(str_index_name, ': TAR Primary Type ignoring')        \n",
    "        pass\n",
    "    ### Flags-based transforming:\n",
    "    else:\n",
    "        ### Indices of NA values collecting:\n",
    "        idx_isna = ser_triangle.loc[ser_triangle.isna()].index\n",
    "        ### Transforming to stock-like series:\n",
    "        if (ser_flags['Processing'] in ['Index', 'Level', 'Level%']):\n",
    "            ser_stock = ser_triangle\n",
    "        elif (ser_flags['Processing'] == 'Flow'):\n",
    "            print(str_index_name, ': Transformation to stock-like series: Cumulative sum')\n",
    "            ### Filling empty values:\n",
    "            ser_triangle = ser_triangle.fillna(0)\n",
    "            ### Cumulative sum for each observation date calculating:\n",
    "            ser_stock = ser_triangle.groupby('Observation_Date').cumsum()\n",
    "            ### Dropping NA values:\n",
    "            ser_stock.loc[idx_isna] = np.NaN\n",
    "        else:\n",
    "            print(str_index_name, ': Transformation to stock-like series: Cumulative product')\n",
    "            ### Filling empty values:\n",
    "            ser_triangle = ser_triangle.fillna(0)\n",
    "            ### Percents to multipliers converting:\n",
    "            ser_stock = 1 + ser_triangle / 100\n",
    "            ### Calculating with needed periodicity:\n",
    "            if (ser_flags['Frequency'] == 'Monthly'):\n",
    "                int_step = dict_cumprod_step[ser_flags['Processing']]\n",
    "                ### Year-by-year cumprod with rebasing:\n",
    "                ser_stock = ser_stock.groupby('Observation_Date').apply(yoy_to_level, int_step).swaplevel().sort_index()\n",
    "            ### Dropping NA values:\n",
    "            ser_stock.loc[idx_isna] = np.NaN\n",
    "        ### Seasonality adjustment testing:\n",
    "        if bool_perform_sa:\n",
    "            print(str_index_name, ': Seasonality adjustment')            \n",
    "            ### Filling empty values:            \n",
    "            ser_stock = ser_stock.groupby('Observation_Date').ffill()\n",
    "            ser_stock = ser_stock.groupby('Observation_Date').apply(perform_x13_sa).swaplevel().sort_index()\n",
    "            ### Dropping NA values:\n",
    "            ser_stock.loc[idx_isna] = np.NaN                        \n",
    "        ### Transforming to YoY series:\n",
    "        if (ser_flags['Processing'] == 'Index'):\n",
    "            ### Debasing only:\n",
    "            print(str_index_name, ': Transformation to YoY series: Debasing')            \n",
    "            ser_yoy = ser_stock - ser_flags['Base']\n",
    "        elif (ser_flags['Processing'] in ['Flow', 'Level']):    \n",
    "            ### Simple difference:\n",
    "            print(str_index_name, ': Transformation to YoY series: Simple difference')\n",
    "            ### Shifting lag defining:\n",
    "            if (ser_flags['Frequency'] in dict_yoy_shift.keys()):\n",
    "                int_yoy_shift = dict_yoy_shift[ser_flags['Frequency']]\n",
    "            else:\n",
    "                int_yoy_shift = dict_yoy_shift['Other']            \n",
    "            ### Stock-like series differing:\n",
    "            ser_yoy = ser_stock.groupby('Observation_Date', group_keys = False).apply(lambda ser_obs_date: ser_obs_date - ser_obs_date.shift(int_yoy_shift))\n",
    "        else:      \n",
    "            ### Difference with dividing:\n",
    "            print(str_index_name, ': Transformation to YoY series: Difference with dividing')\n",
    "            ### Shifting lag defining:\n",
    "            if (ser_flags['Frequency'] in dict_yoy_shift.keys()):\n",
    "                int_yoy_shift = dict_yoy_shift[ser_flags['Frequency']]\n",
    "            else:\n",
    "                int_yoy_shift = dict_yoy_shift['Other']\n",
    "            ### Stock-like series differing:\n",
    "            ser_yoy = ser_stock.groupby('Observation_Date', group_keys = False)\\\n",
    "                               .apply(lambda ser_obs_date: (ser_obs_date / ser_obs_date.shift(int_yoy_shift) - 1))  \n",
    "        ser_yoy.name = 'YoY'\n",
    "        ### Z-scoring across the observation dates:\n",
    "        print(ser_name.index.get_level_values(0)[0], ': Z-scoring across the observation dates')\n",
    "        ser_yoy_z = ser_yoy.groupby('Observation_Date').apply(by_date_z_score, int_winsorize_bound, flo_winsorize_tolerance, int_winsorize_steps_limit, int_min_years,\n",
    "                                                              ser_flags['Basic_Ticker'])\n",
    "        ### Adding results to matrix cube:\n",
    "        pd.concat([ser_yoy_z], keys = [str_index_name], names = ['Index_Name']).to_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, format = 'table',\n",
    "                                                                                       complevel = 9, append = True, mode = 'a',\n",
    "                                                                                       min_itemsize = {'Index_Name': int_max_name_length})    \n",
    "#    ### Results output:\n",
    "#    return pd.concat([ser_yoy_z], keys = [str_index_name], names = ['Index_Name'])\n",
    "\n",
    "### Economic Indices vector loading:\n",
    "ser_history_bday = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_bday_history)\n",
    "ser_history_bday = ser_history_bday.reindex(df_flags_typed.index, level = 'Index_Name')\n",
    "### Testing:\n",
    "if os.path.isfile(str_path_bb_matrix_hdf):\n",
    "    os.remove(str_path_bb_matrix_hdf)\n",
    "ser_history_bday_test = ser_history_bday.loc[ser_history_bday.index.get_level_values('Index_Name').unique()[[0, 21]], All, All]\n",
    "### Maximum length calculating (for HDF manipulations):\n",
    "int_max_name_length = max(ser_history_bday.index.levels[0].str.len())\n",
    "### Data transforming:\n",
    "ser_history_bday_test.groupby('Index_Name').apply(complex_transform, idx_date_range, df_flags_typed, int_max_name_length, int_min_years)\n",
    "#set_res_test = ser_history_bday_test.groupby('Index_Name', group_keys = False).apply(complex_transform, idx_date_range, df_flags, int_max_name_length, int_min_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP: RESULTS CHECK\n",
    "\n",
    "#ser_history_bday_test.to_excel('Data_Files/Test_Files/Processing_test.xlsx', merge_cells = False)\n",
    "#set_res_test.to_excel('Data_Files/Test_Files/Processing_test.xlsx', merge_cells = False)\n",
    "#import tables\n",
    "#tables.file._open_files.close_all()\n",
    "#pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z).to_excel('Data_Files/Test_Files/Processing_test.xlsx', merge_cells = False)\n",
    "ser_history_bday.loc['UKMSVTVJ Index', All, All]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING OBSERVATION DATE VECTOR EXTRACTION\n",
    "\n",
    "def get_obs_date_vector(str_ticker, str_date, bool_exact_date = False, bool_drop_levels = False):\n",
    "    ### Vector for exact date:\n",
    "    if bool_exact_date:\n",
    "        ser_obs_date = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == str_ticker & Observation_Date == str_date')\n",
    "    ### Vector for nearest date:        \n",
    "    else:\n",
    "        ### Loading full ticker series:        \n",
    "        ser_z_scored = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == str_ticker & Observation_Date <= str_date')\n",
    "        ### Extracting data for max date less or equal to needed date:\n",
    "        ser_obs_date = ser_z_scored.loc[All, All, [ser_z_scored.index.levels[-1].max()]]\n",
    "    ### Dropping constant index levels if needed:\n",
    "    if bool_drop_levels:\n",
    "        return ser_obs_date.droplevel(['Index_Name', 'Observation_Date'])\n",
    "    else:\n",
    "        return ser_obs_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO RE-EXPORT DATA: Z_SCORED EI DIAGONAL CONSTRUCTING:\n",
    "\n",
    "### Creating container for tickers diagonals:\n",
    "dict_ei_diag = {}\n",
    "### Economic Indices vector loading:\n",
    "ser_history_bday = pd.read_hdf(str_path_bb_idx_hdf, key = str_key_bday_history)\n",
    "### Tickers list preparing:\n",
    "idx_ticker_list = ser_history_bday.index.levels[0]\n",
    "### Looping over tickers:\n",
    "for iter_ticker in idx_ticker_list:\n",
    "    print(iter_ticker)\n",
    "    ### Loading matrix for each ticker:\n",
    "    ser_iter_matrix = pd.read_hdf(str_path_bb_matrix_hdf, key = str_key_matrix_z, where = 'Index_Name == iter_ticker').droplevel('Index_Name')\n",
    "    ### Extracting unique observation dates for each ticker later than diagonal start date:\n",
    "    idx_date_list = ser_iter_matrix.loc[All, date_diag_start : ].index.dropna().get_level_values('Observation_Date').unique()\n",
    "    ### Creating future diagonal vector:\n",
    "    ser_iter_diag = pd.Series(np.NaN, idx_date_list)    \n",
    "    ### Determining first valid date and first date to place z-scoring results on diagonal:\n",
    "    ### Looping over unique dates:\n",
    "    for iter_date in idx_date_list:\n",
    "        ### Trying to get the latest data date observation:\n",
    "        try:\n",
    "            ser_iter_diag[iter_date] = ser_iter_matrix.loc[All, iter_date].dropna()[-1]\n",
    "        except:\n",
    "            pass\n",
    "    ### Checking for data earlier than diagonal start date:\n",
    "    if ((ser_iter_matrix.index.get_level_values('Observation_Date').unique().min() < date_diag_start) & pd.notna(ser_iter_diag.values[0])):\n",
    "        ### Turning diagonal start date column to diagonal with data dates as index dates:\n",
    "        date_before_diag = ser_iter_matrix.index.levels[-1][list(ser_iter_matrix.index.levels[-1]).index(idx_date_list[0]) - 1]\n",
    "        ### Ticker values on the date to turn:\n",
    "        list_iter_values_to_turn = ser_iter_matrix.loc[ : idx_date_list[0] - pd.offsets.Day(1), [idx_date_list[0]]].values\n",
    "        ### First appearance dates extracting:\n",
    "        ser_iter_release = ser_iter_matrix.loc[ : idx_date_list[0] - pd.offsets.Day(1), :].groupby('Data_Date')\\\n",
    "                                                         .apply(lambda ser_date: ser_date.droplevel('Data_Date').first_valid_index())\n",
    "        ### Implementing lag before the announcement for the first valid observation date (3 years of data dates with equal first valid index):\n",
    "        date_second_obs = pd.to_datetime(ser_iter_release.loc[ser_iter_release > ser_iter_release.min()].values[0])\n",
    "        ins_second_obs_lag = (date_second_obs - ser_iter_release.loc[ser_iter_release > ser_iter_release.min()].index[0]).days\n",
    "        ser_iter_release.loc[ser_iter_release < date_second_obs] = ser_iter_release.loc[ser_iter_release < date_second_obs].index + pd.offsets.Day(ins_second_obs_lag)\n",
    "        ### Taking announcement dates as an index for firsat column values:\n",
    "        list_iter_index_to_turn = ser_iter_release.values\n",
    "        ### Modified column to turn:\n",
    "        ser_iter_to_turn = pd.Series(list_iter_values_to_turn, index = list_iter_index_to_turn)\n",
    "        ### Dropping repeated dates and empty dates:\n",
    "        ser_iter_to_turn = ser_iter_to_turn.groupby(level = 0).apply(lambda ser_date: ser_date[-1]).dropna()\n",
    "        ### Cutting series not to intersect with diagonal index:\n",
    "        ser_iter_to_turn = ser_iter_to_turn.loc[ : idx_date_list[0] - pd.offsets.Day(1)]\n",
    "        ### Joining series:\n",
    "        ser_iter_diag = pd.concat([ser_iter_to_turn, ser_iter_diag], axis = 0).sort_index() \n",
    "    ### Reindexation to business daily vector and forward filling:\n",
    "    ser_iter_diag = ser_iter_diag.ffill().reindex(idx_date_range).ffill()\n",
    "    ### Saving ticker diagonal to the container:\n",
    "    dict_ei_diag[iter_ticker] = ser_iter_diag\n",
    "### Aggregating ticker diagonals:\n",
    "ser_diagonal_z = pd.concat(dict_ei_diag, axis = 0)\n",
    "ser_diagonal_z.index.names = ['Index_Name', 'Date']\n",
    "ser_diagonal_z.name = 'EI_diagonal'\n",
    "### Saving results to hdf file:\n",
    "ser_diagonal_z.to_hdf(str_path_bb_diag_hdf, key = str_key_diag_daily_z, mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: DIAGONALS PLOTTING\n",
    "\n",
    "str_test_ticker = 'DGNOCHNG Index'\n",
    "ser_diagonal = pd.read_hdf(str_path_bb_diag_hdf, key = str_key_diag_daily_z)\n",
    "#ser_diagonal = pd.read_hdf(str_path_bb_diag_hdf, key = str_key_diag_daily_raw)\n",
    "ser_eco_ind = ser_diagonal.loc[str_test_ticker, All].droplevel(0).dropna()\n",
    "date_xlim_start = ser_eco_ind.index.min()\n",
    "date_xlim_finish = ser_eco_ind.index.max()\n",
    "flo_ylim_min = ser_eco_ind.loc[date_xlim_start : date_xlim_finish].min()\n",
    "flo_ylim_max = ser_eco_ind.loc[date_xlim_start : date_xlim_finish].max()\n",
    "tup_ylim = (flo_ylim_min - abs(flo_ylim_min) / 10, flo_ylim_max + abs(flo_ylim_max) / 10)\n",
    "ax_eco_ind = ser_eco_ind.plot(figsize = (20, 7.5), title = str_test_ticker + ' diagonal', xlim = (date_xlim_start, date_xlim_finish), ylim = tup_ylim, x_compat = True)\n",
    "ax_eco_ind.xaxis.set_major_locator(mdates.YearLocator(base = 1))                             \n",
    "ax_eco_ind.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))                \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN TO TESTING: DIAGONALS EXPORT TO MS EXCEL\n",
    "\n",
    "ser_diagonal_z = pd.read_hdf(str_path_bb_diag_hdf, key = str_key_diag_daily_z)\n",
    "ser_diagonal_z.unstack('Index_Name').to_excel('Data_Files/Test_Files/Diag_Eco_Indices_z.xlsx', merge_cells = False)\n",
    "ser_diagonal_raw = pd.read_hdf(str_path_bb_diag_hdf, key = str_key_diag_daily_raw)\n",
    "ser_diagonal_raw.unstack('Index_Name').to_excel('Data_Files/Test_Files/Diag_Eco_Indices_raw.xlsx', merge_cells = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
