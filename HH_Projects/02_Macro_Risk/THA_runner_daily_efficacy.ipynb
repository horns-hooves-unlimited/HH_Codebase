{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THA DAILY MODES COMPARING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULES IMPORT\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL PARAMETERS INITIALIZATION\n",
    "\n",
    "### Common constants:\n",
    "All = slice(None)\n",
    "### Data loading paths:\n",
    "str_path_bb_hdf = 'Data_Files/Source_Files/Bloomberg_prepared.h5'\n",
    "str_key_ret_monthly = 'bb_ret_monthly'\n",
    "str_key_mcap = 'bb_mcap'\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(io = str_path_universe, sheet_name = 0, header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index()    \n",
    "    ### Results output:\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEASURE STATISTICS CALCULATOR\n",
    "\n",
    "def measure_stats(df_measures, arr_back_period = [99]):\n",
    "    ### Declaring local constants & variables:\n",
    "    dict_stats = {}\n",
    "    ### Stats calculating:\n",
    "    for iter_measure in df_measures.columns:\n",
    "        dict_period = {}\n",
    "        for iter_back_period in arr_back_period:\n",
    "            ser_iter_measure = df_measures[iter_measure].dropna()\n",
    "            idx_iter_range = pd.date_range(end = ser_iter_measure.index[-1], periods = iter_back_period * 12, freq = 'BM')\n",
    "            ser_iter_measure = ser_iter_measure[idx_iter_range]            \n",
    "            ser_iter_stats = pd.Series()\n",
    "            ser_iter_stats['count'] = ser_iter_measure.count()\n",
    "            ser_iter_stats['min'] = ser_iter_measure.min()\n",
    "            ser_iter_stats['max'] = ser_iter_measure.max()        \n",
    "            ser_iter_stats['mean'] = ser_iter_measure.mean()\n",
    "            ser_iter_stats['std'] = ser_iter_measure.std()\n",
    "            ser_iter_stats['median'] = ser_iter_measure.median()        \n",
    "            ser_iter_stats['perc_25'] = ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['perc_75'] = ser_iter_measure.quantile(0.75, 'midpoint')\n",
    "            ser_iter_stats['iq_range'] = ser_iter_measure.quantile(0.75, 'midpoint') - ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['mean_abs'] = ser_iter_measure.abs().mean()\n",
    "            ser_iter_stats['t_stat'] = (ser_iter_measure.mean() / ser_iter_measure.std()) * np.sqrt(ser_iter_measure.count())  \n",
    "            dict_period[iter_back_period] = ser_iter_stats\n",
    "        dict_stats[iter_measure] = pd.concat(dict_period, axis = 1)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_stats, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EFFICACY MEASURES FOR SINGLE FACTOR\n",
    "\n",
    "def single_factor_multiple_efficacy_measures(ser_factor, ser_return, ser_weight, arr_measure, return_shift = 0, arr_truncate = [2.5, 2.0]):\n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    dict_measure = {}\n",
    "    set_std_needed = {'fmb_std_eqw', 'fmb_std_weighted'}\n",
    "    num_precision = 5 # For quintile bins rounding and borders controlling\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining MatLab style prctile function:\n",
    "    def prctile(ser_to_perc, p):\n",
    "        ### Sorted list preparing:\n",
    "        list_to_perc = ser_to_perc.dropna().values\n",
    "        list_sorted = np.sort(list_to_perc)\n",
    "        ### Length calculating:\n",
    "        num_len = len(list_to_perc)    \n",
    "        ### Prctile calculating:\n",
    "        num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "        ### Results output:\n",
    "        return num_result\n",
    "    ### Defining quintile bins distribution:\n",
    "    def quintile_distribution(ser_iter_group, num_bins = 5, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True):\n",
    "        ### Bins preparing:\n",
    "        list_bin = list(np.arange(0, 100, 100 / num_bins))[1 : ]\n",
    "        list_bin = [round(iter_element / 100, 2) for iter_element in list_bin]\n",
    "        list_bin = [prctile(ser_iter_group, iter_element) for iter_element in list_bin]\n",
    "        list_bin = [ser_iter_group.min() - abs(ser_iter_group.min())] + list_bin + [ser_iter_group.max() + abs(ser_iter_group.max())]\n",
    "        list_bin = [round(iter_element, num_precision) for iter_element in list_bin]\n",
    "        if bool_populate_last:\n",
    "            list_bin[-2] -= 10 ** (-num_precision)\n",
    "        ### Bins distribution:\n",
    "        ser_iter_distribution = pd.cut(ser_iter_group, bins = list_bin, labels = range(num_bins), \n",
    "                                       right = bool_right_included, include_lowest = bool_include_lowest, precision = num_precision)\n",
    "        ### Results output:\n",
    "        return ser_iter_distribution    \n",
    "    ### Defining get_measure group level function:\n",
    "    def get_measure(df_to_measure, iter_measure):\n",
    "        ### Checking data sufficiency:\n",
    "        if (len(df_to_measure.dropna().index) > 1):           \n",
    "            ### Measure calculating:\n",
    "            if (iter_measure == 'ic_spearman'):\n",
    "                ### Spearmen information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values\n",
    "                num_result = ss.spearmanr(list_factor, list_return, nan_policy = 'omit').correlation\n",
    "            if (iter_measure == 'ic_pearson'):\n",
    "                ### Pearson information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values                \n",
    "                num_result = ss.pearsonr(list_factor, list_return)[0]\n",
    "            if (iter_measure == 'fmb_eqw'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (equal weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return']].dropna()['Return'].values\n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values\n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_std_eqw'):             \n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()['Return'].values                \n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_std_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]                 \n",
    "            if (iter_measure == 'fmb_std_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values                \n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_std_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]  \n",
    "            if (iter_measure == 'clp'):\n",
    "                ### Constant leverage portfolio signed normalized multiplication sum:                \n",
    "                ser_clp_weighted = df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Factor']\n",
    "                ser_clp_weighted = ser_clp_weighted * df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Weight'].transform(np.sqrt)\n",
    "                ser_clp_weighted.loc[ser_clp_weighted < 0] = -ser_clp_weighted / ser_clp_weighted[ser_clp_weighted < 0].sum()\n",
    "                ser_clp_weighted.loc[ser_clp_weighted > 0] = ser_clp_weighted / ser_clp_weighted[ser_clp_weighted > 0].sum()\n",
    "                num_result = (ser_clp_weighted * df_to_measure['Return']).sum()\n",
    "            if ('qtl' in iter_measure):\n",
    "                ### Interquntile range:\n",
    "                num_bins = int(iter_measure.split('qtl')[1])   \n",
    "                df_to_measure = df_to_measure[['Factor', 'Return', 'Constant']].dropna()\n",
    "                df_to_measure['Return'] = df_to_measure['Return'] - df_to_measure['Return'].mean()\n",
    "                df_to_measure['Factor'] = df_to_measure['Factor'].round(num_precision)\n",
    "                ### Distribution factor values between quintile bins:\n",
    "                ser_qtl_bins = quintile_distribution(df_to_measure['Factor'], num_bins, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True)\n",
    "                ser_qtl_bins.name = 'Bin'\n",
    "                ### Mean return for each bin calculating:\n",
    "                df_to_measure = df_to_measure.join(ser_qtl_bins)\n",
    "                df_qtl_rets = df_to_measure.loc[(All), ['Return', 'Bin']]\n",
    "                df_qtl_rets.set_index('Bin', append = True, inplace = True)\n",
    "                ser_qtl_rets = df_qtl_rets.unstack('Bin').mean(axis = 0).droplevel(0).squeeze()\n",
    "                num_result = ser_qtl_rets.iloc[-1] - ser_qtl_rets.iloc[0]                                 \n",
    "        else:                          \n",
    "            num_result = np.NaN\n",
    "        ### Preparing results: \n",
    "        return num_result\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    if (ser_weight.count() == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ser_weight = ser_weight.reset_index('Market', drop = True)\n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted, ser_weight], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return', 'Weight']\n",
    "    df_to_measure['Constant'] = 1\n",
    "    if set_std_needed.intersection(set(arr_measure)):\n",
    "        ser_factor_std = df_to_measure.dropna()['Factor'].groupby('Date').apply(ison_standartize, arr_truncate = arr_truncate, within_market = False)\n",
    "        df_to_measure['Factor_std'] = ser_factor_std.reindex(df_to_measure.index)\n",
    "    ### Looping efficacy measures for calculating measures timeseries:\n",
    "    for iter_measure in arr_measure:\n",
    "        dict_measure[iter_measure] = df_to_measure.groupby('Date').apply(get_measure, iter_measure = iter_measure)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_measure, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SINGLE EFFICACY MEASURE FOR MULTIPLE FACTORS\n",
    "    \n",
    "def multiple_factor_single_efficacy_measure_stats(df_factors, ser_return, ser_weight, str_measure, num_back_period = 99, \n",
    "                                                  num_horizon = 12, list_region_xmo = ['DM', 'EM', 'FM']): \n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    list_months = [1, 2, 3, 6, 9 ,12]\n",
    "    list_range = [iter_month - 1 for iter_month in list_months if iter_month <= num_horizon]\n",
    "    ### Defining full universe expanding for date:\n",
    "    def universe_reindex(iter_group, idx_universe):\n",
    "        df_iter_result = iter_group.unstack('Date').reindex(idx_universe).sort_index().stack('Date', dropna = False)\n",
    "        ### Results output:\n",
    "        return df_iter_result   \n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result   \n",
    "    ### Defining by date correrlation function:\n",
    "    def corr_by_date(iter_group):\n",
    "        num_iter_corr = iter_group['Corr_factor_minus'].corr(iter_group['Corr_factor_plus'])\n",
    "        ### Results output:\n",
    "        return num_iter_corr       \n",
    "    ### Preparing expanded universe for autocorrelation performing:\n",
    "    df_factors_region = df_factors.loc[(All, All, list_region_xmo), :]\n",
    "    idx_date_range = df_factors_region.index.get_level_values(0).unique()\n",
    "    idx_universe = df_factors_region.index.get_level_values(1).unique()\n",
    "    df_factors_full = df_factors_region.reset_index('Market', drop = True).groupby('Date', group_keys = False).apply(universe_reindex, idx_universe).swaplevel()\n",
    "    ### Factors looping:\n",
    "    dict_factors_measures = {} ### Container for all factor measure stats\n",
    "    dict_factors_vectors = {} ### Container for all factor measure vectors\n",
    "    dict_factors_autocorr = {} ### Container for autocorrelation results\n",
    "    for iter_factor in df_factors.columns:\n",
    "        ### Shifts looping for factors measures stats:\n",
    "        ### Stats calculation:\n",
    "        dict_factor_stats = {} ### Container for iterated factor stats\n",
    "        dict_factor_vectors = {} ### Container for iterated factor measure vectors\n",
    "#        for iter_shift in range(num_horizon):\n",
    "        for iter_shift in list_range:            \n",
    "#            df_factor_filtered = df_factors[iter_factor].loc[All, All, list_region_xmo]\n",
    "            df_factor_filtered = df_factors_region[iter_factor]\n",
    "            df_iter_shift_measure = single_factor_multiple_efficacy_measures(df_factor_filtered, ser_return, ser_weight, [str_measure], iter_shift, list_truncate)\n",
    "            df_iter_shift_stats = measure_stats(df_iter_shift_measure, [num_back_period])\n",
    "            dict_factor_stats[iter_shift] = df_iter_shift_stats.loc[['mean', 't_stat'], (str_measure, num_back_period)]\n",
    "            dict_factor_vectors[iter_shift] = df_iter_shift_measure\n",
    "        ### Aggegating factor measure stats:\n",
    "        df_iter_factor_stats = pd.concat(dict_factor_stats, axis = 1)\n",
    "        df_iter_factor_stats.columns = df_iter_factor_stats.columns + 1        \n",
    "        dict_factors_measures[iter_factor] = df_iter_factor_stats\n",
    "        ### Aggegating factor measure vectors:        \n",
    "        df_iter_factor_vectors = pd.concat(dict_factor_vectors, axis = 1)    \n",
    "        df_iter_factor_vectors.columns = df_iter_factor_vectors.columns.droplevel(1) + 1\n",
    "        dict_factors_vectors[iter_factor] = df_iter_factor_vectors        \n",
    "        ### Autocorrelation calculation:\n",
    "        ser_iter_factor = df_factors_full[iter_factor]\n",
    "        ser_iter_factor_plus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[1 : ]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ser_iter_factor_minus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[: -1]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ### Artificial series combining for indexes synchronization:        \n",
    "        ser_iter_factor_plus_shifted = ser_iter_factor_plus.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, 1)\n",
    "        df_iter_factor_to_corr = pd.concat([ser_iter_factor_minus, ser_iter_factor_plus_shifted], axis = 1)\n",
    "        df_iter_factor_to_corr.columns = ['Corr_factor_minus', 'Corr_factor_plus']\n",
    "        dict_factors_autocorr[iter_factor] = pd.Series(df_iter_factor_to_corr.groupby('Date').apply(corr_by_date).mean(), index = ['autocorr'])\n",
    "    ### Results output:\n",
    "    df_factors_measures_stats = pd.concat(dict_factors_measures, axis = 0)\n",
    "    df_factors_autocorr =  pd.concat(dict_factors_autocorr, axis = 1).transpose()\n",
    "    df_factors_vectors = pd.concat(dict_factors_vectors, axis = 0)\n",
    "#    df_factors_coeff = df_factors_measures_stats.loc[(All, 'mean'), list_months].reset_index(1, drop = True)\n",
    "    df_factors_coeff = df_factors_measures_stats.loc[(All, 'mean'), All].reset_index(1, drop = True)    \n",
    "    df_factors_coeff.columns = [('coeff_' + str(iter_column)) for iter_column in df_factors_coeff.columns]\n",
    "#    df_factors_t_stat = df_factors_measures_stats.loc[(All, 't_stat'), list_months].reset_index(1, drop = True)\n",
    "    df_factors_t_stat = df_factors_measures_stats.loc[(All, 't_stat'), All].reset_index(1, drop = True)    \n",
    "    df_factors_t_stat.columns = [('t_' + str(iter_column)) for iter_column in df_factors_t_stat.columns]\n",
    "    df_factors_result = pd.concat([df_factors_autocorr, df_factors_coeff, df_factors_t_stat], axis = 1)    \n",
    "    return (df_factors_result, df_factors_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST: FACTOR CORRELATION\n",
    "\n",
    "### Aggragated Factor loading:\n",
    "ser_factor_agg = pd.read_csv('Data_Files/Test_Files/acadian_mode_test_factor_agg_ML_like.csv', sep = ';', header = 0, index_col = [0, 1, 2], parse_dates = [0],\n",
    "                                 squeeze = True, keep_default_na = False, na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                                                                       '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'])\n",
    "ser_factor_agg.name = 'Aggregated'\n",
    "### Sources factors loading:\n",
    "ser_factor_source = pd.read_csv('Data_Files/Test_Files/acadian_mode_test_factor_source_ML_like.csv', sep = ';', header = 0, index_col = [0, 1, 2, 3], parse_dates = [1],\n",
    "                                 squeeze = True, keep_default_na = False, na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                                                                       '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'])\n",
    "ser_factor_source.name = 'Source'\n",
    "### Factors uniting:\n",
    "df_factors = ser_factor_source.unstack('Source')\n",
    "df_factors.columns.name = ''\n",
    "df_factors = pd.concat([df_factors, ser_factor_agg], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST: EFFICACY COMPARING PREPARATIONS\n",
    "\n",
    "### Measures parameters:\n",
    "list_measure = ['fmb_weighted'] # Efficacy measures list\n",
    "list_back_period = [99, 10, 5] # Look back periods\n",
    "int_horizon = 12 # Measure stats horizon\n",
    "list_ison = ['DM', 'EM', 'FM'] ### Region filter\n",
    "list_truncate = [2.5, 2.0] # Standartization boundaries\n",
    "bool_within_market = True # Standartization way\n",
    "### ISON universe loading:\n",
    "ser_ison = ison_membership_converting(str_path_universe, '2020-07-31') \n",
    "### Returns loading:\n",
    "ser_returns = pd.read_hdf(str_path_bb_hdf, key = str_key_ret_monthly).loc[['USD'], All, All].droplevel('Currency') ### \n",
    "ser_returns = ser_returns.groupby('Country').shift(periods = -1).to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "ser_returns = ser_returns.loc[All, All, list_ison]\n",
    "### Market caps loading:\n",
    "ser_mcap = pd.read_hdf(str_path_bb_hdf, key = str_key_mcap).loc[All, All, list_ison]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuum__DM_EM_FM\n",
      "Continuum__DM\n",
      "Continuum__EM\n",
      "Continuum__FM\n",
      "PRS__DM_EM_FM\n",
      "PRS__DM\n",
      "PRS__EM\n",
      "PRS__FM\n",
      "Aggregated__DM_EM_FM\n",
      "Aggregated__DM\n",
      "Aggregated__EM\n",
      "Aggregated__FM\n"
     ]
    }
   ],
   "source": [
    "### TEST: EFFICACY CALCULATING\n",
    "\n",
    "### Results container:\n",
    "dict_measure_stats = {}\n",
    "dict_measure_vectors = {}\n",
    "### Algorythms looping:\n",
    "for iter_factor in df_factors.columns:\n",
    "    ### Region groups looping:\n",
    "    for iter_region in [['DM', 'EM', 'FM'], ['DM'], ['EM'], ['FM']]: # [['DM', 'EM', 'FM']]: # ['DM']: # ['EM']: # ['FM']: #\n",
    "        ### Iteration ID generating:\n",
    "        str_iter_id = iter_factor + '__' + '_'.join(iter_market for iter_market in iter_region)\n",
    "        print(str_iter_id)\n",
    "        ### Measure calculating:\n",
    "        dict_measure_stats[str_iter_id], dict_measure_vectors[str_iter_id] = multiple_factor_single_efficacy_measure_stats(df_factors[iter_factor].to_frame(), \n",
    "                                                                                                                           ser_returns * 100, \n",
    "                                                                                                                           ser_mcap,\n",
    "                                                                                                                           list_measure[0], \n",
    "                                                                                                                           list_back_period[0], \n",
    "                                                                                                                           int_horizon, \n",
    "                                                                                                                           iter_region)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>autocorr</th>\n",
       "      <th>t_1</th>\n",
       "      <th>t_2</th>\n",
       "      <th>t_3</th>\n",
       "      <th>t_6</th>\n",
       "      <th>t_9</th>\n",
       "      <th>t_12</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorythm</th>\n",
       "      <th>Region</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Continuum</th>\n",
       "      <th>DM_EM_FM</th>\n",
       "      <td>0.993840</td>\n",
       "      <td>2.806142</td>\n",
       "      <td>2.551687</td>\n",
       "      <td>2.314779</td>\n",
       "      <td>2.379724</td>\n",
       "      <td>2.379971</td>\n",
       "      <td>2.152676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DM</th>\n",
       "      <td>0.996110</td>\n",
       "      <td>2.121494</td>\n",
       "      <td>2.014944</td>\n",
       "      <td>1.806585</td>\n",
       "      <td>1.935240</td>\n",
       "      <td>2.015106</td>\n",
       "      <td>2.042690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>0.996070</td>\n",
       "      <td>2.152529</td>\n",
       "      <td>1.791051</td>\n",
       "      <td>1.700996</td>\n",
       "      <td>1.712632</td>\n",
       "      <td>1.526648</td>\n",
       "      <td>1.038696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FM</th>\n",
       "      <td>0.989251</td>\n",
       "      <td>2.562720</td>\n",
       "      <td>1.868360</td>\n",
       "      <td>1.535706</td>\n",
       "      <td>1.017324</td>\n",
       "      <td>1.297020</td>\n",
       "      <td>1.070225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">PRS</th>\n",
       "      <th>DM_EM_FM</th>\n",
       "      <td>0.934851</td>\n",
       "      <td>-0.253080</td>\n",
       "      <td>-0.307125</td>\n",
       "      <td>-0.298002</td>\n",
       "      <td>-1.211445</td>\n",
       "      <td>-0.884931</td>\n",
       "      <td>-0.668338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DM</th>\n",
       "      <td>0.921414</td>\n",
       "      <td>0.416858</td>\n",
       "      <td>0.031986</td>\n",
       "      <td>0.002970</td>\n",
       "      <td>-0.736413</td>\n",
       "      <td>-0.925701</td>\n",
       "      <td>-0.222600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>0.943617</td>\n",
       "      <td>-1.223064</td>\n",
       "      <td>-1.046822</td>\n",
       "      <td>-1.267704</td>\n",
       "      <td>-1.770831</td>\n",
       "      <td>-1.172625</td>\n",
       "      <td>-1.031510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FM</th>\n",
       "      <td>0.947902</td>\n",
       "      <td>2.342601</td>\n",
       "      <td>1.554191</td>\n",
       "      <td>1.929376</td>\n",
       "      <td>0.561992</td>\n",
       "      <td>-0.026736</td>\n",
       "      <td>0.486957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Aggregated</th>\n",
       "      <th>DM_EM_FM</th>\n",
       "      <td>0.954889</td>\n",
       "      <td>1.194266</td>\n",
       "      <td>0.970361</td>\n",
       "      <td>1.072447</td>\n",
       "      <td>0.733588</td>\n",
       "      <td>0.723103</td>\n",
       "      <td>0.534806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DM</th>\n",
       "      <td>0.954760</td>\n",
       "      <td>1.101287</td>\n",
       "      <td>0.915001</td>\n",
       "      <td>1.095461</td>\n",
       "      <td>0.962133</td>\n",
       "      <td>0.744806</td>\n",
       "      <td>0.976561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>0.971690</td>\n",
       "      <td>-0.124564</td>\n",
       "      <td>-0.100005</td>\n",
       "      <td>-0.257225</td>\n",
       "      <td>-0.705924</td>\n",
       "      <td>-0.176273</td>\n",
       "      <td>-0.818799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FM</th>\n",
       "      <td>0.981308</td>\n",
       "      <td>2.575182</td>\n",
       "      <td>1.778352</td>\n",
       "      <td>1.747897</td>\n",
       "      <td>0.854387</td>\n",
       "      <td>0.633123</td>\n",
       "      <td>0.610114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     autocorr       t_1       t_2       t_3       t_6  \\\n",
       "Algorythm  Region                                                       \n",
       "Continuum  DM_EM_FM  0.993840  2.806142  2.551687  2.314779  2.379724   \n",
       "           DM        0.996110  2.121494  2.014944  1.806585  1.935240   \n",
       "           EM        0.996070  2.152529  1.791051  1.700996  1.712632   \n",
       "           FM        0.989251  2.562720  1.868360  1.535706  1.017324   \n",
       "PRS        DM_EM_FM  0.934851 -0.253080 -0.307125 -0.298002 -1.211445   \n",
       "           DM        0.921414  0.416858  0.031986  0.002970 -0.736413   \n",
       "           EM        0.943617 -1.223064 -1.046822 -1.267704 -1.770831   \n",
       "           FM        0.947902  2.342601  1.554191  1.929376  0.561992   \n",
       "Aggregated DM_EM_FM  0.954889  1.194266  0.970361  1.072447  0.733588   \n",
       "           DM        0.954760  1.101287  0.915001  1.095461  0.962133   \n",
       "           EM        0.971690 -0.124564 -0.100005 -0.257225 -0.705924   \n",
       "           FM        0.981308  2.575182  1.778352  1.747897  0.854387   \n",
       "\n",
       "                          t_9      t_12  \n",
       "Algorythm  Region                        \n",
       "Continuum  DM_EM_FM  2.379971  2.152676  \n",
       "           DM        2.015106  2.042690  \n",
       "           EM        1.526648  1.038696  \n",
       "           FM        1.297020  1.070225  \n",
       "PRS        DM_EM_FM -0.884931 -0.668338  \n",
       "           DM       -0.925701 -0.222600  \n",
       "           EM       -1.172625 -1.031510  \n",
       "           FM       -0.026736  0.486957  \n",
       "Aggregated DM_EM_FM  0.723103  0.534806  \n",
       "           DM        0.744806  0.976561  \n",
       "           EM       -0.176273 -0.818799  \n",
       "           FM        0.633123  0.610114  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEST: EFFICACY COMPARING\n",
    "\n",
    "### Results preparing (stats):\n",
    "df_factor_measure_stats = pd.concat(dict_measure_stats).droplevel(1)\n",
    "df_factor_measure_stats.index.name = 'KEY'\n",
    "df_factor_measure_stats.reset_index(inplace = True)\n",
    "df_factor_measure_stats['Algorythm'] = df_factor_measure_stats['KEY'].str.split('__').str[0]\n",
    "df_factor_measure_stats['Region'] = df_factor_measure_stats['KEY'].str.split('__').str[1]\n",
    "df_factor_measure_stats = df_factor_measure_stats.set_index(['Algorythm', 'Region']).drop('KEY', axis = 1)\n",
    "#df_factor_measure_stats.to_excel('Data_Files/Test_Files/acadian_mode_test_measure_stats.xlsx', merge_cells = False)\n",
    "### Results preparing (vectors):\n",
    "df_measure_vectors = pd.concat(dict_measure_vectors).droplevel(1)\n",
    "df_measure_vectors.index.names = ['KEY', 'Date']\n",
    "df_measure_vectors.reset_index(inplace = True)\n",
    "df_measure_vectors['Algorythm'] = df_measure_vectors['KEY'].str.split('__').str[0]\n",
    "df_measure_vectors['Region'] = df_measure_vectors['KEY'].str.split('__').str[1]\n",
    "df_measure_vectors = df_measure_vectors.set_index(['Algorythm', 'Region', 'Date']).drop('KEY', axis = 1)\n",
    "#df_measure_vectors.to_excel('Data_Files/Test_Files/acadian_mode_test_measure_vectors.xlsx', merge_cells = False)\n",
    "df_factor_measure_stats[['autocorr', 't_1', 't_2', 't_3', 't_6', 't_9', 't_12']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
