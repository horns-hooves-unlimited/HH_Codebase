{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZATION\n",
    "\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE\n",
    "\n",
    "def weighted_average(ser_to_manage, ser_weight = pd.Series()):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if (len(ser_weight.index) == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_to_manage.index)\n",
    "        ser_weight.name = 'Weight'\n",
    "    ### Clearing and docking vectors:\n",
    "    df_to_manage = ser_to_manage.to_frame().join(ser_weight, how = 'left').dropna()\n",
    "    df_to_manage.columns = ['Data', 'Weight']\n",
    "    ### Result calculating:\n",
    "    num_result = np.NaN\n",
    "    if (len(df_to_manage.index) > 0):\n",
    "        num_result = df_to_manage['Data'].dot(df_to_manage['Weight']) / sum(df_to_manage['Weight'])      \n",
    "    \n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES\n",
    "\n",
    "def multistep_standartize(ser_to_manage, arr_truncate, ser_weight = pd.Series(), reuse_outliers = False, center_result = True, full_result = False):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd      \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if (len(ser_weight.index) == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_to_manage.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_to_manage.dropna().copy() \n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Clearing and docking vectors:        \n",
    "        index_iter = ser_data_iter.index.intersection(ser_weight_iter.index)\n",
    "        ser_data_iter = ser_data_iter[index_iter]\n",
    "        ser_weight_iter = ser_weight_iter[index_iter] \n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        if not (reuse_outliers):\n",
    "            ### Saving to result and excluding from further calculations truncated values:     \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):      \n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_to_manage.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR SEPARATE SERIES\n",
    "\n",
    "def ison_standartize(ser_to_manage, arr_truncate, ser_weight = pd.Series(), reuse_outliers = False, center_result = True, full_result = False, within_market = False):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Multi-step standartizing:\n",
    "    if (within_market):\n",
    "    ### Within market standartizing:\n",
    "        ser_result = ser_to_manage.groupby(by = 'Market', group_keys = False).apply(multistep_standartize, arr_truncate, ser_weight, \n",
    "                                                                                    reuse_outliers, center_result, full_result)\n",
    "    else:\n",
    "    ### Full universe standartizing:\n",
    "        ser_result = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result)\n",
    "    ### Results output:\n",
    "    return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK\n",
    "\n",
    "def single_factor_standartize(ser_factor, arr_truncate, ser_weight = pd.Series(), reuse_outliers = False, center_result = True, within_market = False):\n",
    "    ### Defining by date standartizing function:\n",
    "    def by_date_standartize(df_date, arr_truncate, reuse_outliers, center_result, within_market):\n",
    "        ### ISON standartizing:\n",
    "        ser_date = ison_standartize(df_date['Factor'], arr_truncate, df_date['Weight'], reuse_outliers, center_result, False, within_market)\n",
    "        ser_date = ser_date.reindex(df_date.index)\n",
    "        ### Result output:\n",
    "        return ser_date\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Weights preparing:\n",
    "    if (len(ser_weight.index) == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Multi-step standartizing:        \n",
    "    df_factor = ser_factor.to_frame().join(ser_weight, how = 'left')\n",
    "    df_factor.columns = ['Factor', 'Weight']\n",
    "#    ser_result = df_factor.groupby('Date').apply\\\n",
    "#    (lambda iter_df: ison_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False, within_market))\n",
    "    ser_result = df_factor.groupby('Date', group_keys = False).apply(by_date_standartize, arr_truncate, reuse_outliers, center_result, within_market)\n",
    "    ### Results output:\n",
    "    ser_result.name = ser_factor.name\n",
    "    return ser_result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING GROUP MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK FOR MULTIPLE FACTORS\n",
    "\n",
    "def multi_factor_standartize(df_factor, arr_truncate, ser_weight = pd.Series(), reuse_outliers = False, center_result = True, within_market = False):\n",
    "    \n",
    "    dict_standartized = {}\n",
    "    ### Single factor standartizing:\n",
    "    for iter_factor in df_factor.columns:\n",
    "        dict_standartized[iter_factor] = single_factor_standartize(df_factor[iter_factor], arr_truncate, ser_weight, \n",
    "                                                                   reuse_outliers, center_result, within_market)\n",
    "    ### Concatenating to dataframe:\n",
    "    df_result = pd.concat(dict_standartized, axis = 1)\n",
    "    ### Results output:\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EFFICACY MEASURES FOR SINGLE FACTOR\n",
    "\n",
    "def single_factor_multiple_efficacy_measures(ser_factor, ser_return, ser_weight, arr_measure, return_shift = 0, arr_truncate = [2.5, 2.0]):\n",
    "    ### Importing standard modules and data-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import statsmodels.api as sm\n",
    "    from scipy import stats as ss\n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    dict_measure = {}\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining get_measure group level function:\n",
    "    def get_measure(df_to_measure, iter_measure):\n",
    "        ### Checking data sufficiency:\n",
    "        if (len(df_to_measure.dropna().index) > 1):           \n",
    "            ### Measure calculating:\n",
    "            if (iter_measure == 'ic_spearman'):\n",
    "                ### Spearmen information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values\n",
    "                num_result = ss.spearmanr(list_factor, list_return, nan_policy = 'omit').correlation\n",
    "            if (iter_measure == 'ic_pearson'):\n",
    "                ### Pearson information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values                \n",
    "                num_result = ss.pearsonr(list_factor, list_return)[0]\n",
    "            if (iter_measure == 'fmb_eqw'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (equal weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return']].dropna()['Return'].values\n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values\n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_std_eqw'):             \n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()['Return'].values                \n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_std_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]                 \n",
    "            if (iter_measure == 'fmb_std_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values                \n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_std_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]  \n",
    "            if (iter_measure == 'clp'):\n",
    "                ### Constant leverage portfolio signed normalized multiplication sum:                \n",
    "                ser_clp_weighted = df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Factor']\n",
    "                ser_clp_weighted = ser_clp_weighted * df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Weight'].transform(np.sqrt)\n",
    "                ser_clp_weighted.loc[ser_clp_weighted < 0] = -ser_clp_weighted / ser_clp_weighted[ser_clp_weighted < 0].sum()\n",
    "                ser_clp_weighted.loc[ser_clp_weighted > 0] = ser_clp_weighted / ser_clp_weighted[ser_clp_weighted > 0].sum()\n",
    "                num_result = (ser_clp_weighted * df_to_measure['Return']).sum()\n",
    "                                  \n",
    "        else:                          \n",
    "            num_result = np.NaN\n",
    "        ### Preparing results: \n",
    "        return num_result\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    if (ser_weight.count() == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ser_weight = ser_weight.reset_index('Market', drop = True)\n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted, ser_weight], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return', 'Weight']\n",
    "    df_to_measure['Constant'] = 1\n",
    "    ser_factor_std = df_to_measure.dropna()['Factor'].groupby('Date').apply(ison_standartize, arr_truncate = arr_truncate, within_market = False)\n",
    "    df_to_measure['Factor_std'] = ser_factor_std.reindex(df_to_measure.index)\n",
    "    ### Looping efficacy measures for calculating measures timeseries:\n",
    "    for iter_measure in arr_measure:\n",
    "        dict_measure[iter_measure] = df_to_measure.groupby('Date').apply(get_measure, iter_measure = iter_measure)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_measure, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEASURE STATISTICS CALCULATOR\n",
    "\n",
    "def measure_stats(df_measures, arr_back_period = [99]):\n",
    "    ### Importing standard modules and data-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Declaring local constants & variables:\n",
    "    dict_stats = {}\n",
    "    ### Stats calculating:\n",
    "    for iter_measure in df_measures.columns:\n",
    "        dict_period = {}\n",
    "        for iter_back_period in arr_back_period:\n",
    "            ser_iter_measure = df_measures[iter_measure].dropna()\n",
    "            idx_iter_range = pd.date_range(end = ser_iter_measure.index[-1], periods = iter_back_period * 12, freq = 'BM')\n",
    "            ser_iter_measure = ser_iter_measure[idx_iter_range]            \n",
    "            ser_iter_stats = pd.Series()\n",
    "            ser_iter_stats['count'] = ser_iter_measure.count()\n",
    "            ser_iter_stats['min'] = ser_iter_measure.min()\n",
    "            ser_iter_stats['max'] = ser_iter_measure.max()        \n",
    "            ser_iter_stats['mean'] = ser_iter_measure.mean()\n",
    "            ser_iter_stats['std'] = ser_iter_measure.std()\n",
    "            ser_iter_stats['median'] = ser_iter_measure.median()        \n",
    "            ser_iter_stats['perc_25'] = ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['perc_75'] = ser_iter_measure.quantile(0.75, 'midpoint')\n",
    "            ser_iter_stats['iq_range'] = ser_iter_measure.quantile(0.75, 'midpoint') - ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['mean_abs'] = ser_iter_measure.abs().mean()\n",
    "            ser_iter_stats['t_stat'] = (ser_iter_measure.mean() / ser_iter_measure.std()) * np.sqrt(ser_iter_measure.count())  \n",
    "            dict_period[iter_back_period] = ser_iter_stats\n",
    "        dict_stats[iter_measure] = pd.concat(dict_period, axis = 1)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_stats, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SPECIAL CLP STATS\n",
    "\n",
    "def special_clp_stats(ser_factor, ser_return, ser_weight, return_shift = 0):\n",
    "    ### Importing standard modules and data-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Declaring local constants & variables:    \n",
    "    dict_clp_stats = {}\n",
    "    list_bin_labels = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining function for constant leverage portfolio normalizing:\n",
    "    def get_normalized_clp(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Constant leverage portfolio signed normalized:\n",
    "            ser_clp_weighted.loc[ser_clp_weighted < 0] = -ser_clp_weighted / ser_clp_weighted[ser_clp_weighted < 0].sum()\n",
    "            ser_clp_weighted.loc[ser_clp_weighted > 0] = ser_clp_weighted / ser_clp_weighted[ser_clp_weighted > 0].sum()\n",
    "            ser_result = ser_clp_weighted.copy()\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index)\n",
    "        ### Results output:\n",
    "        return ser_result\n",
    "    ### Defining function for constant leverage portfolio normalizing:\n",
    "    def get_normalized_factor(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Factor signed normalized:\n",
    "            ser_factor_normalized = df_to_measure['Factor']\n",
    "            ser_factor_normalized.loc[ser_factor_normalized < 0] = -ser_factor_normalized / ser_factor_normalized[ser_factor_normalized < 0].sum()\n",
    "            ser_factor_normalized.loc[ser_factor_normalized > 0] = ser_factor_normalized / ser_factor_normalized[ser_factor_normalized > 0].sum()\n",
    "            ser_result = ser_factor_normalized\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index) \n",
    "        ### Results output:\n",
    "        return ser_result            \n",
    "    ### Defining function for returns for constant leverage portfolio:\n",
    "    def get_normalized_return(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Normalized return:  \n",
    "            ser_result = df_to_measure['Return']\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index)            \n",
    "        ### Results output:\n",
    "        return ser_result  \n",
    "    ### Defining MatLab style prctile function:\n",
    "    def prctile(ser_to_perc, p):\n",
    "        ### Sorted list preparing:\n",
    "        list_to_perc = ser_to_perc.dropna().values\n",
    "        list_sorted = np.sort(list_to_perc)\n",
    "        ### Length calculating:\n",
    "        num_len = len(list_to_perc)    \n",
    "        ### Prctile calculating:\n",
    "        num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "        ### Results output:\n",
    "        return num_result\n",
    "    ### Defining quintile bins distribution:\n",
    "    def quartile_distribution(ser_iter_group):\n",
    "        ### Bins preparing:\n",
    "        list_bin = list(np.arange(0, 100, 20))[1 : ]\n",
    "        list_bin = [round(iter_element / 100, 2) for iter_element in list_bin]\n",
    "        list_bin = [prctile(ser_iter_group, iter_element) for iter_element in list_bin]\n",
    "        list_bin = [ser_iter_group.min() - abs(ser_iter_group.min())] + list_bin + [ser_iter_group.max() + abs(ser_iter_group.max())]\n",
    "        ### Bins distribution:\n",
    "        ser_iter_distribution = pd.cut(ser_iter_group, bins = list_bin, labels = list_bin_labels)    \n",
    "        ### Results output:\n",
    "        return ser_iter_distribution   \n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    if (ser_weight.count() == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ser_weight = ser_weight.reset_index('Market', drop = True)      \n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted, ser_weight], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return', 'Weight']\n",
    "    ser_clp_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_clp)\n",
    "    ser_factor_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_factor)\n",
    "    ser_return_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_return)\n",
    "    ### CLP stats calculating:\n",
    "    dict_clp_stats['Average Bias'] = ser_factor_normalized.groupby('Country').mean()\n",
    "#    dict_clp_stats['Temp'] = pd.Series(np.NaN, index = dict_clp_stats['Average Bias'].index)\n",
    "    dict_clp_stats['Weights Sum'] = ser_clp_normalized.groupby('Country').sum()\n",
    "    dict_clp_stats['Average Return'] = ser_return_normalized.groupby('Country').mean()   \n",
    "    dict_clp_stats['Static Contribution'] = dict_clp_stats['Weights Sum'] * dict_clp_stats['Average Return']\n",
    "    dict_clp_stats['Total Contribution'] = (ser_clp_normalized * ser_return_normalized).groupby('Country').sum()\n",
    "    dict_clp_stats['Dynamic Contribution'] = dict_clp_stats['Total Contribution'] - dict_clp_stats['Static Contribution'] \n",
    "    ### CLP active weights calculating:\n",
    "    ser_clp_delta = ser_clp_normalized.unstack('Date').stack('Date', dropna = False).swaplevel().sort_index(level = ['Date', 'Country'])\n",
    "    ser_clp_delta = ser_clp_delta.fillna(0)\n",
    "    num_clp_mean = ser_clp_delta.groupby('Country').mean().abs().sum()\n",
    "    ser_clp_delta = ser_clp_delta.groupby('Country').apply(lambda iter_group: iter_group - iter_group.mean())\n",
    "    num_clp_delta = (ser_clp_delta.abs().groupby('Date').sum() / (ser_clp_delta.abs().groupby('Date').sum() + num_clp_mean)).mean()\n",
    "    df_clp_stats = pd.concat(dict_clp_stats, axis = 1).reindex(df_to_measure.index.get_level_values('Country').unique()).sort_index()\n",
    "    ### Preparing sum:\n",
    "    df_clp_sum = pd.DataFrame([[np.NaN, np.NaN, np.NaN, df_clp_stats['Static Contribution'].sum(), \n",
    "                               df_clp_stats['Total Contribution'].sum(), df_clp_stats['Dynamic Contribution'].sum()]], \n",
    "                              index = ['Sum'], columns = df_clp_stats.columns)\n",
    "    ### Preparing expected based on active weights:\n",
    "    df_clp_expected = pd.DataFrame([[np.NaN, np.NaN, np.NaN, \n",
    "                                     df_clp_stats['Total Contribution'].sum() * (1 - num_clp_delta), np.NaN, df_clp_stats['Total Contribution'].sum() * num_clp_delta]], \n",
    "                              index = ['Expected based on active weights =>'], columns = df_clp_stats.columns)    \n",
    "    ### Adding totals:\n",
    "    df_clp_stats = pd.concat([df_clp_stats, df_clp_sum, df_clp_expected], axis = 0, join = 'inner')\n",
    "    ### CLP Bias calculating:   \n",
    "    ser_clp_quintile = ser_clp_normalized.groupby('Date', group_keys = False).apply(quartile_distribution)\n",
    "    df_clp_bias = ser_clp_quintile.to_frame()  \n",
    "    df_clp_bias.columns = ['Bin']\n",
    "    df_clp_bias['Quintile'] = 1\n",
    "    df_clp_bias = df_clp_bias.set_index('Bin', append = True).unstack('Bin').fillna(0).droplevel(level = 0, axis = 1)\n",
    "    df_clp_bias.columns = list(df_clp_bias.columns)  \n",
    "    df_clp_bias = df_clp_bias[list_bin_labels]\n",
    "    df_clp_bias = df_clp_bias.groupby('Country').mean()\n",
    "    df_clp_bias.loc[:, 'Q5 - Q1'] = df_clp_bias['Q5'] - df_clp_bias['Q1']   \n",
    "    df_clp_bias = df_clp_bias.reindex(df_to_measure.index.get_level_values('Country').unique()).sort_index()\n",
    "    df_clp_bias = df_clp_bias\n",
    "    ### Output results:\n",
    "    return (df_clp_stats, df_clp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SINGLE EFFICACY MEASURE FOR MULTIPLE FACTORS\n",
    "    \n",
    "def multiple_factor_single_efficacy_measure_stats(df_factors, ser_return, ser_weight, str_measure, num_back_period = 99, \n",
    "                                                  num_horizon = 12, list_region_xmo = ['DM', 'EM', 'FM']): \n",
    "    ### Importing standard modules and data-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    list_months = [1, 2, 3, 6, 9 ,12]\n",
    "    ### Defining full universe expanding for date:\n",
    "    def universe_reindex(iter_group, idx_universe):\n",
    "        df_iter_result = iter_group.unstack('Date').reindex(idx_universe).sort_index().stack('Date', dropna = False)\n",
    "        ### Results output:\n",
    "        return df_iter_result   \n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result   \n",
    "    ### Defining by date correrlation function:\n",
    "    def corr_by_date(iter_group):\n",
    "        num_iter_corr = iter_group['Corr_factor_minus'].corr(iter_group['Corr_factor_plus'])\n",
    "        ### Results output:\n",
    "        return num_iter_corr       \n",
    "    ### Preparing expanded universe for autocorrelation performing:\n",
    "    df_factors_region = df_factors.loc[(All, All, list_region_xmo), :]\n",
    "    idx_date_range = df_factors_region.index.get_level_values(0).unique()\n",
    "    idx_universe = df_factors_region.index.get_level_values(1).unique()\n",
    "    df_factors_full = df_factors_region.reset_index('Market', drop = True).groupby('Date', group_keys = False).apply(universe_reindex, idx_universe).swaplevel()\n",
    "    ### Factors looping:\n",
    "    dict_factors_measures = {} ### Container for all factor stats\n",
    "    dict_factors_autocorr = {} ### Container for autocorrelation results\n",
    "    for iter_factor in df_factors.columns:\n",
    "        ### Shifts looping for factors measures stats:\n",
    "        ### Stats calculation:\n",
    "        dict_factor_stats = {} ### Container for iterated factor stats\n",
    "        for iter_shift in range(num_horizon):\n",
    "#            df_factor_filtered = df_factors[iter_factor].loc[All, All, list_region_xmo]\n",
    "            df_factor_filtered = df_factors_region[iter_factor]\n",
    "            df_iter_shift_measure = single_factor_multiple_efficacy_measures(df_factor_filtered, ser_return, ser_weight, [str_measure], iter_shift, list_truncate)\n",
    "            df_iter_shift_stats = measure_stats(df_iter_shift_measure, [num_back_period])\n",
    "            dict_factor_stats[iter_shift] = df_iter_shift_stats.loc[['mean', 't_stat'], (str_measure, num_back_period)]\n",
    "        df_iter_factor_stats = pd.concat(dict_factor_stats, axis = 1)\n",
    "        df_iter_factor_stats.columns = df_iter_factor_stats.columns + 1\n",
    "        dict_factors_measures[iter_factor] = df_iter_factor_stats\n",
    "        ### Autocorrelation calculation:\n",
    "        ser_iter_factor = df_factors_full[iter_factor]\n",
    "        ser_iter_factor_plus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[1 : ]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ser_iter_factor_minus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[: -1]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ### Artificial series combining for indexes synchronization:        \n",
    "        ser_iter_factor_plus_shifted = ser_iter_factor_plus.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, 1)\n",
    "        df_iter_factor_to_corr = pd.concat([ser_iter_factor_minus, ser_iter_factor_plus_shifted], axis = 1)\n",
    "        df_iter_factor_to_corr.columns = ['Corr_factor_minus', 'Corr_factor_plus']\n",
    "        dict_factors_autocorr[iter_factor] = pd.Series(df_iter_factor_to_corr.groupby('Date').apply(corr_by_date).mean(), index = ['autocorr'])\n",
    "    ### Results output:\n",
    "    df_factors_measures_stats = pd.concat(dict_factors_measures, axis = 0)\n",
    "    df_factors_autocorr =  pd.concat(dict_factors_autocorr, axis = 1).transpose()\n",
    "    df_factors_coeff = df_factors_measures_stats.loc[(All, 'mean'), list_months].reset_index(1, drop = True)\n",
    "    df_factors_coeff.columns = [('coeff_' + str(iter_column)) for iter_column in df_factors_coeff.columns]\n",
    "    df_factors_t_stat = df_factors_measures_stats.loc[(All, 't_stat'), list_months].reset_index(1, drop = True)\n",
    "    df_factors_t_stat.columns = [('t_' + str(iter_column)) for iter_column in df_factors_t_stat.columns]\n",
    "    df_factors_result = pd.concat([df_factors_autocorr, df_factors_coeff, df_factors_t_stat], axis = 1)    \n",
    "    return (df_factors_result, df_factors_measures_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM GENERAL MS EXCEL SOURCE\n",
    "\n",
    "def get_market_membership_from_excel():\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Declaring local constants & variables: \n",
    "    path_msci = 'Data_Files/Source_Files/sample_universe.xlsx' ### Path for membership source    \n",
    "    tab_monthly = 'universe_joined'    \n",
    "    arr_markets_needed = ['DM', 'FM', 'EM']   \n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM'}\n",
    "    no_slice = slice(None)\n",
    "    ### Extracting universe data:\n",
    "    df_universe = pd.read_excel(io = path_msci, sheet_name = tab_monthly, skiprows = [0, 2], header = 0, parse_dates = True, \n",
    "                                na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    df_universe = df_universe.loc[no_slice, ['dates', 'region', 'ctry']]\n",
    "    df_universe.columns = ['Date', 'Market', 'Country']\n",
    "    df_universe.set_index(['Date', 'Country'], inplace = True)\n",
    "    ser_universe = df_universe.squeeze()\n",
    "    ser_universe.sort_index(level = [0, 1], inplace = True)\n",
    "    ser_universe.replace(dict_markets, inplace = True)\n",
    "    ser_market_membership = ser_universe[ser_universe.isin(arr_markets_needed)]\n",
    "    ### Results output:\n",
    "    return ser_market_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZATION\n",
    "\n",
    "### Declaring global constants & variables: \n",
    "path_cds_result_hdf = 'Data_Files/Source_Files/CDS_Result.h5' ### Path to blended CDS index\n",
    "key_cds_result = 'cds_blended' ### blended CDS object\n",
    "path_market_cap = 'Data_Files/Source_Files/Market_Cap.h5'\n",
    "key_market_cap = 'mcap'\n",
    "path_return = 'Data_Files/Source_Files/Returns_Integrated.h5'\n",
    "key_return = 'returns'\n",
    "path_growth_potential_hdf = 'Data_Files/Source_Files/OD_Growth_Potential.h5'\n",
    "key_growth_potential = 'growth_potential'\n",
    "float_zero_tolerance = 0.001 ### For float comparision with 0\n",
    "All = slice(None) ### No-slice for MultiIndex level for using in loc method\n",
    "tup_plt_size = (20, 5) ### Plot sizes\n",
    "list_truncate = [2.5, 2.0] ### Truncate boundaries for standartization\n",
    "list_back_period = [99, 10, 5]\n",
    "### Using transitional results:\n",
    "path_transitional_results = 'Data_Files/Test_Files/Factor_transitionals.h5'\n",
    "key_factors = 'factors'\n",
    "key_return = 'returns'\n",
    "key_mcap = 'mcaps'\n",
    "df_factor_filtered = pd.read_hdf(path_transitional_results, key_factors)\n",
    "ser_return_filtered = pd.read_hdf(path_transitional_results, key_return)\n",
    "ser_mcap_filtered = pd.read_hdf(path_transitional_results, key_mcap)\n",
    "### CDS with ISON and market caps and earlier calculated data reading:\n",
    "#ser_cds_res = pd.read_hdf(path_cds_result_hdf, key_cds_result)\n",
    "#ser_market_membership = get_market_membership_from_excel()\n",
    "#ser_mcap = pd.read_hdf(path_market_cap, key_market_cap)\n",
    "#ser_return = pd.read_hdf(path_return, key_return)\n",
    "#df_growth_potential = pd.read_hdf(path_growth_potential_hdf, key_growth_potential).iloc[All, : 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARING WITH MATLAB: EXCEL FILES PREPARING\n",
    "\n",
    "### Returns shifting & saving:\n",
    "ser_return_shifted = ser_return.groupby('Country').shift(periods = -1)\n",
    "ser_return_shifted.name = 'Return_shifted'\n",
    "ser_return_shifted_ison = ser_return_shifted.loc[All, All, ['DM', 'EM', 'FM']]\n",
    "ser_return_shifted_ison = ser_return_shifted_ison.reset_index('Market').replace(['DM', 'EM', 'FM'], [50, 57, 504]).set_index('Market', append = True)\n",
    "#ser_return_shifted_ison.to_excel('Data_Files/Test_Files/Example_Returns.xlsx')\n",
    "### Market Caps saving:\n",
    "ser_mcap_ison = ser_mcap.loc[All, All, ['DM', 'EM', 'FM']]\n",
    "ser_mcap_ison = ser_mcap_ison.reset_index('Market').replace(['DM', 'EM', 'FM'], [50, 57, 504]).set_index('Market', append = True)\n",
    "#ser_mcap_ison.to_excel('Data_Files/Test_Files/Example_Market_Caps.xlsx')\n",
    "### Factors saving:\n",
    "df_growth_potential_ison = df_growth_potential.loc[(All, All, ['DM', 'EM', 'FM']), All]\n",
    "df_growth_potential_ison = df_growth_potential_ison.reset_index('Market').replace(['DM', 'EM', 'FM'], [50, 57, 504]).set_index('Market', append = True)\n",
    "#df_growth_potential_ison.to_excel('Data_Files/Test_Files/Example_OD_Growth_Potential.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PREPARING\n",
    "\n",
    "### Parameters:\n",
    "list_region = ['DM', 'EM', 'FM'] # ['DM'] # ['DM', 'EM'] ### Regions list\n",
    "list_countries_to_exclude = ['VE'] # ['GR', 'UA', 'VE'] ### Countries not to play the game\n",
    "bool_within_market = True # Standartization way\n",
    "str_date_start = '2007-01-01' # Start date to filter returns, market caps and factors\n",
    "idx_date_range = pd.date_range(str_date_start, periods = 999, freq = 'BM')\n",
    "### Returns shifting:\n",
    "ser_return_prepared = ser_return.groupby('Country').shift(periods = -1)\n",
    "### Region and date clearing:\n",
    "ser_return_prepared = ser_return_prepared.loc[idx_date_range, All, list_region]\n",
    "ser_mcap_prepared = ser_mcap.loc[idx_date_range, All, list_region]\n",
    "df_factor_prepared = df_growth_potential.loc[(idx_date_range, All, list_region), All]\n",
    "### Countries filtering:\n",
    "ser_return_prepared = ser_return_prepared.drop(list_countries_to_exclude, level = 'Country')\n",
    "ser_mcap_prepared = ser_mcap_prepared.drop(list_countries_to_exclude, level = 'Country')\n",
    "df_factor_prepared = df_factor_prepared.drop(list_countries_to_exclude, level = 'Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MULTI FACTORS STANDARTIZING\n",
    "\n",
    "df_factor_std = multi_factor_standartize(df_factor_prepared, list_truncate, within_market = bool_within_market)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARING WITH MATLAB: MULTI FACTORS STANDARTIZING\n",
    "\n",
    "df_factor_std['Education'].reset_index('Market', drop = True).unstack('Date').to_excel('Data_Files/Test_Files/Test_PY_education_std.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STANDARTIZED FACTORS FILTERING\n",
    "\n",
    "list_region_filter = ['DM'] #['DM', 'EM', 'FM'] ### Regions filter\n",
    "df_factor_filtered = df_factor_std.loc[(All, All, list_region_filter), :]\n",
    "ser_return_filtered = ser_return_prepared.loc[All, All, list_region_filter]\n",
    "ser_mcap_filtered = ser_mcap_prepared.loc[All, All, list_region_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARING WITH MATLAB: MULTI FACTORS FILTERING\n",
    "\n",
    "df_factor_filtered['High-Tech Export License Fee'].reset_index('Market', drop = True).unstack('Date').sort_index(level = 'Country').\\\n",
    "to_excel('Data_Files/Test_Files/Test_PY_high_tech_filtered.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVING TRANSITIONAL RESULTS\n",
    "\n",
    "#df_factor_filtered.to_hdf(path_transitional_results, key_factors, 'w')\n",
    "#ser_return_filtered.to_hdf(path_transitional_results, key_return)\n",
    "#ser_mcap_filtered.to_hdf(path_transitional_results, key_mcap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADDITIONAL FILTERING FOR TESTING PURPOSES\n",
    "\n",
    "list_region_filter = ['EM', 'FM'] ### Regions filter\n",
    "df_factor_filtered = df_factor_filtered.loc[(All, All, list_region_filter), :]\n",
    "ser_return_filtered = ser_return_filtered.loc[All, All, list_region_filter]\n",
    "ser_mcap_filtered = ser_mcap_filtered.loc[All, All, list_region_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARING WITH MATLAB: PERFORMANCE TEST\n",
    "\n",
    "### Measures to calculate:\n",
    "arr_measure = ['ic_pearson', 'ic_spearman', 'fmb_eqw', 'fmb_weighted', 'fmb_std_eqw', 'fmb_std_weighted', 'clp']\n",
    "### Measures comparing:\n",
    "df_test_perf_measure = single_factor_multiple_efficacy_measures(df_factor_filtered['Education'], ser_return_filtered, ser_mcap_filtered, \n",
    "                                 arr_measure, return_shift = 0, arr_truncate = list_truncate)\n",
    "df_test_perf_measure.to_excel('Data_Files/Test_Files/Test_PY_edu_measures.xlsx')\n",
    "### Measure stats comparing:\n",
    "df_test_perf_stats = measure_stats(df_test_perf_measure, list_back_period)\n",
    "df_test_perf_stats.to_excel('Data_Files/Test_Files/Test_PY_edu_stats.xlsx')\n",
    "### CLP tables comparing:\n",
    "(df_test_clp_stats, df_test_clp_bias) = \\\n",
    "special_clp_stats(df_factor_filtered['Education'], ser_return_filtered, ser_mcap_filtered, return_shift = 0)\n",
    "df_test_clp_stats.to_excel('Data_Files/Test_Files/Test_PY_edu_clp_stats.xlsx')\n",
    "df_test_clp_bias.to_excel('Data_Files/Test_Files/Test_PY_edu_clp_bias.xlsx')\n",
    "### Multiple factors comparing:\n",
    "(df_factors_result, df_factors_measures_stats) = multiple_factor_single_efficacy_measure_stats(\\\n",
    "                                                                    df_factor_filtered[['Education', 'High-Tech Export License Fee', 'Human Resource Potential']], \n",
    "                                                                    ser_return_filtered, ser_mcap_filtered, 'fmb_std_weighted', list_back_period[0], 12)\n",
    "df_factors_result.to_excel('Data_Files/Test_Files/Test_PY_multiple_result.xlsx')\n",
    "df_factors_measures_stats.to_excel('Data_Files/Test_Files/Test_PY_multiple_fmb_std.xlsx')\n",
    "### Multiple xmo factors comparing:\n",
    "list_region_xmo = ['FM'] ### Regions filter\n",
    "(df_factors_result, df_factors_measures_stats) = multiple_factor_single_efficacy_measure_stats(\\\n",
    "                                                                    df_factor_filtered[['Education', 'High-Tech Export License Fee', 'Human Resource Potential']], \n",
    "                                                                    ser_return_filtered, ser_mcap_filtered, 'fmb_std_weighted', list_back_period[0], 12, list_region_xmo)\n",
    "df_factors_result.to_excel('Data_Files/Test_Files/Test_PY_multiple_xmo_result.xlsx')\n",
    "df_factors_measures_stats.to_excel('Data_Files/Test_Files/Test_PY_multiple_xmo_fmb_std.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        Country  Market\n",
       "2007-01-31  AE       FM        0.023936\n",
       "2007-02-28  AE       FM       -0.062659\n",
       "2007-03-30  AE       FM        0.046285\n",
       "2007-04-30  AE       FM        0.170291\n",
       "2007-05-31  AE       FM       -0.022307\n",
       "                                 ...   \n",
       "2019-05-31  ZM       FM             NaN\n",
       "2019-06-28  ZM       FM             NaN\n",
       "2019-07-31  ZM       FM             NaN\n",
       "2019-08-30  ZM       FM             NaN\n",
       "2019-09-30  ZM       FM             NaN\n",
       "Name: Return, Length: 12659, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CONTROL TEST\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Market</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Market, Return]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
