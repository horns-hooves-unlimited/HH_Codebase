{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COUNTRY RISK FACTORS DAILY GENERATOR (TO BE IGNORED IN PRODUCT CODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULES IMPORT (PART OF THE PRODUCT CODE)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INTERNAL PARAMETERS INITIALIZATION (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "import os ### To work with csv files\n",
    "\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/Country_Risks/acadian_universe.xlsx'\n",
    "### PRS data structured:\n",
    "str_path_prs_hdf = 'Data_Files/Source_Files/Country_Risks/PRS_loaded.h5'\n",
    "str_key_prs_full_raw = 'prs_full_raw'\n",
    "str_key_prs_pillars_only_raw = 'prs_pillars_only_raw'\n",
    "str_key_prs_political_risk_pillar_raw = 'prs_political_risk_pillar_raw'\n",
    "str_key_prs_full_converted = 'prs_full_converted'\n",
    "str_key_prs_pillars_only_converted = 'prs_pillars_only_converted'\n",
    "str_key_prs_political_risk_pillar_converted = 'prs_political_risk_pillar_converted'\n",
    "### Continuum data structured:\n",
    "str_path_continuum_hdf = 'Data_Files/Source_Files/Country_Risks/Continuum_loaded.h5'\n",
    "str_key_continuum_composite_raw = 'continuum_composite_indicators_raw'\n",
    "str_key_continuum_gp_pillar_raw = 'continuum_gp_pillar_raw'\n",
    "str_key_continuum_si_pillar_raw = 'continuum_si_pillar_raw'\n",
    "str_key_continuum_politics_raw = 'continuum_politics_raw'\n",
    "str_key_continuum_composite_converted = 'continuum_composite_indicators_converted'\n",
    "str_key_continuum_gp_pillar_converted = 'continuum_gp_pillar_converted'\n",
    "str_key_continuum_si_pillar_converted = 'continuum_si_pillar_converted'\n",
    "str_key_continuum_politics_converted = 'continuum_politics_converted'\n",
    "### Source parameters:\n",
    "str_source_date_start = '1992-01-01' ### Start date for source vectors\n",
    "str_measure_date_start = '1996-08-01' ### Start date for efficacy measures\n",
    "str_ison_date_start = '1994-01-31' ### Start date for ISON Universe\n",
    "str_measure_date_end = '2020-08-31' ### End date for efficacy measures\n",
    "idx_source_date_range = pd.date_range(str_source_date_start, str_measure_date_end, freq = 'B') ### Range for source data filtering\n",
    "idx_test_monthly_date_range = pd.date_range(str_ison_date_start, str_measure_date_end, freq = 'BM') ### Range for source data filtering\n",
    "idx_test_daily_date_range = pd.date_range(str_ison_date_start, str_measure_date_end, freq = 'B') ### Range for source data filtering\n",
    "idx_factor_date_range = pd.date_range(str_source_date_start, str_measure_date_end, freq = 'BM') ### Range for factor data filtering\n",
    "idx_measure_date_range = pd.date_range(str_measure_date_start, str_measure_date_end, freq = 'BM') ### Range for measures calculation\n",
    "### Results saving:\n",
    "str_prs_raw_csv = 'Data_Files/Test_Files/Country_Risks/acadian_mode_prs_raw.csv'\n",
    "str_prs_std_xlsx = 'Data_Files/Test_Files/Country_Risks/acadian_mode_prs_std.xlsx'\n",
    "str_continuum_raw_csv = 'Data_Files/Test_Files/Country_Risks/acadian_mode_continuum_raw.csv'\n",
    "str_continuum_std_xlsx = 'Data_Files/Test_Files/Country_Risks/acadian_mode_continuum_std.xlsx'\n",
    "str_combined_xlsx = 'Data_Files/Test_Files/Country_Risks/acadian_mode_combined.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL PARAMETERS INITIALIZATION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "### Common constants:\n",
    "All = slice(None)\n",
    "\n",
    "### Standartization parameters:\n",
    "list_truncate = [2.5, 2.0] ### Standartization boundaries\n",
    "bool_within_market = True ### Standartization option\n",
    "\n",
    "### ISON filtering options:\n",
    "list_ison = ['DM', 'EM', 'FM'] ### Regions filter to drop NaN region values\n",
    "list_countries_to_exclude = ['VE'] ### Countries not to play the game\n",
    "\n",
    "### Standalone factors weights:\n",
    "list_static_weights = [2.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = math.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE FOR DATAFRAME COLUMNS (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def columns_average(df_series, list_weights = False):\n",
    "    ### Equal weights list creating:\n",
    "    if isinstance(list_weights, bool):\n",
    "        list_weights = [1] * len(df_series.columns)\n",
    "    ### Dataframe of weights initialising:\n",
    "    df_weights = pd.DataFrame([list_weights] * len(df_series.index), index = df_series.index, columns = df_series.columns)\n",
    "    ### Zeroing weights for NaN values:\n",
    "    for iter_col in df_weights.columns:\n",
    "        df_weights.loc[df_series[iter_col].isna(), iter_col] = 0\n",
    "    ### Weighted mean calulating:\n",
    "    df_means = (df_series * df_weights).sum(axis = 1) / df_weights.sum(axis = 1)    \n",
    "    ### Results output:\n",
    "    return df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def multistep_standartize(ser_data_source, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):  \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_data_source.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_data_source.dropna()\n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        if reuse_outliers:\n",
    "            ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        else:\n",
    "            ### Saving to result and excluding from further calculations truncated values:             \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):\n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_data_source.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR CROSS-SECTION (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_standartize(ser_to_manage, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False, within_market = False):\n",
    "    ### Multi-step standartizing:\n",
    "    if (within_market):\n",
    "    ### Within market standartizing:\n",
    "        ser_result = ser_to_manage.groupby(by = 'Market', group_keys = False).apply(multistep_standartize, arr_truncate, ser_weight, \n",
    "                                                                                                  reuse_outliers, center_result, full_result)\n",
    "    else:\n",
    "    ### Full universe standartizing:\n",
    "        ser_result = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result)\n",
    "    ### Results output:\n",
    "    return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def single_factor_standartize_daily(ser_factor, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, within_market = False):\n",
    "    ### Weights preparing:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "        ser_weight.name = 'Weight'\n",
    "    ### Multi-step standartizing:        \n",
    "    df_factor = ser_factor.to_frame().join(ser_weight, how = 'left')\n",
    "    df_factor.columns = ['Factor', 'Weight']\n",
    "    ser_result = ison_standartize(df_factor['Factor'], arr_truncate, df_factor['Weight'], reuse_outliers, center_result, False, within_market)\n",
    "    ### Results output:\n",
    "    ser_result.name = ser_factor.name\n",
    "    return ser_result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(io = str_path_universe, sheet_name = 0, header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEAN MOMENTUM FUNCTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def mean_momentum(ser_country_source, list_weight, int_mean_min):\n",
    "    try:\n",
    "        ### Weight setting\n",
    "        ser_weight = pd.Series(list_weight[ -len(ser_country_source.index) : ], ser_country_source.index)  \n",
    "        ### Weighted mean calculation:\n",
    "        return weighted_average(ser_country_source, ser_weight, int_mean_min)\n",
    "    except KeyError:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MATLAB STYLE PRCTILE (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def prctile_matlab(ser_to_perc, p):\n",
    "    ### Sorted list preparing:\n",
    "    list_sorted = sorted(ser_to_perc.dropna().values)\n",
    "    ### Length calculating:\n",
    "    num_len = len(list_sorted)    \n",
    "    ### Prctile calculating:\n",
    "    num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING FILTERING DATE INTERVAL, REINDEXING FILTERED VECTOR TO BUSINESS DATES/MONTHS FREQUENCY AND FILLING DATA (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def get_country_interval(ser_filtered, date_start, date_end, int_fill_limit = 1):\n",
    "    ### Drop level to avoid stack/unstack manipulations:\n",
    "    ser_filtered = ser_filtered.droplevel('Country')\n",
    "    ### Business day filter:\n",
    "    idx_date_business = pd.date_range(start = date_start, end = date_end, freq = 'B')\n",
    "    try:\n",
    "        ### Frequency checker:\n",
    "        date_first = ser_filtered.first_valid_index()\n",
    "        date_last = ser_filtered.last_valid_index()\n",
    "        ### Resampling to business month:\n",
    "        if ((date_last - date_first).days / len(ser_filtered.dropna().index) > 3.0):          \n",
    "            ser_filtered = ser_filtered.resample('MS').last().resample('BM').last()\n",
    "    except TypeError:\n",
    "        pass\n",
    "    ### Reindexation and forward filling:\n",
    "    ser_reindexed = ser_filtered.resample('B').ffill().fillna(method = 'ffill', limit = int_fill_limit).reindex(idx_date_business).ffill(limit = int_fill_limit)        \n",
    "    ### Results output:\n",
    "    ser_reindexed.index.names = ['Date']        \n",
    "    return ser_reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA LOADING (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "### Sources loading:\n",
    "ser_prs = pd.read_hdf(str_path_prs_hdf, key = str_key_prs_political_risk_pillar_converted).loc['Socioeconomic Conditions', All, All]\\\n",
    "                                                                                          .droplevel('Variable') ### PRS data source\n",
    "ser_continuum = pd.read_hdf(str_path_continuum_hdf, key = str_key_continuum_politics_converted).loc['Sovereign Risk Index', All, All]\\\n",
    "                                                                                               .droplevel('Indicator') ### Continuum data source\n",
    "### ISON Universe loading:\n",
    "ser_ison_daily = ison_membership_converting(str_path_universe, datetime.strptime(str_measure_date_end, '%Y-%m-%d'), bool_daily = True) ### ISON universe, bus-daily vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING PRS BASED FACTOR CREATING FUNCTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def get_prs_factor(iter_date):\n",
    "    ### Momentum parameters:\n",
    "    int_mom_hl = 520 ### Without rounding here\n",
    "    int_mom_win = 1300\n",
    "    int_mom_min = 520\n",
    "    ### Weights array:\n",
    "    list_weight = list(map(lambda iter_num: exp_weight_single(int_mom_hl, iter_num), range(int_mom_win)))[::-1]\n",
    "    ### Source load parameters:\n",
    "    date_source_start = pd.to_datetime('1992-01-01') ### Start date for source vectors    \n",
    "    int_fill_limit = 66\n",
    "    date_start_win = np.maximum(iter_date - pd.tseries.offsets.BDay(int_mom_win - 1), date_source_start)\n",
    "    date_start_loc = np.maximum(iter_date - pd.tseries.offsets.BDay(int_mom_win + int_fill_limit), date_source_start)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    ser_iter_source_raw = ser_prs.loc[date_start_loc : iter_date, All]  \n",
    "    ### Data source resampling:\n",
    "    ser_iter_source = ser_iter_source_raw.groupby('Country').apply(get_country_interval, date_start_win, iter_date, int_fill_limit).swaplevel().sort_index()\n",
    "    ### Source performing:\n",
    "    ser_iter_delta = ser_iter_source.groupby('Country').diff() / ser_iter_source.groupby('Country').shift()   \n",
    "    ser_iter_delta = ser_iter_delta.replace([np.inf, -np.inf], np.NaN)    \n",
    "    ser_iter_delta.index.names = ['Date', 'Country']\n",
    "    ### Momentum factor calculation:\n",
    "    ser_iter_factor = ser_iter_delta.groupby('Country').apply(mean_momentum, list_weight, int_mom_min)\n",
    "    ser_iter_factor.name = 'PRS'\n",
    "    ### Sign changing:\n",
    "    ser_iter_factor = -ser_iter_factor\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_prs_raw_csv, mode = 'a', header = not os.path.exists(str_prs_raw_csv))\n",
    "    ### Results output:\n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING CONTINUUM BASED FACTOR CREATING FUNCTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def get_continuum_factor(iter_date):\n",
    "    ### Momentum parameters:\n",
    "    int_mom_hl = 520 ### Without rounding here\n",
    "    int_mom_win = 1300\n",
    "    int_mom_min = 520\n",
    "    ### Weights array:\n",
    "    list_weight = list(map(lambda iter_num: exp_weight_single(int_mom_hl, iter_num), range(int_mom_win)))[::-1]\n",
    "    ### Source load parameters:\n",
    "    date_source_start = pd.to_datetime('2005-07-01') ### Start date for source vectors    \n",
    "    int_fill_limit = 66\n",
    "    date_start_win = np.maximum(iter_date - pd.tseries.offsets.BDay(int_mom_win - 1), date_source_start)\n",
    "    date_start_loc = np.maximum(iter_date - pd.tseries.offsets.BDay(int_mom_win + int_fill_limit), date_source_start)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    ser_iter_source_raw = ser_continuum.loc[date_start_loc : iter_date, All]  \n",
    "    ### Data source resampling:\n",
    "    ser_iter_source = ser_iter_source_raw.groupby('Country').apply(get_country_interval, date_start_win, iter_date, int_fill_limit).swaplevel().sort_index()\n",
    "    ### Source performing:\n",
    "    ser_iter_delta = ser_iter_source.groupby('Country').diff() / ser_iter_source.groupby('Country').shift()   \n",
    "    ser_iter_delta = ser_iter_delta.replace([np.inf, -np.inf], np.NaN)    \n",
    "    ser_iter_delta.index.names = ['Date', 'Country']\n",
    "    ### Momentum factor calculation:\n",
    "    ser_iter_factor = ser_iter_delta.groupby('Country').apply(mean_momentum, list_weight, int_mom_min)\n",
    "    ser_iter_factor.name = 'Continuum'\n",
    "    ### Sign changing:\n",
    "    ser_iter_factor = -ser_iter_factor\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_continuum_raw_csv, mode = 'a', header = not os.path.exists(str_continuum_raw_csv))\n",
    "    ### Results output:\n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2021-02-09 11:12:41.719852\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Index' object has no attribute 'swaplevel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8fda9622038f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m### PRS based factor calculating:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mser_iter_prs_factor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_prs_factor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;31m### Next rows are performing standartizing and should be substituted by framework script:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m### ISON Universe for the date loading (should be substituted by SQL query):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-e6c72a3937ce>\u001b[0m in \u001b[0;36mget_prs_factor\u001b[1;34m(iter_date)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mser_iter_source_raw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mser_prs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdate_start_loc\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0miter_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAll\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m### Data source resampling:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mser_iter_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mser_iter_source_raw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Country'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_country_interval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_start_win\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint_fill_limit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswaplevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;31m### Source performing:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mser_iter_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mser_iter_source\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Country'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mser_iter_source\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Country'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshift\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mswaplevel\u001b[1;34m(self, i, j, copy)\u001b[0m\n\u001b[0;32m   3620\u001b[0m            \u001b[0mthe\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0minnermost\u001b[0m \u001b[0mlevels\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3621\u001b[0m         \"\"\"\n\u001b[1;32m-> 3622\u001b[1;33m         \u001b[0mnew_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswaplevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3623\u001b[0m         return self._constructor(self._values, index=new_index, copy=copy).__finalize__(\n\u001b[0;32m   3624\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Index' object has no attribute 'swaplevel'"
     ]
    }
   ],
   "source": [
    "### TESTING: PERFORMING FACTOR FOR DATE RANGE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "### Removing csv files before loop running:\n",
    "if (os.path.exists(str_prs_raw_csv)):\n",
    "    os.remove(str_prs_raw_csv)\n",
    "if (os.path.exists(str_continuum_raw_csv)):\n",
    "    os.remove(str_continuum_raw_csv)  \n",
    "### Dictionary for date vectors collecting:\n",
    "dict_prs_factor_by_date = {}\n",
    "dict_continuum_factor_by_date = {}\n",
    "dict_combo_factor_by_date = {}\n",
    "### Local testing parameters:\n",
    "int_interval = 10 ### Interval of progress displaying\n",
    "date_start = datetime.utcnow() ### Start time of calculations\n",
    "date_control = datetime.utcnow() ### Control time to display\n",
    "idx_test_date_range = idx_test_monthly_date_range[0 : 10] # idx_test_monthly_date_range # idx_test_monthly_date_range[130 : 140] # idx_test_monthly_date_range[310 : 320] # \n",
    "### Test performing:\n",
    "print('Start time:', date_start)\n",
    "for iter_num, iter_date in enumerate(idx_test_date_range):\n",
    "    ### Progress printing:\n",
    "    if not (divmod(iter_num, int_interval)[1]):\n",
    "        if iter_num:\n",
    "            print('Counter marker:', iter_num, '/', len(idx_test_date_range))\n",
    "            timedelta_interval = datetime.utcnow() - date_control\n",
    "            print('Time interval since last marker:', datetime.utcnow() - date_control)            \n",
    "            print('Average interval for single date:', str(timedelta_interval / int_interval))\n",
    "        date_control = datetime.utcnow()\n",
    "          \n",
    "    ### PRS based factor calculating:\n",
    "    ser_iter_prs_factor = get_prs_factor(iter_date)  \n",
    "    ### Next rows are performing standartizing and should be substituted by framework script:\n",
    "    ### ISON Universe for the date loading (should be substituted by SQL query):\n",
    "    ser_ison_iter_date = ser_ison_daily.loc[iter_date, All].droplevel('Date')\n",
    "    ### Factor ISONing:\n",
    "    ser_iter_prs_factor = ser_iter_prs_factor.to_frame().join(ser_ison_iter_date, how = 'left').set_index('Market', append = True).squeeze() \n",
    "    ### Regions clearing:\n",
    "    ser_iter_prs_factor = ser_iter_prs_factor.loc[All, list_ison]\n",
    "    ### Countries filtering:\n",
    "    ser_iter_prs_factor = ser_iter_prs_factor.drop(list_countries_to_exclude, level = 'Country')   \n",
    "    ### Standalone factor standartizing:    \n",
    "    ser_iter_prs_factor_std = single_factor_standartize_daily(ser_iter_prs_factor, list_truncate, within_market = bool_within_market)\\\n",
    "                                                                 .droplevel(['Market']).sort_index()\n",
    "    ### End of standartizing procedure\n",
    "\n",
    "    ### Continuum based factor calculating:\n",
    "    ser_iter_continuum_factor = get_continuum_factor(iter_date)  \n",
    "    ### Next rows are performing standartizing and should be substituted by framework script:\n",
    "    ### ISON Universe for the date loading (should be substituted by SQL query):\n",
    "    ser_ison_iter_date = ser_ison_daily.loc[iter_date, All].droplevel('Date')\n",
    "    ### Factor ISONing:\n",
    "    ser_iter_continuum_factor = ser_iter_continuum_factor.to_frame().join(ser_ison_iter_date, how = 'left').set_index('Market', append = True).squeeze() \n",
    "    ### Regions clearing:\n",
    "    ser_iter_continuum_factor = ser_iter_continuum_factor.loc[All, list_ison]\n",
    "    ### Countries filtering:\n",
    "    ser_iter_continuum_factor = ser_iter_continuum_factor.drop(list_countries_to_exclude, level = 'Country')   \n",
    "    ### Standalone factor standartizing:    \n",
    "    ser_iter_continuum_factor_std = single_factor_standartize_daily(ser_iter_continuum_factor, list_truncate, within_market = bool_within_market)\\\n",
    "                                                                    .droplevel(['Market']).sort_index()\n",
    "    ### End of standartizing procedure\n",
    "\n",
    "    ### Concatenating factors for averaging:\n",
    "    df_iter_weighted_factor = pd.concat([ser_iter_prs_factor_std, ser_iter_continuum_factor_std], axis = 1, sort = False)\n",
    "    ### Factors combining:\n",
    "    ser_iter_combo_factor = columns_average(df_iter_weighted_factor, list_static_weights).sort_index()   \n",
    "    ### Next rows are performing standartizing and should be substituted by framework script:\n",
    "    ### ISON Universe for the date loading (should be substituted by SQL query):\n",
    "    ser_ison_iter_date = ser_ison_daily.loc[iter_date, All].droplevel('Date')\n",
    "    ### Factor ISONing:\n",
    "    ser_iter_combo_factor = ser_iter_combo_factor.to_frame().join(ser_ison_iter_date, how = 'left').set_index('Market', append = True).squeeze() \n",
    "\n",
    "    ### Standalone factor standartizing:    \n",
    "    ser_iter_combo_factor_std = single_factor_standartize_daily(ser_iter_combo_factor, list_truncate, within_market = bool_within_market)\\\n",
    "                                                               .droplevel(['Market']).sort_index()    \n",
    "    ### End of standartizing procedure    \n",
    "    ser_iter_combo_factor_std.name = 'Combo Factor'    \n",
    "    ### Collecting date result for comparision with research mode results:\n",
    "    dict_combo_factor_by_date[iter_date] = ser_iter_combo_factor_std  \n",
    "    \n",
    "date_finish = datetime.utcnow()\n",
    "### Overall statistics printing:\n",
    "print('Finish time:', date_finish)\n",
    "print('Full interval:', date_finish - date_start)\n",
    "print('Average interval for single date:', str((date_finish - date_start) / len(idx_test_date_range)))\n",
    "### Results aggregating for comparision with research mode results:\n",
    "ser_prs_factor_full = pd.concat(dict_prs_factor_by_date, axis = 0, sort = False, names = ['Date']).sort_index()\n",
    "ser_continuum_factor_full = pd.concat(dict_continuum_factor_by_date, axis = 0, sort = False, names = ['Date']).sort_index()\n",
    "ser_combo_factor_full = pd.concat(dict_combo_factor_by_date, axis = 0, sort = False, names = ['Date']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: PERFORMING FACTOR FOR DATE RANGE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "ser_prs_factor_full.to_excel(str_prs_std_xlsx, merge_cells = False)\n",
    "ser_continuum_factor_full.to_excel(str_continuum_std_xlsx, merge_cells = False)\n",
    "pd.concat([ser_prs_factor_full, ser_continuum_factor_full, ser_combo_factor_full], axis = 1).to_excel(str_combined_xlsx, merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
