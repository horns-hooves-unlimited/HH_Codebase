{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM GENERAL MS EXCEL SOURCE\n",
    "def get_market_membership_from_excel(path_msci):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Declaring local constants & variables: \n",
    "    tab_monthly = 'universe_joined'    \n",
    "    arr_markets_needed = ['DM', 'FM', 'EM']   \n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM'}\n",
    "    no_slice = slice(None)\n",
    "    ### Extracting universe data:\n",
    "    df_universe = pd.read_excel(io = path_msci, sheet_name = tab_monthly, skiprows = [0, 2], header = 0, parse_dates = True, \n",
    "                                na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    df_universe = df_universe.loc[no_slice, ['dates', 'region', 'ctry']]\n",
    "    df_universe.columns = ['Date', 'Market', 'Country']\n",
    "    df_universe.set_index(['Date', 'Country'], inplace = True)\n",
    "    ser_universe = df_universe.squeeze()\n",
    "    ser_universe.sort_index(level = [0, 1], inplace = True)\n",
    "    ser_universe.replace(dict_markets, inplace = True)\n",
    "    ser_market_membership = ser_universe[ser_universe.isin(arr_markets_needed)]\n",
    "    \n",
    "    return ser_market_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION RATING DATA FROM GENERAL MS EXCEL SOURCE\n",
    "def get_ratings_from_excel(path_xsdm, path_scale, path_msci):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Declaring local constants & variables:  \n",
    "    arr_currency = ['LC', 'FC'] ### For correct sheet selection in XSDM file\n",
    "    arr_to_skip = list(range(5)) ### For rows skipping in XSDM file\n",
    "    arr_to_subtract = ['\\(P\\)', 'u', ' \\*.?$'] ### For ratings clearing \n",
    "    str_to_subtract_excess = '|'.join(arr_to_subtract)\n",
    "    str_to_highlight_mark = '(\\*.)'  ### For future reviews markers extracting\n",
    "    dict_map_to_replace = {'DD': 'D', 'NR': np.NaN, 'RD': 'D', 'SD': 'D', 'WD': np.NaN, 'WR': np.NaN, 'nan': np.NaN} ### For ratings harmonizing\n",
    "    dict_map_marks = {'-': -1, '+': 1} ### For markers digitalising\n",
    "    dict_table = {} ### For sheets accumulation\n",
    "    ### Collecting sheets:\n",
    "    with pd.ExcelFile(path_xsdm) as file_excel:\n",
    "        for str_sheet_name in file_excel.sheet_names:\n",
    "            arr_sheet_code = str_sheet_name.split(' ')[0].split('_')\n",
    "            if (len(arr_sheet_code) > 2):\n",
    "                if (arr_sheet_code[1] in arr_currency):\n",
    "                    ser_table = pd.read_excel(file_excel, str_sheet_name, squeeze = True, skiprows = arr_to_skip, index_col = 0, parse_dates = True)\n",
    "                    dict_table[str_sheet_name.split(' ')[0]] = ser_table.stack(dropna = False).sort_index()\n",
    "    ### Aggregating tables:\n",
    "    ser_common = pd.concat(dict_table)\n",
    "    ser_common.index.names = ['Code', 'Date', 'Country']\n",
    "    ser_common = ser_common.astype(str)\n",
    "    ### Splitting 'Rating Code' for future analysis:\n",
    "    df_common = ser_common.to_frame().reset_index().set_index('Code')\n",
    "    df_common.index = pd.MultiIndex.from_tuples(df_common.index.str.split('_').to_list())\n",
    "    df_common.index.names = ['Agency', 'Currency', 'Type']\n",
    "    ### Rating dataFrame polishing:\n",
    "    df_rating = df_common.set_index(['Date', 'Country'], drop = True, append = True).sort_index()        \n",
    "    df_rating.columns = ['Rating']\n",
    "    ### Future reviews markers extracting:\n",
    "#    df_rating['Initial'] = df_rating['Rating']\n",
    "    df_rating['Mark'] = df_rating['Rating'].str.extract(pat = str_to_highlight_mark, expand = False).str.slice(-1)    \n",
    "    ### Markers digitalising:\n",
    "    for map_key, map_value in dict_map_marks.items():\n",
    "        df_rating['Mark'] = df_rating['Mark'].replace(map_key, map_value)    \n",
    "    ### Suffixes and prefixes handling:    \n",
    "    df_rating['Rating'] = df_rating['Rating'].str.replace(pat = str_to_subtract_excess, repl = '', regex = True)\n",
    "    ### Ratings harmonizing with scaling table:\n",
    "    for map_key, map_value in dict_map_to_replace.items():\n",
    "        df_rating['Rating'] = df_rating['Rating'].replace(map_key, map_value)\n",
    "    ### Reading scaling table:\n",
    "    df_scale = pd.read_excel(path_scale, index_col = 'Rank')\n",
    "    ### Scaling dataFrame polishing:    \n",
    "    df_scale.rename_axis('Agency', axis = 1, inplace = True)\n",
    "    df_scale = df_scale.stack(dropna = False).to_frame().swaplevel().reset_index(1)\n",
    "    df_scale.rename(columns = {0 : 'Rating'}, inplace = True)\n",
    "    df_scale.set_index('Rating', append = True, inplace = True)\n",
    "    ### Adding rank column to result dataframe:\n",
    "    df_rating = df_rating.join(df_scale, on = ['Agency', 'Rating'], how = 'left')    \n",
    "#    df_result.drop(['Initial', 'Rating'], axis = 1, inplace = True)\n",
    "    df_rating.drop(['Rating'], axis = 1, inplace = True)  \n",
    "    ### Receiving MSCI membership data:    \n",
    "    ser_market_membership = get_market_membership_from_excel(path_msci)\n",
    "    ### Adding membership column to result dataframe:    \n",
    "    df_result = df_rating.join(ser_market_membership, on = ['Date', 'Country'], how = 'left')\n",
    "    df_result = df_result.set_index('Market', drop = True, append = True)    \n",
    "    ### Reordering indexes and value columns:\n",
    "    df_result = df_result[['Rank', 'Mark']]\n",
    "    df_result = df_result.reorder_levels(['Agency', 'Currency', 'Type', 'Date', 'Market', 'Country'])\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHTS GENERATOR\n",
    "def get_exp_weights_series(window_len = 5, halflife_len = 3):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Array of regressioon window day numbers descending:\n",
    "    arr_weight_days = np.arange(window_len, 0, -1) - 1\n",
    "    ### Creating weights series:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    arr_weights = np.exp(math.log(num_period_factor) * arr_weight_days)\n",
    "    ser_weights = pd.Series(arr_weights)        \n",
    "    ser_weights.name = 'Weight'\n",
    "    \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    \n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALL RATINGS IMPORTING\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "### Declaring global constants & variables: \n",
    "path_msci = 'Data_Files/Source_Files/sample_universe.xlsx' ### Path for membership source\n",
    "path_xsdm = 'Data_Files/Source_Files/XSDM.xlsx' ### Path to rating data file\n",
    "path_scale = 'Data_Files/Source_Files/Ratings_Scale.xlsx' ### Path to rating comparision file \n",
    "All = slice(None)\n",
    "### Receiving aggregated ratings table:\n",
    "df_rating_full = get_ratings_from_excel(path_xsdm, path_scale, path_msci)\n",
    "### Dropping non-universe countries:\n",
    "### Comment the next line to get full ranking history (without dropping observations not covered by ISON Universe)\n",
    "#df_rating_full = df_rating_full.loc[(All, All, All, All, ['DM', 'FM', 'EM'], All), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTER\n",
    "print(df_rating_full.loc[('SP', All, All, All, All, All), :].dropna(how = 'all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARING MDY'S RATING TYPES (ISSUER vs DEBT)\n",
    "### Creating table for comparision:\n",
    "df_mdy = df_rating_full.loc['MDY']['Rank'].unstack(level = 'Type')\n",
    "df_mdy.sort_index(level = ['Currency', 'Date', 'Country'], inplace = True)\n",
    "### Table with both type's ratings available:\n",
    "ser_mdy_both = df_mdy.dropna()['DEBT'] - df_mdy.dropna()['ISSUER']\n",
    "### Table with different rating values:\n",
    "ser_mdy_differ = ser_mdy_both[ser_mdy_both != 0]\n",
    "ser_mdy_differ.name = 'Delta'\n",
    "#print(df_mdy.tail())\n",
    "### Comparision process:\n",
    "print('MDY Ratings common stats: Number of all observations:', len(df_mdy.index))\n",
    "print('MDY Ratings common stats: Number of observations with at least one type\\'s rating available:', \n",
    "      '{:.2%}'.format(len(df_mdy.dropna(how = 'all').index) / len(df_mdy.index)), '(', len(df_mdy.dropna(how = 'all').index), ')')\n",
    "print('MDY Ratings common stats: Number of observations with both types ratings available:', \n",
    "      '{:.2%}'.format(len(df_mdy.dropna().index) / len(df_mdy.index)), '(', len(df_mdy.dropna().index), ')')\n",
    "print('MDY Ratings common stats: Number of observations with only DEBT rating available:', \n",
    "      '{:.2%}'.format(len(df_mdy.loc[df_mdy['DEBT'].notna() & df_mdy['ISSUER'].isna()].index) / len(df_mdy.index)), \n",
    "      '(', len(df_mdy.loc[df_mdy['DEBT'].notna() & df_mdy['ISSUER'].isna()].index), ')')\n",
    "print('MDY Ratings common stats: Observations with only DEBT rating available distribution:','\\n',\n",
    "      df_mdy['DEBT'].loc[df_mdy['DEBT'].notna() & df_mdy['ISSUER'].isna()].groupby(['Currency', 'Market']).count())\n",
    "print('MDY Ratings common stats: Number of observations with only ISSUER rating available:', \n",
    "      '{:.2%}'.format(len(df_mdy.loc[df_mdy['DEBT'].isna() & df_mdy['ISSUER'].notna()].index) / len(df_mdy.index)), \n",
    "      '(', len(df_mdy.loc[df_mdy['DEBT'].isna() & df_mdy['ISSUER'].notna()].index), ')')\n",
    "print('MDY Ratings common stats: Observations with only ISSUER rating available distribution:','\\n',\n",
    "      df_mdy['ISSUER'].loc[df_mdy['DEBT'].isna() & df_mdy['ISSUER'].notna()].groupby(['Currency', 'Market']).count())\n",
    "print('MDY Ratings both ratings available stats', '(', len(ser_mdy_both.index), ')', ': Number of observations with equal ratings values:', \n",
    "      '{:.2%}'.format(len(ser_mdy_both[ser_mdy_both == 0].index) / len(ser_mdy_both.index)), \n",
    "      '(', len(ser_mdy_both[ser_mdy_both == 0].index), ')')\n",
    "print('MDY Ratings both ratings available stats', '(', len(ser_mdy_both.index), ')', ': Number of observations with different ratings values:', \n",
    "      '{:.2%}'.format(len(ser_mdy_both[ser_mdy_both != 0].index) / len(ser_mdy_both.index)), \n",
    "      '(', len(ser_mdy_both[ser_mdy_both != 0].index), ')')\n",
    "print('MDY Ratings different rating values', '(', len(ser_mdy_differ.index), ')', ': Currencies & Markets distribution:', '\\n',\n",
    "      ser_mdy_differ.abs().groupby(['Currency', 'Market']).agg(['count', 'min', 'max', 'mean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE'S CHOOSING AND JOINING\n",
    "### Moving Types to columns and flattening columns to one level:\n",
    "df_rating_type = df_rating_full.unstack('Type')\n",
    "df_rating_type.sort_index(level = ['Agency', 'Currency', 'Date', 'Country'], inplace = True)\n",
    "df_rating_type.columns = ['_'.join(iter_tup_col).rstrip('_') for iter_tup_col in df_rating_type.columns.values]\n",
    "### Adding final columns:\n",
    "df_rating_type['Rank'] = np.NaN\n",
    "df_rating_type['Mark'] = np.NaN\n",
    "### Filling final columns:\n",
    "df_rating_type.loc[('FITCH', All), 'Rank'] = df_rating_type['Rank_DEBT']\n",
    "df_rating_type.loc[('FITCH', All), 'Mark'] = df_rating_type['Mark_DEBT']\n",
    "df_rating_type.loc[('SP', All), 'Rank'] = df_rating_type['Rank_ISSUER']\n",
    "df_rating_type.loc[('SP', All), 'Mark'] = df_rating_type['Mark_ISSUER']\n",
    "df_rating_type.loc[('MDY', All), 'Rank'] = df_rating_type['Rank_ISSUER']\n",
    "df_rating_type.loc[('MDY', All), 'Mark'] = df_rating_type['Mark_ISSUER']\n",
    "df_rating_type.loc[(df_rating_type.index.get_level_values(0) == 'MDY') & df_rating_type['Rank'].isna(), 'Mark'] = df_rating_type['Mark_DEBT']\n",
    "df_rating_type.loc[('MDY', All), 'Rank'] = df_rating_type.loc[('MDY', All), 'Rank'].combine_first(df_rating_type.loc[('MDY', All), 'Rank_DEBT'])\n",
    "### Removing source data columns:\n",
    "df_rating_type = df_rating_type[['Rank', 'Mark']]\n",
    "#df_rating_type.to_excel('Data_Files/Test_Files/Types_Merging.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARING RATING CURRENCIES (FC vs LC)\n",
    "### Creating table for comparision:\n",
    "df_cur = df_rating_type['Rank'].unstack('Currency')\n",
    "df_cur.sort_index(level = ['Agency', 'Date', 'Country'], inplace = True)\n",
    "#df_cur.to_excel('Data_Files/Test_Files/Currencies_Merging.xlsx', merge_cells = False)\n",
    "### Table with both type's ratings available:\n",
    "ser_cur_both = df_cur.dropna()['FC'] - df_cur.dropna()['LC']\n",
    "### Table with different rating values:\n",
    "ser_cur_differ = ser_cur_both[ser_cur_both != 0]\n",
    "ser_cur_differ.name = 'Delta'\n",
    "### LC only rating table:\n",
    "ser_cur_LC = df_cur['LC'].loc[df_cur['FC'].isna() & df_cur['LC'].notna()]\n",
    "### LC only countries:\n",
    "arr_cur_keys = ser_cur_LC.groupby(['Agency', 'Market', 'Country']).groups.keys()\n",
    "### Comparision process:\n",
    "print('Ratings common stats: Number of all observations:', len(df_cur.index))\n",
    "print('Ratings common stats: Number of observations with at least one currencie\\'s rating available:', \n",
    "      '{:.2%}'.format(len(df_cur.dropna(how = 'all').index) / len(df_cur.index)), '(', len(df_cur.dropna(how = 'all').index), ')')\n",
    "print('Ratings common stats: Number of observations with both ratings available:', \n",
    "      '{:.2%}'.format(len(df_cur.dropna().index) / len(df_cur.index)), '(', len(df_cur.dropna().index), ')')\n",
    "print('Ratings common stats: Number of observations with only FC rating available:', \n",
    "      '{:.2%}'.format(len(df_cur.loc[df_cur['FC'].notna() & df_cur['LC'].isna()].index) / len(df_cur.index)), \n",
    "      '(', len(df_cur.loc[df_cur['FC'].notna() & df_cur['LC'].isna()].index), ')')\n",
    "print('Ratings common stats: Observations with only FC rating available distribution:','\\n',\n",
    "      df_cur['FC'].loc[df_cur['FC'].notna() & df_cur['LC'].isna()].groupby(['Market', 'Agency']).count())\n",
    "print('Ratings common stats: Number of observations with only LC rating available:', \n",
    "      '{:.2%}'.format(len(df_cur.loc[df_cur['FC'].isna() & df_cur['LC'].notna()].index) / len(df_cur.index)), \n",
    "      '(', len(df_cur.loc[df_cur['FC'].isna() & df_cur['LC'].notna()].index), ')')\n",
    "print('Ratings common stats: Observations with only LC rating available distribution:','\\n',\n",
    "      df_cur['LC'].loc[df_cur['FC'].isna() & df_cur['LC'].notna()].groupby(['Market', 'Agency']).count())\n",
    "print('Both ratings available stats', '(', len(ser_cur_both.index), ')', ': Number of observations with equal ratings values:', \n",
    "      '{:.2%}'.format(len(ser_cur_both[ser_cur_both == 0].index) / len(ser_cur_both.index)), \n",
    "      '(', len(ser_cur_both[ser_cur_both == 0].index), ')')\n",
    "print('Both ratings available stats', '(', len(ser_cur_both.index), ')', ': Number of observations with different ratings values:', \n",
    "      '{:.2%}'.format(len(ser_cur_both[ser_cur_both != 0].index) / len(ser_cur_both.index)), \n",
    "      '(', len(ser_cur_both[ser_cur_both != 0].index), ')')\n",
    "print('Different ratings values', '(', len(ser_cur_differ.index), ')', ': Markets distribution:', '\\n',\n",
    "      ser_cur_differ.abs().groupby(['Market', 'Agency']).agg(['count', 'min', 'max', 'mean']))\n",
    "print('LC only ratings stats: Countries distribution:','\\n', ser_cur_LC.groupby(['Agency', 'Market', 'Country']).agg(['count', 'min', 'max', 'mean']))\n",
    "for iter_tup in arr_cur_keys:\n",
    "#    df_cur.loc[(iter_tup[0], All, iter_tup[1], iter_tup[2])].plot(figsize = (5, 2), title = str(iter_tup[0]) + ' / ' + str(iter_tup[1]) + ' / ' + str(iter_tup[2]),\n",
    "#                                                                  use_index = False, xticks = [])\n",
    "    iter_df_lc = df_cur.dropna().loc[(iter_tup[0], All, iter_tup[1], iter_tup[2])]\n",
    "    if (len(iter_df_lc.index) > 0):\n",
    "        print('LC only ratings stats: Number of observations for', iter_tup, 'with different ratings:',\n",
    "              '{:.2%}'.format(len(iter_df_lc.loc[iter_df_lc['FC'] == iter_df_lc['LC']].index) / len(iter_df_lc.index)))\n",
    "        print('LC only ratings stats: Ratio FC to LC ratings for', iter_tup, ':',\n",
    "              '{:.2%}'.format(iter_df_lc['FC'].mean() / iter_df_lc['LC'].mean()))\n",
    "    else:\n",
    "        print('LC only ratings stats: No observations to compare currency ratings for:', iter_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('FITCH', 'TW')\n",
      "('FITCH', 'UG')\n"
     ]
    }
   ],
   "source": [
    "### CURRENCY'S CHOOSING AND JOINING\n",
    "df_rating_cur = df_rating_type.unstack('Currency')\n",
    "df_rating_cur.sort_index(level = ['Agency', 'Date', 'Country'], inplace = True)\n",
    "df_rating_cur.columns = ['_'.join(iter_tup_col).rstrip('_') for iter_tup_col in df_rating_cur.columns.values]\n",
    "### Adding final columns:\n",
    "df_rating_cur['Rank'] = df_rating_cur['Rank_FC']\n",
    "df_rating_cur['Mark'] = df_rating_cur['Mark_FC']\n",
    "### Additional filling final columns with LC values in case if all FC values are empty:\n",
    "arr_lc_keys = df_rating_cur.loc[df_rating_cur['Rank_FC'].isna() & df_rating_cur['Rank_LC'].notna()].groupby(['Agency', 'Country']).groups.keys()\n",
    "for iter_tup in arr_lc_keys:\n",
    "    if (len(df_rating_cur['Rank_FC'].dropna().loc[(iter_tup[0], All, All, iter_tup[1])].index) == 0):\n",
    "        print(iter_tup)\n",
    "        iter_tup_index = (iter_tup[0], All, All, iter_tup[1])\n",
    "        df_rating_cur.loc[iter_tup_index, 'Mark'] = df_rating_cur.loc[iter_tup_index, 'Mark_LC']          \n",
    "        df_rating_cur.loc[iter_tup_index, 'Rank'] = df_rating_cur.loc[iter_tup_index, 'Rank_LC']\n",
    "df_rating_cur = df_rating_cur[['Rank', 'Mark']]        \n",
    "#df_rating_cur.to_excel('Data_Files/Test_Files/Currencies_Merging.xlsx', merge_cells = False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADDITIONAL REVISION MARKERS RESEARCH\n",
    "### Function for const mark length counting:\n",
    "def mark_length(ser_group_agency):\n",
    "    ser_mark_length = pd.Series(0, index = ser_group_agency.index)\n",
    "    ser_mark_length = ser_group_agency.groupby(((ser_group_agency != ser_group_agency.shift(1)) & (ser_group_agency.isna())).cumsum()).cumcount()\n",
    "    ser_mark_length.loc[ser_group_agency.isna() | ser_group_agency.shift(-1).notna()] = 0\n",
    "    return ser_mark_length\n",
    "### Common providers table:\n",
    "df_mark_plus = df_rating_cur.unstack('Agency').swaplevel(axis = 1).sort_index(ascending = [True, False], axis = 1).sort_index(level = ['Country', 'Date'])\n",
    "#print(df_mark_plus)\n",
    "for iter_agency in df_mark_plus.columns.get_level_values(0).unique():\n",
    "#    print(iter_agency)\n",
    "    ### Selecting provider data:\n",
    "    df_iter_plus = df_mark_plus[iter_agency]\n",
    "    ### Calculating revision mark length:\n",
    "    df_iter_plus = df_iter_plus.assign(Mark_Length = df_iter_plus['Mark'].groupby('Country').apply(lambda iter_group: mark_length(iter_group)))\n",
    "    df_iter_plus.replace(to_replace = 0, value = np.NaN, inplace = True)  \n",
    "    ### Calculating revisions after mark revision period end:\n",
    "    df_iter_plus = df_iter_plus.assign(Mark_Delta = df_iter_plus['Rank'].groupby('Country').diff().shift(-1))\n",
    "#    df_iter_plus.loc[df_iter_plus['Mark_Length'].isna(), 'Mark_Delta'] = np.NaN\n",
    "#    df_iter_plus.loc[df_iter_plus['Mark_Delta'].isna(), 'Mark_Length'] = np.NaN  \n",
    "    df_iter_plus['Mark_Delta'] = np.where(df_iter_plus['Mark_Length'].isna(), np.NaN, df_iter_plus['Mark_Delta'])\n",
    "    df_iter_plus['Mark_Length'] = np.where(df_iter_plus['Mark_Delta'].isna(), np.NaN, df_iter_plus['Mark_Length'])    \n",
    "    ### Marked ranks proportion printing:\n",
    "    print(iter_agency, ': Any marks proportion in total not-NaN ranks number:', \n",
    "          '{:.2%}'.format(df_iter_plus['Mark'].count() / df_iter_plus['Rank'].count()),\n",
    "          '(', df_iter_plus['Mark'].count(), 'observations )')\n",
    "    print(iter_agency, ': Positive marks proportion in total not-NaN ranks number:', \n",
    "          '{:.2%}'.format(len(df_iter_plus.loc[df_iter_plus['Mark'] == 1].index) / df_iter_plus['Rank'].count()),\n",
    "          '(', len(df_iter_plus.loc[df_iter_plus['Mark'] == 1].index), 'observations )')    \n",
    "    print(iter_agency, ': Negative marks proportion in total not-NaN ranks number:', \n",
    "          '{:.2%}'.format(len(df_iter_plus.loc[df_iter_plus['Mark'] == -1].index) / df_iter_plus['Rank'].count()),\n",
    "          '(', len(df_iter_plus.loc[df_iter_plus['Mark'] == -1].index), 'observations )')     \n",
    "    ### Marked ranks unbroken periods length printing:    \n",
    "    print(iter_agency, ': Average unbroken marked period length for any mark:', round(df_iter_plus['Mark_Length'].mean(), 2))\n",
    "    print(iter_agency, ': Average unbroken marked period length for positive mark:', \n",
    "          round(df_iter_plus.loc[df_iter_plus['Mark'] == 1, 'Mark_Length'].mean(), 2))    \n",
    "    print(iter_agency, ': Average unbroken marked period length for negative mark:', \n",
    "          round(df_iter_plus.loc[df_iter_plus['Mark'] == -1, 'Mark_Length'].mean(), 2)) \n",
    "    ### Marked ranks unbroken periods results printing:     \n",
    "    print(iter_agency, ': Marked periods number:', df_iter_plus['Mark_Delta'].count())   \n",
    "    print(iter_agency, ': Marked periods, that ends with revisions, proportion:',\n",
    "          '{:.2%}'.format(len(df_iter_plus.loc[df_iter_plus['Mark_Delta'].abs() > 0].index) / df_iter_plus['Mark_Delta'].count()),\n",
    "          '(', len(df_iter_plus.loc[df_iter_plus['Mark_Delta'].abs() > 0].index), 'periods )') \n",
    "    print(iter_agency, ': Marked periods, that with no revision, proportion:',\n",
    "          '{:.2%}'.format(len(df_iter_plus.loc[df_iter_plus['Mark_Delta'] == 0].index) / df_iter_plus['Mark_Delta'].count()),\n",
    "          '(', len(df_iter_plus.loc[df_iter_plus['Mark_Delta'] == 0].index), 'periods )')   \n",
    "    ### Positively marked ranks unbroken periods results printing:     \n",
    "    print(iter_agency, ': Positively marked periods number:', df_iter_plus.loc[df_iter_plus['Mark'] == 1, 'Mark_Delta'].count()) \n",
    "    if (df_iter_plus.loc[df_iter_plus['Mark'] == 1, 'Mark_Delta'].count() > 0):\n",
    "        print(iter_agency, ': Positively marked periods, that ends with revisions, proportion:',\n",
    "              '{:.2%}'.format(len(df_iter_plus.loc[(df_iter_plus['Mark'] == 1) & (df_iter_plus['Mark_Delta'].abs() > 0)].index) \\\n",
    "                              / df_iter_plus.loc[df_iter_plus['Mark'] == 1, 'Mark_Delta'].count()),\n",
    "              '(', len(df_iter_plus.loc[(df_iter_plus['Mark'] == 1) & (df_iter_plus['Mark_Delta'].abs() > 0)].index), 'periods )') \n",
    "        print(iter_agency, ': Positively marked periods, that ends with positive revisions, proportion:',\n",
    "              '{:.2%}'.format(len(df_iter_plus.loc[(df_iter_plus['Mark'] == 1) & (df_iter_plus['Mark_Delta'] > 0)].index) \\\n",
    "                              / df_iter_plus.loc[df_iter_plus['Mark'] == 1, 'Mark_Delta'].count()),\n",
    "              '(', len(df_iter_plus.loc[(df_iter_plus['Mark'] == 1) & (df_iter_plus['Mark_Delta'] > 0)].index), 'periods )') \n",
    "        print(iter_agency, ': Positively marked periods, that ends with negative revisions, proportion:',\n",
    "              '{:.2%}'.format(len(df_iter_plus.loc[(df_iter_plus['Mark'] == 1) & (df_iter_plus['Mark_Delta'] < 0)].index) \\\n",
    "                              / df_iter_plus.loc[df_iter_plus['Mark'] == 1, 'Mark_Delta'].count()),\n",
    "              '(', len(df_iter_plus.loc[(df_iter_plus['Mark'] == 1) & (df_iter_plus['Mark_Delta'] < 0)].index), 'periods )')     \n",
    "        print(iter_agency, ': Positively marked periods, that ends with no revision, proportion:',\n",
    "              '{:.2%}'.format(len(df_iter_plus.loc[(df_iter_plus['Mark'] == 1) & (df_iter_plus['Mark_Delta'] == 0)].index) \\\n",
    "                              / df_iter_plus.loc[df_iter_plus['Mark'] == 1, 'Mark_Delta'].count()),\n",
    "              '(', len(df_iter_plus.loc[(df_iter_plus['Mark'] == 1) & (df_iter_plus['Mark_Delta'] == 0)].index), 'periods )') \n",
    "        print(iter_agency, ': Positively marked periods ends mean positive revisions delta:',\n",
    "              round(df_iter_plus.loc[(df_iter_plus['Mark'] == 1) & (df_iter_plus['Mark_Delta'] > 0), 'Mark_Delta'].mean(), 2))\n",
    "    ### Negatively marked ranks unbroken periods results printing:      \n",
    "    print(iter_agency, ': Negatively marked periods number:', df_iter_plus.loc[df_iter_plus['Mark'] == -1, 'Mark_Delta'].count())\n",
    "    if (df_iter_plus.loc[df_iter_plus['Mark'] == -1, 'Mark_Delta'].count() > 0):    \n",
    "        print(iter_agency, ': Negatively marked periods, that ends with revisions, proportion:',\n",
    "              '{:.2%}'.format(len(df_iter_plus.loc[(df_iter_plus['Mark'] == -1) & (df_iter_plus['Mark_Delta'].abs() > 0)].index) \\\n",
    "                              / df_iter_plus.loc[df_iter_plus['Mark'] == -1, 'Mark_Delta'].count()),\n",
    "              '(', len(df_iter_plus.loc[(df_iter_plus['Mark'] == -1) & (df_iter_plus['Mark_Delta'].abs() > 0)].index), 'periods )') \n",
    "        print(iter_agency, ': Negatively marked periods, that ends with positive revisions, proportion:',\n",
    "              '{:.2%}'.format(len(df_iter_plus.loc[(df_iter_plus['Mark'] == -1) & (df_iter_plus['Mark_Delta'] > 0)].index) \\\n",
    "                              / df_iter_plus.loc[df_iter_plus['Mark'] == -1, 'Mark_Delta'].count()),\n",
    "              '(', len(df_iter_plus.loc[(df_iter_plus['Mark'] == -1) & (df_iter_plus['Mark_Delta'] > 0)].index), 'periods )') \n",
    "        print(iter_agency, ': Negatively marked periods, that ends with negative revisions, proportion:',\n",
    "              '{:.2%}'.format(len(df_iter_plus.loc[(df_iter_plus['Mark'] == -1) & (df_iter_plus['Mark_Delta'] < 0)].index) \\\n",
    "                              / df_iter_plus.loc[df_iter_plus['Mark'] == -1, 'Mark_Delta'].count()),\n",
    "              '(', len(df_iter_plus.loc[(df_iter_plus['Mark'] == -1) & (df_iter_plus['Mark_Delta'] < 0)].index), 'periods )')     \n",
    "        print(iter_agency, ': Negatively marked periods, that with no revision, proportion:',\n",
    "              '{:.2%}'.format(len(df_iter_plus.loc[(df_iter_plus['Mark'] == -1) & (df_iter_plus['Mark_Delta'] == 0)].index) \\\n",
    "                              / df_iter_plus.loc[df_iter_plus['Mark'] == -1, 'Mark_Delta'].count()),\n",
    "              '(', len(df_iter_plus.loc[(df_iter_plus['Mark'] == -1) & (df_iter_plus['Mark_Delta'] == 0)].index), 'periods )') \n",
    "        print(iter_agency, ': Negatively marked periods ends mean negative revisions delta:',\n",
    "              round(df_iter_plus.loc[(df_iter_plus['Mark'] == -1) & (df_iter_plus['Mark_Delta'] < 0), 'Mark_Delta'].mean(), 2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTING POSITIVELY MARKED RANKS:\n",
    "\n",
    "#df_rating_cur.loc[df_rating_cur['Mark'].notna(), 'Rank'] = df_rating_cur['Rank'] + df_rating_cur['Mark']\n",
    "\n",
    "### ZERO AVOIDING\n",
    "df_rating_cur.loc[(df_rating_cur['Mark'].notna() & ((df_rating_cur['Rank'] + df_rating_cur['Mark']) == 0)), 'Mark'] = df_rating_cur['Mark'] * 2\n",
    "df_rating_cur.loc[df_rating_cur['Mark'].notna(), 'Rank'] = df_rating_cur['Rank'] + df_rating_cur['Mark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RATINGS ANALYZING:\n",
    "### Creating table for comparision:\n",
    "df_agency = df_rating_cur['Rank'].unstack('Agency')\n",
    "df_agency.sort_index(level = ['Date', 'Country'], inplace = True)\n",
    "#df_agency.to_excel('Data_Files/Test_Files/Agencies_Unstacked.xlsx', merge_cells = False)\n",
    "### Creating auxiliar tables:\n",
    "dict_col = {}\n",
    "for iter_num, iter_col in enumerate(df_agency.columns):\n",
    "    dict_col[iter_col] = df_agency.columns.to_list()[ : iter_num] + df_agency.columns.to_list()[(iter_num + 1) : ]\n",
    "df_solo = df_agency[df_agency.isna().sum(axis = 1) == 2]\n",
    "df_duo = df_agency[df_agency.isna().sum(axis = 1) == 1]\n",
    "df_trio = df_agency.dropna()\n",
    "ser_difference = df_agency.dropna(thresh = 2).max(axis = 1) - df_agency.dropna(thresh = 2).min(axis = 1)\n",
    "ser_trio_difference = df_agency.dropna().max(axis = 1) - df_agency.dropna().min(axis = 1)\n",
    "### Comparision process:\n",
    "print('Ratings common stats: Number of all observations:', len(df_agency.index))\n",
    "print('Ratings common stats: Number of observations without any rating:', \n",
    "      '{:.2%}'.format(len(df_agency[df_agency.isna().all(axis = 1)].index) / len(df_agency.index)), '(', len(df_agency[df_agency.isna().all(axis = 1)].index), ')')\n",
    "print('No ratings stats', '(', len(df_agency[df_agency.isna().all(axis = 1)].index), ')', ': Countries distribution:', '\\n',\n",
    "      df_agency[df_agency.isna().all(axis = 1)].reset_index('Date').groupby(['Market', 'Country'])['Date'].agg(['first', 'last', 'size']))\n",
    "print('Ratings common stats: Number of observations with at least one rating available:', \n",
    "      '{:.2%}'.format(len(df_agency.dropna(how = 'all').index) / len(df_agency.index)), '(', len(df_agency.dropna(how = 'all').index), ')')\n",
    "print('Ratings common stats: Number of observations with exactly one rating available:', \n",
    "      '{:.2%}'.format(len(df_solo.index) / len(df_agency.index)), '(', len(df_solo.index), ')')\n",
    "print('One rating stats', '(', len(df_solo.index), ')', ': Countries distribution:', '\\n',\n",
    "      df_solo.reset_index('Date').groupby(['Market', 'Country'])['Date'].agg(['first', 'last', 'size']))\n",
    "print('One rating stats', '(', len(df_solo.index), ')', ': Agencies distribution:')\n",
    "for iter_col in df_solo.columns:\n",
    "    print(iter_col, ':',  '{:.2%}'.format(df_solo[iter_col].count() / len(df_solo.index)), '(', df_solo[iter_col].count(), ')')\n",
    "print('One rating stats', '(', len(df_solo.index), ')', ': Markets distribution:', '\\n', df_solo.groupby('Market').count()) \n",
    "print('Ratings common stats: Mean maximum pairwise difference for observations, where not all ranks are equal:', round(ser_difference[ser_difference != 0].mean(), 2))  \n",
    "print('Ratings common stats: Number of observations with exactly two ratings available:', \n",
    "      '{:.2%}'.format(len(df_duo.index) / len(df_agency.index)), '(', len(df_duo.index), ')')\n",
    "print('Two ratings stats', '(', len(df_duo.index), ')', ': Agencies distribution:')\n",
    "for iter_col in df_duo.columns:\n",
    "    iter_count = len(df_duo[dict_col[iter_col]].dropna().index)\n",
    "    iter_ser_diff = df_duo[dict_col[iter_col]].dropna().iloc[All, 0] - df_duo[dict_col[iter_col]].dropna().iloc[All, 1]\n",
    "    iter_ser_diff = iter_ser_diff[iter_ser_diff != 0].abs()\n",
    "#iter_ser_diff.mean()    \n",
    "    print(dict_col[iter_col], 'not NaN pairs:',\n",
    "          '{:.2%}'.format(iter_count / len(df_duo.index)), '(', iter_count, ') , equal observations part:', \n",
    "          '{:.2%}'.format(df_duo[dict_col[iter_col]].dropna().iloc[:, 0].ne(df_duo[dict_col[iter_col]].dropna().iloc[:, 1], axis = 0).sum() / iter_count),\n",
    "          '(', df_duo[dict_col[iter_col]].dropna().iloc[:, 0].ne(df_duo[dict_col[iter_col]].dropna().iloc[:, 1], axis = 0).sum(), ')')       \n",
    "    print(dict_col[iter_col], 'not NaN pairs:',\n",
    "          '{:.2%}'.format(iter_count / len(df_duo.index)), '(', iter_count, ') , not equal observations mean difference:', round(iter_ser_diff.mean(), 2))      \n",
    "print('Ratings common stats: Number of observations with all ratings available:', \n",
    "      '{:.2%}'.format(len(df_trio.index) / len(df_agency.index)), '(', len(df_trio.index), ')')\n",
    "print('Three ratings stats: All ratings equal:', '{:.2%}'.format(df_trio.eq(df_trio.iloc[:, 0], axis = 0).all(axis = 1).sum() / len(df_trio.index)),\n",
    "      '(', df_trio.eq(df_trio.iloc[:, 0], axis = 0).all(axis = 1).sum(), ')')\n",
    "print('Three ratings stats', '(', len(df_trio.index), ')', ': Two of three ratings equality distribution:')\n",
    "num_pair_sum = 0\n",
    "for iter_col in df_trio.columns:\n",
    "    iter_pair_sum = df_trio[dict_col[iter_col][0]].eq(df_trio[dict_col[iter_col][1]]).sum() - df_trio.eq(df_trio.iloc[:, 0], axis = 0).all(axis = 1).sum()\n",
    "    num_pair_sum = num_pair_sum + iter_pair_sum\n",
    "    print('Equal just pair', dict_col[iter_col], ':', '{:.2%}'.format(iter_pair_sum / len(df_trio.index)), '(', iter_pair_sum, ')')   \n",
    "print('Three ratings stats: Total equal pairs number:', '{:.2%}'.format(num_pair_sum / len(df_trio.index)), '(', num_pair_sum, ')')   \n",
    "print('Three ratings stats: All different values:',\n",
    "      '{:.2%}'.format(1 - (num_pair_sum + df_trio.eq(df_trio.iloc[:, 0], axis = 0).all(axis = 1).sum()) / len(df_trio.index)),\n",
    "      '(', len(df_trio.index) - (df_trio.eq(df_trio.iloc[:, 0], axis = 0).all(axis = 1).sum() + num_pair_sum), ')')   \n",
    "print('Three ratings stats: Mean maximum pairwise difference for observations, where not all ranks are equal:', \n",
    "      round(ser_trio_difference[ser_trio_difference != 0].mean(), 2))  \n",
    "df_agency.reset_index(level = 'Market', drop = True, inplace = True)\n",
    "### Additional cross-sectional stats generating and saving:\n",
    "#df_agency['Delta'] = (df_agency.max(axis = 1) - df_agency.min(axis = 1)).abs()\n",
    "#df_agency.unstack('Country').reorder_levels([1, 0], axis = 1).sort_index(level = [0, 1], axis = 1).to_excel('Data_Files/Test_Files/Countries.xlsx', merge_cells = False)\n",
    "#df_agency.sort_values('Delta', ascending = False).index.get_level_values(1).unique()\n",
    "### Additional timline stats generating and saving:\n",
    "#df_delta = df_agency.sort_index(level = ['Country', 'Date']).copy()\n",
    "#arr_delta = []\n",
    "#for iter_col in df_delta.columns:\n",
    "#    df_delta[iter_col + '_delta'] =  np.NaN\n",
    "#for iter_country in df_delta.index.get_level_values(1).unique():\n",
    "#    iter_df = df_delta.loc[(All, iter_country), :].sort_index()\n",
    "#    for iter_col in df_agency.columns:\n",
    "#        iter_df[iter_col + '_delta'] = (iter_df[iter_col] - iter_df[iter_col].shift()).abs()\n",
    "#    arr_delta.append(iter_df)\n",
    "#df_delta = pd.concat(arr_delta).sort_index(level = ['Country', 'Date'])\n",
    "#df_delta.unstack('Country').reorder_levels([1, 0], axis = 1).sort_index(level = [0, 1], \n",
    "#                                                                        axis = 1)[['AR', 'GR', 'ID', 'IE', 'PE', 'PT']].to_excel('Data_Files/Test_Files/Agencies.xlsx', \n",
    "#                                                                                                                                 merge_cells = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COLLAPSING VARIANTS COMPARING\n",
    "### Function for const length counting:\n",
    "def const_length(ser_group_agency):\n",
    "    ser_group_weight = pd.Series(1, index = ser_group_agency.index)\n",
    "    ser_group_weight = ser_group_agency.groupby((ser_group_agency != ser_group_agency.shift(1)).cumsum()).cumcount() + 1\n",
    "    return ser_group_weight\n",
    "### Table for rankings keeping:\n",
    "df_rating_agency = df_rating_cur['Rank'].unstack('Agency').sort_index(level = ['Country', 'Date'])\n",
    "### Array of agencies names:\n",
    "arr_agency = df_rating_agency.columns\n",
    "### Table for collapsing variants collecting:\n",
    "df_rating_res = pd.DataFrame(index = df_rating_agency.index)\n",
    "df_rating_res['Median'] = round(df_rating_agency.median(axis = 1), 2)\n",
    "df_rating_res['Mean'] = round(df_rating_agency.mean(axis = 1), 2)\n",
    "### Table of freshness weight types:\n",
    "#arr_weight_kind = ['month minus', 'month over', 'year over', 'month trigger', 'year trigger', 'month half life']\n",
    "arr_weight_kind = ['month half life']\n",
    "### Weights for half-life:\n",
    "#num_window = 310\n",
    "num_halflife = 3\n",
    "#ser_weights_halflife = get_exp_weights_series(window_len = num_window, halflife_len = num_halflife)[::-1].reset_index(drop = True)\n",
    "\n",
    "### Freshness weights calculation:\n",
    "for iter_kind in arr_weight_kind:\n",
    "    df_rating_agency = df_rating_cur['Rank'].unstack('Agency').sort_index(level = ['Country', 'Date'], ascending = [True, True])\n",
    "    ### Initialising weights vector\n",
    "    df_rating_weight = pd.DataFrame(index = df_rating_agency.index)\n",
    "    ### Unchanged datepoints quantity calculating:\n",
    "    for iter_agency in arr_agency:\n",
    "        df_rating_weight[iter_agency] = 1\n",
    "        df_rating_weight[iter_agency] = df_rating_agency[iter_agency].groupby(['Country']).apply(lambda iter_group: const_length(iter_group)) \n",
    "    ### Freshness weights calculation:\n",
    "    if (iter_kind == 'month minus'):\n",
    "        ### Weight as unchanged datepoints quantity with changed sign:      \n",
    "        df_rating_weight = -df_rating_weight.sub(df_rating_weight.sum(axis = 1), axis = 0)\n",
    "    if (iter_kind == 'month over'):        \n",
    "        ### Weight as inversed unchanged datepoints quantity:\n",
    "        df_rating_weight = 1 / df_rating_weight\n",
    "    if (iter_kind == 'year over'):\n",
    "        ### Weight as inversed unchanged datepoints years quantity:\n",
    "        df_rating_weight = ((df_rating_weight - 1) // 12) + 1  \n",
    "        df_rating_weight = 1 / df_rating_weight         \n",
    "    if (iter_kind == 'month trigger'):\n",
    "        ### Weight as inversed unchanged datepoints quantity revised only on any change:\n",
    "        df_rating_weight = df_rating_weight.sub(df_rating_weight.min(axis = 1) - 1, axis = 0)\n",
    "        df_rating_weight = 1 / df_rating_weight  \n",
    "    if (iter_kind == 'year trigger'):\n",
    "        ### Weight as inversed unchanged datepoints years quantity revised only on any change:\n",
    "        df_rating_weight = df_rating_weight.sub(df_rating_weight.min(axis = 1) - 1, axis = 0)        \n",
    "        df_rating_weight = ((df_rating_weight - 1) // 12) + 1              \n",
    "        df_rating_weight = 1 / df_rating_weight \n",
    "    if (iter_kind == 'month half life'):\n",
    "        ### Weight as exponentially weighted unchanged datepoints quantity revised only on any change:\n",
    "        for iter_agency in df_rating_agency.columns:\n",
    "            df_rating_weight[iter_agency] = get_exp_weight_single(num_halflife, df_rating_weight[iter_agency])\n",
    "#            pd.Series(ser_weights_halflife[df_rating_weight[iter_agency]].values, index = df_rating_weight[iter_agency].index)             \n",
    "    ### Zeroing of weights of NaN rank values:\n",
    "    df_rating_weight[df_rating_agency.isna()] = 0    \n",
    "    ### Zeroing NaN rank values:\n",
    "#    df_rating_agency.fillna(0, inplace = True)\n",
    "    ### Freshness means calculation:\n",
    "    df_rating_weight  = df_rating_weight.div(df_rating_weight.sum(axis = 1), axis = 0)  \n",
    "    df_rating_res['Fresh ' + iter_kind] = round(df_rating_agency.mul(df_rating_weight, axis = 0).sum(axis = 1), 2)\n",
    "\n",
    "### Freshness means completion\n",
    "df_rating_res[df_rating_agency.isna().sum(axis = 1) == 3] = np.NaN\n",
    "df_rating_res[(round(df_rating_res, 0) == 0) & (df_rating_res <= 0)] = -0.5 - 1 / 10000000\n",
    "df_rating_res[(round(df_rating_res, 0) == 0) & (df_rating_res > 0)] = 0.5 + 1 / 10000000\n",
    "### Aggregated table:\n",
    "#df_rating_collapsed_plus = pd.concat([df_rating_agency, df_rating_weight, df_rating_weight_natural, df_rating_res], axis = 1, \n",
    "#                                sort = True).sort_index(level = ['Country', 'Date'])\n",
    "df_rating_collapsed = pd.concat([df_rating_agency, df_rating_res], axis = 1, sort = True).sort_index(level = ['Country', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTER\n",
    "round(df_rating_collapsed.dropna(how = 'all'), 0).to_excel('Data_Files/Test_Files/Test_Monthly_Rounded.xlsx', merge_cells = False)      \n",
    "round(df_rating_collapsed.dropna(how = 'all'), 2).to_excel('Data_Files/Test_Files/Test_Monthly_Not_Rounded.xlsx', merge_cells = False)    \n",
    "#round(df_rating_collapsed, 0).to_excel('Data_Files/Test_Files/Collapsing_Full_Marked_03.xlsx', merge_cells = False)\n",
    "#df_rating_collapsed_plus.to_excel('Data_Files/Test_Files/Test_Weighting.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESULTS SAVING\n",
    "ser_ranking = round(df_rating_collapsed.iloc[:, -1].dropna(), 0)\n",
    "ser_ranking.name = 'Collapsed'\n",
    "path_collapsed_monthly = 'Data_Files/Source_Files/Collapsed_Monthly_Non_Zero.h5'\n",
    "key_collapsed_monthly = 'Rank'\n",
    "ser_ranking.to_hdf(path_collapsed_monthly, key_collapsed_monthly, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DAILY RESAMPLING FOR REVISION MONTHS   \n",
    "df_rating_agg = df_rating_collapsed.copy()\n",
    "df_rating_agg.drop(['Mean'], axis = 1, inplace = True)\n",
    "df_rating_agg.columns = ['FITCH', 'MDY', 'SP', 'Median', 'COLLAPSED']\n",
    "df_rating_agg = df_rating_agg[['FITCH', 'MDY', 'SP']]\n",
    "dict_ser_deltas = {}\n",
    "### Main loop:\n",
    "for iter_agency in df_rating_agg.columns: ### Agencies iterating\n",
    "    ### Grades series defining:\n",
    "    iter_ser_grades = df_rating_agg[iter_agency].sort_index(level = ['Country', 'Date'])\n",
    "    iter_ser_grades = iter_ser_grades.groupby('Country').filter(lambda iter_group: len(iter_group.dropna()) > 0)\n",
    "    iter_ser_grades = iter_ser_grades.groupby('Country').apply(lambda iter_group: iter_group[iter_group.ffill().dropna().index]).reset_index(0, drop = True)\n",
    "    iter_ser_grades.fillna(0, inplace = True)\n",
    "    ### First valid index for each country:\n",
    "    iter_ser_first_valid = iter_ser_grades.abs().groupby('Country').apply(lambda iter_group: iter_group.iloc[0 : 1]).reset_index(0, drop = True)\n",
    "    ### Deltas series calculation:\n",
    "    iter_ser_deltas = iter_ser_grades.dropna().groupby('Country').diff().sort_index(level = ['Country', 'Date'], ascending = [True, True]).abs().dropna()\n",
    "    iter_ser_deltas = iter_ser_deltas.loc[iter_ser_deltas > 0]\n",
    "    iter_ser_revision = pd.concat([iter_ser_first_valid, iter_ser_deltas], axis = 0).sort_index(level = ['Country', 'Date'])\n",
    "    iter_ser_revision = iter_ser_revision / iter_ser_revision\n",
    "    iter_ser_revision = iter_ser_revision.reset_index()\n",
    "    iter_ser_revision['Month'] = iter_ser_revision['Date'].dt.to_period('M')\n",
    "    iter_ser_revision = iter_ser_revision.set_index(['Country', 'Month'], drop = True).drop('Date', axis = 1)\n",
    "    iter_ser_revision = iter_ser_revision.groupby(['Country', 'Month']).apply(lambda iter_group: iter_group.droplevel(0).resample('B').ffill())\n",
    "    iter_ser_revision.reset_index(1, drop = True, inplace = True)\n",
    "    iter_ser_revision = iter_ser_revision.drop('Market', axis = 1).squeeze()\n",
    "    iter_ser_revision.to_excel('Data_Files/Test_Files/Revisions_Daily_' + iter_agency + '.xlsx', merge_cells = False)\n",
    "\n",
    "\n",
    "    with pd.ExcelWriter('Data_Files/Test_Files/Revisions_Daily.xlsx') as writer:\n",
    "        iter_ser_deltas.to_excel(writer, sheet_name = iter_agency, merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AGENCIES BEHAVIOR AND DISTRIBUTION ANALYZING\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "### Constants for universe filtering:\n",
    "arr_tup_index = [(All, All, All), (All, 'DM', All), (All, 'EM', All), (All, 'FM', All)]\n",
    "arr_tup_market = ['All', 'DM', 'EM', 'FM']\n",
    "#arr_tup_index = [(All, All, All), (All, 'DM', All), (All, ['EM', 'FM'], All)]\n",
    "#arr_tup_market = ['All', 'DM', 'EM & FM']\n",
    "### Constants for crosstab plotting:\n",
    "arr_ranks = np.arange(-12, 11, 1)\n",
    "arr_ranks = arr_ranks[arr_ranks != 0]\n",
    "### Constants for rolling calculations:\n",
    "arr_roll_max = [12, 36, 60]\n",
    "### Function for calculating length of constant rating periods:\n",
    "def rolling_count(iter_delta):\n",
    "    if (iter_delta == 0):\n",
    "        rolling_count.iter_counter = rolling_count.iter_counter + 1\n",
    "        rolling_count.iter_result = 0\n",
    "    else:\n",
    "        rolling_count.iter_result = rolling_count.iter_counter\n",
    "        rolling_count.iter_counter = 1\n",
    "    return rolling_count.iter_result\n",
    "rolling_count.iter_counter = 1 \n",
    "rolling_count.iter_result = 0\n",
    "### Receiving MSCI membership data:    \n",
    "ser_market_membership = get_market_membership_from_excel(path_msci)\n",
    "### Preparing data for analysing:\n",
    "df_rating_agg = df_rating_collapsed.copy()\n",
    "#df_rating_agg.drop(['Median', 'Mean'], axis = 1, inplace = True)\n",
    "#df_rating_agg.columns = ['FITCH', 'MDY', 'SP', 'COLLAPSED']\n",
    "df_rating_agg.drop(['Mean'], axis = 1, inplace = True)\n",
    "df_rating_agg.columns = ['FITCH', 'MDY', 'SP', 'Median', 'COLLAPSED']\n",
    "df_rating_agg['COLLAPSED'] = round(df_rating_agg['COLLAPSED'], 0)\n",
    "df_rating_agg['Median'] = round(df_rating_agg['Median'], 0)\n",
    "df_rating_agg = df_rating_agg.reindex(sorted(df_rating_agg.columns), axis = 1)\n",
    "### Main loop:\n",
    "for iter_agency in df_rating_agg.columns: ### Agencies iterating\n",
    "    for iter_num, iter_tup_market in enumerate(arr_tup_index):  ### Universe filters iterating\n",
    "        ### Grades series defining:\n",
    "        iter_ser_grades = df_rating_agg.loc[iter_tup_market, iter_agency].dropna().sort_index(level = ['Country', 'Date'], ascending = [True, True])\n",
    "        ### Counter for country rankings:\n",
    "        iter_ser_counter = iter_ser_grades.groupby('Country').count()\n",
    "        ### Deltas series calculation:\n",
    "        iter_ser_deltas = df_rating_agg.loc[All, iter_agency].dropna().groupby('Country').diff().sort_index(level = ['Country', 'Date'], ascending = [True, True])\n",
    "        ### Rolling revisions number for every date calculation:\n",
    "        arr_df_roll_flag_date = {}\n",
    "        arr_df_roll_flag_market = {}    \n",
    "        arr_df_crosstab = {}        \n",
    "        for iter_roll_max in arr_roll_max:\n",
    "            num_roll_max = iter_roll_max\n",
    "            num_roll_min = 2 # num_roll_max // 2\n",
    "            iter_ser_flag = iter_ser_deltas.copy()\n",
    "            iter_ser_flag[iter_ser_flag != 0] = iter_ser_flag / iter_ser_flag.abs()\n",
    "            iter_ser_flag_plus = iter_ser_flag.copy()\n",
    "            iter_ser_flag_plus[iter_ser_flag_plus != 1] = 0\n",
    "            iter_ser_flag_minus = iter_ser_flag.copy()\n",
    "            iter_ser_flag_minus[iter_ser_flag_minus != -1] = 0\n",
    "            iter_ser_roll_flag_plus = iter_ser_flag_plus.groupby('Country').rolling(window = num_roll_max, min_periods = num_roll_min).sum()\n",
    "            iter_ser_roll_flag_plus = iter_ser_roll_flag_plus.reset_index(level = 0, drop = True)\n",
    "            iter_ser_roll_flag_minus = iter_ser_flag_minus.abs().groupby('Country').rolling(window = num_roll_max, min_periods = num_roll_min).sum()\n",
    "            iter_ser_roll_flag_minus = iter_ser_roll_flag_minus.reset_index(level = 0, drop = True)\n",
    "            iter_ser_roll_flag = iter_ser_roll_flag_plus + iter_ser_roll_flag_minus\n",
    "            iter_df_roll_flag = pd.concat([iter_ser_roll_flag, iter_ser_roll_flag_plus, iter_ser_roll_flag_minus], axis = 1)\n",
    "            iter_df_roll_flag.sort_index(level = ['Country', 'Date'], ascending = [True, True], inplace = True)  \n",
    "            iter_df_roll_flag = iter_df_roll_flag.loc[iter_tup_market, All]\n",
    "            iter_df_roll_flag.columns = ['Any', 'Plus', 'Minus']\n",
    "            iter_df_roll_flag_date = iter_df_roll_flag.groupby('Date').sum()          \n",
    "            arr_df_roll_flag_date[num_roll_max] = iter_df_roll_flag_date\n",
    "            iter_df_roll_flag_market_all = iter_df_roll_flag.groupby('Date')\n",
    "            iter_df_roll_flag_market_revised = iter_df_roll_flag[iter_df_roll_flag['Any'] > 0].groupby('Date')\n",
    "            iter_df_roll_flag_market = iter_df_roll_flag_market_revised.count().div(iter_df_roll_flag_market_all.count())['Any']\n",
    "            iter_df_roll_flag_market.fillna(0, inplace = True)\n",
    "            arr_df_roll_flag_market[num_roll_max] = iter_df_roll_flag_market            \n",
    "            ### Transparency matrix preparing:\n",
    "            iter_ser_crosstab = df_rating_agg.loc[All, iter_agency].dropna()\n",
    "            num_crosstab_plus = iter_roll_max\n",
    "            iter_ser_crosstab_step = iter_ser_crosstab.groupby('Country').shift(-num_crosstab_plus)\n",
    "            df_crosstab = pd.concat([iter_ser_crosstab, iter_ser_crosstab_step], ignore_index = True, axis = 1)\n",
    "            df_crosstab = df_crosstab.loc[iter_tup_market, All]    \n",
    "            df_crosstab.sort_index(level = ['Country', 'Date'], ascending = [True, True], inplace = True)\n",
    "            df_crosstab.columns = ['Rank t', 'Rank t + ' + str(num_crosstab_plus)]\n",
    "            df_crosstab.dropna(inplace = True)\n",
    "            df_crosstab = round(df_crosstab, 0)\n",
    "            df_crosstab = df_crosstab.astype(int)\n",
    "            df_crosstab = pd.crosstab(df_crosstab['Rank t'], df_crosstab['Rank t + ' + str(num_crosstab_plus)], normalize = 0, dropna = False)\n",
    "            df_crosstab = df_crosstab.reindex(arr_ranks, axis = 0)\n",
    "            df_crosstab = df_crosstab.reindex(arr_ranks, axis = 1)\n",
    "            df_crosstab.fillna(0.00, inplace = True)\n",
    "            df_crosstab = round(df_crosstab, 2)      \n",
    "            arr_df_crosstab[num_crosstab_plus] = df_crosstab      \n",
    "        ### Constant periods lengths series calculation:\n",
    "        iter_ser_deltas = iter_ser_deltas.shift(-1)    \n",
    "        iter_ser_distances = iter_ser_deltas.apply(rolling_count)\n",
    "        iter_ser_distances = iter_ser_distances.loc[iter_tup_market]\n",
    "        iter_ser_deltas = iter_ser_deltas.shift()  \n",
    "        iter_ser_deltas = iter_ser_deltas.loc[iter_tup_market]\n",
    "        ### Deltas number for countries calculation:        \n",
    "        iter_ser_deltas_countries = iter_ser_deltas.dropna()[iter_ser_deltas != 0].groupby('Country').count()\n",
    "        iter_ser_deltas_countries = iter_ser_deltas_countries / iter_ser_deltas.dropna().groupby('Country').count()\n",
    "        iter_ser_deltas_countries.fillna(0, inplace = True)\n",
    "        iter_ser_deltas_countries = iter_ser_deltas_countries * iter_ser_deltas.dropna().groupby('Country').count()\n",
    "        \n",
    "        iter_ser_deltas_countries.name = 'Country revisions number'\n",
    "        ### Mean deltas for universe filter:\n",
    "        iter_ser_deltas_mean = pd.Series(iter_ser_deltas_countries.mean(), index = iter_ser_deltas_countries.index)        \n",
    "        iter_ser_deltas_mean.name = iter_agency + ' / ' + arr_tup_market[iter_num] + ' mean'\n",
    "        ### Filtered universe grades distribution drawing:\n",
    "        (iter_ser_grades.groupby(iter_ser_grades).count() / iter_ser_grades.count()).plot(kind = 'bar', \n",
    "                                                                                          title = iter_agency + ' / ' + arr_tup_market[iter_num] + ' - distribution')\n",
    "        plt.show()        \n",
    "        ### Timeline for filtered market time-series aggregating:\n",
    "        grouper_market_by_country = iter_ser_grades.groupby('Country')\n",
    "        ### Timeline for filtered market plotting:\n",
    "        grouper_market_by_date = iter_ser_grades.groupby('Date')\n",
    "        df_timeline_market = pd.concat([grouper_market_by_date.max(), grouper_market_by_date.quantile(0.75), grouper_market_by_date.median(), \n",
    "                                        grouper_market_by_date.quantile(0.25), grouper_market_by_date.min()], axis = 1)\n",
    "        df_timeline_market.columns = ['max', 'quantile 3/4', 'median', 'quantile 1/4', 'min']\n",
    "        df_timeline_market.plot(figsize = (15, 5), title = iter_agency + ' / ' + arr_tup_market[iter_num] + ' - timeline')\n",
    "        plt.show()        \n",
    "        ### Timeline for filtered countries plotting:\n",
    "        dict_color = {'boxes': 'DarkBlue', 'medians': 'Green', 'whiskers': 'Blue', 'caps': 'Black'}\n",
    "        iter_ser_grades.unstack('Country').reindex(iter_ser_grades.unstack('Country').median().sort_values().index, axis = 1).plot.box(figsize = (20, 5), \n",
    "                                                                                title = iter_agency + ' / ' + arr_tup_market[iter_num] + ' - by country box plot',\n",
    "                                                                                color = dict_color, sym = 'r+', whis = [10, 90])\n",
    "        plt.show()\n",
    "        ### Revisions number for filtered countries plotting:        \n",
    "        iter_ser_deltas_countries.plot.bar(figsize = (20, 5), title = iter_agency + ' / ' + arr_tup_market[iter_num] + ' - by country revisions number',\n",
    "                                           legend = True, width = 0.5)\n",
    "        iter_ser_deltas_mean.plot(color = 'Orange', legend = True)\n",
    "        plt.show()\n",
    "        for iter_roll_max in arr_roll_max:        \n",
    "            ### Transition probability matrix plotting:\n",
    "            num_crosstab_plus = iter_roll_max\n",
    "            df_crosstab = arr_df_crosstab[num_crosstab_plus]\n",
    "            plt.figure(figsize = (10, 10))\n",
    "            plot_heatmap = sns.heatmap(df_crosstab, linewidths = 0.1, square = True, annot = True, annot_kws = {'size': 8},\n",
    "                        cmap = 'GnBu')\n",
    "            plot_heatmap.set_title(iter_agency + ': ' + arr_tup_market[iter_num] + ' markets: Transition probability (k = ' + str(num_crosstab_plus) + ')')\n",
    "            plt.show()      \n",
    "            ### Revisions number by date plotting:\n",
    "            num_roll_max = iter_roll_max\n",
    "            iter_df_roll_flag_date = arr_df_roll_flag_date[num_roll_max]\n",
    "            plot_roll_flag_date = iter_df_roll_flag_date.plot(figsize = (20, 5), x_compat = True)\n",
    "            plot_roll_flag_date.set_title(iter_agency + ' / ' +  arr_tup_market[iter_num] + ': Revisions number in last ' + str(num_roll_max) + ' months')\n",
    "            plot_roll_flag_date.xaxis.set_major_locator(mdates.YearLocator())\n",
    "            plt.gcf().autofmt_xdate()\n",
    "            plt.show() \n",
    "            \n",
    "            iter_df_roll_flag_market = arr_df_roll_flag_market[num_roll_max]\n",
    "            plot_roll_flag_market = iter_df_roll_flag_market.plot(figsize = (20, 3), x_compat = True)\n",
    "            plot_roll_flag_market.set_title(iter_agency + ' / ' +  arr_tup_market[iter_num] + ': Revised members proportion in last ' + str(num_roll_max) + ' months')\n",
    "            plot_roll_flag_market.xaxis.set_major_locator(mdates.YearLocator())    \n",
    "            plt.gcf().autofmt_xdate()\n",
    "            plt.show()             \n",
    "        ### Displaying statistics for filtered universe:\n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Countries covered:', iter_ser_counter.count())   \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Average observations per country:', round(iter_ser_counter.mean(), 2)) \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Mean coverage per country:', '{:.2%}'.format((iter_ser_counter / (ser_market_membership.groupby('Country').size() - 1)).mean()))\n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Investment grade:', '{:.2%}'.format(len(iter_ser_grades[iter_ser_grades > 0].index) / iter_ser_grades.count()), \n",
    "              '(', len(iter_ser_grades[iter_ser_grades > 0].index), ')')\n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Non-investment grade:', '{:.2%}'.format(len(iter_ser_grades[iter_ser_grades < 0].index) / iter_ser_grades.count()), \n",
    "              '(', len(iter_ser_grades[iter_ser_grades < 0].index), ')')      \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Any revisions frequency:', '{:.2%}'.format(len(iter_ser_deltas.dropna()[iter_ser_deltas != 0].index) / iter_ser_grades.count()), \n",
    "              '(', len(iter_ser_deltas.dropna()[iter_ser_deltas != 0].index), ')')\n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Positive revisions frequency:', '{:.2%}'.format(len(iter_ser_deltas[iter_ser_deltas > 0].index) / iter_ser_grades.count()), \n",
    "              '(', len(iter_ser_deltas[iter_ser_deltas > 0].index), ')')  \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Negative revisions frequency:', '{:.2%}'.format(len(iter_ser_deltas[iter_ser_deltas < 0].index) / iter_ser_grades.count()), \n",
    "              '(', len(iter_ser_deltas[iter_ser_deltas < 0].index), ')')\n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Average revisions number for country:', round(iter_ser_deltas_countries.mean(), 2))        \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', iter_ser_grades.count(), ')',\n",
    "              ': Average stable rating period lentgh (months):', \n",
    "              round(iter_ser_distances[iter_ser_distances > 0].mean(), 2)) \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean revision absolute step:', \n",
    "              round(iter_ser_deltas.abs()[iter_ser_deltas != 0].mean(), 2))   \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean revision positive step:', \n",
    "              round(iter_ser_deltas[iter_ser_deltas > 0].mean(), 2))\n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean revision negative step:', \n",
    "              round(iter_ser_deltas[iter_ser_deltas < 0].mean(), 2))\n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean cross-sectional interquantile range (between 0.9 and 0.1 quantile):', \n",
    "              round((grouper_market_by_date.quantile(0.90) - grouper_market_by_date.quantile(0.10)).mean(), 2))\n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean cross-sectional median:', \n",
    "              round(grouper_market_by_date.median().mean(), 2))       \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean country interquantile range (between 0.9 and 0.1 quantile):', \n",
    "              round((grouper_market_by_country.quantile(0.90) - grouper_market_by_country.quantile(0.10)).mean(), 2))\n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Median country interquantile range (between 0.9 and 0.1 quantile):', \n",
    "              round((grouper_market_by_country.quantile(0.90) - grouper_market_by_country.quantile(0.10)).median(), 2))       \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets:  Mean absolute deviation for countries mean:', \n",
    "              round(grouper_market_by_country.mad().mean(), 2))         \n",
    "        print(iter_agency, ':', arr_tup_market[iter_num], 'markets:  Mean absolute deviation for countries median:', \n",
    "              round(grouper_market_by_country.mad().median(), 2))        \n",
    "        for iter_roll_max in arr_roll_max:\n",
    "            num_roll_max = iter_roll_max\n",
    "            iter_df_roll_flag_date = arr_df_roll_flag_date[num_roll_max]\n",
    "            iter_df_roll_flag_market = arr_df_roll_flag_market[num_roll_max]\n",
    "            print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', len(iter_df_roll_flag_date.index), 'datepoints)',\n",
    "                  ': Datepoints with at least one revision in last', num_roll_max,'months:', \n",
    "                  '{:.2%}'.format(len(iter_df_roll_flag_date[iter_df_roll_flag_date['Any'] > 0].index) / len(iter_df_roll_flag_date.index)), \n",
    "                  '(', len(iter_df_roll_flag_date[iter_df_roll_flag_date['Any'] > 0].index), ')')\n",
    "            print(iter_agency, ':', arr_tup_market[iter_num], 'markets (', len(iter_df_roll_flag_date.index), 'datepoints)',\n",
    "                  ': Datepoints with no revisions in last', num_roll_max,'months:', \n",
    "                  '{:.2%}'.format(len(iter_df_roll_flag_date[iter_df_roll_flag_date['Any'] == 0].index) / len(iter_df_roll_flag_date.index)), \n",
    "                  '(', len(iter_df_roll_flag_date[iter_df_roll_flag_date['Any'] == 0].index), ')')\n",
    "            print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean any revisions number in last', num_roll_max,'months from datepoint:',\n",
    "                  round(iter_df_roll_flag_date['Any'].mean(), 2)) \n",
    "            print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean positive revisions number in last', num_roll_max,'months from datepoint:',\n",
    "                  round(iter_df_roll_flag_date['Plus'].mean(), 2)) \n",
    "            print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean negative revisions number in last', num_roll_max,'months from datepoint:',\n",
    "                  round(iter_df_roll_flag_date['Minus'].mean(), 2))   \n",
    "            print(iter_agency, ':', arr_tup_market[iter_num], 'markets: Mean proportion of members revised in last', num_roll_max,'months from market:',\n",
    "                  '{:.2%}'.format(round(iter_df_roll_flag_market.mean(), 4)))         \n",
    "\n",
    "#        if (iter_num == 1):\n",
    "#            break\n",
    "        break        \n",
    "#    if (iter_agency == 'FITCH'):\n",
    "#        break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVING COLLAPSE RANKING TO HDF FILE\n",
    "ser_ranking = df_rating_agg['COLLAPSED']\n",
    "ser_ranking.name = 'Rank'\n",
    "path_collapsed = 'Data_Files/Source_Files/Collapsed_Rank_Not_Marked.h5'\n",
    "path_collapsed_marked = 'Data_Files/Source_Files/Collapsed_Rank_Marked.h5'\n",
    "key_collapsed = 'Rank'\n",
    "#ser_ranking.to_hdf(path_collapsed, key_collapsed, mode = 'w', format = 'fixed')\n",
    "#ser_ranking.to_hdf(path_collapsed_marked, key_collapsed, mode = 'w', format = 'fixed')\n",
    "#ser_ranking.to_excel('Data_Files/Test_Files/Test_Collapsed_Rank_Marked.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COUNTRY ISO CODES EXTRACTOR\n",
    "def get_country_codes(use_local_copy = False):  \n",
    "    import pandas as pd\n",
    "    \n",
    "    if (use_local_copy):\n",
    "        url_country_code = 'Data_Files/Source_Files/countrycode.html'\n",
    "    else:\n",
    "        url_country_code = 'https://countrycode.org/'\n",
    "    df_full_codes = pd.read_html(url_country_code, index_col = 'COUNTRY')[0]\n",
    "    df_full_codes[['ISO SHORT', 'ISO LONG']] = df_full_codes['ISO CODES'].str.split(' / ', expand = True)\n",
    "    df_result = df_full_codes[['ISO SHORT', 'ISO LONG']]      \n",
    "    df_result.index = df_result.index.str.upper()\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INTERACTIVE RANKING WORLD MAP VISUALIZING\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon, Polygon    \n",
    "import bokeh.plotting as b_pl\n",
    "import bokeh.models as b_md    \n",
    "from bokeh.palettes import inferno, viridis, cividis, magma, plasma, all_palettes\n",
    "from bokeh.layouts import widgetbox\n",
    "from bokeh.layouts import column as b_col\n",
    "from bokeh.layouts import row as b_row\n",
    "from bokeh.models.widgets import DateSlider, Select\n",
    "from bokeh.models import CustomJS\n",
    "\n",
    "path_collapsed = 'Data_Files/Source_Files/Collapsed_Rank_Marked.h5'\n",
    "key_collapsed = 'Rank'\n",
    "ser_ranking = pd.read_hdf(path_collapsed, key_collapsed)\n",
    "\n",
    "path_countries_map_shp = 'Data_Files/Source_Files/Geo_info/ne_110m_admin_0_countries.shp'\n",
    "\n",
    "### Integrating long codes to ranking data table:\n",
    "df_ranking = ser_ranking.to_frame()\n",
    "df_ranking.index.set_names('ISO SHORT', level = 2, inplace = True)\n",
    "df_country_codes = get_country_codes().set_index('ISO SHORT', drop = True)\n",
    "df_ranking = df_ranking.join(df_country_codes, on = 'ISO SHORT', how = 'left')\n",
    "### Creating data table for class evolution through month/year to further connect with Slider bokeh widget:\n",
    "df_ranking.reset_index(inplace = True)\n",
    "df_ranking.rename(columns={'ISO LONG': 'Country'}, inplace = True)\n",
    "df_ranking['Year_Month'] = df_ranking['Date'].dt.to_period('M')\n",
    "df_ranking = df_ranking[['Year_Month', 'Market', 'Country', 'Rank']]\n",
    "df_ranking.set_index(['Year_Month'], inplace = True)\n",
    "### Exporting geo data:\n",
    "gdf_file_countries = gpd.read_file(path_countries_map_shp)[['ADMIN', 'ADM0_A3', 'geometry']]\n",
    "gdf_file_countries.columns = ['Country_Name', 'Country_Long_Code', 'Country_Geometry']\n",
    "gdf_file_countries.sort_values('Country_Long_Code', inplace = True)\n",
    "gdf_file_countries = gdf_file_countries[gdf_file_countries['Country_Name'] != 'Antarctica']\n",
    "### Converting polygons to coordinates arrays dataframe for bokech patches:\n",
    "arr_names = []\n",
    "arr_codes = []\n",
    "arr_arr_x = []\n",
    "arr_arr_y = []\n",
    "### Extacting polygons from gdf file:\n",
    "for country_counter, (country_name, country_long_code, country_geometry) in gdf_file_countries.iterrows():\n",
    "    ### For group of polygons:\n",
    "    if isinstance(country_geometry,  MultiPolygon):\n",
    "        for country_polygon in country_geometry:\n",
    "            arr_names.append(country_name)            \n",
    "            arr_codes.append(country_long_code)\n",
    "            arr_arr_x.append(list(country_polygon.exterior.coords.xy[0]))\n",
    "            arr_arr_y.append(list(country_polygon.exterior.coords.xy[1]))             \n",
    "    ### For single polygons:            \n",
    "    else:\n",
    "        country_polygon = country_geometry\n",
    "        arr_names.append(country_name)               \n",
    "        arr_codes.append(country_long_code)\n",
    "        arr_arr_x.append(list(country_polygon.exterior.coords.xy[0]))\n",
    "        arr_arr_y.append(list(country_polygon.exterior.coords.xy[1]))  \n",
    "### Constructing dataframe with row for each polygon:            \n",
    "tup_country_coords = tuple(zip(arr_names, arr_codes, arr_arr_x, arr_arr_y))\n",
    "df_country_coords = pd.DataFrame(list(tup_country_coords), columns = ['Country_Name', 'Country_Long_Code', 'Coord_X_Array', 'Coord_Y_Array'])\n",
    "df_country_coords.set_index(['Country_Long_Code', 'Country_Name'], inplace = True)\n",
    "\n",
    "### Configuring latest known MSCI data as initial world map classification data source:\n",
    "df_last_date = df_ranking.loc[df_ranking.index.values.max()]\n",
    "df_last_date = df_last_date.reset_index()\n",
    "df_last_date.set_index('Country', inplace = True)\n",
    "df_last_date.drop('Year_Month', axis = 1 ,inplace = True)\n",
    "df_country_show = df_country_coords.merge(df_last_date, how = 'left', left_on = 'Country_Long_Code', right_index = True)\n",
    "### Class select additional data connectors constructing:\n",
    "dict_classes_select = {'Developed Markets': 'DM', 'Emerging Markets': 'EM', 'Frontier Markets': 'FM', 'All Markets': 'ALL'}\n",
    "dict_classes_mirror = dict(zip(dict_classes_select.values(), dict_classes_select.keys()))\n",
    "arr_classes_select = list(dict_classes_select.keys())\n",
    "arr_classes_index = list(dict_classes_select.values())\n",
    "### Creating transit dataframe as container for data transfer betwwen widgets and figures:\n",
    "df_meta_transit = pd.DataFrame([str(df_ranking.index.values.max()), 'ALL'], index = ['Year_Month', 'Market']).T\n",
    "### Preparing by date market status dictionary to dyhnamic changing cds sources\n",
    "dict_market = {}\n",
    "dict_ranking = {}\n",
    "for iter_year_month in df_ranking.sort_index(level = 'Year_Month').index.get_level_values('Year_Month').unique():\n",
    "    iter_ser_year_month = df_ranking.loc[iter_year_month, ['Market', 'Country']].reset_index(drop = True).set_index('Country').squeeze()\n",
    "    dict_market[str(iter_year_month)] = iter_ser_year_month.reindex(df_country_show.index.get_level_values(0)).values\n",
    "    dict_ranking[str(iter_year_month)] = {}\n",
    "    for iter_market in arr_classes_index:       \n",
    "        iter_arr_include = [iter_market]\n",
    "        if (iter_market == 'ALL'):\n",
    "            iter_arr_include = arr_classes_index[ : -1]\n",
    "        iter_ser_market = df_ranking.loc[iter_year_month].reset_index(drop = True).set_index('Country')\n",
    "        iter_ser_market = iter_ser_market.loc[iter_ser_market['Market'].isin(iter_arr_include)]\n",
    "        dict_ranking[str(iter_year_month)][iter_market] = iter_ser_market.reindex(df_country_show.index.get_level_values(0))['Rank'].values\n",
    "### Defining output to notebook or to html file:\n",
    "b_pl.output_notebook()\n",
    "#b_pl.output_file('Ranking_evolution.html')\n",
    "### Bokeh Data Source defining:\n",
    "src_meta_transit =  b_pl.ColumnDataSource(df_meta_transit) ### For data transfer betwwen elements\n",
    "src_country_show =  b_pl.ColumnDataSource(df_country_show.reset_index()) ### For world map borders\n",
    "### Initialising worldmap classes evolution figure:\n",
    "str_fig_worldmap_toolbar =  'pan, wheel_zoom, reset'\n",
    "tup_fig_worldmap_size = (800, 400)\n",
    "str_fig_worldmap_title = 'Universe debt ranking evolution'\n",
    "fig_world_map = b_pl.figure(tools = str_fig_worldmap_toolbar, active_scroll = 'wheel_zoom', \n",
    "                            plot_width = tup_fig_worldmap_size[0], plot_height = tup_fig_worldmap_size[1],\n",
    "                            title = str_fig_worldmap_title)\n",
    "### Tuning worldmap figure:\n",
    "fig_world_map.axis.visible = False\n",
    "fig_world_map.xgrid.visible = False\n",
    "fig_world_map.ygrid.visible = False\n",
    "fig_world_map.toolbar.autohide = True\n",
    "### Creating hover inspection to worldmap figure:    \n",
    "fig_world_map.add_tools(b_md.HoverTool(tooltips = [('Country', '@Country_Name'), ('Market Class', '@Market'), ('Ranking', '@Rank{0,0}')], \n",
    "                                       formatters = {'Rank': 'numeral'}))\n",
    "### Colors categorising and mapping:\n",
    "palette_red_green = all_palettes['Plasma'][256][-79 : -10][::3] + all_palettes['Viridis'][256][-69 : ][::-1][::3]\n",
    "linear_cm_rank = b_md.LinearColorMapper(low = -12, high = 10, palette = palette_red_green, nan_color = 'silver')\n",
    "#inferno(23), viridis(23), cividis(23), magma(23), plasma(23)\n",
    "#all_palettes['Viridis'][256][-23:][::-1]\n",
    "### Drawing world map:\n",
    "patches_world_map = fig_world_map.patches('Coord_X_Array', 'Coord_Y_Array', source = src_country_show,\n",
    "                                          color = {'field': 'Rank', 'transform': linear_cm_rank}, fill_alpha = 1.0, line_color = 'lightgray', \n",
    "                                          line_width = 1.0)\n",
    "### Adding rank value color bar to rank world map figure:\n",
    "color_bar_class = b_md.ColorBar(color_mapper = linear_cm_rank, label_standoff = 8, width = 20, height = tup_fig_worldmap_size[1] - 100,\n",
    "                                border_line_color = None, orientation = 'vertical', location = (0, 50),\n",
    "                                ticker = b_md.AdaptiveTicker(desired_num_ticks = 23),\n",
    "                                formatter = b_md.NumeralTickFormatter(format = '0.'))\n",
    "fig_world_map.add_layout(color_bar_class, 'right')\n",
    "\n",
    "### Drawing and tuning date slider widget, creating a trigger for it's changes:\n",
    "callback_date_slider = CustomJS(args = dict(fig_map_to_update = fig_world_map, title_map_main_part = str_fig_worldmap_title, \n",
    "                                            cds_world = src_country_show, cds_transit = src_meta_transit,\n",
    "                                            arr_classes_mirror = dict_classes_mirror,\n",
    "                                            arr_dict_market = dict_market, arr_dict_ranking = dict_ranking),\n",
    "                                code = \"\"\"                             \n",
    "                                       var market_selected = cds_transit.data['Market'];                                \n",
    "                                       var date_chosen = new Date(cb_obj.value);\n",
    "                                       var arr_months = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "                                                         'July', 'August', 'September', 'October', 'November', 'December'];\n",
    "                                       var date_month_name = arr_months[date_chosen.getMonth()];\n",
    "\n",
    "                                       fig_map_to_update.title.text = title_map_main_part + ': ' + arr_classes_mirror[market_selected] + ' | ';\n",
    "                                       fig_map_to_update.title.text = fig_map_to_update.title.text  + date_chosen.getFullYear() + ' / ' + date_month_name;\n",
    "                                       var date_year_month = date_chosen.getFullYear().toString();\n",
    "                                       if (date_chosen.getMonth() > 8)\n",
    "                                       {\n",
    "                                           date_year_month = date_year_month + '-' + (date_chosen.getMonth() + 1).toString();\n",
    "                                       }\n",
    "                                        else\n",
    "                                       {\n",
    "                                           date_year_month = date_year_month + '-' + '0' + (date_chosen.getMonth() + 1).toString();\n",
    "                                       }                                                                         \n",
    "                                       cds_world.data['Market'] = arr_dict_market[date_year_month];\n",
    "                                       cds_world.data['Rank'] = arr_dict_ranking[date_year_month][market_selected];\n",
    "                                       cds_world.change.emit();                               \n",
    "\n",
    "                                       fig_map_to_update.change.emit(); \n",
    "\n",
    "                                       cds_transit.data['Year_Month'] = date_year_month;\n",
    "                                       cds_transit.change.emit();                                        \n",
    "                                       \"\"\")\n",
    "date_min = ser_ranking.index.get_level_values(level = 0).min()\n",
    "date_max = ser_ranking.index.get_level_values(level = 0).max()\n",
    "slider_dates = DateSlider(title = 'Date to show ranking status', width = tup_fig_worldmap_size[0] - 50, \n",
    "                          start = date_min, end = date_max, step = 1, value = date_max)\n",
    "slider_dates.callback_policy = 'throttle'\n",
    "slider_dates.js_on_change('value', callback_date_slider)\n",
    "slider_dates.tooltips = False\n",
    "\n",
    "### Creating select widget for choosing class tuning (including connection between selected value and range slider boundaries):\n",
    "callback_class_select = CustomJS(args = dict(fig_map_to_update = fig_world_map, title_map_main_part = str_fig_worldmap_title, \n",
    "                                             cds_world = src_country_show, cds_transit = src_meta_transit,\n",
    "                                            arr_classes_select = dict_classes_select, arr_dict_ranking = dict_ranking),\n",
    "                                 code = \"\"\"\n",
    "                                        var date_year_month = cds_transit.data['Year_Month'].toString();\n",
    "                                        var date_year = date_year_month.substring(0, 4);\n",
    "                                        var date_month = Number(date_year_month.substring(5)) - 1;\n",
    "                                        var market_selected = arr_classes_select[cb_obj.value];  \n",
    "                                        var arr_months = ['January', 'February', 'March', 'April', 'May', 'June', \n",
    "                                                          'July', 'August', 'September', 'October', 'November', 'December'];\n",
    "                                        var date_month_name = arr_months[date_month];\n",
    "                                        fig_map_to_update.title.text = title_map_main_part + ': ' + market_selected + ' | ';\n",
    "                                        fig_map_to_update.title.text = title_map_main_part + ': ' + cb_obj.value + ' | ';\n",
    "                                        fig_map_to_update.title.text = fig_map_to_update.title.text  + date_year + ' / ' + date_month_name;\n",
    "                                        cds_world.data['Rank'] = arr_dict_ranking[date_year_month][market_selected];                                      \n",
    "                                        cds_world.change.emit();                               \n",
    "                                        fig_map_to_update.change.emit();\n",
    "                                        \n",
    "                                        cds_transit.data['Market'] = market_selected;\n",
    "                                        cds_transit.change.emit(); \n",
    "                                        \"\"\")\n",
    "select_class = Select(title = 'Select market class to show:', options = arr_classes_select, value = arr_classes_select[-1], callback = callback_class_select)\n",
    "\n",
    "### Constructing common layout: \n",
    "layout_world = b_col(widgetbox(select_class), fig_world_map, widgetbox(slider_dates))\n",
    "b_pl.show(layout_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPO WEIGHTED ILLUSTRATING\n",
    "import matplotlib.pyplot as plt\n",
    "num_window = 310\n",
    "num_halflife = 3\n",
    "ser_weights_halflife = get_exp_weights_series(window_len = num_window, halflife_len = num_halflife).reset_index(drop = True)\n",
    "ser_weights_halflife.index = ser_weights_halflife.index - 310\n",
    "#ser_weights_halflife.to_excel('Data_Files/Test_Files/Expo_Weights_360_3.xlsx', merge_cells = False)  \n",
    "ser_weights_halflife.plot(figsize = (15, 2), title = 'Expo weights with 310 month array length and 3 month half-life period plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COLLAPSED / PROVIDERS REVISIONS CORRELATION EXCLUDING COLLAPSED REVISION INITIATOR\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "All = slice(None)\n",
    "### Ranking vector reading:\n",
    "df_rating_collapsed = pd.read_excel('Data_Files/Test_Files/Test_Collapsing_Full.xlsx')\n",
    "#df_rating_collapsed = pd.read_excel('Data_Files/Test_Files/Test_Collapsing_Full_Marked_Plus.xlsx')\n",
    "df_rating_collapsed = df_rating_collapsed.set_index(['Date', 'Market', 'Country'])\n",
    "df_rating_collapsed.sort_index(level = ['Country', 'Date'])\n",
    "df_rating_agg = df_rating_collapsed.copy()\n",
    "df_rating_agg['COLLAPSED'] = round(df_rating_agg['Fresh month half life'], 0)\n",
    "df_rating_agg.drop(['Fresh month half life', 'Mean'], axis = 1, inplace = True)\n",
    "### Delta vector preparing:\n",
    "num_delta_step = 1\n",
    "df_rating_delta = df_rating_agg.groupby('Country').diff(num_delta_step).sort_index(level = ['Country', 'Date'], ascending = [True, True])\n",
    "### Droppping constant collapsed ranking countries:\n",
    "df_rating_delta = df_rating_delta.loc[df_rating_delta['COLLAPSED'].groupby('Country').filter(lambda iter_group: iter_group.abs().sum() != 0).index]\n",
    "### Main loop for providers:\n",
    "for iter_agency in df_rating_delta.columns.to_list():\n",
    "    if (iter_agency != 'COLLAPSED'):\n",
    "        ### Shifting leg:\n",
    "        for num_shifter in (np.arange(5) + 1):\n",
    "            ### Selecting columns for correlation:\n",
    "            df_iter_agency_delta = df_rating_delta[['COLLAPSED', iter_agency]]\n",
    "            ### Adding shifted providers delta ranking:\n",
    "            df_iter_agency_delta = df_iter_agency_delta.assign(**{iter_agency + ' shifted': df_iter_agency_delta[iter_agency].shift(-num_shifter).values})  \n",
    "            ### Dropping zero delta COLLAPSED values:            \n",
    "            df_iter_agency_delta = df_iter_agency_delta.loc[df_iter_agency_delta['COLLAPSED'] != 0]                \n",
    "            ### Dropping non-zero non-shifted delta provider values (excluding observations where COLLAPSED ranking changed by provider's ranking):\n",
    "            df_iter_agency_delta = df_iter_agency_delta.loc[df_iter_agency_delta[iter_agency] == 0]\n",
    "            ### Calculating correlation:\n",
    "            df_iter_agency_corr = df_iter_agency_delta.corr()       \n",
    "            df_iter_agency_corr.index.set_names(['Agency'], inplace = True)  \n",
    "            ### Printing correlation result:\n",
    "            print('Provider:', iter_agency, ', shift:', num_shifter, ':', round(df_iter_agency_corr.loc['COLLAPSED', iter_agency + ' shifted'], 2))\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESULTS SAVING\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_collapsed = 'Data_Files/Source_Files/Collapsed_Rank_Not_Marked.h5'\n",
    "path_collapsed_marked = 'Data_Files/Source_Files/Collapsed_Rank_Marked.h5'\n",
    "key_collapsed = 'Rank'\n",
    "ser_ranking_no_impact = pd.read_hdf(path_collapsed, key_collapsed)\n",
    "ser_ranking_with_impact = pd.read_hdf(path_collapsed_marked, key_collapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MARKS IMPACT ANALYSIS\n",
    "ser_diff_no_impact = ser_ranking_no_impact.dropna().groupby('Country').diff().abs()\n",
    "print(ser_diff_no_impact[ser_diff_no_impact > 0].count())\n",
    "print(ser_diff_no_impact[ser_diff_no_impact > 0].mean())\n",
    "ser_diff_with_impact = ser_ranking_with_impact.dropna().groupby('Country').diff().abs()\n",
    "print(ser_diff_with_impact[ser_diff_with_impact > 0].count())\n",
    "print(ser_diff_with_impact[ser_diff_with_impact > 0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
