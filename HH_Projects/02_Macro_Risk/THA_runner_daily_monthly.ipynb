{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THA STANDARTIZE PLAYGROUND ACADIAN MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULES IMPORT (PART OF THE PRODUCT CODE)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version:  0.25.3\n",
      "python version:  3.7.4\n"
     ]
    }
   ],
   "source": [
    "## VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "print('pandas version: ', pd.__version__)\n",
    "print('python version: ', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INTERNAL PARAMETERS INITIALIZATION (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "import os ### To work with csv files\n",
    "\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### PRS data:\n",
    "str_path_prs_hdf = 'Data_Files/Source_Files/Country_Risks/PRS_loaded.h5'\n",
    "str_key_prs_pillars_only_converted = 'prs_pillars_only_converted'\n",
    "### Continuum data:\n",
    "str_path_continuum_hdf = 'Data_Files/Source_Files/Country_Risks/Continuum_loaded.h5'\n",
    "str_key_continuum_politics_converted = 'continuum_politics_converted'\n",
    "### Test Sources weights:\n",
    "str_path_weights_xlsx = 'Data_Files/Test_Files/Test_Weights.xlsx'\n",
    "### General daily-mode ranges parameters:\n",
    "str_source_date_start = '1992-01-01' ### Start date for source vectors\n",
    "str_measure_date_start = '1996-08-01' ### Start date for efficacy measures\n",
    "str_ison_date_start = '1994-01-31' ### Start date for ISON Universe\n",
    "str_measure_date_end = '2020-08-31' ### End date for efficacy measures\n",
    "idx_source_date_range = pd.date_range(str_source_date_start, str_measure_date_end, freq = 'B') ### Range for source data filtering\n",
    "idx_test_monthly_date_range = pd.date_range(str_ison_date_start, str_measure_date_end, freq = 'BM') ### Range for source data filtering\n",
    "idx_test_daily_date_range = pd.date_range(str_ison_date_start, str_measure_date_end, freq = 'B') ### Range for source data filtering\n",
    "idx_factor_date_range = pd.date_range(str_source_date_start, str_measure_date_end, freq = 'BM') ### Range for factor data filtering\n",
    "idx_measure_date_range = pd.date_range(str_measure_date_start, str_measure_date_end, freq = 'BM') ### Range for measures calculation\n",
    "### Results saving:\n",
    "str_test_factor_full_csv = 'Data_Files/Test_Files/acadian_mode_test_factor_full.csv'\n",
    "str_test_autocorr_csv = 'Data_Files/Test_Files/acadian_mode_test_autocorr.csv'\n",
    "str_test_factor_source_csv = 'Data_Files/Test_Files/acadian_mode_test_factor_source.csv'\n",
    "str_test_factor_agg_csv = 'Data_Files/Test_Files/acadian_mode_test_factor_agg.csv'\n",
    "str_test_factor_res_xlsx = 'Data_Files/Test_Files/acadian_mode_test_factor_res.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL PARAMETERS INITIALIZATION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "### Common constants:\n",
    "All = slice(None)\n",
    "\n",
    "### ISON filtering options:\n",
    "list_ison = ['DM', 'EM', 'FM'] ### Regions filter to drop NaN region values\n",
    "list_countries_to_exclude = ['VE'] ### Countries not to play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(io = str_path_universe, sheet_name = 0, header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = math.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE FOR DATAFRAME COLUMNS (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def columns_average(df_series, list_weights = False):\n",
    "    ### Equal weights list creating:\n",
    "    if isinstance(list_weights, bool):\n",
    "        list_weights = [1] * len(df_series.columns)\n",
    "    ### Dataframe of weights initialising:\n",
    "    df_weights = pd.DataFrame([list_weights] * len(df_series.index), index = df_series.index, columns = df_series.columns)\n",
    "    ### Zeroing weights for NaN values:\n",
    "    for iter_col in df_weights.columns:\n",
    "        df_weights.loc[df_series[iter_col].isna(), iter_col] = 0\n",
    "    ### Weighted mean calulating:\n",
    "    ser_means = ((df_series * df_weights).sum(axis = 1) / df_weights.sum(axis = 1))\n",
    "    ### Results output:\n",
    "    return ser_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING ACADIAN STYLE TWO-STEP FACTOR VECTOR STANDARTIZATION FOR REGION WITHIN CROSS-SECTION \"SINGLE DATE / SINGLE REGION\" PDF VERSION (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def td_two_stage_standardize(ser_factor):\n",
    "    ### Limits definition:\n",
    "    flo_trunc_limit_1 = 2.5\n",
    "    flo_trunc_limit_2 = 2.0\n",
    "    ### Preliminary statistics calculation:\n",
    "    flo_std = np.nanstd(ser_factor, axis = 0, ddof = 1)\n",
    "    flo_mean = np.nanmean(ser_factor, axis = 0)\n",
    "    ### Preliminary z-scoring:\n",
    "    ser_score = (ser_factor - flo_mean) / flo_std\n",
    "    ### Constant vector checking:\n",
    "    if np.isclose(flo_std, 0.0):\n",
    "        ser_score = ser_factor - ser_factor\n",
    "    ### First winsorization step:\n",
    "    ser_score.loc[ser_score < (-1.0 * flo_trunc_limit_1)] = -1.0 * flo_trunc_limit_1\n",
    "    ser_score.loc[ser_score > flo_trunc_limit_1] = flo_trunc_limit_1\n",
    "    ### First limit precence marker:\n",
    "    ser_on_limit = (ser_score.abs() == flo_trunc_limit_1)\n",
    "    ### Check if first step do some truncation:    \n",
    "    if ser_on_limit.any():\n",
    "        ### Under the limit values marking:\n",
    "        ser_off_limit = (ser_score.abs() != flo_trunc_limit_1)\n",
    "        ### Separating truncated values to perform further transformations with under the limit values only:\n",
    "        ser_score_trunc_1 = ser_score.copy()\n",
    "        ser_score_trunc_1.loc[ser_off_limit] = 0.0\n",
    "        ### Dropping truncaterd values for further performance:\n",
    "        ser_score.loc[ser_on_limit] = np.NaN\n",
    "        ### Repeated statistics calculation:\n",
    "        flo_std = np.nanstd(ser_score, axis = 0, ddof = 1)\n",
    "        flo_mean = np.nanmean(ser_score, axis = 0)\n",
    "        ### Constant vector checking:\n",
    "        if np.isclose(flo_std, 0.0):\n",
    "            ser_score = ser_score - ser_score\n",
    "        else:\n",
    "            ### Second z-scoring:\n",
    "            ser_score = (ser_score - flo_mean) / flo_std\n",
    "        ### Dropping truncaterd values for further performance:\n",
    "        ser_score.loc[ser_on_limit] = np.NaN\n",
    "        ### Second winsorization step:  \n",
    "        ser_score.loc[ser_score < (-1.0 * flo_trunc_limit_2)] = -1.0 * flo_trunc_limit_2\n",
    "        ser_score.loc[ser_score > flo_trunc_limit_2] = flo_trunc_limit_2\n",
    "        ### Preparing for truncated values adding:\n",
    "        ser_score.loc[ser_on_limit] = 0.0\n",
    "        ### Vectors union:\n",
    "        ser_score = ser_score + ser_score_trunc_1\n",
    "        ### Final demean:\n",
    "        flo_mean = np.nanmean(ser_score, axis = 0)  \n",
    "        ser_score = ser_score - flo_mean\n",
    "    ### Results output:\n",
    "    return ser_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING ACADIAN STYLE TWO-STEP FACTOR VECTOR STANDARTIZATION FOR REGION WITHIN CROSS-SECTION \"SINGLE DATE / SINGLE REGION\" EXTENDED (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def td_two_stage_standardize_extended(ser_factor):\n",
    "    ### Limits definition:\n",
    "    flo_trunc_limit_1 = 2.5\n",
    "    flo_trunc_limit_2 = 2.0\n",
    "    ### Preliminary statistics calculation:\n",
    "    flo_std_1 = np.nanstd(ser_factor, axis = 0, ddof = 1)\n",
    "    flo_mean_1 = np.nanmean(ser_factor, axis = 0)\n",
    "    ### Constant vector checking:\n",
    "    if np.isclose(flo_std_1, 0.0):\n",
    "        ser_score = ser_factor - ser_factor\n",
    "    else:\n",
    "        ### First z-scoring:\n",
    "        ser_score = (ser_factor - flo_mean_1) / flo_std_1\n",
    "    ### First winsorization step:\n",
    "    ser_score.loc[ser_score < (-1.0 * flo_trunc_limit_1)] = -1.0 * flo_trunc_limit_1\n",
    "    ser_score.loc[ser_score > flo_trunc_limit_1] = flo_trunc_limit_1\n",
    "    ### First limit precence marker:\n",
    "    ser_on_limit = (ser_score.abs() == flo_trunc_limit_1)\n",
    "    ### Under the limit values marking:\n",
    "    ser_off_limit = (ser_score.abs() != flo_trunc_limit_1)\n",
    "    ### Separating truncated values to perform further transformations with under the limit values only:\n",
    "    ser_score_trunc_1 = ser_score.copy()\n",
    "    ser_score_trunc_1.loc[ser_off_limit] = 0.0\n",
    "    ### Dropping truncaterd values for further performance:\n",
    "    ser_score.loc[ser_on_limit] = np.NaN\n",
    "    ### Repeated statistics calculation:\n",
    "    flo_std_2 = np.nanstd(ser_score, axis = 0, ddof = 1)\n",
    "    flo_mean_2 = np.nanmean(ser_score, axis = 0)\n",
    "    ### Constant vector checking:\n",
    "    if np.isclose(flo_std_2, 0.0):\n",
    "        ser_score = ser_score - ser_score\n",
    "    else:\n",
    "        ### Second z-scoring:\n",
    "        ser_score = (ser_score - flo_mean_2) / flo_std_2\n",
    "    ### Dropping truncaterd values for further performance:\n",
    "    ser_score.loc[ser_on_limit] = np.NaN\n",
    "    ### Second winsorization step:  \n",
    "    ser_score.loc[ser_score < (-1.0 * flo_trunc_limit_2)] = -1.0 * flo_trunc_limit_2\n",
    "    ser_score.loc[ser_score > flo_trunc_limit_2] = flo_trunc_limit_2\n",
    "    ### Preparing for truncated values adding:\n",
    "    ser_score.loc[ser_on_limit] = 0.0\n",
    "    ### Vectors union:\n",
    "    ser_score = ser_score + ser_score_trunc_1\n",
    "    ### Final demean:\n",
    "    flo_mean = np.nanmean(ser_score, axis = 0)  \n",
    "    ser_score = ser_score - flo_mean\n",
    "    ### Results output:\n",
    "    return ser_score, flo_std_1, flo_mean_1, flo_std_2, flo_mean_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING PRELIMINARY DATA EXTRACTION (SHOULKD BE SUBSTITUTED BY SQL QUERY)\n",
    "\n",
    "def get_history_window(iter_date, ser_source_raw, int_extended_win):\n",
    "    ### Start date for source vectors:\n",
    "    date_source_start = pd.to_datetime('1992-01-01')     \n",
    "    ### Start date for extended window defining:\n",
    "    date_start_loc = np.maximum(iter_date - pd.tseries.offsets.BDay(int_extended_win), date_source_start)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):    \n",
    "    ser_history_raw = ser_source_raw.loc[date_start_loc : iter_date, All]\n",
    "    ### Results output:\n",
    "    return ser_history_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING FILTERING DATE INTERVAL, REINDEXING FILTERED VECTOR TO BUSINESS DATES/MONTHS FREQUENCY AND FILLING DATA (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def get_country_interval(ser_filtered, date_start, date_end, int_fill_limit = 1):\n",
    "    ### Drop level to avoid stack/unstack manipulations:\n",
    "    ser_filtered = ser_filtered.droplevel('Country')\n",
    "    ### Business day filter:\n",
    "    idx_date_business = pd.date_range(start = date_start, end = date_end, freq = 'B')\n",
    "    try:\n",
    "        ### Frequency checker:\n",
    "        date_first = ser_filtered.first_valid_index()\n",
    "        date_last = ser_filtered.last_valid_index()\n",
    "        ### Resampling to business month:\n",
    "        if ((date_last - date_first).days / len(ser_filtered.dropna().index) > 3.0):          \n",
    "            ser_filtered = ser_filtered.resample('MS').last().resample('BM').last()\n",
    "    except TypeError:\n",
    "        pass\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    ### Reindexation and forward filling:\n",
    "    ser_reindexed = ser_filtered.resample('B').ffill().fillna(method = 'ffill', limit = int_fill_limit).reindex(idx_date_business).ffill(limit = int_fill_limit)        \n",
    "    ### Results output:\n",
    "    ser_reindexed.index.names = ['Date']        \n",
    "    return ser_reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING FILTERING DATE INTERVAL, REINDEXING FILTERED VECTOR TO BUSINESS DATES/MONTHS FREQUENCY AND FILLING DATA (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def get_monthly_interval(ser_filtered, date_start, date_end, int_fill_limit = 1):\n",
    "    ### Drop level to avoid stack/unstack manipulations:\n",
    "    ser_filtered = ser_filtered.droplevel('Country')\n",
    "    ### Business day filter:\n",
    "    idx_date_business = pd.date_range(start = date_start, end = date_end, freq = 'BM')\n",
    "    try:\n",
    "        ### Frequency checker:\n",
    "        date_first = ser_filtered.first_valid_index()\n",
    "        date_last = ser_filtered.last_valid_index()\n",
    "        ### Resampling to business month:      \n",
    "        ser_filtered = ser_filtered.resample('MS').last().resample('BM').last()\n",
    "    except TypeError:\n",
    "        pass\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    ### Reindexation and forward filling:\n",
    "    ser_reindexed = ser_filtered.reindex(idx_date_business).ffill(limit = int_fill_limit)        \n",
    "    ### Results output:\n",
    "    ser_reindexed.index.names = ['Date']        \n",
    "    return ser_reindexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEAN MOMENTUM FUNCTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def mean_momentum(ser_country_source, list_weight, int_mean_min):\n",
    "    try:\n",
    "        ### Weight setting\n",
    "        ser_weight = pd.Series(list_weight[ -len(ser_country_source.index) : ], ser_country_source.index)  \n",
    "        ### Weighted mean calculation:\n",
    "        return weighted_average(ser_country_source, ser_weight, int_mean_min)\n",
    "    except KeyError:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_factor(ser_source_adopted):\n",
    "    ### Last not empty value extraction:\n",
    "    ser_factor = ser_source_adopted.dropna().groupby('Country').apply(lambda ser_group: ser_group.values[-1])\n",
    "    ### Resulsts output:\n",
    "    return ser_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING CHANGE FACTOR FACTOR CREATING FUNCTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def get_change_factor(ser_source_adopted, int_mom_win, int_mom_min, int_mom_hl):\n",
    "    ### Forward filling limitation:\n",
    "    int_fill_limit = 66\n",
    "    ### Start date for source vectors:\n",
    "    date_source_start = pd.to_datetime('1991-12-31')     \n",
    "    ### Start date for window defining:    \n",
    "    date_start_win = np.maximum(iter_date - pd.tseries.offsets.BDay(int_mom_win - 1), date_source_start)    \n",
    "    ### Weights array:\n",
    "    list_weight = list(map(lambda iter_num: exp_weight_single(int_mom_hl, iter_num), range(int_mom_win)))[::-1]    \n",
    "    ### Data source resampling:\n",
    "    ser_data = ser_source_adopted.groupby('Country').apply(get_country_interval, date_start_win, iter_date, int_fill_limit).swaplevel().sort_index()\n",
    "\n",
    "    ### Source performing:\n",
    "    ser_delta = ser_data.groupby('Country').diff() / ser_data.groupby('Country').shift()   \n",
    "    ser_delta = ser_delta.replace([np.inf, -np.inf], np.NaN)    \n",
    "    ser_delta.index.names = ['Date', 'Country'] \n",
    "    ### Momentum factor calculation:\n",
    "    ser_factor = ser_delta.groupby('Country').apply(mean_momentum, list_weight, int_mom_min)\n",
    "    ### Results output:\n",
    "    return ser_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING CHANGE FACTOR FACTOR CREATING FUNCTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def get_change_factor_only(ser_data, list_weight, int_mom_min):\n",
    "    ### Momentum factor calculation:\n",
    "    ser_factor = ser_data.groupby('Country').apply(mean_momentum, list_weight, int_mom_min)\n",
    "    ### Results output:\n",
    "    return ser_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_recover_demean_std(ser_factor_raw):\n",
    "    ### If factor have values:\n",
    "    if (ser_factor_raw.count() > 0):\n",
    "        ### Outliers clipping through the two step standartizing and recovering:\n",
    "        (ser_factor_raw_std_prelim, flo_std_1, flo_mean_1, flo_std_2, flo_mean_2) = td_two_stage_standardize_extended(ser_factor_raw)\n",
    "        ser_factor_rec = (ser_factor_raw_std_prelim * flo_std_2 + flo_mean_2) * flo_std_1 + flo_mean_1  \n",
    "        ### By market demeaning for clipped data vector:\n",
    "        ser_factor_rec = ser_factor_rec.groupby('Market').apply(lambda ser_region: ser_region - ser_region.mean())\n",
    "        ### Winsorized & demeaned by market data vector standartizing and saving:\n",
    "        ser_factor_std = td_two_stage_standardize_extended(ser_factor_rec)[0]\n",
    "    ### If factor is all NaN:\n",
    "    else:\n",
    "        ser_factor_std = ser_factor_raw\n",
    "    ### Results output:\n",
    "    return ser_factor_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA LOADING (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "ser_ison_daily = ison_membership_converting(str_path_universe, datetime.strptime(str_measure_date_end, '%Y-%m-%d'), bool_daily = True) ### ISON universe, bus-daily vector\n",
    "dict_source_raw = {}\n",
    "### PRS pillars table to use as a factor data source:\n",
    "dict_source_raw['PRS'] = pd.read_hdf(str_path_prs_hdf, key = str_key_prs_pillars_only_converted)\\\n",
    "                           .loc[['Economic Risk Rating', 'Financial Risk Rating', 'Political Risk Rating'], All, All]\n",
    "dict_source_raw['PRS'].index.set_names('Pillar', level = 'Variable', inplace = True)\n",
    "### Continuum pillars table to use as a factor data source:\n",
    "dict_source_raw['Continuum'] = pd.read_hdf(str_path_continuum_hdf, key = str_key_continuum_politics_converted)\\\n",
    "                                 .loc[['External Adjustment Capacity', 'Institutional Robustness', 'Medium-Term Growth Potential', 'Social Inclusion'], All, All]\n",
    "dict_source_raw['Continuum'].index.set_names('Pillar', level = 'Indicator', inplace = True)\n",
    "### Weights loading:\n",
    "ser_weights = pd.read_excel(io = str_path_weights_xlsx, sheet_name = 0, header = 0, index_col = [0, 1, 2],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False).astype(float).squeeze().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2021-05-05 10:34:58.981387\n",
      "Finish time: 2021-05-05 10:35:01.736368\n",
      "Full interval: 0:00:02.754981\n",
      "Average interval for single date: 0:00:02.754981\n"
     ]
    }
   ],
   "source": [
    "### TESTING: PERFORMING FACTOR FOR DATE RANGE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "### Removing csv files before loop running:\n",
    "if (os.path.exists(str_test_autocorr_csv)):\n",
    "    os.remove(str_test_autocorr_csv)   \n",
    "if (os.path.exists(str_test_factor_full_csv)):\n",
    "    os.remove(str_test_factor_full_csv) \n",
    "if (os.path.exists(str_test_factor_source_csv)):\n",
    "    os.remove(str_test_factor_source_csv)     \n",
    "if (os.path.exists(str_test_factor_agg_csv)):\n",
    "    os.remove(str_test_factor_agg_csv)\n",
    "### THA-calculation constants:\n",
    "flo_similarity = 5 * (10 ** (-6))   \n",
    "flo_tha_ratio = 0.9 ### THA progression ratio\n",
    "int_tha_length = 24 ### THA horizon length  \n",
    "### Change factor parameters:\n",
    "int_extended_win = 260 * 6 ### Extended window length, business days\n",
    "int_fill_limit = 3 # 66 ### Forward filling limitation\n",
    "int_mom_win = 60 # 1300 ### Rolling window length\n",
    "int_mom_hl_short = 3 # 66 ### Half-life and Minimal rolling window length for short factor\n",
    "int_mom_hl_long = 24 # 520 ### Half-life and Minimal rolling window length for long factor\n",
    "date_source_start = pd.to_datetime('1992-01-01') ### Start date for source vectors\n",
    "list_weight_short = list(map(lambda iter_num: exp_weight_single(int_mom_hl_short, iter_num), range(int_mom_win)))[::-1] ### Weights for short change\n",
    "list_weight_long = list(map(lambda iter_num: exp_weight_single(int_mom_hl_long, iter_num), range(int_mom_win)))[::-1] ### Weights for long change\n",
    "### Local testing parameters:\n",
    "int_interval = 10 ### Interval of progress displaying\n",
    "date_start = datetime.utcnow() ### Start time of calculations\n",
    "date_control = datetime.utcnow() ### Control time to display\n",
    "idx_test_date_range = idx_test_monthly_date_range[0 : 1] # idx_test_monthly_date_range # idx_test_monthly_date_range[0 : 10] # idx_test_monthly_date_range[0 : 60] # \n",
    "#idx_test_date_range = idx_test_daily_date_range[0: 100]\n",
    "### Test performing:\n",
    "print('Start time:', date_start)\n",
    "for iter_num, iter_date in enumerate(idx_test_date_range):\n",
    "    ### Progress printing:\n",
    "    if not (divmod(iter_num, int_interval)[1]):\n",
    "        if iter_num:\n",
    "            print('Counter marker:', iter_num, '/', len(idx_test_date_range))\n",
    "            timedelta_interval = datetime.utcnow() - date_control\n",
    "            print('Time interval since last marker:', datetime.utcnow() - date_control)            \n",
    "            print('Average interval for single date:', str(timedelta_interval / int_interval))\n",
    "        date_control = datetime.utcnow()\n",
    "    ### ISON daily vector loading:\n",
    "    ser_ison_iter_date = ser_ison_daily.loc[iter_date, All].droplevel('Date')\n",
    "    ### Start date for window defining:    \n",
    "    date_start_win = np.maximum(iter_date - pd.tseries.offsets.BMonthEnd(int_mom_win - 1), date_source_start)  \n",
    "    ### Sources looping:\n",
    "    for iter_source in dict_source_raw:\n",
    "        ### Source data portion loading:\n",
    "        ser_iter_adopted = dict_source_raw[iter_source].groupby('Pillar')\\\n",
    "                                                      .apply(lambda ser_pillar: get_history_window(iter_date, ser_pillar.droplevel('Pillar'), int_extended_win))\n",
    "        ### Check for not empty source vector:\n",
    "        if (len(ser_iter_adopted) > 0):\n",
    "            ### Data source resampling:\n",
    "            ser_iter_interval = ser_iter_adopted.groupby(['Pillar', 'Country'])\\\n",
    "                                            .apply(lambda ser_country: get_monthly_interval(ser_country.droplevel('Pillar'), date_start_win, iter_date, int_fill_limit))\\\n",
    "                                            .swaplevel().sort_index()\n",
    "            ### Raw factors calculating:\n",
    "            ser_iter_level_factor = ser_iter_adopted.groupby('Pillar').apply(lambda ser_pillar: get_level_factor(ser_pillar.droplevel('Pillar')))\n",
    "            ser_iter_level_factor.name = 'Level'\n",
    "            ser_iter_long_factor = ser_iter_interval.groupby('Pillar')\\\n",
    "                                                    .apply(lambda ser_pillar: get_change_factor_only(ser_pillar.droplevel('Pillar'), list_weight_long, int_mom_hl_long))\n",
    "            ser_iter_long_factor = ser_iter_level_factor - ser_iter_long_factor\n",
    "            ser_iter_long_factor.name = 'Long'            \n",
    "            ser_iter_short_factor = ser_iter_interval.groupby('Pillar')\\\n",
    "                                                     .apply(lambda ser_pillar: get_change_factor_only(ser_pillar.droplevel('Pillar'), list_weight_short, int_mom_hl_short))\n",
    "            ser_iter_short_factor = ser_iter_level_factor - ser_iter_short_factor            \n",
    "            ser_iter_short_factor.name = 'Short' \n",
    "            ### Deleting change factors for all Continuum pillars:\n",
    "            if (iter_source == 'Continuum'):\n",
    "                ser_iter_long_factor.loc[All] = np.NaN\n",
    "                ser_iter_short_factor.loc[All] = np.NaN                \n",
    "            ### Adding region information:\n",
    "            df_iter_factor_raw = pd.concat([ser_iter_level_factor, ser_iter_short_factor, ser_iter_long_factor], axis = 1).join(ser_ison_iter_date, how = 'left')\\\n",
    "                                                                                                                          .set_index('Market', append = True)\n",
    "            ### Regions clearing:\n",
    "            df_iter_factor_raw = df_iter_factor_raw.loc[(All, All, list_ison), All]\n",
    "            ### Countries filtering:\n",
    "            df_iter_factor_raw = df_iter_factor_raw.drop(list_countries_to_exclude, level = 'Country')\n",
    "            ### Preparing for two step standardizing:\n",
    "            ser_iter_factor_raw = df_iter_factor_raw.stack(dropna = False)\n",
    "            ser_iter_factor_raw.index.set_names('Factor', level = -1, inplace = True)\n",
    "            ser_iter_factor_raw = ser_iter_factor_raw.reorder_levels(['Pillar', 'Factor', 'Country', 'Market'])\n",
    "            ser_iter_factor_raw.name = 'Raw'\n",
    "            ### Standardize -> Recover -> Demean by Region -> Standardize again:\n",
    "            ser_iter_factor_std = ser_iter_factor_raw.groupby(['Pillar', 'Factor'])\\\n",
    "                                                     .apply(lambda ser_factor_raw: std_recover_demean_std(ser_factor_raw.droplevel(['Pillar', 'Factor'])))\n",
    "            ser_iter_factor_std.name = 'Std'\n",
    "            ### Object to save:\n",
    "            df_iter_factor_to_save = pd.concat([ser_iter_factor_raw, ser_iter_factor_std], axis = 1)\n",
    "            df_iter_factor_to_save = pd.concat({iter_source: pd.concat({iter_date: df_iter_factor_to_save}, names = ['Date'])}, names = ['Source'])\\\n",
    "                                       .reorder_levels(['Source', 'Pillar', 'Factor', 'Date', 'Country', 'Market'])         \n",
    "            ### Saving source factors data to the table:\n",
    "            df_iter_factor_to_save.to_csv(str_test_factor_full_csv, sep = ';', mode = 'a', header = not os.path.exists(str_test_factor_full_csv))\n",
    "            ### Autocorrection calculating for business-end-of-month:\n",
    "            if (iter_date == (iter_date + pd.tseries.offsets.BMonthEnd(0))):\n",
    "                date_prev = iter_date - pd.tseries.offsets.BMonthEnd(1)\n",
    "                ### Extract previous tha-factor vector from csv file (should be substituted by SQL query):\n",
    "                if (os.path.exists(str_test_autocorr_csv)):\n",
    "                    ser_iter_factor_std_prev = pd.read_csv(str_test_factor_full_csv, sep = ';', index_col = list(range(6)), header = 0, squeeze = True, parse_dates = [3],\n",
    "                                                           na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                                                        '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\\\n",
    "                                                 .loc[([iter_source], All, All, [date_prev], All, All), 'Std'].droplevel(['Source', 'Date'])\n",
    "                else:\n",
    "                    ser_iter_factor_std_prev = pd.Series(np.NaN, index = ser_iter_factor_std.index)\n",
    "                ser_iter_factor_std.name = 'Current'\n",
    "                ser_iter_factor_std_prev.name = 'Previous'    \n",
    "                ### Autocorrelation calculating:\n",
    "                df_iter_history = pd.concat([ser_iter_factor_std, ser_iter_factor_std_prev], axis = 1)\n",
    "                ser_iter_autocorr = df_iter_history.groupby(['Pillar', 'Factor', 'Market'], group_keys = False)\\\n",
    "                                                  .apply(lambda df_market: df_market['Current'].corr(df_market['Previous']))\n",
    "                ### Autocorrelation coefficients calculating and saving (should be substituted by SQL query):\n",
    "                ser_iter_autocorr_dated = pd.concat({iter_source: pd.concat({iter_date: ser_iter_autocorr}, names = ['Date'])}, names = ['Source'])\\\n",
    "                                            .reorder_levels(['Source', 'Pillar', 'Factor', 'Date', 'Market'])            \n",
    "                ser_iter_autocorr_dated.name = 'Autocorr'\n",
    "                ser_iter_autocorr_dated.to_csv(str_test_autocorr_csv, sep = ';', mode = 'a', header = not os.path.exists(str_test_autocorr_csv))\n",
    "            ### Extract autocorr data (should be substituted by SQL query):\n",
    "            ser_autocorr_vector = pd.read_csv(str_test_autocorr_csv, sep = ';', index_col = [0, 1, 2, 3, 4], header = 0, squeeze = True, parse_dates = [3],\n",
    "                                              na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                                           '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\\\n",
    "                                       .loc[[iter_source], All, All, All, All].droplevel('Source')\n",
    "            ### First month-end check:            \n",
    "            if (ser_autocorr_vector.count() > 0):\n",
    "                ### Excluding ones (repeated values):\n",
    "                ser_autocorr_vector.loc[ser_autocorr_vector >= (1.0 - flo_similarity)] = np.NaN                \n",
    "                ### Autocorrelation mean:\n",
    "                ser_autocorr_cum_mean = ser_autocorr_vector.groupby(['Pillar', 'Factor', 'Market'], group_keys = False).mean().clip(lower = 0.0)\n",
    "                ### Quarterly mean converting to monthly:\n",
    "                if (iter_source == 'Continuum'):\n",
    "                    ser_autocorr_cum_mean = ser_autocorr_cum_mean ** (1 / 3)\n",
    "                ### THA-coefficient calculating:                       \n",
    "                ser_tha_coeff = ser_autocorr_cum_mean\\\n",
    "                                        .transform(lambda iter_mean: sum(map(lambda iter_num: (flo_tha_ratio * iter_mean) ** iter_num, range(int_tha_length))) / 2)\n",
    "            else:\n",
    "                ser_tha_coeff = ser_autocorr_vector.droplevel('Date')\n",
    "            ### Adding empty Markets to further fillna and fillna (first date of new region appearance):\n",
    "            ser_tha_coeff = ser_tha_coeff.unstack(['Pillar', 'Factor']).reindex(['DM', 'EM', 'FM']).stack(['Pillar', 'Factor'], dropna = False)\\\n",
    "                                         .reorder_levels(['Pillar', 'Factor', 'Market']).fillna(2.0).sort_index()\n",
    "            ser_tha_coeff.name = 'THA_Coeff'\n",
    "            ### THA-adjusted z-score calculating:\n",
    "            ser_iter_factor_std.name = 'Factor'\n",
    "            df_iter_factor_tha = ser_iter_factor_std.to_frame().join(ser_tha_coeff).sort_index()\n",
    "            ser_iter_factor_tha = df_iter_factor_tha['Factor'] * df_iter_factor_tha['THA_Coeff']\n",
    "            ### Consolidated factor for each pillar of the source:\n",
    "            ser_iter_factor_agg = ser_iter_factor_tha.unstack('Factor').groupby('Pillar', group_keys = False).apply(columns_average)\n",
    "            ### Source level consolidating and saving:\n",
    "            if (len(ser_iter_factor_agg.index.get_level_values('Market').unique()) > 1):\n",
    "                ser_iter_source = ser_iter_factor_agg.unstack('Pillar').groupby('Market', group_keys = False)\\\n",
    "                                                     .apply(lambda df_region: columns_average(df_region, ser_weights.loc[iter_source, All, df_region.index[0][0]].values))\n",
    "            ### If we have single region for particular date:\n",
    "            else:\n",
    "                df_region = ser_iter_factor_agg.unstack('Pillar') \n",
    "                ser_iter_source = columns_average(df_region, ser_weights.loc[iter_source, All, df_region.index[0][0]].values)\n",
    "            ### Source level saving:    \n",
    "            ser_iter_source_dated = pd.concat({iter_source: pd.concat({iter_date: ser_iter_source.sort_index(level = 'Country')}, names = ['Date'])}, names = ['Source'])\\\n",
    "                                      .reorder_levels(['Source', 'Date', 'Country', 'Market'])\n",
    "            ser_iter_source_dated.name = 'Source_Factor'\n",
    "            ser_iter_source_dated.to_csv(str_test_factor_source_csv, sep = ';', mode = 'a', header = not os.path.exists(str_test_factor_source_csv))\n",
    "    ### Source factors aggregating, z-scoring and saving:\n",
    "    df_iter_full = pd.read_csv(str_test_factor_source_csv, sep = ';', index_col = [0, 1, 2, 3], header = 0, squeeze = True, parse_dates = [1],\n",
    "                           na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                        '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\\\n",
    "                                       .loc[All, [iter_date], All, All].unstack('Source')\n",
    "    ser_iter_full = columns_average(df_iter_full).groupby('Market').apply(lambda df_market: td_two_stage_standardize_extended(df_market)[0])\n",
    "    ser_iter_full.name = 'Consolidated_Factor'\n",
    "    ser_iter_full.to_csv(str_test_factor_agg_csv, sep = ';', mode = 'a', header = not os.path.exists(str_test_factor_agg_csv))\n",
    "    \n",
    "date_finish = datetime.utcnow()\n",
    "### Overall statistics printing:\n",
    "print('Finish time:', date_finish)\n",
    "print('Full interval:', date_finish - date_start)\n",
    "print('Average interval for single date:', str((date_finish - date_start) / len(idx_test_date_range)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Factor  Market  Country\n",
       "Level   DM      AT         1.479758\n",
       "                AU        -1.114699\n",
       "                BE        -0.390645\n",
       "                CA         0.227383\n",
       "                CH         3.106888\n",
       "                             ...   \n",
       "Short   DM      NO         0.070196\n",
       "                NZ         0.354459\n",
       "                SE         0.791418\n",
       "                SG         0.849237\n",
       "                US         0.218713\n",
       "Length: 63, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_iter_factor_agg_test = ser_iter_factor_tha.unstack('Pillar').groupby('Factor', group_keys = False).apply(columns_average)\n",
    "ser_iter_factor_agg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Source     Pillar                        Region\n",
       "Continuum  External Adjustment Capacity  DM        3.0\n",
       "                                         EM        3.0\n",
       "                                         FM        3.0\n",
       "           Institutional Robustness      DM        1.0\n",
       "                                         EM        1.0\n",
       "                                         FM        1.0\n",
       "           Medium-Term Growth Potential  DM        1.0\n",
       "                                         EM        1.0\n",
       "                                         FM        1.0\n",
       "           Social Inclusion              DM        1.0\n",
       "                                         EM        1.0\n",
       "                                         FM        1.0\n",
       "PRS        Economic Risk Rating          DM        1.0\n",
       "                                         EM        1.0\n",
       "                                         FM        1.0\n",
       "           Financial Risk Rating         DM        1.0\n",
       "                                         EM        1.0\n",
       "                                         FM        1.0\n",
       "           Political Risk Rating         DM        1.0\n",
       "                                         EM        1.0\n",
       "                                         FM        1.0\n",
       "Name: Weight, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
