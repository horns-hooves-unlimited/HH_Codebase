{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS NOTEBOOK IS PREPARING FACTOR DATA VECTORS FOR MARKET RISK THEME (GLOBAL COUNTRY MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STANDART MODULES INITIALISING\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXTRACTING DATA FROM MATLAB-STYLED XLSX FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants declaring:\n",
    "#path_msci_data = 'Data_Files/Source_Files/sample_data.xlsx'\n",
    "path_msci_data = 'Data_Files/Source_Files/sample_data.xlsx'\n",
    "tab_monthly = 'monthly_data'\n",
    "tab_daily = 'daily_returns'\n",
    "tab_ivol = 'ivol_data'\n",
    "tab_map = 'country_map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting universe data:\n",
    "df_universe = pd.read_excel(io = path_msci_data, sheet_name = tab_monthly, skiprows = [0, 2], header = 0,\n",
    "                            na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                         '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'])\n",
    "df_universe = df_universe.loc[:, ['dates', 'region', 'ctry']]\n",
    "df_universe.columns = ['Date', 'Market', 'Code']\n",
    "df_universe.set_index(['Code', 'Date'], inplace = True)\n",
    "ser_universe = df_universe.squeeze()\n",
    "ser_universe.sort_index(level = [0, 1], inplace = True)\n",
    "ser_universe.replace({50 : 'DM', 57 : 'EM', 504 : 'FM'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtering market universe:\n",
    "arr_markets_needed = ['DM', 'FM', 'EM']\n",
    "ser_market_membership = ser_universe[ser_universe.isin(arr_markets_needed)]\n",
    "index_market = ser_market_membership.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting returns data:\n",
    "df_returns = pd.read_excel(io = path_msci_data, sheet_name = tab_daily, skiprows = [0, 2], header = 0,\n",
    "                           na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                        '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'])\n",
    "df_returns = df_returns.loc[:, ['dates', 'ctry', 'retusd', 'retloc']]\n",
    "df_returns.columns = ['Date', 'Code', 'Ret USD', 'Ret LOC']\n",
    "df_returns.set_index(['Code', 'Date'], inplace = True)\n",
    "df_returns.sort_index(level = [0, 1], inplace = True)\n",
    "ser_realized_ret_USD = df_returns['Ret USD'].copy()\n",
    "ser_realized_ret_LOC = df_returns['Ret LOC'].copy()\n",
    "### Appending returns on ethalon date vector:\n",
    "date_first = date(1992, 1, 1)\n",
    "date_last = date(2018, 12, 31)\n",
    "index_dates = pd.date_range(date_first, date_last, freq = 'B')\n",
    "### Reindexation and forward filling procedure for USD returns:\n",
    "dict_realized_ret_USD = {}\n",
    "ser_ret_index_USD = pd.Series(np.NaN, index = ser_realized_ret_USD.index)\n",
    "for iter_country in ser_realized_ret_USD.index.get_level_values(0).unique():\n",
    "    ser_ret_index_USD[iter_country] = (1 + ser_realized_ret_USD[iter_country]).cumprod()\n",
    "    ser_ret_index_USD[iter_country].iloc[0] = 1\n",
    "    ser_ret_index_USD_iter = ser_ret_index_USD[iter_country].reindex(index_dates, method = 'ffill')\n",
    "    ser_ret_index_USD_iter.fillna(method = 'ffill', inplace = True)    \n",
    "    ser_realized_ret_USD_iter = (ser_ret_index_USD_iter / ser_ret_index_USD_iter.shift(1) - 1)\n",
    "    dict_realized_ret_USD[iter_country] = ser_realized_ret_USD_iter\n",
    "ser_realized_ret_USD = pd.concat(dict_realized_ret_USD)  \n",
    "ser_realized_ret_USD.index.names = ['Code', 'Date']\n",
    "ser_realized_ret_USD.sort_index(level = [0, 1], inplace = True)\n",
    "### Reindexation and forward filling procedure for LOC returns:\n",
    "dict_realized_ret_LOC = {}\n",
    "ser_ret_index_LOC = pd.Series(np.NaN, index = ser_realized_ret_LOC.index)\n",
    "for iter_country in ser_realized_ret_LOC.index.get_level_values(0).unique():\n",
    "    ser_ret_index_LOC[iter_country] = (1 + ser_realized_ret_LOC[iter_country]).cumprod()\n",
    "    ser_ret_index_LOC[iter_country].iloc[0] = 1   \n",
    "    ser_ret_index_LOC_iter = ser_ret_index_LOC[iter_country].reindex(index_dates, method = 'ffill')\n",
    "    ser_ret_index_LOC_iter.fillna(method = 'ffill', inplace = True)\n",
    "    ser_realized_ret_LOC_iter = (ser_ret_index_LOC_iter / ser_ret_index_LOC_iter.shift(1) - 1)   \n",
    "    dict_realized_ret_LOC[iter_country] = ser_realized_ret_LOC_iter\n",
    "ser_realized_ret_LOC = pd.concat(dict_realized_ret_LOC)    \n",
    "ser_realized_ret_LOC.index.names = ['Code', 'Date']\n",
    "ser_realized_ret_LOC.sort_index(level = [0, 1], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting ivol data:\n",
    "df_ivol = pd.read_excel(io = path_msci_data, sheet_name = tab_ivol, skiprows = [0, 2], header = 0,\n",
    "                        na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                     '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'])\n",
    "df_ivol = df_ivol.loc[:, ['dates', 'ctry', 'ivol3m', 'vrp3m']]\n",
    "df_ivol.columns = ['Date', 'Code', 'IVol 3m', 'VRP 3m']\n",
    "df_ivol.set_index(['Code', 'Date'], inplace = True)\n",
    "df_ivol.sort_index(level = [0, 1], inplace = True)\n",
    "ser_ivol3m = df_ivol['IVol 3m']\n",
    "ser_vrp3m = df_ivol['VRP 3m']\n",
    "### Appending returns on ethalon date vector:\n",
    "date_first = date(1992, 1, 1)\n",
    "date_last = date(2018, 12, 31)\n",
    "index_dates = pd.date_range(date_first, date_last, freq = 'B')\n",
    "dict_ivol3m = {}\n",
    "for iter_country in ser_ivol3m.index.get_level_values(0).unique():  \n",
    "    dict_ivol3m[iter_country] = ser_ivol3m[iter_country].reindex(index_dates, method = 'ffill')\n",
    "#    ser_ivol3m[iter_country] = ser_ivol3m[iter_country] - ser_ivol3m[iter_country].shift(1) ## CHANGES: DELTA CALCULATING TRANSFERRED TO FUNCTION\n",
    "ser_ivol3m = pd.concat(dict_ivol3m)    \n",
    "ser_ivol3m.index.names = ['Code', 'Date']\n",
    "ser_ivol3m.sort_index(level = [0, 1], inplace = True)\n",
    "dict_vrp3m = {}\n",
    "for iter_country in ser_vrp3m.index.get_level_values(0).unique():    \n",
    "    dict_vrp3m[iter_country] = ser_vrp3m[iter_country].reindex(index_dates, method = 'ffill')    \n",
    "ser_vrp3m = pd.concat(dict_vrp3m)    \n",
    "ser_vrp3m.index.names = ['Code', 'Date']\n",
    "ser_vrp3m.sort_index(level = [0, 1], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACTING MRI INDEX\n",
    "path_mri_index_hdf = 'Data_Files/Source_Files/mri_released_index.h5'\n",
    "object_released_mri_hdf = 'released_MRI_data'\n",
    "ser_mri_released = pd.read_hdf(path_mri_index_hdf, object_released_mri_hdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL WEIGHTS GENERATOR\n",
    "def get_exp_weights(window_years = 5, halflife_months = 3):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Array of regressioon window day numbers descending:\n",
    "#    arr_weight_days = np.arange(num_year_work_days * window_years + 1, 0, -1)\n",
    "    arr_weight_days = np.arange(num_year_work_days * window_years, 0, -1) - 1 ## CHANGES: ONE POINT SHIFT\n",
    "    ### Creating weights series:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round((num_year_work_days / num_year_months * halflife_months)))\n",
    "    arr_weights = np.exp(math.log(num_period_factor) * arr_weight_days)\n",
    "    ser_weights = pd.Series(arr_weights)        \n",
    "    ser_weights.name = 'Weight'\n",
    "    \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTS TO SERIES BINDER\n",
    "def bind_exp_weights(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Creating weights series:\n",
    "    if (weighting_kind == 'equal'):\n",
    "        ser_weights = pd.Series(1, index = ser_returns.index)\n",
    "    if (weighting_kind == 'expo'):       \n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]  ## CHANGES: NOT COUNT, BUT SIZE\n",
    "#        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.count() : ]        \n",
    "        ser_weights.index = ser_returns.index\n",
    "    if (weighting_kind == 'expo_cond'):\n",
    "        ser_condition = abs(ser_condition - ser_condition.iloc[-1])\n",
    "        ser_condition = ser_condition.sort_values(ascending = False)\n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]  ## CHANGES: NOT COUNT, BUT SIZE\n",
    "#        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.count() : ]            \n",
    "        ser_weights = pd.Series(ser_weights.values, ser_condition.index)\n",
    "        ser_weights.sort_index(inplace = True)\n",
    "        ser_weights.name = 'Weight'\n",
    "        \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL VOLATILITY CALCULATOR\n",
    "def get_expvol_value(ser_returns, ser_weights):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Exponential volatility calculating:\n",
    "    expvol_result = np.NaN\n",
    "    ser_returns = ser_returns.dropna()\n",
    "#    if (ser_returns.count() > num_year_work_days // 2): ## MOVED TO HIGHER LEVEL FUNCTION\n",
    "#        ser_weights = bind_exp_weights(ser_returns, weighting_kind, window_years, halflife_months, ser_condition) ## MOVED TO HIGHER LEVEL FUNCTION\n",
    "    index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "    ### Exponential volatility calculating:\n",
    "    expvol_y = ser_returns[index_rolling]\n",
    "    expvol_w = ser_weights[index_rolling]             \n",
    "    expvol_w = expvol_w / expvol_w.sum()\n",
    "    expvol_result = np.sqrt(expvol_w.dot(expvol_y * expvol_y)) * np.sqrt(num_year_work_days)\n",
    "        \n",
    "    return expvol_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL VOLATILITY SERIES BUILDER\n",
    "def get_expvol_series(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Flattening MSCI changes by logarythm\n",
    "    ser_returns = np.log(1 + ser_returns)\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)\n",
    "    ### Main loop performing:\n",
    "    ser_expvol = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.mean()\n",
    "                if (ser_iter_returns.size > 0):\n",
    "                    if (ser_condition.count() > 0):\n",
    "                        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                    else:\n",
    "                        ser_iter_condition = pd.Series(np.NaN)                     \n",
    "                    ser_iter_weights = bind_exp_weights(ser_iter_returns, weighting_kind, window_years, halflife_months, ser_iter_condition) ## CHANGES: ADDED       \n",
    "                ser_iter_returns.dropna(inplace = True)     \n",
    "                if (ser_iter_returns.count() > num_year_work_days // 2):\n",
    "                    expvol_result = get_expvol_value(ser_iter_returns, ser_iter_weights)\n",
    "                    ser_expvol.loc[iter_country, iter_date] = expvol_result\n",
    "                    \n",
    "    return ser_expvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVENT RISK FACTOR STANDALONE CALCULATION\n",
    "ser_expvol1m = get_expvol_series(ser_market_membership, ser_realized_ret_LOC, weighting_kind = 'expo', window_years = 5, halflife_months = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_expvol1m - AR 29-Dec-2006: 0.20481093607134887\n",
      "ser_expvol1m - US 29-Dec-2006: 0.07665226549320425\n",
      "ser_expvol1m - cross-sectional mean min: 0.11338560985788693\n",
      "ser_expvol1m - cross-sectional mean mean: 0.20672018872197573\n",
      "ser_expvol1m - cross-sectional mean max: 0.6481339155959455\n",
      "ser_expvol1m - cross-sectional mean stdev: 0.0701514409982021\n",
      "ser_expvol1m - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### EVENT RISK FACTOR TESTING:\n",
    "print('ser_expvol1m - AR 29-Dec-2006:', ser_expvol1m.loc['AR' , '2006-12-29'])\n",
    "print('ser_expvol1m - US 29-Dec-2006:', ser_expvol1m.loc['US' , '2006-12-29'])\n",
    "ser_expvol1m_mean = pd.Series(np.NaN, index = ser_expvol1m.index.get_level_values(1).unique())\n",
    "for iter_date in ser_expvol1m_mean.index:  \n",
    "    ser_expvol1m_mean[iter_date] = ser_expvol1m.loc[:, iter_date].mean()\n",
    "ser_expvol1m_mean.sort_index(inplace = True)\n",
    "print('ser_expvol1m - cross-sectional mean min:', ser_expvol1m_mean.min())\n",
    "print('ser_expvol1m - cross-sectional mean mean:', ser_expvol1m_mean.mean())\n",
    "print('ser_expvol1m - cross-sectional mean max:', ser_expvol1m_mean.max())\n",
    "print('ser_expvol1m - cross-sectional mean stdev:', ser_expvol1m_mean.std())\n",
    "print('ser_expvol1m - cross-sectional mean mean:', ser_expvol1m_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOWVOL FACTOR STANDALONE CALCULATION\n",
    "ser_expvol24m = get_expvol_series(ser_market_membership, ser_realized_ret_LOC, weighting_kind = 'expo', window_years = 5, halflife_months = 24)\n",
    "ser_lowvol_base = 1 / (ser_expvol24m * ser_expvol24m)\n",
    "ser_lowvol_base = ser_lowvol_base.swaplevel()\n",
    "ser_lowvol_base.sort_index(inplace = True)\n",
    "ser_lowvol = pd.Series(np.NaN, index = ser_lowvol_base.index)\n",
    "for iter_date in ser_lowvol.index.get_level_values(0).unique():  \n",
    "    ser_lowvol[iter_date] = (ser_lowvol_base[iter_date] / ser_lowvol_base[iter_date].sum())\n",
    "ser_lowvol = ser_lowvol.swaplevel()\n",
    "ser_lowvol.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_lowvol - AR 29-Dec-2006: 0.006992748913125383\n",
      "ser_lowvol - US 29-Dec-2006: 0.034252552068587024\n",
      "ser_lowvol - cross-sectional mean min: 0.02040816326530611\n",
      "ser_lowvol - cross-sectional mean mean: 0.02253254854186519\n",
      "ser_lowvol - cross-sectional mean max: 0.04545454545454546\n",
      "ser_lowvol - cross-sectional mean stdev: 0.006446261259307378\n",
      "ser_lowvol - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### LOWVOL FACTOR TESTING:\n",
    "print('ser_lowvol - AR 29-Dec-2006:', ser_lowvol.loc['AR' , '2006-12-29'])\n",
    "print('ser_lowvol - US 29-Dec-2006:', ser_lowvol.loc['US' , '2006-12-29'])\n",
    "ser_lowvol_mean = pd.Series(np.NaN, index = ser_lowvol.index.get_level_values(1).unique())\n",
    "for iter_date in ser_lowvol_mean.index:  \n",
    "    ser_lowvol_mean[iter_date] = ser_lowvol.loc[:, iter_date].mean()\n",
    "ser_lowvol_mean.sort_index(inplace = True)\n",
    "print('ser_lowvol - cross-sectional mean min:', ser_lowvol_mean.min())\n",
    "print('ser_lowvol - cross-sectional mean mean:', ser_lowvol_mean.mean())\n",
    "print('ser_lowvol - cross-sectional mean max:', ser_lowvol_mean.max())\n",
    "print('ser_lowvol - cross-sectional mean stdev:', ser_lowvol_mean.std())\n",
    "print('ser_lowvol - cross-sectional mean mean:', ser_lowvol_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VOLATILITY SURPRISE FACTOR STANDALONE CALCULATION\n",
    "ser_expvol1m_cond = get_expvol_series(ser_market_membership, ser_realized_ret_LOC, weighting_kind = 'expo_cond', window_years = 5, halflife_months = 1,\n",
    "                                      ser_condition = ser_mri_released)\n",
    "ser_expvol1m_surp = -np.log(ser_expvol1m / ser_expvol1m_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_expvol1m_cond - AR 29-Dec-2006: 0.20111505918084296\n",
      "ser_expvol1m_cond - US 29-Dec-2006: 0.08030428355472215\n",
      "ser_expvol1m_cond - cross-sectional mean min: 0.1235471422932931\n",
      "ser_expvol1m_cond - cross-sectional mean mean: 0.20781308329697862\n",
      "ser_expvol1m_cond - cross-sectional mean max: 0.6554988133255066\n",
      "ser_expvol1m_cond - cross-sectional mean stdev: 0.07722141721218073\n",
      "ser_expvol1m_cond - cross-sectional mean mean: 234\n",
      "ser_expvol1m_surp - AR 29-Dec-2006: -0.01821011250865118\n",
      "ser_expvol1m_surp - US 29-Dec-2006: 0.04654380270481559\n",
      "ser_expvol1m_surp - cross-sectional mean min: -0.4467730689545881\n",
      "ser_expvol1m_surp - cross-sectional mean mean: 0.006006031222997517\n",
      "ser_expvol1m_surp - cross-sectional mean max: 0.2584758919150355\n",
      "ser_expvol1m_surp - cross-sectional mean stdev: 0.10665189556657083\n",
      "ser_expvol1m_surp - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### VOLATILITY SURPRISE FACTOR TESTING:\n",
    "print('ser_expvol1m_cond - AR 29-Dec-2006:', ser_expvol1m_cond.loc['AR' , '2006-12-29'])\n",
    "print('ser_expvol1m_cond - US 29-Dec-2006:', ser_expvol1m_cond.loc['US' , '2006-12-29'])\n",
    "ser_expvol1m_cond_mean = pd.Series(np.NaN, index = ser_expvol1m_cond.index.get_level_values(1).unique())\n",
    "for iter_date in ser_expvol1m_cond_mean.index:  \n",
    "    ser_expvol1m_cond_mean[iter_date] = ser_expvol1m_cond.loc[:, iter_date].mean()\n",
    "ser_expvol1m_cond_mean.sort_index(inplace = True)\n",
    "print('ser_expvol1m_cond - cross-sectional mean min:', ser_expvol1m_cond_mean.min())\n",
    "print('ser_expvol1m_cond - cross-sectional mean mean:', ser_expvol1m_cond_mean.mean())\n",
    "print('ser_expvol1m_cond - cross-sectional mean max:', ser_expvol1m_cond_mean.max())\n",
    "print('ser_expvol1m_cond - cross-sectional mean stdev:', ser_expvol1m_cond_mean.std())\n",
    "print('ser_expvol1m_cond - cross-sectional mean mean:', ser_expvol1m_cond_mean.count())\n",
    "print('ser_expvol1m_surp - AR 29-Dec-2006:', ser_expvol1m_surp.loc['AR' , '2006-12-29'])\n",
    "print('ser_expvol1m_surp - US 29-Dec-2006:', ser_expvol1m_surp.loc['US' , '2006-12-29'])\n",
    "ser_expvol1m_surp_mean = pd.Series(np.NaN, index = ser_expvol1m_surp.index.get_level_values(1).unique())\n",
    "for iter_date in ser_expvol1m_surp_mean.index:  \n",
    "    ser_expvol1m_surp_mean[iter_date] = ser_expvol1m_surp.loc[:, iter_date].mean()\n",
    "ser_expvol1m_mean.sort_index(inplace = True)\n",
    "print('ser_expvol1m_surp - cross-sectional mean min:', ser_expvol1m_surp_mean.min())\n",
    "print('ser_expvol1m_surp - cross-sectional mean mean:', ser_expvol1m_surp_mean.mean())\n",
    "print('ser_expvol1m_surp - cross-sectional mean max:', ser_expvol1m_surp_mean.max())\n",
    "print('ser_expvol1m_surp - cross-sectional mean stdev:', ser_expvol1m_surp_mean.std())\n",
    "print('ser_expvol1m_surp - cross-sectional mean mean:', ser_expvol1m_surp_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING SKEWNESS CALCULATOR\n",
    "def get_skewness_value(ser_returns):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy.stats as sc    \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    ### Skewness calculating:\n",
    "    skewness_result = np.NaN\n",
    "    ser_returns = ser_returns.dropna()\n",
    "    if (ser_returns.count() > num_year_work_days // 2):\n",
    "        skewness_result = sc.skew(ser_returns, bias = False)\n",
    "        \n",
    "    return skewness_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING SKEWNESS SERIES BUILDER\n",
    "def get_skewness_series(ser_market_membership, ser_returns, window_years = 2):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Main loop performing:\n",
    "    ser_skewness = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date].dropna()     \n",
    "                if (ser_iter_returns.count() > num_year_work_days // 2):\n",
    "                    skewness_result = get_skewness_value(ser_iter_returns)\n",
    "                    ser_skewness.loc[iter_country, iter_date] = skewness_result\n",
    "\n",
    "    return ser_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TAIL RISK FACTOR STANDALONE CALCULATION\n",
    "ser_tailrisk = get_skewness_series(ser_market_membership, ser_realized_ret_LOC, window_years = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_tailrisk - AR 29-Dec-2006: 0.03928688949001212\n",
      "ser_tailrisk - US 29-Dec-2006: 0.08036479847853283\n",
      "ser_tailrisk - cross-sectional mean min: -0.49624621808319275\n",
      "ser_tailrisk - cross-sectional mean mean: -0.08681784121651422\n",
      "ser_tailrisk - cross-sectional mean max: 0.3488438785320589\n",
      "ser_tailrisk - cross-sectional mean stdev: 0.1673956680922374\n",
      "ser_tailrisk - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### TAIL RISK FACTOR TESTING:\n",
    "print('ser_tailrisk - AR 29-Dec-2006:', ser_tailrisk.loc['AR' , '2006-12-29'])\n",
    "print('ser_tailrisk - US 29-Dec-2006:', ser_tailrisk.loc['US' , '2006-12-29'])\n",
    "ser_tailrisk_mean = pd.Series(np.NaN, index = ser_tailrisk.index.get_level_values(1).unique())\n",
    "for iter_date in ser_tailrisk_mean.index:  \n",
    "    ser_tailrisk_mean[iter_date] = ser_tailrisk.loc[:, iter_date].mean()\n",
    "ser_tailrisk_mean.sort_index(inplace = True)\n",
    "print('ser_tailrisk - cross-sectional mean min:', ser_tailrisk_mean.min())\n",
    "print('ser_tailrisk - cross-sectional mean mean:', ser_tailrisk_mean.mean())\n",
    "print('ser_tailrisk - cross-sectional mean max:', ser_tailrisk_mean.max())\n",
    "print('ser_tailrisk - cross-sectional mean stdev:', ser_tailrisk_mean.std())\n",
    "print('ser_tailrisk - cross-sectional mean mean:', ser_tailrisk_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VRP FACTOR SERIES BUILDER\n",
    "def get_market_series(ser_market_membership, ser_returns, window_years = 5):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Main loop performing:\n",
    "    ser_market = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date].dropna()     \n",
    "                if (ser_iter_returns.count() > num_year_work_days // 4):\n",
    "                    ser_market.loc[iter_country, iter_date] = ser_returns.loc[iter_country, iter_date]\n",
    "\n",
    "    return ser_market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VRP FACTOR STANDALONE CALCULATION\n",
    "#ser_vrp_factor = ser_vrp3m.reindex(ser_market_membership.index)\n",
    "ser_vrp_factor = get_market_series(ser_market_membership, ser_vrp3m, window_years = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_vrp_factor - AR 29-Dec-2006: 0.00163\n",
      "ser_vrp_factor - US 29-Dec-2006: -0.004396\n",
      "ser_vrp_factor - cross-sectional mean min: -0.023387673469387766\n",
      "ser_vrp_factor - cross-sectional mean mean: 0.002256157878726162\n",
      "ser_vrp_factor - cross-sectional mean max: 0.036017938775510204\n",
      "ser_vrp_factor - cross-sectional mean stdev: 0.00794579999880406\n",
      "ser_vrp_factor - cross-sectional mean mean: 237\n"
     ]
    }
   ],
   "source": [
    "### VRP FACTOR TESTING:\n",
    "print('ser_vrp_factor - AR 29-Dec-2006:', ser_vrp_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_vrp_factor - US 29-Dec-2006:', ser_vrp_factor.loc['US' , '2006-12-29'])\n",
    "ser_vrp_factor_mean = pd.Series(np.NaN, index = ser_vrp_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_vrp_factor_mean.index:  \n",
    "    ser_vrp_factor_mean[iter_date] = ser_vrp_factor.loc[:, iter_date].mean()\n",
    "ser_vrp_factor_mean.sort_index(inplace = True)\n",
    "print('ser_vrp_factor - cross-sectional mean min:', ser_vrp_factor_mean.min())\n",
    "print('ser_vrp_factor - cross-sectional mean mean:', ser_vrp_factor_mean.mean())\n",
    "print('ser_vrp_factor - cross-sectional mean max:', ser_vrp_factor_mean.max())\n",
    "print('ser_vrp_factor - cross-sectional mean stdev:', ser_vrp_factor_mean.std())\n",
    "print('ser_vrp_factor - cross-sectional mean mean:', ser_vrp_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTED AVERAGE CALCULATOR\n",
    "def get_average_value(ser_returns, ser_weights):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Rolling average calculating:\n",
    "    average_result = np.NaN  \n",
    "    ser_returns = ser_returns.dropna()\n",
    "    if (ser_returns.count() > num_year_work_days // 4):\n",
    "        index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "        ### Exponential volatility calculating:\n",
    "        average_x = ser_returns[index_rolling]\n",
    "        average_w = ser_weights[index_rolling]                    \n",
    "        average_result = average_x.dot(average_w) / sum(average_w)        \n",
    "        \n",
    "    return average_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTED AVERAGE SERIES BUILDER\n",
    "def get_average_series(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Initialising delta series:\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)        \n",
    "    ### Main loop performing:\n",
    "    ser_average = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "                if (ser_iter_returns.size > 0):\n",
    "                    if (ser_condition.count() > 0):\n",
    "                        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                    else:\n",
    "                        ser_iter_condition = pd.Series(np.NaN)                      \n",
    "                    ser_iter_weights = bind_exp_weights(ser_iter_returns, weighting_kind, window_years, halflife_months, ser_iter_condition)                 \n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.shift(1)        \n",
    "                ser_iter_returns = ser_iter_returns.dropna()[ser_iter_returns != 0]\n",
    "                if (ser_iter_returns.count() > num_year_work_days // 4):\n",
    "                    average_result = get_average_value(ser_iter_returns, ser_iter_weights)\n",
    "                    ser_average.loc[iter_country, iter_date] = average_result\n",
    "    ser_average.sort_index(level = [0, 1], inplace = True)\n",
    "    \n",
    "    return ser_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPLIED VOLATILITY MOMENTUM FACTOR STANDALONE CALCULATION\n",
    "ser_ivolmom_1m = get_average_series(ser_market_membership, ser_ivol3m, weighting_kind = 'expo', window_years = 5, halflife_months = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_ivolmom_1m - AR 29-Dec-2006: -5.728833933486413e-05\n",
      "ser_ivolmom_1m - US 29-Dec-2006: -0.0001643642124898838\n",
      "ser_ivolmom_1m - cross-sectional mean min: -0.0007719988881806511\n",
      "ser_ivolmom_1m - cross-sectional mean mean: 6.00820444231159e-05\n",
      "ser_ivolmom_1m - cross-sectional mean max: 0.0011461061607235408\n",
      "ser_ivolmom_1m - cross-sectional mean stdev: 0.0002536959457065397\n",
      "ser_ivolmom_1m - cross-sectional mean mean: 237\n"
     ]
    }
   ],
   "source": [
    "### IMPLIED VOLATILITY MOMENTUM FACTOR TESTING:\n",
    "print('ser_ivolmom_1m - AR 29-Dec-2006:', ser_ivolmom_1m.loc['AR' , '2006-12-29'])\n",
    "print('ser_ivolmom_1m - US 29-Dec-2006:', ser_ivolmom_1m.loc['US' , '2006-12-29'])\n",
    "ser_ivolmom_1m_mean = pd.Series(np.NaN, index = ser_ivolmom_1m.index.get_level_values(1).unique())\n",
    "for iter_date in ser_ivolmom_1m_mean.index:  \n",
    "    ser_ivolmom_1m_mean[iter_date] = ser_ivolmom_1m.loc[:, iter_date].mean()\n",
    "ser_ivolmom_1m_mean.sort_index(inplace = True)\n",
    "print('ser_ivolmom_1m - cross-sectional mean min:', ser_ivolmom_1m_mean.min())\n",
    "print('ser_ivolmom_1m - cross-sectional mean mean:', ser_ivolmom_1m_mean.mean())\n",
    "print('ser_ivolmom_1m - cross-sectional mean max:', ser_ivolmom_1m_mean.max())\n",
    "print('ser_ivolmom_1m - cross-sectional mean stdev:', ser_ivolmom_1m_mean.std())\n",
    "print('ser_ivolmom_1m - cross-sectional mean mean:', ser_ivolmom_1m_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPLIED VOLATILITY MOMENTUM FACTOR STANDALONE CALCULATION\n",
    "ser_ivolmom_12m = get_average_series(ser_market_membership, ser_ivol3m, weighting_kind = 'expo', window_years = 5, halflife_months = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_ivolmom_12m - AR 29-Dec-2006: 2.4071504646865075e-05\n",
      "ser_ivolmom_12m - US 29-Dec-2006: -1.281313465985619e-05\n",
      "ser_ivolmom_12m - cross-sectional mean min: -8.265832098596306e-05\n",
      "ser_ivolmom_12m - cross-sectional mean mean: 5.48849915627575e-06\n",
      "ser_ivolmom_12m - cross-sectional mean max: 9.861384692768506e-05\n",
      "ser_ivolmom_12m - cross-sectional mean stdev: 2.4997621734106227e-05\n",
      "ser_ivolmom_12m - cross-sectional mean mean: 237\n"
     ]
    }
   ],
   "source": [
    "### IMPLIED VOLATILITY MOMENTUM FACTOR TESTING:\n",
    "print('ser_ivolmom_12m - AR 29-Dec-2006:', ser_ivolmom_12m.loc['AR' , '2006-12-29'])\n",
    "print('ser_ivolmom_12m - US 29-Dec-2006:', ser_ivolmom_12m.loc['US' , '2006-12-29'])\n",
    "ser_ivolmom_12m_mean = pd.Series(np.NaN, index = ser_ivolmom_12m.index.get_level_values(1).unique())\n",
    "for iter_date in ser_ivolmom_12m_mean.index:  \n",
    "    ser_ivolmom_12m_mean[iter_date] = ser_ivolmom_12m.loc[:, iter_date].mean()\n",
    "ser_ivolmom_12m_mean.sort_index(inplace = True)\n",
    "print('ser_ivolmom_12m - cross-sectional mean min:', ser_ivolmom_12m_mean.min())\n",
    "print('ser_ivolmom_12m - cross-sectional mean mean:', ser_ivolmom_12m_mean.mean())\n",
    "print('ser_ivolmom_12m - cross-sectional mean max:', ser_ivolmom_12m_mean.max())\n",
    "print('ser_ivolmom_12m - cross-sectional mean stdev:', ser_ivolmom_12m_mean.std())\n",
    "print('ser_ivolmom_12m - cross-sectional mean mean:', ser_ivolmom_12m_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_factors = {'short_term_event_risk': -ser_expvol1m,\n",
    "                'low_vol_anomaly': ser_lowvol,\n",
    "                'vol_surprise_event_risk': ser_expvol1m_surp,\n",
    "                'tail_risk': -ser_tailrisk,\n",
    "                'ivol_momentum_1m': ser_ivolmom_1m,\n",
    "                'ivol_momentum_12m': ser_ivolmom_12m,                \n",
    "                'vrp': ser_vrp_factor}\n",
    "path_results_data = 'Data_Files/Test_Files/sample_results.xlsx'\n",
    "with pd.ExcelWriter(path_results_data) as writer:\n",
    "    for iter_factor in dict_factors:\n",
    "        dict_factors[iter_factor].swaplevel().to_excel(writer, iter_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FUNCTION ### NOT USED FOR NOW !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "def iter_standartize(ser_to_manage, arr_truncates = [2.5, 2.0], reuse_outliers = False, center_result = True):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd     \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_full = ser_to_manage.copy()\n",
    "    ser_data_full = ser_data_full.dropna()\n",
    "    ser_data_iter = ser_data_full.copy() \n",
    "    ser_data_full.replace(ser_data_full.values, 0, inplace = True)    \n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncates:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = ser_data_iter.mean()\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        if not (reuse_outliers):\n",
    "            ### Saving to result and excluding from further calculations truncated values:     \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):      \n",
    "        ser_result = ser_data_full - ser_data_full.mean()\n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "            \n",
    "    return [ser_result, arr_mean, arr_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FACTOR SCORING GENERATOR ### NOT USED FOR NOW !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "def get_scored_factors(dict_factors, ser_market_membership, score_grouping = 'within', \n",
    "                       score_boundaries = [2.5, 2.0], score_reuse_outliers = False, score_center_result = True):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd   \n",
    " \n",
    "    ### Defining loop variants:\n",
    "    dict_result_raw = {}\n",
    "    raw_factor_suffix = '_raw'\n",
    "    dict_result_scored = {}    \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260    \n",
    "    num_year_months = 12\n",
    "    ### Looping factors:\n",
    "    for iter_factor in dict_factors:\n",
    "        ### Reforming and naming series for future performing:\n",
    "        ser_factor = dict_factors[iter_factor].swaplevel().sort_index(level = [0, 1])\n",
    "        ser_factor.name = iter_factor      \n",
    "        dict_result_raw[iter_factor + raw_factor_suffix] = ser_factor        \n",
    "        ### Scoring factor:\n",
    "        ### Defining constants for standatize procedure:        \n",
    "        arr_ser_scored = []\n",
    "        arr_dates = []                \n",
    "        ### Scoring for no grouping:            \n",
    "        if (score_grouping == 'full'):\n",
    "            for iter_date in ser_factor.index.get_level_values(0).unique():\n",
    "                ser_iter_factor = ser_factor.loc[iter_date].dropna()\n",
    "                if (ser_iter_factor.count() > 0):\n",
    "                    ser_iter_score = iter_standartize(ser_iter_factor, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                    arr_ser_scored.append(ser_iter_score)\n",
    "                    arr_dates.append(iter_date)\n",
    "            ser_factor_scored = pd.concat(arr_ser_scored, axis = 0, keys = arr_dates).sort_index(level = [0, 1])\n",
    "        ### Scoring for markets grouping:                            \n",
    "        if (score_grouping == 'within'):               \n",
    "            df_to_score = pd.concat([ser_factor, ser_market_membership.swaplevel().sort_index(level = [0, 1])], axis = 1, join = 'inner')\n",
    "            df_to_score.index.names = ['Date', 'Code']\n",
    "            df_to_score.set_index('Market', append = True, inplace = True)\n",
    "            df_to_score.sort_index(level = [0, 1, 2], inplace = True)                \n",
    "            arr_ser_scored = []\n",
    "            for iter_date in df_to_score.index.get_level_values(0).unique():\n",
    "                for iter_market in df_to_score.loc[iter_date, :, :].index.get_level_values(2).unique():\n",
    "                    df_to_score_iter = df_to_score.loc[iter_date, :, iter_market]\n",
    "                    ser_iter_factor = df_to_score_iter[iter_factor].dropna()\n",
    "                    if (ser_iter_factor.count() > 0):\n",
    "                        ser_iter_score = iter_standartize(ser_iter_factor, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                        ser_iter_score.reset_index('Market', drop = True, inplace = True)\n",
    "                        arr_ser_scored.append(ser_iter_score)\n",
    "            ser_factor_scored = pd.concat(arr_ser_scored, axis = 0).sort_index(level = [0, 1])\n",
    "        ### Aggregating factors to dictionary:    \n",
    "        ser_factor_scored.index.names = ['Date', 'Code']    \n",
    "        dict_result_scored[iter_factor] = ser_factor_scored\n",
    "        print(iter_factor, 'prepared')\n",
    "    ### Collecting factor tables to dictionary:\n",
    "    df_factors_raw = pd.concat(dict_result_raw, axis = 1, join = 'outer') \n",
    "    df_factors_scored = pd.concat(dict_result_scored, axis = 1, join = 'outer')    \n",
    "    \n",
    "    return [df_factors_raw, df_factors_scored]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short_term_event_risk prepared\n",
      "low_vol_anomaly prepared\n",
      "vol_surprise_event_risk prepared\n",
      "tail_risk prepared\n",
      "ivol_momentum_1m prepared\n",
      "ivol_momentum_12m prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in sign\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in sign\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vrp prepared\n"
     ]
    }
   ],
   "source": [
    " ### NOT USED FOR NOW !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "dict_factors = {'short_term_event_risk': -ser_expvol1m,\n",
    "                'low_vol_anomaly': ser_lowvol,\n",
    "                'vol_surprise_event_risk': ser_expvol1m_surp,\n",
    "                'tail_risk': -ser_tailrisk,\n",
    "                'ivol_momentum_1m': ser_ivolmom_1m,\n",
    "                'ivol_momentum_12m': ser_ivolmom_12m,                \n",
    "                'vrp': ser_vrp_factor}\n",
    "[df_factors_raw, df_factors_scored] = get_scored_factors(dict_factors, ser_market_membership, score_grouping = 'within', score_boundaries = [2.5, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
