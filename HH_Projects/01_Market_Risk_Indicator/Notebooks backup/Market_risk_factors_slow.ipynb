{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS NOTEBOOK IS PREPARING FACTOR DATA VECTORS FOR MARKET RISK THEME (GLOBAL COUNTRY MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STANDART MODULES INITIALISING\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXTRACTING DATA FROM MATLAB-STYLED XLSX FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants declaring:\n",
    "path_msci_data = 'Data_Files/Source_Files/sample_data.xlsx'\n",
    "tab_monthly = 'monthly_data'\n",
    "tab_daily = 'daily_returns'\n",
    "tab_ivol = 'ivol_data'\n",
    "tab_map = 'country_map'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting universe data:\n",
    "df_universe = pd.read_excel(io = path_msci_data, sheet_name = tab_monthly, skiprows = [0, 2], header = 0)\n",
    "df_universe = df_universe.loc[:, ['dates', 'region', 'ctry']]\n",
    "df_universe.columns = ['Date', 'Market', 'Code']\n",
    "df_universe.set_index(['Code', 'Date'], inplace = True)\n",
    "ser_universe = df_universe.squeeze()\n",
    "ser_universe.sort_index(level = [0, 1], inplace = True)\n",
    "ser_universe.replace({50 : 'DM', 57 : 'EM', 504 : 'FM'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filtering market universe:\n",
    "arr_markets_needed = ['DM', 'FM', 'EM']\n",
    "ser_market_membership = ser_universe[ser_universe.isin(arr_markets_needed)]\n",
    "index_market = ser_market_membership.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting returns data:\n",
    "df_returns = pd.read_excel(io = path_msci_data, sheet_name = tab_daily, skiprows = [0, 2], header = 0)\n",
    "df_returns = df_returns.loc[:, ['dates', 'ctry', 'retusd', 'retloc']]\n",
    "df_returns.columns = ['Date', 'Code', 'Ret USD', 'Ret LOC']\n",
    "df_returns.set_index(['Code', 'Date'], inplace = True)\n",
    "df_returns.sort_index(level = [0, 1], inplace = True)\n",
    "ser_realized_ret_USD = df_returns['Ret USD']\n",
    "ser_realized_ret_LOC = df_returns['Ret LOC']\n",
    "### Appending returns on ethalon date vector:\n",
    "date_first = date(1992, 1, 1)\n",
    "date_last = date(2018, 12, 31)\n",
    "index_dates = pd.date_range(date_first, date_last, freq = 'B')\n",
    "ser_ret_index_USD = pd.Series(np.NaN, index = ser_realized_ret_USD.index)\n",
    "for iter_country in ser_realized_ret_USD.index.get_level_values(0).unique():\n",
    "    ser_ret_index_USD[iter_country] = (1 + ser_realized_ret_USD[iter_country]).cumprod()\n",
    "    ser_ret_index_USD[iter_country].fillna(method = 'ffill', inplace = True)\n",
    "    ser_ret_index_USD[iter_country].iloc[0] = 1\n",
    "    ser_ret_index_USD[iter_country].reindex(index_dates, method = 'ffill')\n",
    "    ser_realized_ret_USD[iter_country] = (ser_ret_index_USD[iter_country] / ser_ret_index_USD[iter_country].shift(1) - 1)\n",
    "ser_ret_index_LOC = pd.Series(np.NaN, index = ser_realized_ret_LOC.index)\n",
    "for iter_country in ser_realized_ret_LOC.index.get_level_values(0).unique():\n",
    "    ser_ret_index_LOC[iter_country] = (1 + ser_realized_ret_LOC[iter_country]).cumprod()\n",
    "    ser_ret_index_LOC[iter_country].fillna(method = 'ffill', inplace = True)\n",
    "    ser_ret_index_LOC[iter_country].iloc[0] = 1   \n",
    "    ser_ret_index_LOC[iter_country].reindex(index_dates, method = 'ffill')\n",
    "    ser_realized_ret_LOC[iter_country] = (ser_ret_index_LOC[iter_country] / ser_ret_index_LOC[iter_country].shift(1) - 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting ivol data:\n",
    "df_ivol = pd.read_excel(io = path_msci_data, sheet_name = tab_ivol, skiprows = [0, 2], header = 0)\n",
    "df_ivol = df_ivol.loc[:, ['dates', 'ctry', 'ivol3m', 'vrp3m']]\n",
    "df_ivol.columns = ['Date', 'Code', 'IVol 3m', 'VRP 3m']\n",
    "df_ivol.set_index(['Code', 'Date'], inplace = True)\n",
    "df_ivol.sort_index(level = [0, 1], inplace = True)\n",
    "ser_ivol3m = df_ivol['IVol 3m']\n",
    "ser_vrp3m = df_ivol['VRP 3m']\n",
    "### Appending returns on ethalon date vector:\n",
    "date_first = date(1992, 1, 1)\n",
    "date_last = date(2018, 12, 31)\n",
    "index_dates = pd.date_range(date_first, date_last, freq = 'B')\n",
    "for iter_country in ser_ivol3m.index.get_level_values(0).unique():\n",
    "    ser_ivol3m[iter_country].reindex(index_dates, method = 'ffill')    \n",
    "#    ser_ivol3m[iter_country] = ser_ivol3m[iter_country] - ser_ivol3m[iter_country].shift(1)\n",
    "for iter_country in ser_vrp3m.index.get_level_values(0).unique():    \n",
    "    ser_vrp3m[iter_country].reindex(index_dates, method = 'ffill')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACTING MRI INDEX\n",
    "path_mri_index_hdf = 'Data_Files/Source_Files/mri_released_index.h5'\n",
    "object_released_mri_hdf = 'released_MRI_data'\n",
    "ser_mri_released = pd.read_hdf(path_mri_index_hdf, object_released_mri_hdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL WEIGHTS GENERATOR\n",
    "def get_exp_weights(window_years = 5, halflife_months = 3):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Array of regressioon window day numbers descending:\n",
    "#    arr_weight_days = np.arange(num_year_work_days * window_years + 1, 0, -1)\n",
    "    arr_weight_days = np.arange(num_year_work_days * window_years, 0, -1) - 1\n",
    "    ### Creating weights series:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round((num_year_work_days / num_year_months * halflife_months)))\n",
    "    arr_weights = np.exp(math.log(num_period_factor) * arr_weight_days)\n",
    "    ser_weights = pd.Series(arr_weights)        \n",
    "    ser_weights.name = 'Weight'\n",
    "    \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTS TO SERIES BINDER\n",
    "def bind_exp_weights(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Need to replace ser_returns with ser_returns.index !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!    \n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Creating weights series:\n",
    "    if (weighting_kind == 'equal'):\n",
    "        ser_weights = pd.Series(1, index = ser_returns.index)\n",
    "    if (weighting_kind == 'expo'):       \n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]\n",
    "#        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.count() : ]        \n",
    "        ser_weights.index = ser_returns.index\n",
    "    if (weighting_kind == 'expo_cond'):\n",
    "        ### Need to move conditonal logic outside of the loop. leave only sorting inside of the function !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        ser_condition = abs(ser_condition - ser_condition.iloc[-1])\n",
    "        ser_condition = ser_condition.sort_values(ascending = False)\n",
    "#        ser_condition = ser_condition.sort_values(ascending = False, kind = 'mergesort')        \n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]\n",
    "#        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.count() : ]            \n",
    "        ser_weights = pd.Series(ser_weights.values, ser_condition.index)\n",
    "        ser_weights.sort_index(inplace = True)\n",
    "        ser_weights.name = 'Weight'\n",
    "        \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL VOLATILITY CALCULATOR\n",
    "def get_expvol_value(ser_returns, ser_weights):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Exponential volatility calculating:\n",
    "    expvol_result = np.NaN\n",
    "    ser_returns = ser_returns.dropna()\n",
    "#    if (ser_returns.count() > num_year_work_days // 2):\n",
    "        ### Need to have minimum data count parameter!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!   \n",
    "#        ser_weights = bind_exp_weights(ser_returns, weighting_kind, window_years, halflife_months, ser_condition)\n",
    "    index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "    ### Exponential volatility calculating:\n",
    "    expvol_y = ser_returns[index_rolling]\n",
    "    expvol_w = ser_weights[index_rolling]             \n",
    "    expvol_w = expvol_w / expvol_w.sum()\n",
    "    expvol_result = np.sqrt(expvol_w.dot(expvol_y * expvol_y)) * np.sqrt(num_year_work_days)\n",
    "        \n",
    "    return expvol_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL VOLATILITY SERIES BUILDER\n",
    "def get_expvol_series(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Flattening MSCI changes by logarythm\n",
    "    ### Need to move flattening logic outside of the function !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    ser_returns = np.log(1 + ser_returns)\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)\n",
    "    ser_window_weights = get_exp_weights(window_years, halflife_months)\n",
    "    ### Main loop performing:\n",
    "    ser_expvol = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                index_iter_full = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "                ser_iter_returns = ser_returns[iter_country][index_iter_full]\n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.mean() \n",
    "                ser_iter_returns.dropna(inplace = True)\n",
    "                index_iter_ret = ser_iter_returns.index                              \n",
    "                if (ser_iter_returns.count() > num_year_work_days // 2):\n",
    "                    if (weighting_kind == 'equal'):\n",
    "                        ser_iter_weights = pd.Series(1, index = index_iter_ret)                    \n",
    "                    if (weighting_kind == 'expo'):                  \n",
    "                        ser_iter_weights = pd.Series(ser_window_weights.values, index = index_iter_full)\n",
    "                        ser_iter_weights = ser_iter_weights[index_iter_ret]                    \n",
    "                    if (weighting_kind == 'expo_cond'):\n",
    "                        ser_iter_weights = pd.Series(ser_window_weights.values, index = index_iter_full)\n",
    "                        ser_iter_weights = ser_iter_weights[index_iter_ret]                   \n",
    "                        ser_iter_condition = ser_condition[index_iter_ret]\n",
    "                        ser_iter_condition = abs(ser_iter_condition - ser_iter_condition.iloc[-1])\n",
    "                        ser_iter_condition = ser_iter_condition.sort_values(ascending = False)\n",
    "                        ser_iter_weights = pd.Series(ser_iter_weights.values, ser_iter_condition.index)\n",
    "                        ser_iter_weights.sort_index(inplace = True)                     \n",
    "                    ### Need to have minimum data count parameter !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    ### Exponential volatility calculating:\n",
    "                    expvol_y = ser_iter_returns\n",
    "                    expvol_w = ser_iter_weights            \n",
    "                    expvol_w = expvol_w / expvol_w.sum()\n",
    "                    expvol_result = np.sqrt(expvol_w.dot(expvol_y * expvol_y)) * np.sqrt(num_year_work_days)                    \n",
    "                    ser_expvol.loc[iter_country, iter_date] = expvol_result\n",
    "                    \n",
    "    return ser_expvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_returns = ser_realized_ret_LOC.copy()\n",
    "ser_returns = np.log(1 + ser_returns)\n",
    "ser_condition = ser_mri_released.copy()\n",
    "iter_country = 'BE'\n",
    "iter_date = pd.Timestamp('2000-10-31')\n",
    "num_year_work_days = 260\n",
    "window_years = 5\n",
    "halflife_months = 3\n",
    "ser_iter_returns = ser_realized_ret_LOC[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "ser_iter_returns = ser_iter_returns - ser_iter_returns.mean() #### Need to transfer\n",
    "ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "ser_iter_condition = abs(ser_iter_condition - ser_iter_condition.iloc[-1])\n",
    "ser_iter_condition = ser_iter_condition.sort_values(ascending = False)\n",
    "ser_iter_weights = get_exp_weights(window_years, halflife_months)[- ser_iter_returns.size : ]\n",
    "ser_iter_weights = pd.Series(ser_iter_weights.values, ser_iter_condition.index)\n",
    "ser_iter_weights.sort_index(inplace = True)\n",
    "ser_iter_weights.name = 'Weight'\n",
    "ser_iter_returns.dropna(inplace = True)\n",
    "\n",
    "ser_iter_returns = ser_iter_returns.dropna()\n",
    "index_rolling = ser_iter_returns.index.intersection(ser_iter_weights.index)           \n",
    "expvol_y = ser_iter_returns[index_rolling]\n",
    "expvol_w = ser_iter_weights[index_rolling]             \n",
    "expvol_w = expvol_w / expvol_w.sum()\n",
    "expvol_result = np.sqrt(expvol_w.dot(expvol_y * expvol_y)) * np.sqrt(num_year_work_days)\n",
    "print('Test result:', expvol_result)\n",
    "print('Old result: 0.21947494831168715')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVENT RISK FACTOR STANDALONE CALCULATION\n",
    "ser_expvol1m = get_expvol_series(ser_market_membership, ser_realized_ret_LOC, weighting_kind = 'expo', window_years = 5, halflife_months = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_expvol1m - AR 29-Dec-2006: 0.20481093607134887\n",
      "ser_expvol1m - US 29-Dec-2006: 0.07665226549320425\n",
      "ser_expvol1m - cross-sectional mean min: 0.11338560985788693\n",
      "ser_expvol1m - cross-sectional mean mean: 0.20651407109697165\n",
      "ser_expvol1m - cross-sectional mean max: 0.6481642751441703\n",
      "ser_expvol1m - cross-sectional mean stdev: 0.07026043107191231\n",
      "ser_expvol1m - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### EVENT RISK FACTOR TESTING:\n",
    "print('ser_expvol1m - AR 29-Dec-2006:', ser_expvol1m.loc['AR' , '2006-12-29'])\n",
    "print('ser_expvol1m - US 29-Dec-2006:', ser_expvol1m.loc['US' , '2006-12-29'])\n",
    "ser_expvol1m_mean = pd.Series(np.NaN, index = ser_expvol1m.index.get_level_values(1).unique())\n",
    "for iter_date in ser_expvol1m_mean.index:  \n",
    "    ser_expvol1m_mean[iter_date] = ser_expvol1m.loc[:, iter_date].mean()\n",
    "ser_expvol1m_mean.sort_index(inplace = True)\n",
    "print('ser_expvol1m - cross-sectional mean min:', ser_expvol1m_mean.min())\n",
    "print('ser_expvol1m - cross-sectional mean mean:', ser_expvol1m_mean.mean())\n",
    "print('ser_expvol1m - cross-sectional mean max:', ser_expvol1m_mean.max())\n",
    "print('ser_expvol1m - cross-sectional mean stdev:', ser_expvol1m_mean.std())\n",
    "print('ser_expvol1m - cross-sectional mean mean:', ser_expvol1m_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOWVOL FACTOR STANDALONE CALCULATION\n",
    "ser_expvol24m = get_expvol_series(ser_market_membership, ser_realized_ret_LOC, weighting_kind = 'expo', window_years = 5, halflife_months = 24)\n",
    "ser_lowvol_base = 1 / (ser_expvol24m * ser_expvol24m)\n",
    "ser_lowvol_base = ser_lowvol_base.swaplevel()\n",
    "ser_lowvol_base.sort_index(inplace = True)\n",
    "ser_lowvol = pd.Series(np.NaN, index = ser_lowvol_base.index)\n",
    "for iter_date in ser_lowvol.index.get_level_values(0).unique():  \n",
    "    ser_lowvol[iter_date] = (ser_lowvol_base[iter_date] / ser_lowvol_base[iter_date].sum())\n",
    "ser_lowvol = ser_lowvol.swaplevel()\n",
    "ser_lowvol.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_lowvol - AR 29-Dec-2006: 0.006992748913125383\n",
      "ser_lowvol - US 29-Dec-2006: 0.034252552068587024\n",
      "ser_lowvol - cross-sectional mean min: 0.02040816326530611\n",
      "ser_lowvol - cross-sectional mean mean: 0.02253254854186519\n",
      "ser_lowvol - cross-sectional mean max: 0.04545454545454546\n",
      "ser_lowvol - cross-sectional mean stdev: 0.006446261259307378\n",
      "ser_lowvol - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### LOWVOL FACTOR TESTING:\n",
    "print('ser_lowvol - AR 29-Dec-2006:', ser_lowvol.loc['AR' , '2006-12-29'])\n",
    "print('ser_lowvol - US 29-Dec-2006:', ser_lowvol.loc['US' , '2006-12-29'])\n",
    "ser_lowvol_mean = pd.Series(np.NaN, index = ser_lowvol.index.get_level_values(1).unique())\n",
    "for iter_date in ser_lowvol_mean.index:  \n",
    "    ser_lowvol_mean[iter_date] = ser_lowvol.loc[:, iter_date].mean()\n",
    "ser_lowvol_mean.sort_index(inplace = True)\n",
    "print('ser_lowvol - cross-sectional mean min:', ser_lowvol_mean.min())\n",
    "print('ser_lowvol - cross-sectional mean mean:', ser_lowvol_mean.mean())\n",
    "print('ser_lowvol - cross-sectional mean max:', ser_lowvol_mean.max())\n",
    "print('ser_lowvol - cross-sectional mean stdev:', ser_lowvol_mean.std())\n",
    "print('ser_lowvol - cross-sectional mean mean:', ser_lowvol_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VOLATILITY SURPRISE FACTOR STANDALONE CALCULATION\n",
    "ser_expvol1m_cond = get_expvol_series(ser_market_membership, ser_realized_ret_LOC, weighting_kind = 'expo_cond', window_years = 5, halflife_months = 1,\n",
    "                                      ser_condition = ser_mri_released)\n",
    "ser_expvol1m_surp = -np.log(ser_expvol1m / ser_expvol1m_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_expvol1m_cond - AR 29-Dec-2006: 0.20111505918084296\n",
      "ser_expvol1m_cond - US 29-Dec-2006: 0.08030428355472215\n",
      "ser_expvol1m_cond - cross-sectional mean min: 0.1235471422932931\n",
      "ser_expvol1m_cond - cross-sectional mean mean: 0.20781125053599298\n",
      "ser_expvol1m_cond - cross-sectional mean max: 0.6554988121197827\n",
      "ser_expvol1m_cond - cross-sectional mean stdev: 0.07721835002621177\n",
      "ser_expvol1m_cond - cross-sectional mean mean: 234\n",
      "ser_expvol1m_surp - AR 29-Dec-2006: -0.01821011250865118\n",
      "ser_expvol1m_surp - US 29-Dec-2006: 0.04654380270481559\n",
      "ser_expvol1m_surp - cross-sectional mean min: -0.4467730689545792\n",
      "ser_expvol1m_surp - cross-sectional mean mean: 0.0060044612564703\n",
      "ser_expvol1m_surp - cross-sectional mean max: 0.2573344798530834\n",
      "ser_expvol1m_surp - cross-sectional mean stdev: 0.10663886264647311\n",
      "ser_expvol1m_surp - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### VOLATILITY SURPRISE FACTOR TESTING:\n",
    "print('ser_expvol1m_cond - AR 29-Dec-2006:', ser_expvol1m_cond.loc['AR' , '2006-12-29'])\n",
    "print('ser_expvol1m_cond - US 29-Dec-2006:', ser_expvol1m_cond.loc['US' , '2006-12-29'])\n",
    "ser_expvol1m_cond_mean = pd.Series(np.NaN, index = ser_expvol1m_cond.index.get_level_values(1).unique())\n",
    "for iter_date in ser_expvol1m_cond_mean.index:  \n",
    "    ser_expvol1m_cond_mean[iter_date] = ser_expvol1m_cond.loc[:, iter_date].mean()\n",
    "ser_expvol1m_cond_mean.sort_index(inplace = True)\n",
    "print('ser_expvol1m_cond - cross-sectional mean min:', ser_expvol1m_cond_mean.min())\n",
    "print('ser_expvol1m_cond - cross-sectional mean mean:', ser_expvol1m_cond_mean.mean())\n",
    "print('ser_expvol1m_cond - cross-sectional mean max:', ser_expvol1m_cond_mean.max())\n",
    "print('ser_expvol1m_cond - cross-sectional mean stdev:', ser_expvol1m_cond_mean.std())\n",
    "print('ser_expvol1m_cond - cross-sectional mean mean:', ser_expvol1m_cond_mean.count())\n",
    "print('ser_expvol1m_surp - AR 29-Dec-2006:', ser_expvol1m_surp.loc['AR' , '2006-12-29'])\n",
    "print('ser_expvol1m_surp - US 29-Dec-2006:', ser_expvol1m_surp.loc['US' , '2006-12-29'])\n",
    "ser_expvol1m_surp_mean = pd.Series(np.NaN, index = ser_expvol1m_surp.index.get_level_values(1).unique())\n",
    "for iter_date in ser_expvol1m_surp_mean.index:  \n",
    "    ser_expvol1m_surp_mean[iter_date] = ser_expvol1m_surp.loc[:, iter_date].mean()\n",
    "ser_expvol1m_mean.sort_index(inplace = True)\n",
    "print('ser_expvol1m_surp - cross-sectional mean min:', ser_expvol1m_surp_mean.min())\n",
    "print('ser_expvol1m_surp - cross-sectional mean mean:', ser_expvol1m_surp_mean.mean())\n",
    "print('ser_expvol1m_surp - cross-sectional mean max:', ser_expvol1m_surp_mean.max())\n",
    "print('ser_expvol1m_surp - cross-sectional mean stdev:', ser_expvol1m_surp_mean.std())\n",
    "print('ser_expvol1m_surp - cross-sectional mean mean:', ser_expvol1m_surp_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Code\n",
       "AT    0.184245\n",
       "AU    0.165522\n",
       "BE    0.218800\n",
       "CA    0.373403\n",
       "CH    0.198943\n",
       "DE    0.303769\n",
       "DK    0.199148\n",
       "ES    0.359159\n",
       "FI    0.639754\n",
       "FR    0.279475\n",
       "GB    0.217396\n",
       "HK    0.331460\n",
       "IE    0.237979\n",
       "IT    0.299907\n",
       "JP    0.193879\n",
       "NL    0.264758\n",
       "NO    0.267469\n",
       "NZ    0.240489\n",
       "PT    0.228004\n",
       "SE    0.365983\n",
       "SG    0.328698\n",
       "US    0.229374\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser_expvol1m_cond[:, '2000-10-31']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING SKEWNESS CALCULATOR\n",
    "def get_skewness_value(ser_returns):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy.stats as sc    \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    ### Skewness calculating:\n",
    "    skewness_result = np.NaN\n",
    "    ser_returns = ser_returns.dropna()\n",
    "    if (ser_returns.count() > num_year_work_days // 2):\n",
    "        ### Need to have minimum data count parameter!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!   \n",
    "        skewness_result = sc.skew(ser_returns, bias = False)\n",
    "        \n",
    "    return skewness_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING SKEWNESS SERIES BUILDER\n",
    "def get_skewness_series(ser_market_membership, ser_returns, window_years = 2):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Main loop performing:\n",
    "    ser_skewness = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date].dropna()     \n",
    "                ### Change loc to iloc !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                if (ser_iter_returns.count() > num_year_work_days // 2):\n",
    "                    ### Need to have minimum data count parameter !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    skewness_result = get_skewness_value(ser_iter_returns)\n",
    "                    ser_skewness.loc[iter_country, iter_date] = skewness_result\n",
    "\n",
    "    return ser_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TAIL RISK FACTOR STANDALONE CALCULATION\n",
    "ser_tailrisk = get_skewness_series(ser_market_membership, ser_realized_ret_LOC, window_years = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_tailrisk - AR 29-Dec-2006: 0.03928688949001212\n",
      "ser_tailrisk - US 29-Dec-2006: 0.08036479847853283\n",
      "ser_tailrisk - cross-sectional mean min: -0.49624621808319275\n",
      "ser_tailrisk - cross-sectional mean mean: -0.08681784121651422\n",
      "ser_tailrisk - cross-sectional mean max: 0.3488438785320589\n",
      "ser_tailrisk - cross-sectional mean stdev: 0.1673956680922374\n",
      "ser_tailrisk - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### TAIL RISK FACTOR TESTING:\n",
    "print('ser_tailrisk - AR 29-Dec-2006:', ser_tailrisk.loc['AR' , '2006-12-29'])\n",
    "print('ser_tailrisk - US 29-Dec-2006:', ser_tailrisk.loc['US' , '2006-12-29'])\n",
    "ser_tailrisk_mean = pd.Series(np.NaN, index = ser_tailrisk.index.get_level_values(1).unique())\n",
    "for iter_date in ser_tailrisk_mean.index:  \n",
    "    ser_tailrisk_mean[iter_date] = ser_tailrisk.loc[:, iter_date].mean()\n",
    "ser_tailrisk_mean.sort_index(inplace = True)\n",
    "print('ser_tailrisk - cross-sectional mean min:', ser_tailrisk_mean.min())\n",
    "print('ser_tailrisk - cross-sectional mean mean:', ser_tailrisk_mean.mean())\n",
    "print('ser_tailrisk - cross-sectional mean max:', ser_tailrisk_mean.max())\n",
    "print('ser_tailrisk - cross-sectional mean stdev:', ser_tailrisk_mean.std())\n",
    "print('ser_tailrisk - cross-sectional mean mean:', ser_tailrisk_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### VRP FACTOR SERIES BUILDER\n",
    "def get_market_series(ser_market_membership, ser_returns, window_years = 5):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Main loop performing:\n",
    "    ser_market = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date].dropna()     \n",
    "                ### Change loc to iloc !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                if (ser_iter_returns.count() > num_year_work_days // 4):\n",
    "                    ### Need to have minimum data count parameter !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    ser_market.loc[iter_country, iter_date] = ser_returns.loc[iter_country, iter_date]\n",
    "\n",
    "    return ser_market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VRP FACTOR STANDALONE CALCULATION\n",
    "#ser_vrp_factor = ser_vrp3m.reindex(ser_market_membership.index)\n",
    "ser_vrp_factor = get_market_series(ser_market_membership, ser_vrp3m, window_years = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_vrp_factor - AR 29-Dec-2006: 0.00163\n",
      "ser_vrp_factor - US 29-Dec-2006: -0.004396\n",
      "ser_vrp_factor - cross-sectional mean min: -0.023387673469387766\n",
      "ser_vrp_factor - cross-sectional mean mean: 0.002256157878726162\n",
      "ser_vrp_factor - cross-sectional mean max: 0.036017938775510204\n",
      "ser_vrp_factor - cross-sectional mean stdev: 0.00794579999880406\n",
      "ser_vrp_factor - cross-sectional mean mean: 237\n"
     ]
    }
   ],
   "source": [
    "### VRP FACTOR TESTING:\n",
    "print('ser_vrp_factor - AR 29-Dec-2006:', ser_vrp_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_vrp_factor - US 29-Dec-2006:', ser_vrp_factor.loc['US' , '2006-12-29'])\n",
    "ser_vrp_factor_mean = pd.Series(np.NaN, index = ser_vrp_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_vrp_factor_mean.index:  \n",
    "    ser_vrp_factor_mean[iter_date] = ser_vrp_factor.loc[:, iter_date].mean()\n",
    "ser_vrp_factor_mean.sort_index(inplace = True)\n",
    "print('ser_vrp_factor - cross-sectional mean min:', ser_vrp_factor_mean.min())\n",
    "print('ser_vrp_factor - cross-sectional mean mean:', ser_vrp_factor_mean.mean())\n",
    "print('ser_vrp_factor - cross-sectional mean max:', ser_vrp_factor_mean.max())\n",
    "print('ser_vrp_factor - cross-sectional mean stdev:', ser_vrp_factor_mean.std())\n",
    "print('ser_vrp_factor - cross-sectional mean mean:', ser_vrp_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTED AVERAGE CALCULATOR\n",
    "def get_average_value(ser_returns, ser_weights):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Rolling average calculating:\n",
    "    average_result = np.NaN  \n",
    "    ser_returns = ser_returns.dropna()\n",
    "    if (ser_returns.count() > num_year_work_days // 4):\n",
    "#        ser_weights = bind_exp_weights(ser_returns, weighting_kind, window_years, halflife_months, ser_condition)       \n",
    "        index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "        ### Exponential volatility calculating:\n",
    "        average_x = ser_returns[index_rolling]\n",
    "        average_w = ser_weights[index_rolling]                    \n",
    "        average_result = average_x.dot(average_w) / sum(average_w)        \n",
    "        \n",
    "    return average_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTED AVERAGE SERIES BUILDER\n",
    "def get_average_series(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)\n",
    "    ser_window_weights = get_exp_weights(window_years, halflife_months)\n",
    "    ### Main loop performing:\n",
    "    ser_average = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                index_iter_full = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "                ser_iter_returns = ser_returns[iter_country][index_iter_full]\n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.shift(1)        \n",
    "                ser_iter_returns.dropna(inplace = True)                \n",
    "                ser_iter_returns = ser_iter_returns[ser_iter_returns != 0]                \n",
    "                index_iter_ret = ser_iter_returns.index                              \n",
    "                if (ser_iter_returns.count() > num_year_work_days // 4):\n",
    "                    if (weighting_kind == 'equal'):\n",
    "                        ser_iter_weights = pd.Series(1, index = index_iter_ret)                    \n",
    "                    if (weighting_kind == 'expo'):                  \n",
    "                        ser_iter_weights = pd.Series(ser_window_weights.values, index = index_iter_full)\n",
    "                        ser_iter_weights = ser_iter_weights[index_iter_ret]                    \n",
    "                    if (weighting_kind == 'expo_cond'):\n",
    "                        ser_iter_weights = pd.Series(ser_window_weights.values, index = index_iter_full)\n",
    "                        ser_iter_weights = ser_iter_weights[index_iter_ret]                   \n",
    "                        ser_iter_condition = ser_condition[index_iter_ret]\n",
    "                        ser_iter_condition = abs(ser_iter_condition - ser_iter_condition.iloc[-1])\n",
    "                        ser_iter_condition = ser_iter_condition.sort_values(ascending = False)\n",
    "                        ser_iter_weights = pd.Series(ser_iter_weights.values, ser_iter_condition.index)\n",
    "                        ser_iter_weights.sort_index(inplace = True)                     \n",
    "                    ### Need to have minimum data count parameter !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    ### Exponential average calculating:\n",
    "                    average_x = ser_iter_returns\n",
    "                    average_w = ser_iter_weights                    \n",
    "                    average_result = average_x.dot(average_w) / sum(average_w)                  \n",
    "                    ser_average.loc[iter_country, iter_date] = average_result\n",
    "                    \n",
    "    return ser_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPLIED VOLATILITY MOMENTUM FACTOR STANDALONE CALCULATION\n",
    "ser_ivolmom_1m = get_average_series(ser_market_membership, ser_ivol3m, weighting_kind = 'expo', window_years = 5, halflife_months = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_ivolmom_1m - AR 29-Dec-2006: -5.728833933486413e-05\n",
      "ser_ivolmom_1m - US 29-Dec-2006: -0.0001643642124898838\n",
      "ser_ivolmom_1m - cross-sectional mean min: -0.0007719988881806511\n",
      "ser_ivolmom_1m - cross-sectional mean mean: 6.00820444231159e-05\n",
      "ser_ivolmom_1m - cross-sectional mean max: 0.0011461061607235408\n",
      "ser_ivolmom_1m - cross-sectional mean stdev: 0.0002536959457065397\n",
      "ser_ivolmom_1m - cross-sectional mean mean: 237\n"
     ]
    }
   ],
   "source": [
    "### IMPLIED VOLATILITY MOMENTUM FACTOR TESTING:\n",
    "print('ser_ivolmom_1m - AR 29-Dec-2006:', ser_ivolmom_1m.loc['AR' , '2006-12-29'])\n",
    "print('ser_ivolmom_1m - US 29-Dec-2006:', ser_ivolmom_1m.loc['US' , '2006-12-29'])\n",
    "ser_ivolmom_1m_mean = pd.Series(np.NaN, index = ser_ivolmom_1m.index.get_level_values(1).unique())\n",
    "for iter_date in ser_ivolmom_1m_mean.index:  \n",
    "    ser_ivolmom_1m_mean[iter_date] = ser_ivolmom_1m.loc[:, iter_date].mean()\n",
    "ser_ivolmom_1m_mean.sort_index(inplace = True)\n",
    "print('ser_ivolmom_1m - cross-sectional mean min:', ser_ivolmom_1m_mean.min())\n",
    "print('ser_ivolmom_1m - cross-sectional mean mean:', ser_ivolmom_1m_mean.mean())\n",
    "print('ser_ivolmom_1m - cross-sectional mean max:', ser_ivolmom_1m_mean.max())\n",
    "print('ser_ivolmom_1m - cross-sectional mean stdev:', ser_ivolmom_1m_mean.std())\n",
    "print('ser_ivolmom_1m - cross-sectional mean mean:', ser_ivolmom_1m_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPLIED VOLATILITY MOMENTUM FACTOR STANDALONE CALCULATION\n",
    "ser_ivolmom_12m = get_average_series(ser_market_membership, ser_ivol3m, weighting_kind = 'expo', window_years = 5, halflife_months = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_ivolmom_12m - AR 29-Dec-2006: 2.4071504646865075e-05\n",
      "ser_ivolmom_12m - US 29-Dec-2006: -1.281313465985619e-05\n",
      "ser_ivolmom_12m - cross-sectional mean min: -8.265832098596306e-05\n",
      "ser_ivolmom_12m - cross-sectional mean mean: 5.48849915627575e-06\n",
      "ser_ivolmom_12m - cross-sectional mean max: 9.861384692768506e-05\n",
      "ser_ivolmom_12m - cross-sectional mean stdev: 2.4997621734106227e-05\n",
      "ser_ivolmom_12m - cross-sectional mean mean: 237\n"
     ]
    }
   ],
   "source": [
    "### IMPLIED VOLATILITY MOMENTUM FACTOR TESTING:\n",
    "print('ser_ivolmom_12m - AR 29-Dec-2006:', ser_ivolmom_12m.loc['AR' , '2006-12-29'])\n",
    "print('ser_ivolmom_12m - US 29-Dec-2006:', ser_ivolmom_12m.loc['US' , '2006-12-29'])\n",
    "ser_ivolmom_12m_mean = pd.Series(np.NaN, index = ser_ivolmom_12m.index.get_level_values(1).unique())\n",
    "for iter_date in ser_ivolmom_12m_mean.index:  \n",
    "    ser_ivolmom_12m_mean[iter_date] = ser_ivolmom_12m.loc[:, iter_date].mean()\n",
    "ser_ivolmom_12m_mean.sort_index(inplace = True)\n",
    "print('ser_ivolmom_12m - cross-sectional mean min:', ser_ivolmom_12m_mean.min())\n",
    "print('ser_ivolmom_12m - cross-sectional mean mean:', ser_ivolmom_12m_mean.mean())\n",
    "print('ser_ivolmom_12m - cross-sectional mean max:', ser_ivolmom_12m_mean.max())\n",
    "print('ser_ivolmom_12m - cross-sectional mean stdev:', ser_ivolmom_12m_mean.std())\n",
    "print('ser_ivolmom_12m - cross-sectional mean mean:', ser_ivolmom_12m_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FUNCTION\n",
    "def iter_standartize(ser_to_manage, arr_truncates = [2.5, 2.0], reuse_outliers = False, center_result = True):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd     \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_full = ser_to_manage.copy()\n",
    "    ser_data_full = ser_data_full.dropna()\n",
    "    ser_data_iter = ser_data_full.copy() \n",
    "    ser_data_full.replace(ser_data_full.values, 0, inplace = True)    \n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncates:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = ser_data_iter.mean()\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        if not (reuse_outliers):\n",
    "            ### Saving to result and excluding from further calculations truncated values:     \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):      \n",
    "        ser_result = ser_data_full - ser_data_full.mean()\n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "            \n",
    "    return [ser_result, arr_mean, arr_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FACTOR SCORING GENERATOR\n",
    "def get_scored_factors(dict_factors, ser_market_membership, score_grouping = 'within', \n",
    "                       score_boundaries = [2.5, 2.0], score_reuse_outliers = False, score_center_result = True):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd   \n",
    " \n",
    "    ### Defining loop variants:\n",
    "    dict_result_raw = {}\n",
    "    raw_factor_suffix = '_raw'\n",
    "    dict_result_scored = {}    \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260    \n",
    "    num_year_months = 12\n",
    "    ### Looping factors:\n",
    "    for iter_factor in dict_factors:\n",
    "        ### Reforming and naming series for future performing:\n",
    "        ser_factor = dict_factors[iter_factor].swaplevel().sort_index(level = [0, 1])\n",
    "        ser_factor.name = iter_factor      \n",
    "        dict_result_raw[iter_factor + raw_factor_suffix] = ser_factor        \n",
    "        ### Scoring factor:\n",
    "        ### Defining constants for standatize procedure:        \n",
    "        arr_ser_scored = []\n",
    "        arr_dates = []                \n",
    "        ### Scoring for no grouping:            \n",
    "        if (score_grouping == 'full'):\n",
    "            for iter_date in ser_factor.index.get_level_values(0).unique():\n",
    "                ser_iter_factor = ser_factor.loc[iter_date].dropna()\n",
    "                if (ser_iter_factor.count() > 0):\n",
    "                    ser_iter_score = iter_standartize(ser_iter_factor, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                    arr_ser_scored.append(ser_iter_score)\n",
    "                    arr_dates.append(iter_date)\n",
    "            ser_factor_scored = pd.concat(arr_ser_scored, axis = 0, keys = arr_dates).sort_index(level = [0, 1])\n",
    "        ### Scoring for markets grouping:                            \n",
    "        if (score_grouping == 'within'):               \n",
    "            df_to_score = pd.concat([ser_factor, ser_market_membership.swaplevel().sort_index(level = [0, 1])], axis = 1, join = 'inner')\n",
    "            df_to_score.index.names = ['Date', 'Code']\n",
    "            df_to_score.set_index('Market', append = True, inplace = True)\n",
    "            df_to_score.sort_index(level = [0, 1, 2], inplace = True)                \n",
    "            arr_ser_scored = []\n",
    "            for iter_date in df_to_score.index.get_level_values(0).unique():\n",
    "                for iter_market in df_to_score.loc[iter_date, :, :].index.get_level_values(2).unique():\n",
    "                    df_to_score_iter = df_to_score.loc[iter_date, :, iter_market]\n",
    "                    ser_iter_factor = df_to_score_iter[iter_factor].dropna()\n",
    "                    if (ser_iter_factor.count() > 0):\n",
    "                        ser_iter_score = iter_standartize(ser_iter_factor, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                        ser_iter_score.reset_index('Market', drop = True, inplace = True)\n",
    "                        arr_ser_scored.append(ser_iter_score)\n",
    "            ser_factor_scored = pd.concat(arr_ser_scored, axis = 0).sort_index(level = [0, 1])\n",
    "        ### Aggregating factors to dictionary:    \n",
    "        ser_factor_scored.index.names = ['Date', 'Code']    \n",
    "        dict_result_scored[iter_factor] = ser_factor_scored\n",
    "        print(iter_factor, 'prepared')\n",
    "    ### Collecting factor tables to dictionary:\n",
    "    df_factors_raw = pd.concat(dict_result_raw, axis = 1, join = 'outer') \n",
    "    df_factors_scored = pd.concat(dict_result_scored, axis = 1, join = 'outer')    \n",
    "    \n",
    "    return [df_factors_raw, df_factors_scored]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short_term_event_risk prepared\n",
      "low_vol_anomaly prepared\n",
      "vol_surprise_event_risk prepared\n",
      "tail_risk prepared\n",
      "ivol_momentum_1m prepared\n",
      "ivol_momentum_12m prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in sign\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in sign\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vrp prepared\n"
     ]
    }
   ],
   "source": [
    "dict_factors = {'short_term_event_risk': -ser_expvol1m,\n",
    "                'low_vol_anomaly': ser_lowvol,\n",
    "                'vol_surprise_event_risk': ser_expvol1m_surp,\n",
    "                'tail_risk': -ser_tailrisk,\n",
    "                'ivol_momentum_1m': ser_ivolmom_1m,\n",
    "                'ivol_momentum_12m': ser_ivolmom_12m,                \n",
    "                'vrp': ser_vrp_factor}\n",
    "[df_factors_raw, df_factors_scored] = get_scored_factors(dict_factors, ser_market_membership, score_grouping = 'within', score_boundaries = [2.5, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ivol_momentum_12m_raw         -0.000003\n",
       "ivol_momentum_1m_raw          -0.000058\n",
       "low_vol_anomaly_raw            0.006993\n",
       "short_term_event_risk_raw     -0.204811\n",
       "tail_risk_raw                 -0.039287\n",
       "vol_surprise_event_risk_raw   -0.018210\n",
       "vrp_raw                        0.001600\n",
       "Name: (2006-12-29 00:00:00, AR), dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_factors_raw.loc['2006-12-29', 'AR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD \n",
    "def market_risk_factors(dict_factors, ser_market_membership, score_all = True, \n",
    "                        score_grouping = 'within', score_boundaries = [2.5, 2.0], score_reuse_outliers = False, score_center_result = True):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd   \n",
    "    import scipy.stats as sc\n",
    "    ### Defining loop variants:\n",
    "    dict_ser_factor = {}\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260    \n",
    "    num_year_months = 12    \n",
    "    index_market = ser_market_membership.index\n",
    "    ### Looping factors:\n",
    "    for iter_factor in dict_factors:\n",
    "        arr_ser_source = dict_factors[iter_factor].copy()\n",
    "        if (iter_factor == 'eventrisk'):\n",
    "            ser_factor = (-1) * get_expvol_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])\n",
    "        if (iter_factor == 'lowvol'):\n",
    "            ser_factor = (-1) * get_expvol_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])              \n",
    "        if (iter_factor == 'volsurprise'):\n",
    "            ser_factor = get_expvol_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])\n",
    "            ser_factor = ser_factor.divide(get_expvol_series(index_market, arr_ser_source[0], arr_ser_source[4], arr_ser_source[5], arr_ser_source[6], arr_ser_source[7]))\n",
    "            ser_factor = (-1) * np.log(ser_factor)\n",
    "        if (iter_factor == 'tailrisk'):\n",
    "            ser_source = pd.Series(np.NaN, index = arr_ser_source[0].index)\n",
    "            for iter_country in arr_ser_source[0].index.get_level_values(0).unique():\n",
    "                ser_source[iter_country] = arr_ser_source[0][iter_country] - arr_ser_source[0][iter_country].shift(1)            \n",
    "            ser_factor = pd.Series(np.NaN, index = index_market)\n",
    "            for iter_country in index_market.get_level_values(0).unique():\n",
    "                if (iter_country in ser_source.index.get_level_values(0).unique()):\n",
    "                    for iter_date in index_market.get_level_values(1).unique():\n",
    "                        ser_iter_source = ser_source.loc[iter_country, iter_date - pd.offsets.BusinessDay(num_year_work_days * arr_ser_source[1] - 1) : iter_date].dropna()\n",
    "                        if (len(ser_iter_source) > 0):\n",
    "                            ser_factor.loc[iter_country, iter_date] = sc.skew(ser_iter_source) \n",
    "#        if (iter_factor == 'vrp'):\n",
    "#            ser_factor = arr_ser_source[0]                    \n",
    "        if (iter_factor == 'ivolmom1m'):\n",
    "            ser_factor = get_average_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])\n",
    "        if (iter_factor == 'ivolmom12m'):\n",
    "            ser_factor = get_average_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])\n",
    "        ### Reforming and Naming series for future performing:\n",
    "        ser_factor = ser_factor.swaplevel().sort_index(level = [0, 1])\n",
    "        ser_factor.name = iter_factor      \n",
    "        ### Scoring factor:\n",
    "        if (score_all):\n",
    "        ### Defining constants for standatize procedure:        \n",
    "            arr_ser_scored = []\n",
    "            arr_dates = []                \n",
    "            ### Scoring for no grouping:            \n",
    "            if (score_grouping == 'full'):\n",
    "                for iter_date in ser_factor.index.get_level_values(0).unique():\n",
    "                    ser_iter_factor = ser_factor.loc[iter_date].dropna()\n",
    "                    if (ser_iter_factor.count() > 0):\n",
    "                        ser_iter_score = iter_standartize(ser_iter_factor, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                        arr_ser_scored.append(ser_iter_score)\n",
    "                        arr_dates.append(iter_date)\n",
    "                ser_factor = pd.concat(arr_ser_scored, axis = 0, keys = arr_dates).sort_index(level = [0, 1])\n",
    "            ### Scoring for markets grouping:                            \n",
    "            if (score_grouping == 'within'):               \n",
    "                df_to_score = pd.concat([ser_factor, ser_market_membership.swaplevel().sort_index(level = [0, 1])], axis = 1, join = 'inner')\n",
    "                df_to_score.index.names = ['Date', 'Code']\n",
    "                df_to_score.set_index('Market', append = True, inplace = True)\n",
    "                df_to_score.sort_index(level = [0, 1, 2], inplace = True)                \n",
    "                arr_ser_scored = []\n",
    "                for iter_date in df_to_score.index.get_level_values(0).unique():\n",
    "                    for iter_market in df_to_score.loc[iter_date, :, :].index.get_level_values(2).unique():\n",
    "                        df_to_score_iter = df_to_score.loc[iter_date, :, iter_market]\n",
    "                        ser_iter_factor = df_to_score_iter[iter_factor].dropna()\n",
    "                        if (ser_iter_factor.count() > 0):\n",
    "                            ser_iter_score = iter_standartize(ser_iter_factor, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                            ser_iter_score.reset_index('Market', drop = True, inplace = True)\n",
    "                            arr_ser_scored.append(ser_iter_score)\n",
    "                ser_factor = pd.concat(arr_ser_scored, axis = 0).sort_index(level = [0, 1])\n",
    "        ### Aggregating factors to dictionary:    \n",
    "        ser_factor.index.names = ['Date', 'Code']    \n",
    "        dict_ser_factor[iter_factor] = ser_factor.copy()\n",
    "        print(iter_factor, 'prepared')\n",
    "    ### Collecting factor tables to dictinary:\n",
    "    df_factors = pd.concat(list(dict_ser_factor.values()), axis = 1, join = 'outer')   \n",
    "    \n",
    "    return df_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eventrisk prepared\n",
      "lowvol prepared\n",
      "volsurprise prepared\n",
      "tailrisk prepared\n",
      "ivolmom1m prepared\n",
      "ivolmom12m prepared\n"
     ]
    }
   ],
   "source": [
    "### OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD OLD \n",
    "dict_factors = {'eventrisk': [ser_realized_ret_LOC, 'expo', 5, 1], \n",
    "                'lowvol': [ser_realized_ret_LOC, 'expo', 5, 24], \n",
    "                'volsurprise': [ser_realized_ret_LOC, 'expo', 5, 1, 'expo_cond', 5, 1, ser_mri_released], \n",
    "                'tailrisk': [ser_realized_ret_LOC, 2], \n",
    "#                'VRP': [ser_fake_vrp], \n",
    "                'ivolmom1m': [ser_realized_ret_LOC, 'expo', 5, 1], \n",
    "                'ivolmom12m': [ser_realized_ret_LOC, 'expo', 5, 12]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTED AVERAGE SERIES BUILDER\n",
    "def get_average_series_old(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Initialising delta series:\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)        \n",
    "    ### Main loop performing:\n",
    "    ser_average = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "                if (ser_iter_returns.size > 0):\n",
    "                    if (ser_condition.count() > 0):\n",
    "                        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                    else:\n",
    "                        ser_iter_condition = pd.Series(np.NaN)                      \n",
    "                    ser_iter_weights = bind_exp_weights(ser_iter_returns, weighting_kind, window_years, halflife_months, ser_iter_condition)                 \n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.shift(1)        \n",
    "                ser_iter_returns = ser_iter_returns.dropna()[ser_iter_returns != 0]\n",
    "                ### Need to shift by iloc!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                if (ser_iter_returns.count() > num_year_work_days // 4):\n",
    "                ### Need to have minimum data count parameter!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    average_result = get_average_value(ser_iter_returns, ser_iter_weights)\n",
    "                    ser_average.loc[iter_country, iter_date] = average_result\n",
    "#                if ((iter_country == 'EG') & (iter_date == pd.Timestamp('2011-10-31'))):\n",
    "#                    print(iter_country, '/', iter_date, '/', average_result)\n",
    "#                    print(ser_iter_returns.size)                    \n",
    "#                    print(ser_iter_returns.count())\n",
    "#                    print(ser_iter_returns.sum())                    \n",
    "#                    print(ser_iter_returns.min())\n",
    "#                    print(ser_iter_returns.max())\n",
    "#                    print(ser_iter_returns.mean())\n",
    "#                    print(ser_iter_returns.head())\n",
    "#                    print(ser_iter_returns.tail())                     \n",
    "#                    break\n",
    "    ser_average.sort_index(level = [0, 1], inplace = True)\n",
    "    \n",
    "    return ser_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL VOLATILITY SERIES BUILDER\n",
    "def get_expvol_series_old(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Flattening MSCI changes by logarythm\n",
    "    ### Need to move flattening logic outside of the function !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    ser_returns = np.log(1 + ser_returns)\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)\n",
    "    ### Main loop performing:\n",
    "    ser_expvol = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.mean()\n",
    "                if (ser_iter_returns.size > 0):\n",
    "                    if (ser_condition.count() > 0):\n",
    "                        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                    else:\n",
    "                        ser_iter_condition = pd.Series(np.NaN)                     \n",
    "                    ser_iter_weights = bind_exp_weights(ser_iter_returns, weighting_kind, window_years, halflife_months, ser_iter_condition)                           \n",
    "                ser_iter_returns.dropna(inplace = True)     \n",
    "                ### Change loc to iloc !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                \n",
    "                if (ser_iter_returns.count() > num_year_work_days // 2):\n",
    "                    ### Need to have minimum data count parameter !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                    expvol_result = get_expvol_value(ser_iter_returns, ser_iter_weights)\n",
    "                    ser_expvol.loc[iter_country, iter_date] = expvol_result\n",
    "                    \n",
    "    return ser_expvol"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
