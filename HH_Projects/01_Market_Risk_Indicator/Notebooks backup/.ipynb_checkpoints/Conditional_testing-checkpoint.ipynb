{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACTING PREVIOUSLY SAVED DATA FROM HDF5 FILE\n",
    "### Declaring constants:\n",
    "path_market_risk_source_hdf = 'Data_Files/Source_Files/market_risk_source.h5'\n",
    "market_membership_key = 'market_membership_key'\n",
    "realized_ret_USD_key = 'realized_ret_USD_key'\n",
    "realized_ret_LOC_key = 'realized_ret_LOC_key'\n",
    "index_ret_USD_key = 'index_ret_USD_key'\n",
    "index_ret_LOC_key = 'index_ret_LOC_key'\n",
    "ivol3m_key = 'ivol3m_key'\n",
    "vrp3m_key = 'vrp3m_key'\n",
    "path_gri_index_hdf = 'Data_Files/Source_Files/gri_released_index.h5'\n",
    "gri_released_key = 'gri_released_key'\n",
    "gri_start_key = 'gri_start_key'\n",
    "gri_level_perc_key = 'gri_level_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "gri_momentum_perc_key = 'gri_momentum_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "### Exporting data from hdf5 fixed formatted files:\n",
    "ser_market_membership = pd.read_hdf(path_market_risk_source_hdf, market_membership_key)\n",
    "ser_realized_ret_USD = pd.read_hdf(path_market_risk_source_hdf, realized_ret_USD_key)\n",
    "ser_realized_ret_LOC = pd.read_hdf(path_market_risk_source_hdf, realized_ret_LOC_key)\n",
    "ser_index_ret_USD = pd.read_hdf(path_market_risk_source_hdf, index_ret_USD_key)\n",
    "ser_index_ret_LOC = pd.read_hdf(path_market_risk_source_hdf, index_ret_LOC_key)\n",
    "ser_ivol3m = pd.read_hdf(path_market_risk_source_hdf, ivol3m_key)\n",
    "ser_vrp3m = pd.read_hdf(path_market_risk_source_hdf, vrp3m_key)\n",
    "ser_gri_released = pd.read_hdf(path_market_risk_source_hdf, gri_released_key)\n",
    "ser_gri_start_col = pd.read_hdf(path_market_risk_source_hdf, gri_start_key)\n",
    "ser_gri_level_perc = pd.read_hdf(path_market_risk_source_hdf, gri_level_perc_key) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_gri_momentum_perc = pd.read_hdf(path_market_risk_source_hdf, gri_momentum_perc_key) ### ADDED FOR BETA FACTORS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACTING UNIVERSE DATA FOR A PARTICULAR DATE\n",
    "def get_date_membership(iter_date):\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Preparing data for universe filtering:\n",
    "    if (pd.to_datetime(iter_date) == pd.to_datetime(iter_date - pd.offsets.BusinessMonthEnd(0))):\n",
    "        iter_month_end = iter_date\n",
    "    else: \n",
    "        iter_month_end = pd.to_datetime(iter_date - pd.offsets.BusinessMonthEnd(1))    \n",
    "#    ser_iter_membership = pd.read_hdf(path_market_risk_source_hdf, market_membership_key, where = 'Date = iter_month_end')\n",
    "    ser_iter_membership = ser_market_membership.loc[:, iter_month_end : iter_month_end]\n",
    "    ser_iter_membership.rename(index = {iter_month_end : iter_date}, inplace = True)\n",
    "    \n",
    "    return ser_iter_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHTS GENERATOR\n",
    "def get_exp_weights(window_years = 5, halflife_months = 3):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Array of regressioon window day numbers descending:\n",
    "    arr_weight_days = np.arange(num_year_work_days * window_years, 0, -1) - 1\n",
    "    ### Creating weights series:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round((num_year_work_days / num_year_months * halflife_months)))\n",
    "    arr_weights = np.exp(math.log(num_period_factor) * arr_weight_days)\n",
    "    ser_weights = pd.Series(arr_weights)        \n",
    "    ser_weights.name = 'Weight'\n",
    "    \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTS TO SERIES BINDER\n",
    "def bind_exp_weights(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Creating weights series:\n",
    "    if (weighting_kind == 'equal'):\n",
    "        ser_weights = pd.Series(1, index = ser_returns.index)\n",
    "    if (weighting_kind == 'expo'):       \n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]\n",
    "        ser_weights.index = ser_returns.index\n",
    "    if (weighting_kind == 'expo_cond_old'): ### BEFORE UDATING\n",
    "        ser_condition = abs(ser_condition - ser_condition.iloc[-1])\n",
    "        ser_condition = ser_condition.sort_values(ascending = False)\n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]        \n",
    "        ser_weights = pd.Series(ser_weights.values, ser_condition.index)\n",
    "        ser_weights.sort_index(inplace = True)\n",
    "        ser_weights.name = 'Weight'\n",
    "    if (weighting_kind == 'expo_cond'): ### AFTER UDATING\n",
    "            index_sort = ser_returns.dropna().index\n",
    "            ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]\n",
    "            ser_weights.index = ser_returns.index\n",
    "            ser_cond_weights = pd.Series(np.NaN, ser_returns.index)\n",
    "            df_to_sort = pd.concat([ser_returns, ser_condition], axis = 1)\n",
    "            df_to_sort['Number'] = np.arange(ser_returns.size)\n",
    "            df_to_sort.set_index('Number', inplace = True, drop = False)\n",
    "            df_to_sort.dropna(inplace = True)\n",
    "            if (df_to_sort['GRI'].count() > 0):\n",
    "                df_to_sort['GRI'] = abs(df_to_sort['GRI'] - df_to_sort['GRI'].iloc[-1])\n",
    "                df_to_sort.sort_values('GRI', inplace = True, ascending = False)\n",
    "                df_to_sort['Date'] = index_sort.values\n",
    "                df_to_sort.sort_index(inplace = True)           \n",
    "                df_to_sort.set_index('Date', inplace = True)\n",
    "                for (iter_weight_date, iter_weight_row) in df_to_sort.iterrows():\n",
    "                    ser_cond_weights.iloc[int(iter_weight_row['Number'])] = ser_weights.loc[iter_weight_date]\n",
    "        \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL VOLATILITY CALCULATOR\n",
    "def get_expvol_value(ser_returns, ser_weights):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Exponential volatility calculating:\n",
    "    expvol_result = np.NaN\n",
    "    ser_returns = ser_returns.dropna()\n",
    "    index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "    ### Exponential volatility calculating:\n",
    "    expvol_y = ser_returns[index_rolling]\n",
    "    expvol_w = ser_weights[index_rolling]             \n",
    "    expvol_w = expvol_w / expvol_w.sum()\n",
    "    expvol_result = np.sqrt(expvol_w.dot(expvol_y * expvol_y))\n",
    "        \n",
    "    return expvol_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL VOLATILITY SERIES BUILDER\n",
    "def get_expvol_series(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    ### Flattening MSCI changes by logarythm\n",
    "    ser_returns = np.log(1 + ser_returns)\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)\n",
    "    ### Main loop performing:\n",
    "    ser_expvol = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "                ser_iter_returns.name = 'Returns'\n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.mean()\n",
    "                if (ser_iter_returns.size > 0):\n",
    "                    if (ser_condition.count() > 0):\n",
    "                        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                    else:\n",
    "                        ser_iter_condition = pd.Series(np.NaN)                     \n",
    "                    ser_iter_weights = bind_exp_weights(ser_iter_returns, weighting_kind, window_years, halflife_months, ser_iter_condition) ## CHANGES: ADDED       \n",
    "                ser_iter_returns.dropna(inplace = True)\n",
    "                if (ser_iter_returns.count() > num_year_work_days // 2):\n",
    "                    expvol_result = get_expvol_value(ser_iter_returns, ser_iter_weights) * np.sqrt(num_year_work_days)\n",
    "                    ser_expvol.loc[iter_country, iter_date] = expvol_result\n",
    "    return ser_expvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING \n",
    "### GET VOLATILITY SURPRISE FACTOR\n",
    "def get_conditional_vol_factor(iter_date):\n",
    "    ### Importing standard modules and date-special modules (common for all factors):\n",
    "    import numpy as np\n",
    "    import pandas as pd   \n",
    "    from datetime import date\n",
    "    ### Global parameters declaring:\n",
    "    date_start = date(1993, 12, 31)     \n",
    "    ### Constants declaring (common for all factors):  \n",
    "    num_year_work_days = 260\n",
    "    window_years = 5        \n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)  \n",
    "    ser_iter_returns = ser_realized_ret_LOC.loc[:, index_iter_date]   \n",
    "    ### Removing effectively missing return observations    \n",
    "    ser_iter_returns.replace(0, np.nan, inplace = True)    \n",
    "    ser_gri_added = pd.concat([ser_gri_start_col[date_start].loc[ : date_start - pd.offsets.BusinessDay()], ser_gri_released])\n",
    "    ser_iter_condition = ser_gri_added[index_iter_date] \n",
    "    ### Factor calculation:\n",
    "    ser_expvol1m_cond = get_expvol_series(ser_iter_membership, ser_iter_returns, 'expo_cond', window_years = window_years, halflife_months = 1, \n",
    "                                          ser_condition = ser_iter_condition)\n",
    "    ser_iter_factor = ser_expvol1m_cond\n",
    "    \n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING \n",
    "\n",
    "### LOOPER FOR CONDITIONAL VOLATILITY FACTOR\n",
    "#date_range_test = pd.date_range(start = '2010-10-25', periods = 6)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 157]\n",
    "arr_conditional_vol_factor = []\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    arr_conditional_vol_factor.append(get_conditional_vol_factor(iter_date))\n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_conditional_vol_factor = pd.concat(arr_conditional_vol_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_conditional_vol_factor - AR 29-Dec-2006: 0.2107838560995583\n",
      "ser_conditional_vol_factor - US 29-Dec-2006: 0.0817462646666443\n",
      "ser_conditional_vol_factor - cross-sectional mean min: 0.12735794074386794\n",
      "ser_conditional_vol_factor - cross-sectional mean mean: 0.2140489750963062\n",
      "ser_conditional_vol_factor - cross-sectional mean max: 0.6762556480844379\n",
      "ser_conditional_vol_factor - cross-sectional mean stdev: 0.07974120939017841\n",
      "ser_conditional_vol_factor - cross-sectional mean count: 234\n"
     ]
    }
   ],
   "source": [
    "### TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING \n",
    "\n",
    "### CONDITIONAL VOLATILITY FACTOR TESTING:\n",
    "print('ser_conditional_vol_factor - AR 29-Dec-2006:', ser_conditional_vol_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_conditional_vol_factor - US 29-Dec-2006:', ser_conditional_vol_factor.loc['US' , '2006-12-29'])\n",
    "ser_conditional_vol_factor_mean = pd.Series(np.NaN, index = ser_conditional_vol_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_conditional_vol_factor_mean.index:  \n",
    "    ser_conditional_vol_factor_mean[iter_date] = ser_conditional_vol_factor.loc[:, iter_date].mean()\n",
    "ser_conditional_vol_factor_mean.sort_index(inplace = True)\n",
    "print('ser_conditional_vol_factor - cross-sectional mean min:', ser_conditional_vol_factor_mean.min())\n",
    "print('ser_conditional_vol_factor - cross-sectional mean mean:', ser_conditional_vol_factor_mean.mean())\n",
    "print('ser_conditional_vol_factor - cross-sectional mean max:', ser_conditional_vol_factor_mean.max())\n",
    "print('ser_conditional_vol_factor - cross-sectional mean stdev:', ser_conditional_vol_factor_mean.std())\n",
    "print('ser_conditional_vol_factor - cross-sectional mean count:', ser_conditional_vol_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002-02-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "### TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING \n",
    "### Importing standard modules and date-special modules (common for all factors):\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "from datetime import date\n",
    "### Constants declaring:\n",
    "date_start = date(1993, 12, 31)\n",
    "num_year_work_days = 260\n",
    "window_years = 5  \n",
    "### Testing:\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "iter_date = date(2002, 2, 28)\n",
    "date_range_test = pd.date_range(end = iter_date, periods = 1, freq = 'B')\n",
    "ser_result = pd.Series(np.NaN, index = date_range_test)\n",
    "for iter_date in date_range_test:\n",
    "    print(iter_date)\n",
    "#get_conditional_vol_factor(iter_date).loc['US']\n",
    "### ENCODING:\n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)  \n",
    "    ser_iter_returns = ser_realized_ret_LOC.loc[:, index_iter_date]   \n",
    "    ### Removing effectively missing return observations    \n",
    "    ser_iter_returns.replace(0, np.nan, inplace = True)    \n",
    "    ser_gri_added = pd.concat([ser_gri_start_col[date_start].loc[ : date_start - pd.offsets.BusinessDay()], ser_gri_released])\n",
    "    ser_iter_condition = ser_gri_added[index_iter_date] \n",
    "\n",
    "    ### Factor calculation:\n",
    "    #get_expvol_series(ser_iter_membership, ser_iter_returns, 'expo_cond', window_years = window_years, halflife_months = 1, ser_condition = ser_iter_condition).loc['US']\n",
    "    ### ENCODING:\n",
    "    ### Flattening MSCI changes by logarythm\n",
    "    ser_returns = ser_iter_returns.copy()\n",
    "    ser_condition = ser_iter_condition.copy()\n",
    "    weighting_kind = 'expo_cond'\n",
    "    halflife_months = 1\n",
    "    ser_returns = np.log(1 + ser_returns)\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)\n",
    "    ### Main loop performing:\n",
    "    iter_country = 'US'\n",
    "    ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "    ser_iter_returns = ser_iter_returns - ser_iter_returns.mean()\n",
    "    if (ser_iter_returns.size > 0):\n",
    "        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "\n",
    "        if (ser_iter_condition.count() > 0):\n",
    "            \n",
    "            \n",
    "            ser_iter_returns = ser_iter_returns.reset_index(drop = True)\n",
    "            ser_iter_condition = ser_iter_condition.reset_index(drop = True)\n",
    "            ser_iter_weights = get_exp_weights(window_years, halflife_months)[- ser_iter_returns.size : ]\n",
    "            ser_iter_weights.index = ser_iter_returns.index\n",
    "\n",
    "            ser_iter_returns.dropna(inplace = True)\n",
    "            ser_iter_condition = ser_iter_condition[ser_iter_returns.index]\n",
    "            ser_iter_weights = ser_iter_weights[ser_iter_returns.index]\n",
    "            ser_iter_returns = ser_iter_returns.reset_index(drop = True)\n",
    "            if (ser_iter_condition.count() > 0):\n",
    "                ser_iter_returns.name = 'Returns'\n",
    "                ser_iter_condition = ser_iter_condition.reset_index(drop = True)\n",
    "                ser_iter_weights = ser_iter_weights.reset_index(drop = True)\n",
    "                ser_iter_condition = abs(ser_iter_condition - ser_iter_condition.iloc[-1])\n",
    "                df_sorting = pd.concat([ser_iter_condition, ser_iter_returns], axis = 1)\n",
    "                df_sorting.sort_values('GRI', inplace = True, ascending = False)\n",
    "                ser_iter_returns = df_sorting['Returns']\n",
    "                ser_iter_returns = ser_iter_returns.reset_index(drop = True)\n",
    "\n",
    "                expvol_y = ser_iter_returns\n",
    "                expvol_w = ser_iter_weights                   \n",
    "                expvol_w = expvol_w / expvol_w.sum()\n",
    "                expvol_result = np.sqrt(expvol_w.dot(expvol_y * expvol_y)) \n",
    "\n",
    "                expvol_result = expvol_result * np.sqrt(num_year_work_days)\n",
    "                ser_result[iter_date] = expvol_result\n",
    "                \n",
    "        else:\n",
    "            ser_iter_condition = pd.Series(np.NaN)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2002-02-28    0.175407\n",
       "Freq: B, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994-12-30 00:00:00\n",
      "1995-12-29 00:00:00\n",
      "1996-12-31 00:00:00\n",
      "1997-12-31 00:00:00\n",
      "1998-12-31 00:00:00\n",
      "1999-12-31 00:00:00\n",
      "2000-12-29 00:00:00\n",
      "2001-12-31 00:00:00\n",
      "2002-12-31 00:00:00\n",
      "2003-12-31 00:00:00\n",
      "2004-12-31 00:00:00\n",
      "2005-12-30 00:00:00\n",
      "2006-12-29 00:00:00\n",
      "2007-12-31 00:00:00\n",
      "2008-12-31 00:00:00\n",
      "2009-12-31 00:00:00\n",
      "2010-12-31 00:00:00\n",
      "2011-12-30 00:00:00\n",
      "2012-12-31 00:00:00\n",
      "2013-12-31 00:00:00\n",
      "2014-12-31 00:00:00\n",
      "2015-12-31 00:00:00\n",
      "2016-12-30 00:00:00\n",
      "2017-12-29 00:00:00\n",
      "2018-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "### TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING \n",
    "### Importing standard modules and date-special modules (common for all factors):\n",
    "import numpy as np\n",
    "import pandas as pd   \n",
    "from datetime import date\n",
    "### Constants declaring:\n",
    "date_start = date(1993, 12, 31)\n",
    "num_year_work_days = 260\n",
    "window_years = 5  \n",
    "### Testing:\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#iter_date = date(2002, 2, 28)\n",
    "#date_range_test = pd.date_range(end = iter_date, periods = 1, freq = 'B')\n",
    "ser_result = pd.Series(np.NaN, index = date_range_test)\n",
    "for iter_date in date_range_test:\n",
    "    if iter_date.month == 12:\n",
    "        print(iter_date)\n",
    "#get_conditional_vol_factor(iter_date).loc['US']\n",
    "### ENCODING:\n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)  \n",
    "    ser_iter_returns = ser_realized_ret_LOC.loc[:, index_iter_date]   \n",
    "    ### Removing effectively missing return observations    \n",
    "    ser_iter_returns.replace(0, np.nan, inplace = True)    \n",
    "    ser_gri_added = pd.concat([ser_gri_start_col[date_start].loc[ : date_start - pd.offsets.BusinessDay()], ser_gri_released])\n",
    "    ser_iter_condition = ser_gri_added[index_iter_date] \n",
    "\n",
    "    ### Factor calculation:\n",
    "    #get_expvol_series(ser_iter_membership, ser_iter_returns, 'expo_cond', window_years = window_years, halflife_months = 1, ser_condition = ser_iter_condition).loc['US']\n",
    "    ### ENCODING:\n",
    "    ### Flattening MSCI changes by logarythm\n",
    "    ser_returns = ser_iter_returns.copy()\n",
    "    ser_condition = ser_iter_condition.copy()\n",
    "    weighting_kind = 'expo_cond'\n",
    "    halflife_months = 1\n",
    "    ser_returns = np.log(1 + ser_returns)\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)\n",
    "    ### Main loop performing:\n",
    "    iter_country = 'US'\n",
    "    ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "    ser_iter_returns.name = 'Returns'\n",
    "    ser_iter_returns = ser_iter_returns - ser_iter_returns.mean()\n",
    "    if (ser_iter_returns.size > 0):\n",
    "        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "\n",
    "        if (ser_iter_condition.count() > 0):\n",
    "\n",
    "            index_iter_sort = ser_iter_returns.dropna().index\n",
    "            ser_iter_weights = get_exp_weights(window_years, halflife_months)[- ser_iter_returns.size : ]\n",
    "            ser_iter_weights.index = ser_iter_returns.index\n",
    "            ser_cond_weights = pd.Series(np.NaN, ser_iter_returns.index)\n",
    "            df_to_sort = pd.concat([ser_iter_returns, ser_iter_condition], axis = 1)\n",
    "            df_to_sort['Number'] = np.arange(ser_iter_returns.size) #\n",
    "##            df_to_sort = df_to_sort.reset_index() ##\n",
    "            df_to_sort.set_index('Number', inplace = True, drop = False) #\n",
    "            df_to_sort.dropna(inplace = True)\n",
    "            if (df_to_sort['GRI'].count() > 0):\n",
    "                df_to_sort['GRI'] = abs(df_to_sort['GRI'] - df_to_sort['GRI'].iloc[-1])\n",
    "                df_to_sort.sort_values('GRI', inplace = True, ascending = False)\n",
    "                df_to_sort['Date'] = index_iter_sort.values\n",
    "                df_to_sort.sort_index(inplace = True)           \n",
    "                df_to_sort.set_index('Date', inplace = True) #          \n",
    "                df_to_sort['Number'] = df_to_sort['Number'].astype(np.int16)\n",
    "\n",
    "                for (iter_weight_date, iter_weight_row) in df_to_sort.iterrows():\n",
    "                    ser_cond_weights.iloc[int(iter_weight_row['Number'])] = ser_iter_weights.loc[iter_weight_date]\n",
    "\n",
    "                ser_iter_returns = ser_iter_returns.dropna()\n",
    "                index_rolling = ser_iter_returns.index.intersection(ser_cond_weights.index)           \n",
    "                ### Exponential volatility calculating:\n",
    "                expvol_y = ser_iter_returns[index_rolling]\n",
    "                expvol_w = ser_cond_weights[index_rolling]             \n",
    "                expvol_w = expvol_w / expvol_w.sum()\n",
    "                expvol_result = np.sqrt(expvol_w.dot(expvol_y * expvol_y))\n",
    "\n",
    "                expvol_result = expvol_result * np.sqrt(num_year_work_days)\n",
    "                ser_result[iter_date] = expvol_result\n",
    "\n",
    "###            df_to_sort.columns = df_to_sort.columns + '_sort'\n",
    "            \n",
    "###            df_to_rew = pd.concat([ser_iter_returns, ser_iter_weights], axis = 1)\n",
    "##            df_to_rew = df_to_rew.reset_index() ##\n",
    "###            df_to_rew['Number'] = np.arange(ser_iter_returns.size) #\n",
    "##            df_to_rew.set_index('Date', inplace = True) ##\n",
    "###            df_to_rew.loc[df_to_rew['Returns'].isna(), 'Weight'] = 0\n",
    "###            df_to_rew.columns = df_to_rew.columns + '_rew'\n",
    "            \n",
    "                \n",
    "        else:\n",
    "            ser_iter_condition = pd.Series(np.NaN)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_result.to_excel('Data_Files/Test_Files/conditional_testing_plus.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Returns</th>\n",
       "      <th>GRI</th>\n",
       "      <th>Number</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-02-15</th>\n",
       "      <td>-0.012166</td>\n",
       "      <td>0.252743</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>-0.012166</td>\n",
       "      <td>0.753098</td>\n",
       "      <td>1290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-19</th>\n",
       "      <td>0.008513</td>\n",
       "      <td>0.199892</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>-0.018764</td>\n",
       "      <td>0.802080</td>\n",
       "      <td>1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-20</th>\n",
       "      <td>-0.015544</td>\n",
       "      <td>0.197225</td>\n",
       "      <td>1294.0</td>\n",
       "      <td>0.013783</td>\n",
       "      <td>0.827753</td>\n",
       "      <td>1293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-21</th>\n",
       "      <td>-0.018764</td>\n",
       "      <td>0.196251</td>\n",
       "      <td>1292.0</td>\n",
       "      <td>-0.015544</td>\n",
       "      <td>0.854248</td>\n",
       "      <td>1294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-22</th>\n",
       "      <td>0.013783</td>\n",
       "      <td>0.195641</td>\n",
       "      <td>1293.0</td>\n",
       "      <td>0.008513</td>\n",
       "      <td>0.881591</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-25</th>\n",
       "      <td>0.017975</td>\n",
       "      <td>0.183550</td>\n",
       "      <td>1296.0</td>\n",
       "      <td>0.017975</td>\n",
       "      <td>0.909809</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-26</th>\n",
       "      <td>-0.000155</td>\n",
       "      <td>0.123948</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>0.938931</td>\n",
       "      <td>1297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-27</th>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.068722</td>\n",
       "      <td>1298.0</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.968984</td>\n",
       "      <td>1298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-02-28</th>\n",
       "      <td>-0.002633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1299.0</td>\n",
       "      <td>-0.002633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Returns       GRI  Number   Returns    Weight  Number\n",
       "Date                                                              \n",
       "2002-02-15 -0.012166  0.252743  1290.0 -0.012166  0.753098    1290\n",
       "2002-02-18       NaN       NaN     NaN       NaN  0.000000    1291\n",
       "2002-02-19  0.008513  0.199892  1295.0 -0.018764  0.802080    1292\n",
       "2002-02-20 -0.015544  0.197225  1294.0  0.013783  0.827753    1293\n",
       "2002-02-21 -0.018764  0.196251  1292.0 -0.015544  0.854248    1294\n",
       "2002-02-22  0.013783  0.195641  1293.0  0.008513  0.881591    1295\n",
       "2002-02-25  0.017975  0.183550  1296.0  0.017975  0.909809    1296\n",
       "2002-02-26 -0.000155  0.123948  1297.0 -0.000155  0.938931    1297\n",
       "2002-02-27  0.000774  0.068722  1298.0  0.000774  0.968984    1298\n",
       "2002-02-28 -0.002633  0.000000  1299.0 -0.002633  1.000000    1299"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_common = pd.concat([df_to_sort, df_to_rew], axis = 1)\n",
    "df_common.tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
