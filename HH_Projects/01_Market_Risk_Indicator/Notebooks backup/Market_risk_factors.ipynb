{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS NOTEBOOK IS PREPARING FACTOR DATA VECTORS FOR MARKET RISK THEME (GLOBAL COUNTRY MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STANDART MODULES INITIALISING\n",
    "\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXTRACTING MSCI TOTAL RETURNS INDEX DATA FROM XLSX FILE (NOT NEEDED TO IMPLEMENT)\n",
    "\n",
    "### Constants declaring (NOT NEEDED TO IMPLEMENT):\n",
    "path_bloomberg_source = 'Data_Files/Source_Files/msci_data_bloomberg.xlsx'\n",
    "tab_USD = 'MSCI_NET_DAILY_TR_USD'\n",
    "tab_LOC = 'MSCI_NET_DAILY_TR_LOC'\n",
    "path_msci_returns_hdf = 'Data_Files/Source_Files/msci_returns.h5'\n",
    "key_total_ret_ind_USD = 'key_total_ret_ind_USD'\n",
    "key_total_ret_ind_LOC = 'key_total_ret_ind_LOC'\n",
    "key_realized_ret_USD = 'key_realized_ret_USD'\n",
    "key_realized_ret_LOC = 'key_realized_ret_LOC'\n",
    "### Reading fil data (NOT NEEDED TO IMPLEMENT):\n",
    "df_source_USD = pd.read_excel(io = path_bloomberg_source, sheet_name = tab_USD, header = 6)[3 : ]\n",
    "df_source_LOC = pd.read_excel(io = path_bloomberg_source, sheet_name = tab_LOC, header = 6)[3 : ]\n",
    "### Converting data for suitable format (NOT NEEDED TO IMPLEMENT):\n",
    "df_source_USD.set_index('Country Code', inplace = True)\n",
    "ser_total_ret_ind_USD = df_source_USD.stack()\n",
    "ser_total_ret_ind_USD.name = 'Tot Ret Ind USD'\n",
    "ser_total_ret_ind_USD.index.names = ['Date', 'Code']\n",
    "ser_total_ret_ind_USD = ser_total_ret_ind_USD.swaplevel()\n",
    "ser_total_ret_ind_USD.sort_index(inplace = True)\n",
    "ser_realized_ret_USD = pd.Series(np.NaN, index = ser_total_ret_ind_USD.index)\n",
    "for iter_country in ser_realized_ret_USD.index.get_level_values(level = 0).unique():\n",
    "    ser_realized_ret_USD[iter_country] = (ser_total_ret_ind_USD[iter_country] / ser_total_ret_ind_USD[iter_country].shift(1) - 1)\n",
    "ser_realized_ret_USD.name = 'Ret USD'\n",
    "df_source_LOC.set_index('Country Code', inplace = True)\n",
    "ser_total_ret_ind_LOC = df_source_LOC.stack()\n",
    "ser_total_ret_ind_LOC.name = 'Tot Ret Ind LOC'\n",
    "ser_total_ret_ind_LOC.index.names = ['Date', 'Code']\n",
    "ser_total_ret_ind_LOC = ser_total_ret_ind_LOC.swaplevel()\n",
    "ser_total_ret_ind_LOC.sort_index(inplace = True)\n",
    "ser_realized_ret_LOC = pd.Series(np.NaN, index = ser_total_ret_ind_LOC.index)\n",
    "for iter_country in ser_realized_ret_LOC.index.get_level_values(level = 0).unique():\n",
    "    ser_realized_ret_LOC[iter_country] = (ser_total_ret_ind_LOC[iter_country] / ser_total_ret_ind_LOC[iter_country].shift(1) - 1)\n",
    "ser_realized_ret_LOC.name = 'Ret LOC'\n",
    "### Saving results (NOT NEEDED TO IMPLEMENT):\n",
    "ser_total_ret_ind_USD = ser_total_ret_ind_USD.astype('float')\n",
    "ser_total_ret_ind_LOC = ser_total_ret_ind_LOC.astype('float')\n",
    "ser_realized_ret_USD = ser_realized_ret_USD.astype('float')\n",
    "ser_realized_ret_LOC = ser_realized_ret_LOC.astype('float')\n",
    "ser_total_ret_ind_USD.to_hdf(path_msci_returns_hdf, key_total_ret_ind_USD, mode = 'w', format = 'fixed')\n",
    "ser_total_ret_ind_LOC.to_hdf(path_msci_returns_hdf, key_total_ret_ind_LOC, mode = 'a', format = 'fixed')\n",
    "ser_realized_ret_USD.to_hdf(path_msci_returns_hdf, key_realized_ret_USD, mode = 'a', format = 'fixed')\n",
    "ser_realized_ret_LOC.to_hdf(path_msci_returns_hdf, key_realized_ret_LOC, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREPARING RETURNS DATA (NOT NEEDED TO IMPLEMENT)\n",
    "### Constanst declaring (NOT NEEDED TO IMPLEMENT):\n",
    "path_msci_returns_hdf = 'Data_Files/Source_Files/msci_returns.h5'\n",
    "key_total_ret_ind_USD = 'key_total_ret_ind_USD'\n",
    "key_total_ret_ind_LOC = 'key_total_ret_ind_LOC'\n",
    "key_realized_ret_USD = 'key_realized_ret_USD'\n",
    "key_realized_ret_LOC = 'key_realized_ret_LOC'\n",
    "### Extracting MSCI returns data from file (NOT NEEDED TO IMPLEMENT)\n",
    "ser_total_ret_ind_USD = pd.read_hdf(path_msci_returns_hdf, key_total_ret_ind_USD)\n",
    "ser_total_ret_ind_LOC = pd.read_hdf(path_msci_returns_hdf, key_total_ret_ind_LOC)\n",
    "ser_realized_ret_USD = pd.read_hdf(path_msci_returns_hdf, key_realized_ret_USD)\n",
    "ser_realized_ret_LOC = pd.read_hdf(path_msci_returns_hdf, key_realized_ret_LOC)\n",
    "### Checking control values (NOT NEEDED TO IMPLEMENT):\n",
    "#print('LOC TOTAL:', ser_total_ret_ind_LOC.count(),'/' ,ser_total_ret_ind_LOC.sum())\n",
    "#print('USD TOTAL:', ser_total_ret_ind_USD.count(),'/' ,ser_total_ret_ind_USD.sum())\n",
    "#print('LOC RET:', ser_realized_ret_LOC.count(),'/ US SUM:' ,ser_realized_ret_LOC.loc['US', :].sum(), '/ QA SUM:',ser_realized_ret_LOC.loc['QA', :].sum())\n",
    "#print('USD RET:', ser_realized_ret_USD.count(),'/ US SUM:' ,ser_realized_ret_USD.loc['US', :].sum(), '/QA SUM:',ser_realized_ret_USD.loc['QA', :].sum())\n",
    "### Tested successfully:\n",
    "#LOC TOTAL: 216617 / 485451238.28700006\n",
    "#USD TOTAL: 265221 / 761735894.4570001\n",
    "#LOC RET: 216567 / US SUM: 1.5319008807330472 / QA SUM: 0.8297622443894299\n",
    "#USD RET: 265171 / US SUM: 1.5319023941832968 /QA SUM: 0.08727786207278843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACTING MSCI MEMBERSHIP DATA (NOT NEEDED TO IMPLEMENT)\n",
    "arr_markets_needed = ['DM', 'FM', 'EM']\n",
    "path_msci_membership_hdf = 'Data_Files/Source_Files/msci_membership.h5'\n",
    "key_date_membership = 'key_date_membership'\n",
    "ser_date_membership = pd.read_hdf(path_msci_membership_hdf, key_date_membership)\n",
    "ser_market_membership = ser_date_membership[ser_date_membership.isin(arr_markets_needed)].swaplevel().sort_index(level = [0, 1])\n",
    "index_market = ser_market_membership.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL WEIGHTS GENERATOR\n",
    "def get_exp_weights(window_years = 5, halflife_months = 3):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Array of regressioon window day numbers descending:\n",
    "    arr_weight_days = np.arange(num_year_work_days * window_years + 1, 0, -1)\n",
    "    ### Creating weights series:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round((num_year_work_days / num_year_months * halflife_months)))\n",
    "    arr_weights = np.exp(math.log(num_period_factor) * arr_weight_days)\n",
    "    ser_weights = pd.Series(arr_weights)        \n",
    "    ser_weights.name = 'Weight'\n",
    "    \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTS TO SERIES BINDER\n",
    "def bind_exp_weights(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Creating weights series:\n",
    "    if (weighting_kind == 'equal'):\n",
    "        ser_weights = pd.Series(1, index = ser_returns.index)\n",
    "    if (weighting_kind == 'expo'):       \n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.count() : ]\n",
    "        ser_weights.index = ser_returns.index\n",
    "    if (weighting_kind == 'expo_cond'):\n",
    "        ser_condition = abs(ser_condition - ser_condition.iloc[-1])\n",
    "        ser_condition = ser_condition.sort_values(ascending = False)\n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_condition.count() : ]\n",
    "        ser_weights = pd.Series(ser_weights.values, ser_condition.index)\n",
    "        ser_weights.sort_index(inplace = True)\n",
    "        ser_weights.name = 'Weight'\n",
    "        \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL VOLATILITY CALCULATOR\n",
    "def get_expvol_value(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Exponential volatility calculating:\n",
    "    expvol_result = np.NaN\n",
    "    ser_returns = ser_returns.dropna()\n",
    "    if (ser_returns.count() >= num_year_work_days * window_years // 2):\n",
    "        ser_weights = bind_exp_weights(ser_returns, weighting_kind, window_years, halflife_months, ser_condition)\n",
    "        index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "        ### Exponential volatility calculating:\n",
    "        expvol_y = ser_returns[index_rolling]\n",
    "        expvol_w = ser_weights[index_rolling]             \n",
    "        expvol_w = expvol_w / expvol_w.sum()\n",
    "        expvol_result = np.sqrt(expvol_w.dot(expvol_y * expvol_y)) * np.sqrt(num_year_work_days)\n",
    "        \n",
    "    return expvol_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL VOLATILITY SERIES BUILDER\n",
    "def get_expvol_series(index_market, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Flattening MSCI changes by logarythm\n",
    "    ser_returns = np.log(1 + ser_returns)\n",
    "    ### Main loop performing:\n",
    "    ser_expvol = pd.Series(np.NaN, index = index_market)\n",
    "    for iter_country in index_market.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in index_market.get_level_values(1).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years) : iter_date].dropna()          \n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.mean()\n",
    "                if (ser_iter_returns.count() >= num_year_work_days * window_years // 2):\n",
    "                    if (ser_condition.count() > 0):\n",
    "                        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                        ser_iter_condition = ser_iter_condition - ser_iter_condition.mean()\n",
    "                    else:\n",
    "                        ser_iter_condition = pd.Series(np.NaN)\n",
    "                    expvol_result = get_expvol_value(ser_iter_returns, weighting_kind, window_years, halflife_months, ser_iter_condition)\n",
    "                    ser_expvol.loc[iter_country, iter_date] = expvol_result\n",
    "    ser_expvol.sort_index(level = [0, 1], inplace = True)\n",
    "    \n",
    "    return ser_expvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating expvol as base series for factor (NOT NEEDED TO IMPLEMENT):\n",
    "import tables\n",
    "tables.file._open_files.close_all()\n",
    "ser_expvol1m = get_expvol_series(index_market, ser_realized_ret_LOC, weighting_kind = 'expo', window_years = 5, halflife_months = 1) \n",
    "ser_expvol24m = get_expvol_series(index_market, ser_realized_ret_LOC, weighting_kind = 'expo', window_years = 5, halflife_months = 24) \n",
    "ser_fake_GRI = ser_total_ret_ind_LOC['AU']\n",
    "ser_expvol1m_cond = get_expvol_series(index_market, ser_realized_ret_LOC, weighting_kind = 'expo_cond', window_years = 5, halflife_months = 1,\n",
    "                                             ser_condition = ser_fake_GRI)\n",
    "path_msci_expvol_hdf = 'Data_Files/Source_Files/expvol_for_factors.h5'\n",
    "key_expvol1m_LOC = 'key_expvol1m_LOC'\n",
    "key_expvol24m_LOC = 'key_expvol24m_LOC'\n",
    "key_expvol1m_cond_LOC = 'key_expvol1m_cond_LOC'\n",
    "ser_expvol1m.to_hdf(path_msci_expvol_hdf, key_expvol1m_LOC, mode = 'w', format = 'fixed')\n",
    "ser_expvol24m.to_hdf(path_msci_expvol_hdf, key_expvol24m_LOC, mode = 'a', format = 'fixed')\n",
    "ser_expvol1m_cond.to_hdf(path_msci_expvol_hdf, key_expvol1m_cond_LOC, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTED AVERAGE CALCULATOR\n",
    "def get_average_value(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Rolling average calculating:\n",
    "    ser_returns = ser_returns.dropna()\n",
    "    average_result = np.NaN    \n",
    "    if (ser_returns.count() >= num_year_work_days * window_years // 2):\n",
    "        ser_weights = bind_exp_weights(ser_returns, weighting_kind, window_years, halflife_months, ser_condition)\n",
    "        index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "        ### Exponential volatility calculating:\n",
    "        average_x = ser_returns[index_rolling]\n",
    "        average_w = ser_weights[index_rolling]             \n",
    "        average_result = average_x.dot(average_w) / sum(average_w)\n",
    "        \n",
    "    return average_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTED AVERAGE SERIES BUILDER\n",
    "def get_average_series(index_market, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Initialising delta series:\n",
    "    ser_delta = pd.Series(np.NaN, index = ser_returns.index)   \n",
    "    for iter_country in ser_returns.index.get_level_values(0).unique():\n",
    "        ser_delta[iter_country] = ser_returns[iter_country] - ser_returns[iter_country].shift(1)\n",
    "    ser_returns = ser_delta\n",
    "    ### Main loop performing:\n",
    "    ser_average = pd.Series(np.NaN, index = index_market)\n",
    "    for iter_country in index_market.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in index_market.get_level_values(1).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years) : iter_date].dropna()          \n",
    "                if (ser_iter_returns.count() >= num_year_work_days * window_years // 2):\n",
    "                    if (ser_condition.count() > 0):\n",
    "                        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                        ser_iter_condition = ser_iter_condition - ser_iter_condition.mean()\n",
    "                    else:\n",
    "                        ser_iter_condition = pd.Series(np.NaN)\n",
    "                    average_result = get_average_value(ser_iter_returns, weighting_kind, window_years, halflife_months, ser_iter_condition)\n",
    "                    ser_average.loc[iter_country, iter_date] = average_result\n",
    "    ser_average.sort_index(level = [0, 1], inplace = True)\n",
    "    \n",
    "    return ser_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average: 5 years / 1 month: 0.5629990103030383\n",
      "Weighted average: 5 years / 12 months: 0.4951936875458536\n",
      "Weighted average: 5 years / 24 months: 0.4896550581371931\n",
      "Weighted expvol: 5 years / 1 month: 10.1332252337724\n",
      "Weighted expvol: 5 years / 12 months: 9.254597078527707\n",
      "Weighted expvol: 5 years / 24 months: 9.171404880638066\n",
      "Conditional weighted average: 5 years / 1 month: 0.4746031066491625\n",
      "Conditional weighted average: 5 years / 12 months: 0.48027070263210925\n",
      "Conditional weighted average: 5 years / 24 months: 0.4814311172953687\n",
      "Conditional weighted expvol: 5 years / 1 month: 8.709034578041864\n",
      "Conditional weighted expvol: 5 years / 12 months: 9.046579810201605\n",
      "Conditional weighted expvol: 5 years / 24 months: 9.064367209905814\n"
     ]
    }
   ],
   "source": [
    "### Function testing (NOT NEEDED TO IMPLEMENT):\n",
    "ser_test_base = pd.read_excel(io = 'Data_Files/Test_Files/weights_test.xlsx', sheet_name = 'returns', header = None, squeeze = True)\n",
    "ser_test_cond = pd.read_excel(io = 'Data_Files/Test_Files/weights_test.xlsx', sheet_name = 'condition', header = None, squeeze = True)\n",
    "print('Weighted average: 5 years / 1 month:', get_average_value(ser_test_base, weighting_kind = 'expo', window_years = 5, halflife_months = 1))\n",
    "print('Weighted average: 5 years / 12 months:', get_average_value(ser_test_base, weighting_kind = 'expo', window_years = 5, halflife_months = 12))\n",
    "print('Weighted average: 5 years / 24 months:', get_average_value(ser_test_base, weighting_kind = 'expo', window_years = 5, halflife_months = 24))\n",
    "print('Weighted expvol: 5 years / 1 month:', get_expvol_value(ser_test_base, weighting_kind = 'expo', window_years = 5, halflife_months = 1))\n",
    "print('Weighted expvol: 5 years / 12 months:', get_expvol_value(ser_test_base, weighting_kind = 'expo', window_years = 5, halflife_months = 12))\n",
    "print('Weighted expvol: 5 years / 24 months:', get_expvol_value(ser_test_base, weighting_kind = 'expo', window_years = 5, halflife_months = 24))\n",
    "print('Conditional weighted average: 5 years / 1 month:', get_average_value(ser_test_base, weighting_kind = 'expo_cond', window_years = 5, halflife_months = 1,\n",
    "                                                                            ser_condition = ser_test_cond))                     \n",
    "print('Conditional weighted average: 5 years / 12 months:', get_average_value(ser_test_base, weighting_kind = 'expo_cond', window_years = 5, halflife_months = 12, \n",
    "                                                                              ser_condition = ser_test_cond))\n",
    "print('Conditional weighted average: 5 years / 24 months:', get_average_value(ser_test_base, weighting_kind = 'expo_cond', window_years = 5, halflife_months = 24, \n",
    "                                                                              ser_condition = ser_test_cond))\n",
    "print('Conditional weighted expvol: 5 years / 1 month:', get_expvol_value(ser_test_base, weighting_kind = 'expo_cond', window_years = 5, halflife_months = 1, \n",
    "                                                                          ser_condition = ser_test_cond))\n",
    "print('Conditional weighted expvol: 5 years / 12 months:', get_expvol_value(ser_test_base, weighting_kind = 'expo_cond', window_years = 5, halflife_months = 12, \n",
    "                                                                            ser_condition = ser_test_cond))\n",
    "print('Conditional weighted expvol: 5 years / 24 months:', get_expvol_value(ser_test_base, weighting_kind = 'expo_cond', window_years = 5, halflife_months = 24, \n",
    "                                                                            ser_condition = ser_test_cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating average as base series for factors (NOT NEEDED TO IMPLEMENT):\n",
    "import tables\n",
    "tables.file._open_files.close_all()\n",
    "ser_fake_ivol = pd.read_hdf(path_msci_returns_hdf, key_total_ret_ind_LOC)\n",
    "ser_ivol1m_delta = get_average_series(index_market, ser_realized_ret_LOC, weighting_kind = 'expo', window_years = 5, halflife_months = 1) \n",
    "ser_ivol12m_delta = get_average_series(index_market, ser_realized_ret_LOC, weighting_kind = 'expo', window_years = 5, halflife_months = 12)\n",
    "path_ivol_hdf = 'Data_Files/Source_Files/ivol_delta.h5'\n",
    "key_ivol1m_delta = 'key_ivol1m_delta'\n",
    "key_ivol12m_delta = 'key_ivol12m_delta'\n",
    "ser_ivol1m_delta.to_hdf(path_ivol_hdf, key_ivol1m_delta, mode = 'w', format = 'fixed')\n",
    "ser_ivol12m_delta.to_hdf(path_ivol_hdf, key_ivol12m_delta, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FUNCTION\n",
    "def iter_standartize(ser_to_manage, arr_truncates = [2.5, 2.0], reuse_outliers = False, center_result = True):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd     \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_full = ser_to_manage.copy()\n",
    "    ser_data_full = ser_data_full.dropna()\n",
    "    ser_data_iter = ser_data_full.copy() \n",
    "    ser_data_full.replace(ser_data_full.values, 0, inplace = True)    \n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncates:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = ser_data_iter.mean()\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        if not (reuse_outliers):\n",
    "            ### Saving to result and excluding from further calculations truncated values:     \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):      \n",
    "        ser_result = ser_data_full - ser_data_full.mean()\n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "            \n",
    "    return [ser_result, arr_mean, arr_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING FACTOR GENERATOR\n",
    "def market_risk_factors(dict_factors, ser_market_membership, score_all = True, \n",
    "                        score_grouping = 'within', score_boundaries = [2.5, 2.0], score_reuse_outliers = False, score_center_result = True):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd   \n",
    "    import scipy.stats as sc\n",
    "    ### Defining loop variants:\n",
    "    dict_ser_factor = {}\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260    \n",
    "    num_year_months = 12    \n",
    "    index_market = ser_market_membership.index\n",
    "    ### Looping factors:\n",
    "    for iter_factor in dict_factors:\n",
    "        arr_ser_source = dict_factors[iter_factor].copy()\n",
    "        if (iter_factor == 'eventrisk'):\n",
    "            ser_factor = (-1) * get_expvol_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])\n",
    "        if (iter_factor == 'lowvol'):\n",
    "            ser_factor = (-1) * get_expvol_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])              \n",
    "        if (iter_factor == 'volsurprise'):\n",
    "            ser_factor = get_expvol_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])\n",
    "            ser_factor = ser_factor.divide(get_expvol_series(index_market, arr_ser_source[0], arr_ser_source[4], arr_ser_source[5], arr_ser_source[6], arr_ser_source[7]))\n",
    "            ser_factor = (-1) * np.log(ser_factor)\n",
    "        if (iter_factor == 'tailrisk'):\n",
    "            ser_source = pd.Series(np.NaN, index = arr_ser_source[0].index)\n",
    "            for iter_country in arr_ser_source[0].index.get_level_values(0).unique():\n",
    "                ser_source[iter_country] = arr_ser_source[0][iter_country] - arr_ser_source[0][iter_country].shift(1)            \n",
    "            ser_factor = pd.Series(np.NaN, index = index_market)\n",
    "            for iter_country in index_market.get_level_values(0).unique():\n",
    "                if (iter_country in ser_source.index.get_level_values(0).unique()):\n",
    "                    for iter_date in index_market.get_level_values(1).unique():\n",
    "                        ser_iter_source = ser_source.loc[iter_country, iter_date - pd.offsets.BusinessDay(num_year_work_days * arr_ser_source[1]) : iter_date].dropna()\n",
    "                        if (len(ser_iter_source) > 0):\n",
    "                            ser_factor.loc[iter_country, iter_date] = sc.skew(ser_iter_source) \n",
    "#        if (iter_factor == 'vrp'):\n",
    "#            ser_factor = arr_ser_source[0]                    \n",
    "        if (iter_factor == 'ivolmom1m'):\n",
    "            ser_factor = get_average_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])\n",
    "        if (iter_factor == 'ivolmom12m'):\n",
    "            ser_factor = get_average_series(index_market, arr_ser_source[0], arr_ser_source[1], arr_ser_source[2], arr_ser_source[3])\n",
    "        ### Reforming and Naming series for future performing:\n",
    "        ser_factor = ser_factor.swaplevel().sort_index(level = [0, 1])\n",
    "        ser_factor.name = iter_factor      \n",
    "        ### Scoring factor:\n",
    "        if (score_all):\n",
    "        ### Defining constants for standatize procedure:        \n",
    "            arr_ser_scored = []\n",
    "            arr_dates = []                \n",
    "            ### Scoring for no grouping:            \n",
    "            if (score_grouping == 'full'):\n",
    "                for iter_date in ser_factor.index.get_level_values(0).unique():\n",
    "                    ser_iter_factor = ser_factor.loc[iter_date].dropna()\n",
    "                    if (ser_iter_factor.count() > 0):\n",
    "                        ser_iter_score = iter_standartize(ser_iter_factor, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                        arr_ser_scored.append(ser_iter_score)\n",
    "                        arr_dates.append(iter_date)\n",
    "                ser_factor = pd.concat(arr_ser_scored, axis = 0, keys = arr_dates).sort_index(level = [0, 1])\n",
    "            ### Scoring for markets grouping:                            \n",
    "            if (score_grouping == 'within'):               \n",
    "                df_to_score = pd.concat([ser_factor, ser_market_membership.swaplevel().sort_index(level = [0, 1])], axis = 1, join = 'inner')\n",
    "                df_to_score.index.names = ['Date', 'Code']\n",
    "                df_to_score.set_index('Market', append = True, inplace = True)\n",
    "                df_to_score.sort_index(level = [0, 1, 2], inplace = True)                \n",
    "                arr_ser_scored = []\n",
    "                for iter_date in df_to_score.index.get_level_values(0).unique():\n",
    "                    for iter_market in df_to_score.loc[iter_date, :, :].index.get_level_values(2).unique():\n",
    "                        df_to_score_iter = df_to_score.loc[iter_date, :, iter_market]\n",
    "                        ser_iter_factor = df_to_score_iter[iter_factor].dropna()\n",
    "                        if (ser_iter_factor.count() > 0):\n",
    "                            ser_iter_score = iter_standartize(ser_iter_factor, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                            ser_iter_score.reset_index('Market', drop = True, inplace = True)\n",
    "                            arr_ser_scored.append(ser_iter_score)\n",
    "                ser_factor = pd.concat(arr_ser_scored, axis = 0).sort_index(level = [0, 1])\n",
    "        ### Aggregating factors to dictionary:    \n",
    "        ser_factor.index.names = ['Date', 'Code']    \n",
    "        dict_ser_factor[iter_factor] = ser_factor.copy()\n",
    "        print(iter_factor, 'prepared')\n",
    "    ### Collecting factor tables to dictinary:\n",
    "    df_factors = pd.concat(list(dict_ser_factor.values()), axis = 1, join = 'outer')   \n",
    "    \n",
    "    return df_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_msci_expvol_hdf = 'Data_Files/Source_Files/expvol_for_factors.h5'\n",
    "### Extracting realized expvol1m for local returns (eventrisk and volsurprise base series) (NOT NEEDED TO IMPLEMENT):\n",
    "key_expvol1m_LOC = 'key_expvol1m_LOC'\n",
    "ser_expvol1m = pd.read_hdf(path_msci_expvol_hdf, key_expvol1m_LOC)\n",
    "### Extracting realized expvol1m for local returns (lowvol base series) (NOT NEEDED TO IMPLEMENT):\n",
    "key_expvol24m_LOC = 'key_expvol24m_LOC'\n",
    "ser_expvol24m = pd.read_hdf(path_msci_expvol_hdf, key_expvol24m_LOC)\n",
    "### Extracting conditional expvol1m for local returns (volsurprise base series) (NOT NEEDED TO IMPLEMENT):\n",
    "key_expvol1m_cond_LOC = 'key_expvol1m_cond_LOC'\n",
    "ser_expvol1m_cond = pd.read_hdf(path_msci_expvol_hdf, key_expvol1m_cond_LOC)\n",
    "### Extracting fake series data from file (NOT NEEDED TO IMPLEMENT)\n",
    "ser_fake_vrp = pd.read_hdf(path_msci_returns_hdf, key_total_ret_ind_USD)\n",
    "path_ivol_hdf = 'Data_Files/Source_Files/ivol_delta.h5'\n",
    "key_ivol1m_delta = 'key_ivol1m_delta'\n",
    "ser_ivol1m_delta = pd.read_hdf(path_ivol_hdf, key_ivol1m_delta)\n",
    "key_ivol12m_delta = 'key_ivol12m_delta'\n",
    "ser_ivol12m_delta = pd.read_hdf(path_ivol_hdf, key_ivol12m_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eventrisk prepared\n",
      "lowvol prepared\n",
      "volsurprise prepared\n",
      "tailrisk prepared\n",
      "ivolmom1m prepared\n",
      "ivolmom12m prepared\n"
     ]
    }
   ],
   "source": [
    "dict_factors = {'eventrisk': [ser_realized_ret_LOC, 'expo', 5, 1], \n",
    "                'lowvol': [ser_realized_ret_LOC, 'expo', 5, 24], \n",
    "                'volsurprise': [ser_realized_ret_LOC, 'expo', 5, 1, 'expo_cond', 5, 1, ser_fake_GRI], \n",
    "                'tailrisk': [ser_realized_ret_LOC, 2], \n",
    "#                'VRP': [ser_fake_vrp], \n",
    "                'ivolmom1m': [ser_realized_ret_LOC, 'expo', 5, 1], \n",
    "                'ivolmom12m': [ser_realized_ret_LOC, 'expo', 5, 12]}\n",
    "df_factors = market_risk_factors(dict_factors, ser_market_membership, score_all = True,\n",
    "                                 score_grouping = 'within', score_boundaries = [2.5, 2.0], score_reuse_outliers = False, score_center_result = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 / 68\n",
      "                 eventrisk    lowvol  volsurprise  tailrisk  ivolmom1m  \\\n",
      "Date       Code                                                          \n",
      "2018-12-31 SG    -0.795969 -0.535900    -0.523145 -0.595845  -0.141656   \n",
      "           TR    -0.983687 -1.164845    -0.676157 -0.730658   1.203965   \n",
      "           TW     0.844722  1.106156    -0.674338 -0.190422  -0.608496   \n",
      "           US    -1.268074  1.194927    -1.949948  1.875141   1.344401   \n",
      "           ZA     0.256445 -0.416024     1.168463  0.190551   0.371558   \n",
      "\n",
      "                 ivolmom12m  \n",
      "Date       Code              \n",
      "2018-12-31 SG      0.345008  \n",
      "           TR      1.129825  \n",
      "           TW     -0.708280  \n",
      "           US      1.298807  \n",
      "           ZA      0.195492  \n"
     ]
    }
   ],
   "source": [
    "### Factors testing (NOT NEEDED TO IMPLEMENT):\n",
    "print(df_factors['eventrisk'].loc['2018-12-31'].count(), '/', ser_market_membership.loc[:, '2018-12-31'].count())\n",
    "print(df_factors.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
