{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### THIS NOTEBOOK IS PREPARING FACTOR DATA VECTORS FOR MARKET RISK THEME (GLOBAL COUNTRY MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STANDART MODULES INITIALISING\n",
    "\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math \n",
    "from datetime import date\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EXTRACTING MSCI TOTAL RETURNS INDEX DATA FROM XLSX FILE (NOT NEEDED TO IMPLEMENT)\n",
    "\n",
    "### Constants declaring (NOT NEEDED TO IMPLEMENT):\n",
    "path_bloomberg_source = 'Data_Files/Source_Files/msci_data_bloomberg.xlsx'\n",
    "tab_USD = 'MSCI_NET_DAILY_TR_USD'\n",
    "tab_LOC = 'MSCI_NET_DAILY_TR_LOC'\n",
    "path_msci_returns_hdf = 'Data_Files/Source_Files/msci_returns.h5'\n",
    "key_total_ret_ind_USD = 'key_total_ret_ind_USD'\n",
    "key_total_ret_ind_LOC = 'key_total_ret_ind_LOC'\n",
    "key_realized_ret_USD = 'key_realized_ret_USD'\n",
    "key_realized_ret_LOC = 'key_realized_ret_LOC'\n",
    "### Reading fil data (NOT NEEDED TO IMPLEMENT):\n",
    "df_source_USD = pd.read_excel(io = path_bloomberg_source, sheet_name = tab_USD, header = 6)[3 : ]\n",
    "df_source_LOC = pd.read_excel(io = path_bloomberg_source, sheet_name = tab_LOC, header = 6)[3 : ]\n",
    "### Converting data for suitable format (NOT NEEDED TO IMPLEMENT):\n",
    "df_source_USD.set_index('Country Code', inplace = True)\n",
    "ser_total_ret_ind_USD = df_source_USD.stack()\n",
    "ser_total_ret_ind_USD.name = 'Tot Ret Ind USD'\n",
    "ser_total_ret_ind_USD.index.names = ['Date', 'Code']\n",
    "ser_total_ret_ind_USD = ser_total_ret_ind_USD.swaplevel()\n",
    "ser_total_ret_ind_USD.sort_index(inplace = True)\n",
    "ser_realized_ret_USD = pd.Series(np.NaN, index = ser_total_ret_ind_USD.index)\n",
    "for iter_country in ser_realized_ret_USD.index.get_level_values(level = 0).unique():\n",
    "    ser_realized_ret_USD[iter_country] = (ser_total_ret_ind_USD[iter_country] / ser_total_ret_ind_USD[iter_country].shift(1) - 1)\n",
    "ser_realized_ret_USD.name = 'Ret USD'\n",
    "df_source_LOC.set_index('Country Code', inplace = True)\n",
    "ser_total_ret_ind_LOC = df_source_LOC.stack()\n",
    "ser_total_ret_ind_LOC.name = 'Tot Ret Ind LOC'\n",
    "ser_total_ret_ind_LOC.index.names = ['Date', 'Code']\n",
    "ser_total_ret_ind_LOC = ser_total_ret_ind_LOC.swaplevel()\n",
    "ser_total_ret_ind_LOC.sort_index(inplace = True)\n",
    "ser_realized_ret_LOC = pd.Series(np.NaN, index = ser_total_ret_ind_LOC.index)\n",
    "for iter_country in ser_realized_ret_LOC.index.get_level_values(level = 0).unique():\n",
    "    ser_realized_ret_LOC[iter_country] = (ser_total_ret_ind_LOC[iter_country] / ser_total_ret_ind_LOC[iter_country].shift(1) - 1)\n",
    "ser_realized_ret_LOC.name = 'Ret LOC'\n",
    "### Saving results (NOT NEEDED TO IMPLEMENT):\n",
    "ser_total_ret_ind_USD = ser_total_ret_ind_USD.astype('float')\n",
    "ser_total_ret_ind_LOC = ser_total_ret_ind_LOC.astype('float')\n",
    "ser_realized_ret_USD = ser_realized_ret_USD.astype('float')\n",
    "ser_realized_ret_LOC = ser_realized_ret_LOC.astype('float')\n",
    "ser_total_ret_ind_USD.to_hdf(path_msci_returns_hdf, key_total_ret_ind_USD, mode = 'a', format = 'fixed')\n",
    "ser_total_ret_ind_LOC.to_hdf(path_msci_returns_hdf, key_total_ret_ind_LOC, mode = 'a', format = 'fixed')\n",
    "ser_realized_ret_USD.to_hdf(path_msci_returns_hdf, key_realized_ret_USD, mode = 'a', format = 'fixed')\n",
    "ser_realized_ret_LOC.to_hdf(path_msci_returns_hdf, key_realized_ret_LOC, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC TOTAL: 216617 / 485451238.28700006\n",
      "USD TOTAL: 265221 / 761735894.4570001\n",
      "LOC RET: 216567 / US SUM: 1.5319008807330472 / QA SUM: 0.8297622443894299\n",
      "USD RET: 265171 / US SUM: 1.5319023941832968 /QA SUM: 0.08727786207278843\n"
     ]
    }
   ],
   "source": [
    "#### PREPARING RETURNS DATA (NOT NEEDED TO IMPLEMENT)\n",
    "### Constanst declaring (NOT NEEDED TO IMPLEMENT):\n",
    "path_msci_returns_hdf = 'Data_Files/Source_Files/msci_returns.h5'\n",
    "key_total_ret_ind_USD = 'key_total_ret_ind_USD'\n",
    "key_total_ret_ind_LOC = 'key_total_ret_ind_LOC'\n",
    "key_realized_ret_USD = 'key_realized_ret_USD'\n",
    "key_realized_ret_LOC = 'key_realized_ret_LOC'\n",
    "### Extracting MSCI returns data from file (NOT NEEDED TO IMPLEMENT)\n",
    "ser_total_ret_ind_USD = pd.read_hdf(path_msci_returns_hdf, key_total_ret_ind_USD)\n",
    "ser_total_ret_ind_LOC = pd.read_hdf(path_msci_returns_hdf, key_total_ret_ind_LOC)\n",
    "ser_realized_ret_USD = pd.read_hdf(path_msci_returns_hdf, key_realized_ret_USD)\n",
    "ser_realized_ret_LOC = pd.read_hdf(path_msci_returns_hdf, key_realized_ret_LOC)\n",
    "### Checking control values (NOT NEEDED TO IMPLEMENT):\n",
    "print('LOC TOTAL:', ser_total_ret_ind_LOC.count(),'/' ,ser_total_ret_ind_LOC.sum())\n",
    "print('USD TOTAL:', ser_total_ret_ind_USD.count(),'/' ,ser_total_ret_ind_USD.sum())\n",
    "print('LOC RET:', ser_realized_ret_LOC.count(),'/ US SUM:' ,ser_realized_ret_LOC.loc['US', :].sum(), '/ QA SUM:',ser_realized_ret_LOC.loc['QA', :].sum())\n",
    "print('USD RET:', ser_realized_ret_USD.count(),'/ US SUM:' ,ser_realized_ret_USD.loc['US', :].sum(), '/QA SUM:',ser_realized_ret_USD.loc['QA', :].sum())\n",
    "### Tested successfully:\n",
    "#LOC TOTAL: 216617 / 485451238.28700006\n",
    "#USD TOTAL: 265221 / 761735894.4570001\n",
    "#LOC RET: 216567 / US SUM: 1.5319008807330472 / QA SUM: 0.8297622443894299\n",
    "#USD RET: 265171 / US SUM: 1.5319023941832968 /QA SUM: 0.08727786207278843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL WEIGHTS GENERATOR:\n",
    "def get_exp_weights(window_years = 5, halflife_months = 3, result_freq = 'day'):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Array of regressioon window day numbers descending:\n",
    "    arr_weight_days = np.arange(num_year_work_days * window_years + 1, 0, -1)\n",
    "    ### Creating wieghts array:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round((num_year_work_days / num_year_months * halflife_months)))\n",
    "    arr_weights = np.exp(math.log(num_period_factor) * arr_weight_days)\n",
    "    ### Weights adopting for returns period:\n",
    "    if (result_freq == 'day'):\n",
    "        ser_weights = pd.Series(arr_weights)        \n",
    "    if (result_freq == 'month'):        \n",
    "        ser_weights = pd.Series(arr_weights[:: -21])\n",
    "    ser_weights.name = 'Weight'\n",
    "    \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING WEIGHTS TO SERIES BINDER:\n",
    "def bind_exp_weights(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, result_freq = 'day', ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Creating weights series:\n",
    "    if (weighting_kind == 'equal'):\n",
    "        ser_weights = pd.Series(1, index = ser_returns.index)\n",
    "    if (weighting_kind == 'realized'):       \n",
    "        ser_weights = get_exp_weights(window_years, halflife_months, result_freq)[- ser_returns.count() : ]\n",
    "        ser_weights.index = ser_returns.index\n",
    "    if (weighting_kind == 'conditional'):\n",
    "        ser_condition = abs(ser_condition - ser_condition.iloc[-1])\n",
    "        ser_condition = ser_condition.sort_values(ascending = False)\n",
    "        ser_weights = get_exp_weights(window_years, halflife_months, result_freq)[- ser_condition.count() : ]\n",
    "        ser_weights = pd.Series(ser_weights.values, ser_condition.index)\n",
    "        ser_weights.sort_index(inplace = True)\n",
    "        ser_weights.name = 'Weight'\n",
    "        \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing bind_exp_weights (NOT NEEDED TO IMPLEMENT):\n",
    "#ser_returns = pd.Series([10, 20, 30, 40, 50], index = pd.date_range(start = '2018-01-01', periods = 5))\n",
    "#ser_returns.name = 'Returns'\n",
    "#ser_condition = pd.Series([100, 400, 200, 50, 250], index = pd.date_range(start = '2018-01-01', periods = 5))\n",
    "#print(ser_returns)\n",
    "#print('Equal weighted:', bind_exp_weights(ser_returns, weighting_kind = 'equal', window_years = 1, halflife_months = 1, result_freq = 'day'))\n",
    "#print('Date weighted:', bind_exp_weights(ser_returns, weighting_kind = 'realized', window_years = 1, halflife_months = 1, result_freq = 'day'))\n",
    "#print(ser_condition)\n",
    "#print('Conditional weighted:', bind_exp_weights(ser_returns, weighting_kind = 'condition', window_years = 1, halflife_months = 1, result_freq = 'day',\n",
    "#                                          ser_condition = ser_condition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINING EXPONENTIAL VOLATILITY CALCULATOR:\n",
    "def calc_expvol(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, result_freq = 'day', ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12 \n",
    "    ### Flattening MSCI changes by logarythm\n",
    "    ser_returns = np.log(1 + ser_returns)\n",
    "    ### Main loop performing:\n",
    "    ser_expvol = pd.Series(np.NaN, index = ser_returns.index)\n",
    "    iter_counter = 0\n",
    "    for (iter_country, iter_date) in ser_returns.index:\n",
    "        iter_counter = iter_counter + 1\n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * 5) : iter_date].dropna()          \n",
    "        ser_iter_returns = ser_iter_returns - ser_iter_returns.mean()\n",
    "        if (ser_iter_returns.count() >= num_year_work_days * window_years // 2):\n",
    "            if (ser_condition.count() > 0):\n",
    "                ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                ser_iter_condition = ser_iter_condition - ser_iter_condition.mean()\n",
    "            else:\n",
    "                ser_iter_condition = pd.Series(np.NaN)\n",
    "            ser_iter_weights = bind_exp_weights(ser_iter_returns, weighting_kind, window_years, halflife_months, result_freq, ser_iter_condition)\n",
    "            iter_index = ser_iter_returns.index.intersection(ser_iter_weights.index)           \n",
    "            ### Exponential volatility calculating:\n",
    "            expvol_y = ser_iter_returns[iter_index]\n",
    "            expvol_w = ser_iter_weights[iter_index]             \n",
    "            expvol_w = expvol_w / expvol_w.sum()\n",
    "            expvol_results = np.sqrt(expvol_w.dot(expvol_y * expvol_y)) * np.sqrt(num_year_work_days)\n",
    "            ser_expvol.loc[iter_country, iter_date] = expvol_results\n",
    "        if (iter_counter % 1000 == 0):\n",
    "            print(iter_country, iter_date)\n",
    "    return ser_expvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating realized expvol1m for local returns (eventrisk and volsurprise base series):\n",
    "ser_expvol1m_realized = calc_expvol(ser_realized_ret_LOC, weighting_kind = 'realized', window_years = 5, halflife_months = 1, result_freq = 'day')\n",
    "path_msci_expvol_hdf = 'Data_Files/Source_Files/expvol_for_factors.h5'\n",
    "key_expvol1m_realized_LOC = 'key_expvol1m_realized_LOC'\n",
    "ser_expvol1m_realized.to_hdf(path_msci_expvol_hdf, key_expvol1m_realized_LOC, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating realized expvol24m (lowvol base series):\n",
    "ser_expvol24m_realized = calc_expvol(ser_realized_ret_LOC, weighting_kind = 'realized', window_years = 5, halflife_months = 24, result_freq = 'day')\n",
    "path_msci_expvol_hdf = 'Data_Files/Source_Files/expvol_for_factors.h5'\n",
    "key_expvol24m_realized_LOC = 'key_expvol24m_realized_LOC'\n",
    "ser_expvol24m_realized.to_hdf(path_msci_expvol_hdf, key_expvol24m_realized_LOC, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating conditional expvol1m for local returns (volsurprise base series):\n",
    "ser_fake_GRI = ser_total_ret_ind_LOC['AU']\n",
    "ser_expvol1m_conditional = calc_expvol(ser_realized_ret_LOC, weighting_kind = 'conditional', window_years = 5, halflife_months = 1, result_freq = 'day',\n",
    "                                       ser_condition = ser_fake_GRI)\n",
    "path_msci_expvol_hdf = 'Data_Files/Source_Files/expvol_for_factors.h5'\n",
    "key_expvol1m_conditional_LOC = 'key_expvol1m_conditional_LOC'\n",
    "ser_expvol1m_conditional.to_hdf(path_msci_expvol_hdf, key_expvol1m_conditional_LOC, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "tables.file._open_files.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_msci_expvol_hdf = 'Data_Files/Source_Files/expvol_for_factors.h5'\n",
    "### Extracting realized expvol1m for local returns (eventrisk and volsurprise base series) (NOT NEEDED TO IMPLEMENT):\n",
    "key_expvol1m_realized_LOC = 'key_expvol1m_realized_LOC'\n",
    "ser_expvol1m_realized = pd.read_hdf(path_msci_expvol_hdf, key_expvol1m_realized_LOC)\n",
    "### Extracting realized expvol1m for local returns (lowvol base series) (NOT NEEDED TO IMPLEMENT):\n",
    "key_expvol24m_realized_LOC = 'key_expvol24m_realized_LOC'\n",
    "ser_expvol24m_realized = pd.read_hdf(path_msci_expvol_hdf, key_expvol24m_realized_LOC)\n",
    "### Extracting conditional expvol1m for local returns (volsurprise base series) (NOT NEEDED TO IMPLEMENT):\n",
    "key_expvol1m_conditional_LOC = 'key_expvol1m_conditional_LOC'\n",
    "ser_expvol1m_conditional = pd.read_hdf(path_msci_expvol_hdf, key_expvol1m_conditional_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(ser_to_manage, ser_weights):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd        \n",
    "    ### Clearing and docking vectors:\n",
    "    ser_to_manage_filtered = ser_to_manage.dropna()\n",
    "    ser_weights_filtered = ser_weights.dropna()\n",
    "    index_filtered = ser_to_manage_filtered.index.intersection(ser_weights_filtered.index)\n",
    "    ser_to_manage_filtered = ser_to_manage_filtered[index_filtered]\n",
    "    ser_weights_filtered = ser_weights_filtered[index_filtered]\n",
    "    ### Result calculating:\n",
    "    if (ser_to_manage_filtered.count() > 0):\n",
    "        num_result = ser_to_manage_filtered.dot(ser_weights_filtered) / sum(ser_weights_filtered)\n",
    "    else:\n",
    "        num_result = 0\n",
    "    \n",
    "    return num_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_standartize(ser_to_manage, ser_weights, arr_truncates, reuse_outliers = False, center_result = True):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd     \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_full = ser_to_manage.copy()\n",
    "    ser_data_full = ser_data_full.dropna()\n",
    "    ser_data_iter = ser_data_full.copy() \n",
    "    ser_weights_iter = ser_weights.copy()\n",
    "    ser_data_full.replace(ser_data_full.values, 0, inplace = True)    \n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncates:\n",
    "        ### Clearing and docking vectors:        \n",
    "        index_iter = ser_data_iter.index.intersection(ser_weights_iter.index)\n",
    "        ser_data_iter = ser_data_iter[index_iter]\n",
    "        ser_weights_iter = ser_weights_iter[index_iter] \n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weights_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        if not (reuse_outliers):\n",
    "            ### Saving to result and excluding from further calculations truncated values:     \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):      \n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weights) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "            \n",
    "    return [ser_result, arr_mean, arr_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACTING MSCI MEMBERSHIP DATA (NOT NEEDED TO IMPLEMENT)\n",
    "path_msci_membership_hdf = 'Data_Files/Source_Files/msci_membership.h5'\n",
    "key_date_membership = 'key_date_membership'\n",
    "ser_date_membership = pd.read_hdf(path_msci_membership_hdf, key_date_membership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_risk_factors(dict_factors, ser_date_membership, \n",
    "                        score_grouping = 'within', score_boundaries = [2.5, 2.0], score_reuse_outliers = False, score_center_result = True):\n",
    "    ### Importing standard modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Defining loop variants:\n",
    "    dict_ser_factor = {}\n",
    "    ### Defining constants:\n",
    "    num_year_months = 12\n",
    "    ### Looping factors:\n",
    "    for iter_factor in dict_factors:\n",
    "        ser_source = dict_factors[iter_factor].copy()\n",
    "        if (iter_factor == 'ret1mp'):\n",
    "            ser_factor = ser_source\n",
    "        if (iter_factor == 'retnmf'):            \n",
    "            ser_factor = ser_source.reset_index(level = 1)\n",
    "            ser_factor.index = ser_factor.index.shift((-1) * period_shift, 'BM')   \n",
    "            ser_factor.set_index(['Code'], append = True, inplace = True)\n",
    "            ser_factor = ser_factor.squeeze()\n",
    "        if (iter_factor == 'mcap'):\n",
    "            ser_factor = ser_source           \n",
    "        if (iter_factor == 'reversal'):\n",
    "            ser_factor = (-1) * ser_source\n",
    "        if (iter_factor == 'mom12m'):\n",
    "            ser_factor = pd.Series(np.NaN, index = ser_source.index)            \n",
    "            for iter_index in ser_source.index:\n",
    "                ser_iter_index = ser_source.loc[iter_index[0] - pd.offsets.BMonthEnd(num_year_months - 1) : iter_index[0], iter_index[1]]\n",
    "                ser_iter_index.dropna(inplace = True)\n",
    "                if (len(ser_iter_index) > 0):\n",
    "                    ser_factor.loc[iter_index[0], iter_index[1]] = pow((ser_iter_index + 1).prod(), 1 / len(ser_iter_index)) - 1\n",
    "        if (iter_factor == 'mom6mL3m'):\n",
    "            ser_factor = pd.Series(np.NaN, index = ser_source.index)\n",
    "            for iter_index in ser_source.index:\n",
    "                index_start = iter_index[0] - pd.offsets.BMonthEnd(num_year_months - 4)\n",
    "                index_end = iter_index[0] - pd.offsets.BMonthEnd(num_year_months - 9)\n",
    "                ser_iter_index = ser_source.loc[index_start : index_end, iter_index[1]]                \n",
    "                ser_iter_index.dropna(inplace = True)\n",
    "                if (len(ser_iter_index) > 0):\n",
    "                    ser_factor.loc[iter_index[0], iter_index[1]] = pow((ser_iter_index + 1).prod(), 1 / len(ser_iter_index)) - 1   \n",
    "        if (iter_factor == 'eventrisk'):\n",
    "            ser_factor = (-1) * ser_source\n",
    "        ### Naming series for future performing:\n",
    "        ser_factor.name = iter_factor                         \n",
    "        ### Scoring factor:\n",
    "        if (score_all):\n",
    "        ### Defining weights for standatize procedure:\n",
    "            if (score_weights == 'equal'):\n",
    "                ser_weights = pd.Series(1, index = ser_factor.index)\n",
    "            if (score_weights == 'mcap'):\n",
    "                ser_weights = dict_factors['mcap'].copy()\n",
    "            ser_weights.name = 'Weight'\n",
    "            arr_ser_scored = []\n",
    "            arr_dates = []                \n",
    "            ### Scoring for no grouping:            \n",
    "            if (score_grouping == 'full'):\n",
    "                for iter_date in ser_factor.index.get_level_values(0).unique():\n",
    "                    ser_iter_factor = ser_factor.loc[iter_date].dropna()\n",
    "                    ser_iter_weights = ser_weights.loc[iter_date].dropna()\n",
    "                    if ((ser_iter_factor.count() > 0) & ((ser_iter_weights.count() > 0))):\n",
    "                        ser_iter_score = iter_standartize(ser_iter_factor, ser_iter_weights, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                        arr_ser_scored.append(ser_iter_score)\n",
    "                        arr_dates.append(iter_date)\n",
    "                ser_factor = pd.concat(arr_ser_scored, axis = 0, keys = arr_dates).sort_index(level = [0, 1])\n",
    "            ### Scoring for markets grouping:                            \n",
    "            if (score_grouping == 'within'):   \n",
    "                df_to_score = pd.concat([ser_factor, ser_weights, ser_date_membership], axis = 1, join = 'inner')\n",
    "                df_to_score.index.names = ['Date', 'Code']\n",
    "                df_to_score.set_index('Market', append = True, inplace = True)\n",
    "                df_to_score.sort_index(level = [0, 1, 2], inplace = True)\n",
    "                arr_ser_scored = []\n",
    "                for iter_date in df_to_score.index.get_level_values(0).unique():\n",
    "                    for iter_market in df_to_score.loc[iter_date, :, :].index.get_level_values(2).unique():\n",
    "                        df_to_score_iter = df_to_score.loc[iter_date, :, iter_market]\n",
    "                        ser_iter_factor = df_to_score_iter[iter_factor].dropna()\n",
    "                        ser_iter_weights = df_to_score_iter['Weight'].dropna()\n",
    "                        if ((ser_iter_factor.count() > 0) & ((ser_iter_weights.count() > 0))):\n",
    "                            ser_iter_score = iter_standartize(ser_iter_factor, ser_iter_weights, score_boundaries, score_reuse_outliers, score_center_result)[0]\n",
    "                            ser_iter_score.reset_index('Market', drop = True, inplace = True)\n",
    "                            arr_ser_scored.append(ser_iter_score)\n",
    "                ser_factor = pd.concat(arr_ser_scored, axis = 0).sort_index(level = [0, 1])\n",
    "        ### Aggregating factors to dictionary:    \n",
    "        ser_factor.index.names = ['Date', 'Code']    \n",
    "        dict_ser_factor[iter_factor] = ser_factor.copy()     \n",
    "    ### Collecting factor tables to dictinary:\n",
    "    df_factors = pd.concat(list(dict_ser_factor.values()), axis = 1, join = 'outer')   \n",
    "\n",
    "    return df_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
