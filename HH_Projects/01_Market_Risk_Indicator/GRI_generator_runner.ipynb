{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRI GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2377: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->['Asset Group', 'Asset Group Description', 'Asset Code', 'Asset Tab Name', 'Asset Desc', 'Processing Routine']]\n",
      "\n",
      "  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "### EXTRACTING GRI SOURCE DATA FROM GENERAL MS EXCEL SOURCE\n",
    "def get_gri_data_from_excel():\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import date\n",
    "    ### Source xlsx file attributes:\n",
    "    path_gri_data_xlsx = 'Data_Files/Source_Files/mri_data.xlsx'\n",
    "    tab_gri_model = 'Model 01'  \n",
    "    ### Reading Model information from Source model sheet:\n",
    "    df_model_raw = pd.read_excel(path_gri_data_xlsx, sheet_name = tab_gri_model, header = 1, usecols = [0, 1, 2, 3, 4, 5, 6])\n",
    "    ### Group border rows deleting:\n",
    "    df_model_raw = df_model_raw[df_model_raw['Asset Group'] != df_model_raw['Asset Code']]   \n",
    "    ### Dividing list on asset part and MRI weights part:\n",
    "    df_model_asset = df_model_raw[df_model_raw['Asset Group'] != 'MRI'] ### Asset part\n",
    "    df_model_asset.reset_index(drop = True, inplace = True)\n",
    "    df_model_gri = df_model_raw[df_model_raw['Asset Group'] == 'MRI'] ### MRI part\n",
    "    df_model_gri.reset_index(drop = True, inplace = True) \n",
    "    ### Aggregating data from the source xlsx file to pd.DataFrame:\n",
    "    arr_tab_data = []\n",
    "    for iter_index, iter_row in df_model_asset.iterrows():\n",
    "        iter_tab = iter_row['Asset Tab Name']\n",
    "        iter_asset = iter_row['Asset Code']\n",
    "        ser_iter_tab = pd.read_excel(path_gri_data_xlsx, sheet_name = iter_tab, header = 0, index_col = 0, squeeze = True)\n",
    "        ser_iter_tab.name = iter_asset\n",
    "        arr_tab_data.append(ser_iter_tab)\n",
    "    df_source_data = pd.concat(arr_tab_data, axis = 1, join = 'outer')  \n",
    "    df_source_data.index.name = 'Date'\n",
    "    df_source_data = df_source_data.astype('float64')\n",
    "\n",
    "    return [df_model_asset, df_model_gri, df_source_data]\n",
    "    \n",
    "### EXTRACTING SOURCE DATA FROM MS EXCEL FILES AND SAVING TO HDF FILES SCRIPT\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "### Global parameters declaring:\n",
    "date_first = date(1990, 1, 1)\n",
    "date_last = date(2018, 12, 31)\n",
    "### Declaring constants:\n",
    "path_gri_data_hdf = 'Data_Files/Source_Files/gri_data.h5'\n",
    "model_asset_key = 'model_asset_key'\n",
    "model_gri_key = 'model_gri_key'\n",
    "source_data_key = 'source_data_key'\n",
    "### Data extracting:\n",
    "[df_model_asset, df_model_gri, df_source_data] = get_gri_data_from_excel()\n",
    "### closing files that have been previously opened:\n",
    "import tables\n",
    "tables.file._open_files.close_all()\n",
    "### Saving data to hdf5 table formatted files:\n",
    "df_model_asset.to_hdf(path_gri_data_hdf, model_asset_key, mode = 'w', format = 'fixed')\n",
    "df_model_gri.to_hdf(path_gri_data_hdf, model_gri_key, mode = 'a', format = 'fixed')\n",
    "df_source_data.to_hdf(path_gri_data_hdf, source_data_key, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "### Declaring constants:\n",
    "path_gri_data_hdf = 'Data_Files/Source_Files/gri_data.h5'\n",
    "model_asset_key = 'model_asset_key'\n",
    "model_gri_key = 'model_gri_key'\n",
    "source_data_key = 'source_data_key'\n",
    "\n",
    "df_model_asset = pd.read_hdf(path_gri_data_hdf, model_asset_key)\n",
    "df_model_gri = pd.read_hdf(path_gri_data_hdf, model_gri_key)\n",
    "df_source_data = pd.read_hdf(path_gri_data_hdf, source_data_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_z_score(ser_source, min_wnd, max_wnd, winsor_bottom, winsor_top):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Initializing z-score vector:\n",
    "    ser_asset_z_score = pd.Series(np.NaN, index = ser_source.index)\n",
    "    ser_asset_z_score = ser_asset_z_score.astype('float64')\n",
    "    if (ser_source.count() > 0):\n",
    "        ### Initialising index conditions:\n",
    "        asset_start_index = ser_source.first_valid_index() + pd.offsets.BusinessDay(min_wnd - 1)  \n",
    "        iter_date_index = ser_source.index[-1]\n",
    "        ### Checking for at list min_wnd elements of rolling window are not np.NaN:\n",
    "        if (iter_date_index >= asset_start_index):          \n",
    "            ### Isolating rolling window for particular data vector element:\n",
    "            iter_start_index = iter_date_index - pd.offsets.BusinessDay(max_wnd)\n",
    "            ser_iter_source = ser_source.loc[iter_start_index : iter_date_index]        \n",
    "            ser_iter_z_score = (ser_iter_source - ser_iter_source.mean()) / ser_iter_source.std()            \n",
    "            ### Winsorization process:\n",
    "            bool_to_winsor = True            \n",
    "            while (bool_to_winsor): \n",
    "                ### Value based winsorization:                \n",
    "                ser_iter_z_score.clip(lower = winsor_bottom, upper = winsor_top, inplace = True)\n",
    "                ### Recalculating of z scores:\n",
    "                ser_iter_z_score = (ser_iter_z_score - ser_iter_z_score.mean()) / ser_iter_z_score.std()\n",
    "                ### Checking for boundaries:\n",
    "                if (ser_iter_z_score[(ser_iter_z_score <= (winsor_bottom - 0.01)) | (ser_iter_z_score >= (winsor_top + 0.01))].count() == 0):\n",
    "                    bool_to_winsor = False\n",
    "            ### Filling z vector part after the winsorizing (if needed):\n",
    "            ser_asset_z_score.loc[iter_start_index : iter_date_index] = ser_iter_z_score.values\n",
    "    return ser_asset_z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ### TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING TESTING\n",
    "    ### Importing standard modules and date-special modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import date    \n",
    "    ### Declaring constants:\n",
    "    date_start = date(1993, 12, 31)    \n",
    "    path_gri_data_hdf = 'Data_Files/Source_Files/gri_data.h5'\n",
    "    model_asset_key = 'model_asset_key'\n",
    "    model_gri_key = 'model_gri_key'\n",
    "    source_data_key = 'source_data_key'\n",
    "    ### Limitations for rolling windows for z-score calculating:\n",
    "    asset_window_min = 252\n",
    "    asset_window_max = 252 * 100\n",
    "    ### Limitations for z-score winsorizing:\n",
    "    arr_winsor_boundary = [-4, 4]\n",
    "    asset_code = 'iv_rvx'\n",
    "    ser_test = get_rolling_z_score(np.log(df_source_data[asset_code]), asset_window_min, asset_window_max, arr_winsor_boundary[0], arr_winsor_boundary[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gri_value(iter_date):\n",
    "    ### Importing standard modules and date-special modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import date    \n",
    "    ### Declaring constants:\n",
    "    date_first = date(1990, 1, 1)    \n",
    "    date_start = date(1993, 12, 31)    \n",
    "    path_gri_data_hdf = 'Data_Files/Source_Files/gri_data.h5'\n",
    "    model_asset_key = 'model_asset_key'\n",
    "    model_gri_key = 'model_gri_key'\n",
    "    source_data_key = 'source_data_key'\n",
    "    ### Limitations for rolling windows for z-score calculating:\n",
    "    asset_window_min = 252\n",
    "    asset_window_max = 252 * 100\n",
    "    mri_window_max = 260 * 10\n",
    "    ### Limitations for z-score winsorizing:\n",
    "    winsor_bottom = -4\n",
    "    winsor_top = 4    \n",
    "    ### Limitations for moving average for GRI calculation:\n",
    "    ma_max_wnd = 5    \n",
    "    ### Extracting model data from hdf5 file:\n",
    "    df_model_asset = pd.read_hdf(path_gri_data_hdf, model_asset_key)\n",
    "    df_model_gri = pd.read_hdf(path_gri_data_hdf, model_gri_key)\n",
    "    df_source_data = pd.read_hdf(path_gri_data_hdf, source_data_key).loc[ : iter_date]\n",
    "    df_source_data.fillna(method = 'ffill', inplace = True)\n",
    "    index_selected = pd.date_range(date_first, iter_date, freq = 'B')\n",
    "    df_selected_data = df_source_data.reindex(index_selected, method = 'ffill')    \n",
    "    ### Base assets determination (resorting by earliest value):\n",
    "    df_model_asset['Asset Date'] = date(1900, 1, 1)\n",
    "    for (iter_index, asset_code) in df_model_asset['Asset Code'].iteritems():\n",
    "        df_model_asset.loc[iter_index, 'Asset Date'] = df_selected_data[asset_code].dropna().index.min() \n",
    "    df_model_asset.sort_values(['Asset Group', 'Asset Date'], inplace = True)\n",
    "    df_model_asset = df_model_asset.reset_index(drop = True)    \n",
    "    ### Initialising loop visibility variables:          \n",
    "    dict_group_container = {} ### Group vectors collection for gri mean vector calculation   \n",
    "    ### Standartizing loop on group level:\n",
    "    for asset_group_name, df_asset_group in df_model_asset.groupby('Asset Group'):\n",
    "        ### Initialising group visibility variables:\n",
    "        bool_base_asset = True\n",
    "        dict_asset_container = {} ### Asset vectors collection for group mean matrix calculation\n",
    "        ### Standartizing cycle on asset level with the group:\n",
    "        for (asset_index, asset_code) in df_asset_group['Asset Code'].iteritems():\n",
    "            ### Assignment of base asset data set:\n",
    "            if (bool_base_asset):\n",
    "                bool_base_asset = False\n",
    "                ### Performing z scoring for base asset:\n",
    "                ser_base_source = np.log(df_selected_data[asset_code][: iter_date])\n",
    "                ser_base_z_score = get_rolling_z_score(ser_base_source, \n",
    "                                                       asset_window_min, asset_window_max,\n",
    "                                                       winsor_bottom, winsor_top)\n",
    "                ### Calculating ethalon filled quantity before date_start:\n",
    "                int_base_filled = ser_base_source[ : date_start].count()\n",
    "                ### Adding result to dictionary:\n",
    "                dict_asset_container[asset_code] = ser_base_z_score\n",
    "            ### Normalization of other asset's data sets:                \n",
    "            else:\n",
    "                ### Performing z scoring for asset:        \n",
    "                ser_asset_source = np.log(df_selected_data[asset_code][: iter_date])                \n",
    "                ser_asset_z_score = get_rolling_z_score(ser_asset_source, \n",
    "                                                        asset_window_min, asset_window_max,\n",
    "                                                        winsor_bottom, winsor_top)\n",
    "                ### Calculating asset filled quantity:                \n",
    "                int_asset_filled = ser_asset_source[ : date_start].count()          \n",
    "                ### Standartizing asset if it does not have enough initial values:\n",
    "                if (int_asset_filled < int_base_filled * 2 / 3):\n",
    "                    ### RenormaLizing asset z vector with base z vector data:\n",
    "                    if (ser_asset_z_score.first_valid_index() != None):                \n",
    "                        index_asset_start = ser_asset_z_score.first_valid_index() + pd.offsets.BusinessDay(asset_window_min - 1)\n",
    "                        ser_base_z_part = ser_base_z_score.loc[index_asset_start - pd.offsets.BusinessDay(asset_window_min - 1) : iter_date]    \n",
    "                        ser_asset_z_score = ser_asset_z_score * ser_base_z_part.std() + ser_base_z_part.mean()\n",
    "                ### Adding result to dictionary:  \n",
    "                dict_asset_container[asset_code] = ser_asset_z_score   \n",
    "        ### Calculating z vector for group:\n",
    "        ser_group_mean = pd.concat(dict_asset_container, axis = 0, names = ['Asset Code', 'Date'], copy = False)\n",
    "        ser_group_mean = ser_group_mean.groupby('Date').mean()    \n",
    "        ser_group_mean_z = (ser_group_mean - ser_group_mean.mean()) / ser_group_mean.std()  \n",
    "        dict_group_container[asset_group_name] = ser_group_mean_z\n",
    "    ### Model groups mean calculation:    \n",
    "    ser_model_mean = pd.concat(dict_group_container, axis = 0, names = ['Group', 'Date'], copy = False)\n",
    "    ser_model_mean = ser_model_mean.groupby(['Date']).mean()  \n",
    "    ### GRI z-scoring:\n",
    "    ser_gri_z_score = pd.Series(np.NaN, index = ser_model_mean.index)\n",
    "    if (iter_date >= pd.Timestamp(date_start)):\n",
    "        ser_iter_mri = ser_model_mean.loc[iter_date - pd.offsets.BusinessDay(mri_window_max) : iter_date]\n",
    "        ser_iter_z_score = (ser_iter_mri - ser_iter_mri.mean()) / ser_iter_mri.std()\n",
    "        ### Winsorization process:\n",
    "        bool_to_winsor = True            \n",
    "        while (bool_to_winsor):       \n",
    "            ### Value based winsorization:\n",
    "            ser_iter_z_score.clip(lower = winsor_bottom, upper = winsor_top, inplace = True)\n",
    "            ### Recalculating of z scores:\n",
    "            ser_iter_z_score = (ser_iter_z_score - ser_iter_z_score.mean()) / ser_iter_z_score.std()                \n",
    "            ### Checking for boundaries:\n",
    "            if (ser_iter_z_score[(ser_iter_z_score <= (winsor_bottom - 0.01)) | (ser_iter_z_score >= (winsor_top + 0.01))].count() == 0):                    \n",
    "                bool_to_winsor = False    \n",
    "        ser_gri_z_score.loc[iter_date - pd.offsets.BusinessDay(mri_window_max) : iter_date] = ser_iter_z_score.values\n",
    "    ### Calculating z matrix for MRI with winsorization:       \n",
    "    ser_gri_z_ma = ser_gri_z_score.copy()\n",
    "    for iter_shift in np.arange(1, ma_max_wnd):\n",
    "        ser_gri_z_ma = ser_gri_z_ma + ser_gri_z_score.shift(iter_shift)\n",
    "    ser_gri_z_ma = ser_gri_z_ma / ma_max_wnd\n",
    "    \n",
    "    return ser_gri_z_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALL CELL ADDED FOR BETA PERCENTILE FACTORS CALCULATION\n",
    "\n",
    "### DEFINING EXPONENTIAL WEIGHTS GENERATOR\n",
    "def get_exp_weights(window_years = 5, halflife_months = 3):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Array of regressioon window day numbers descending:\n",
    "    arr_weight_days = np.arange(num_year_work_days * window_years, 0, -1) - 1\n",
    "    ### Creating weights series:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round((num_year_work_days / num_year_months * halflife_months)))\n",
    "    arr_weights = np.exp(math.log(num_period_factor) * arr_weight_days)\n",
    "    ser_weights = pd.Series(arr_weights)        \n",
    "    ser_weights.name = 'Weight'\n",
    "    \n",
    "    return ser_weights\n",
    "\n",
    "### DEFINING WEIGHTS TO SERIES BINDER\n",
    "def bind_exp_weights(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Creating weights series:\n",
    "    if (weighting_kind == 'equal'):\n",
    "        ser_weights = pd.Series(1, index = ser_returns.index)\n",
    "    if (weighting_kind == 'expo'):       \n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]\n",
    "        ser_weights.index = ser_returns.index\n",
    "    if (weighting_kind == 'expo_cond'):\n",
    "        ser_condition = abs(ser_condition - ser_condition.iloc[-1])\n",
    "        ser_condition = ser_condition.sort_values(ascending = False)\n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]\n",
    "        ser_weights = pd.Series(ser_weights.values, ser_condition.index)\n",
    "        ser_weights.sort_index(inplace = True)\n",
    "        ser_weights.name = 'Weight'\n",
    "        \n",
    "    return ser_weights\n",
    "\n",
    "### DEFINING WEIGHTED AVERAGE CALCULATOR\n",
    "def get_average_value(ser_returns, ser_weights):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Rolling average calculating:\n",
    "    average_result = np.NaN  \n",
    "    ser_returns = ser_returns.dropna()\n",
    "    index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "    ### Exponential volatility calculating:\n",
    "    average_x = ser_returns[index_rolling]\n",
    "    average_w = ser_weights[index_rolling]                    \n",
    "    average_result = average_x.dot(average_w) / sum(average_w)        \n",
    "        \n",
    "    return average_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR GRI\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import math  \n",
    "### Global parameters declaring:\n",
    "date_start = date(1993, 12, 31)  \n",
    "date_last = date(2018, 12, 31)\n",
    "date_range_test = pd.date_range(date_start, date_last, freq = 'B')\n",
    "#date_test = date(2013, 6, 20)\n",
    "#date_range_test = pd.date_range(date_test, date_test, freq = 'B')\n",
    "### Percentile and signs constants declaring:\n",
    "percentile_years = 10\n",
    "average_years = 5\n",
    "halflife_months = 12\n",
    "num_year_work_days = 260\n",
    "### General looping:\n",
    "dict_gri_vector = {}\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    dict_gri_vector[iter_date] = get_gri_value(iter_date)\n",
    "    if ((iter_counter // 1000) == (iter_counter / 1000)):\n",
    "        print('Vintage progress printout', iter_counter, '/', iter_date)\n",
    "ser_gri_full = pd.concat(dict_gri_vector)\n",
    "ser_gri_released = pd.Series(np.NaN, index = ser_gri_full.index.get_level_values(0).unique())\n",
    "### Percentile series dummies:\n",
    "ser_level_perc_gri = pd.Series(np.NaN, index = ser_gri_full.index.get_level_values(0).unique()) ### ADDED FOR BETA PERCENTILE FACTORS CALCULATION\n",
    "ser_momentum_perc_gri = pd.Series(np.NaN, index = ser_gri_full.index.get_level_values(0).unique()) ### ADDED FOR BETA PERCENTILE FACTORS CALCULATION\n",
    "#ser_momentum_gri = pd.Series(np.NaN, index = ser_gri_full.index.get_level_values(0).unique()) ### FOR TESTING PURPOSES\n",
    "iter_counter = 0\n",
    "for iter_date in ser_gri_released.index:\n",
    "    iter_counter = iter_counter + 1\n",
    "    if ((iter_counter // 1000) == (iter_counter / 1000)):\n",
    "        print('Vectors progress printout', iter_counter, '/', iter_date)    \n",
    "    ### Selecting GRI vintage column for percentiles calculation:\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = percentile_years * num_year_work_days, freq = 'B') ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    index_iter_date_mean = pd.date_range(end = iter_date, periods = average_years * num_year_work_days, freq = 'B') ### ADDED FOR BETA FACTORS CALCULATION            \n",
    "    ser_iter_vintage = ser_gri_full[iter_date] ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    ser_iter_vintage = ser_iter_vintage[index_iter_date] ### ADDED FOR BETA FACTORS CALCULATION    \n",
    "#    ser_gri_released[iter_date] = ser_gri_full.loc[iter_date, iter_date]\n",
    "    ser_gri_released[iter_date] = ser_iter_vintage.loc[iter_date] ### CHANGED\n",
    "    ### Level percentile calculating:    \n",
    "    ser_level_perc_gri[iter_date] = ser_iter_vintage.rank(method = 'average', na_option = 'keep', ascending = True, pct = True).iloc[-1] ### ADDED FOR BETA FACTORS\n",
    "    ### Daily changes for momentum percentiles receiving:\n",
    "    ser_iter_vintage = ser_iter_vintage - ser_iter_vintage.shift(1) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    ### Weighted average calculating for momentum percentiles:\n",
    "    ser_iter_vintage_mean = ser_iter_vintage[index_iter_date_mean] ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    ser_iter_weights = bind_exp_weights(ser_iter_vintage_mean, 'expo', average_years, halflife_months) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    ser_iter_vintage_mean = ser_iter_vintage_mean.dropna() ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    num_iter_average = get_average_value(ser_iter_vintage_mean, ser_iter_weights) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    ser_iter_weights = ser_iter_weights / ser_iter_weights.sum() ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    num_iter_average = num_iter_average * math.sqrt(1 / ((ser_iter_weights * ser_iter_weights).sum())) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    ser_iter_vintage[-1] = num_iter_average ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    ### Momentum percentile calculating:\n",
    "    ser_momentum_perc_gri[iter_date] = ser_iter_vintage.rank(method = 'average', na_option = 'keep', ascending = True, pct = True).iloc[-1] ### ADDED FOR BETA FACTORS\n",
    "#    ser_momentum_gri[iter_date] = num_iter_average ### FOR TESTING PURPOSES\n",
    "ser_gri_released.index.name = 'Date'\n",
    "ser_gri_released.name = 'GRI_MA5'\n",
    "ser_level_perc_gri.index.name = 'Date' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_level_perc_gri.name = 'GRI_Level_Perc' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_momentum_perc_gri.index.name = 'Date' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_momentum_perc_gri.name = 'GRI_Momentum_Perc' ### ADDED FOR BETA FACTORS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_iter_weights min: 8.611314519576692e-05\n",
      "ser_iter_weights mean: 0.0007692307692307699\n",
      "ser_iter_weights max: 0.0027482840812943082\n",
      "ser_iter_weights stdev: 0.0007072385809436454\n",
      "ser_iter_weights count: 1300\n"
     ]
    }
   ],
   "source": [
    "print('ser_iter_weights min:', ser_iter_weights.min())\n",
    "print('ser_iter_weights mean:', ser_iter_weights.mean())\n",
    "print('ser_iter_weights max:', ser_iter_weights.max())\n",
    "print('ser_iter_weights stdev:', ser_iter_weights.std())\n",
    "print('ser_iter_weights count:', ser_iter_weights.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESULTS SAVING\n",
    "\n",
    "path_gri_index_hdf = 'Data_Files/Source_Files/gri_released_index.h5'\n",
    "gri_vector_key = 'gri_vector_key'\n",
    "gri_vintage_key = 'gri_vintage_key'\n",
    "gri_start_key = 'gri_start_key'\n",
    "gri_level_perc_key = 'gri_level_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "gri_momentum_perc_key = 'gri_momentum_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_gri_released.to_hdf(path_gri_index_hdf, gri_vector_key, mode = 'w', format = 'fixed')\n",
    "ser_level_perc_gri.to_hdf(path_gri_index_hdf, gri_level_perc_key, mode = 'a', format = 'fixed') ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_momentum_perc_gri.to_hdf(path_gri_index_hdf, gri_momentum_perc_key, mode = 'a', format = 'fixed') ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_gri_full.index.names = ['Date_Point', 'Date']\n",
    "ser_gri_full.to_hdf(path_gri_index_hdf, gri_vintage_key, mode = 'a', format = 'fixed')\n",
    "ser_gri_full.loc[date_start, :].to_hdf(path_gri_index_hdf, gri_start_key, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x250c5afaf28>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTING\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "path_gri_index_hdf = 'Data_Files/Source_Files/gri_released_index.h5'\n",
    "gri_vector_key = 'gri_vector_key'\n",
    "gri_level_perc_key = 'gri_level_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "gri_momentum_perc_key = 'gri_momentum_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_gri_released = pd.read_hdf(path_gri_index_hdf, gri_vector_key)\n",
    "ser_level_perc_gri = pd.read_hdf(path_gri_index_hdf, gri_level_perc_key)\n",
    "ser_momentum_perc_gri = pd.read_hdf(path_gri_index_hdf, gri_momentum_perc_key)\n",
    "import matplotlib\n",
    "ser_gri_released.plot(figsize = (15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESULTS EXTRACTING \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_gri_index_hdf = 'Data_Files/Source_Files/gri_released_index.h5'\n",
    "gri_vector_key = 'gri_vector_key'\n",
    "gri_start_key = 'gri_start_key'\n",
    "gri_level_perc_key = 'gri_level_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "gri_momentum_perc_key = 'gri_momentum_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "\n",
    "ser_gri_released = pd.read_hdf(path_gri_index_hdf, gri_vector_key)\n",
    "ser_level_perc_gri = pd.read_hdf(path_gri_index_hdf, gri_level_perc_key)\n",
    "ser_momentum_perc_gri = pd.read_hdf(path_gri_index_hdf, gri_momentum_perc_key)\n",
    "ser_gri_start = pd.read_hdf(path_gri_index_hdf, gri_start_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RESULTS EXPORTING\n",
    "\n",
    "ser_level_perc_gri.to_excel('Data_Files/Source_Files/gri_level_perc.xlsx')\n",
    "ser_momentum_perc_gri.to_excel('Data_Files/Source_Files/gri_momentum_perc.xlsx')\n",
    "ser_gri_released.to_excel('Data_Files/Source_Files/gri_released.xlsx')\n",
    "pd.concat([ser_gri_start.droplevel(0).iloc[: -1], ser_gri_released], axis = 0).to_excel('Data_Files/Source_Files/gri_released_plus.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
