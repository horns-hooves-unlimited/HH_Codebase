{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MRI GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STANDART MODULES INITIALISING\n",
    "\n",
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MRI CONSTANTS AND PARAMETERS SETTING\n",
    "\n",
    "### Standart date format for notebook:\n",
    "date_format = '%Y-%m-%d'\n",
    "### MRI dates:\n",
    "date_first = date(1990, 1, 1)\n",
    "date_last = date(2018, 12, 31)\n",
    "date_start = date(1993, 12, 31)\n",
    "### Source xlsx file attributes:\n",
    "path_mri_data_xlsx = 'Data_Files/Source_Files/mri_data.xlsx'\n",
    "mri_model_name = 'Model 01'\n",
    "### HDF5 file with structured source data for selected date interval attributes:\n",
    "path_mri_data_hdf = 'Data_Files/Source_Files/mri_data.h5'\n",
    "key_mri_data_hdf = 'source_data'\n",
    "\n",
    "### Limitations for rolling windows for z-score calculating:\n",
    "asset_window_min = 252\n",
    "asset_window_max = 252 * 100\n",
    "mri_window_max = 260 * 10\n",
    "### Limitations for z-score winsorizing:\n",
    "arr_winsor_boundary = [-4, 4]\n",
    "### Limitations for moving average for MRI calculation:\n",
    "mri_moving_average_window_max = 5\n",
    "### HDF5 with MRI group matrices builded from z-scored means of standartized winsorized weighted z-score matrices for each group asset:\n",
    "path_mri_standart_hdf = 'Data_Files/Source_Files/mri_group_z_matrix.h5'\n",
    "### HDF5 with MRI asset level info:\n",
    "path_mri_assets_hdf = 'Data_Files/Source_Files/mri_released_assets.h5'\n",
    "object_selected_data_hdf = 'selected_data'\n",
    "object_standartized_data_hdf = 'standartized_data'\n",
    "### HDF5 with MRI group level info:\n",
    "path_mri_groups_hdf = 'Data_Files/Source_Files/mri_released_groups.h5'\n",
    "object_diag_grouped_hdf = 'diag_grouped_data'\n",
    "object_perc_grouped_hdf = 'percentile_grouped_data'\n",
    "### HDF5 with MRI level info:\n",
    "path_mri_index_hdf = 'Data_Files/Source_Files/mri_released_index.h5'\n",
    "object_diag_mri_hdf = 'diag_MRI_data'\n",
    "object_released_mri_hdf = 'released_MRI_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MRI DATA AGGREGATING FUNCTION\n",
    "def get_mri_data(source_file_path, source_model_sheet, hdf_file_path, hdf_object_key, date_index, update_hdf = True):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    \n",
    "    ### Reading Model information from Source model sheet:\n",
    "    df_model_raw = pd.read_excel(source_file_path, sheet_name = source_model_sheet, header = 1, usecols = [0, 1, 2, 3, 4, 5, 6])\n",
    "    ### Group border rows deleting:\n",
    "    df_model_raw = df_model_raw[df_model_raw['Asset Group'] != df_model_raw['Asset Code']]   \n",
    "    ### Dividing list on asset part and MRI weights part:\n",
    "    df_model_asset = df_model_raw[df_model_raw['Asset Group'] != 'MRI'] ### Asset part\n",
    "    df_model_asset.reset_index(drop = True, inplace = True)\n",
    "    df_model_mri = df_model_raw[df_model_raw['Asset Group'] == 'MRI'] ### MRI part\n",
    "    df_model_mri.reset_index(drop = True, inplace = True) \n",
    "    ### Extracting source data from initial excel file or from saved hdf\n",
    "    if (update_hdf): \n",
    "        ### Aggregating data from the source xlsx file to pd.DataFrame:\n",
    "        arr_tab_data = []\n",
    "        for iter_index, iter_row in df_model_asset.iterrows():\n",
    "            iter_tab = iter_row['Asset Tab Name']\n",
    "            iter_asset = iter_row['Asset Code']\n",
    "            ser_iter_tab = pd.read_excel(source_file_path, sheet_name = iter_tab, header = 0, index_col = 0, squeeze = True)\n",
    "            ser_iter_tab.name = iter_asset\n",
    "            arr_tab_data.append(ser_iter_tab)\n",
    "        df_source_data = pd.concat(arr_tab_data, axis = 1, join = 'outer')\n",
    "        df_source_data = df_source_data.astype(float)        \n",
    "        df_source_data.to_hdf(hdf_file_path, hdf_object_key, mode = 'w', format = 'fixed', append = False)\n",
    "    else:\n",
    "        df_source_data = pd.read_hdf(hdf_file_path, hdf_object_key)\n",
    "    ### Filtering by date_index and forward filling missing values:\n",
    "    df_source_data.fillna(method = 'ffill', inplace = True)\n",
    "    df_selected_data = df_source_data.reindex(date_index, method = 'ffill')\n",
    "    df_selected_data.index.name = 'Date'\n",
    "    return [df_model_asset, df_model_mri, df_selected_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GETTING MRI DATA FOR FUTURE CALCULATIONS\n",
    "index_mri_date = pd.date_range(date_first, date_last, freq = 'B')\n",
    "#[df_model_asset, df_model_mri, df_selected_data] = get_mri_data(path_mri_data_xlsx, mri_model_name, path_mri_data_hdf, key_mri_data_hdf, \n",
    "#                                                                 index_mri_date, update_hdf = True)\n",
    "[df_model_asset, df_model_mri, df_selected_data] = get_mri_data(path_mri_data_xlsx, mri_model_name, path_mri_data_hdf, key_mri_data_hdf, \n",
    "                                                                index_mri_date, update_hdf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_z_score(ser_source, min_wnd, max_wnd, winsor_bottom, winsor_top):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    ### Calculating rolling mean:\n",
    "    ser_rolling_mean = ser_source.rolling(window = max_wnd, min_periods = min_wnd, win_type = None).mean()\n",
    "    ### Calculating rolling standard deviation:\n",
    "    ser_rolling_std = ser_source.rolling(window = max_wnd, min_periods = min_wnd, win_type = None).std()\n",
    "    ### Calculating rolling z-score:\n",
    "    ser_rolling_z_score = (ser_source - ser_rolling_mean) / ser_rolling_std\n",
    "    ### Initializing resulting variables:\n",
    "    df_z_matrix = pd.DataFrame(np.NaN, index = ser_source.index, columns = ser_source.index)\n",
    "    df_z_matrix = df_z_matrix.astype(float)\n",
    "    ### Calculating z-score matrix:\n",
    "    for iter_end_index in ser_source.index:\n",
    "        ### Isolating rolling window for particular data vector element:\n",
    "        iter_start_index = iter_end_index - pd.offsets.BusinessDay(max_wnd)\n",
    "        ser_iter_source = ser_source.loc[iter_start_index : iter_end_index]\n",
    "        ### Checking for at list min_wnd elements of rolling window are not np.NaN:\n",
    "        if (ser_iter_source.count() >= min_wnd):\n",
    "            ser_iter_z_score = (ser_iter_source - ser_iter_source.mean()) / ser_iter_source.std()            \n",
    "            ### Winsorization process:\n",
    "            bool_to_winsor = True            \n",
    "            while (bool_to_winsor):       \n",
    "                ### Value based winsorization:\n",
    "                ser_iter_z_score.clip(lower = winsor_bottom, upper = winsor_top, inplace = True)\n",
    "                ### Recalculating of z scores:\n",
    "                ser_iter_z_score = (ser_iter_z_score - ser_iter_z_score.mean()) / ser_iter_z_score.std()                \n",
    "                ### Checking for boundaries:\n",
    "                if (ser_iter_z_score[(ser_iter_z_score < winsor_bottom) & (ser_iter_z_score > winsor_top)].size == 0):\n",
    "                    bool_to_winsor = False\n",
    "            ### Filling z matrix column part after the winsorizing (if needed):\n",
    "            df_z_matrix.loc[iter_start_index : iter_end_index, iter_end_index] = ser_iter_z_score.values        \n",
    "    ### Getting winsorized z meanings:     \n",
    "    ser_rolling_z_winsor = pd.Series(list(np.diag(df_z_matrix)), index = ser_source.index)\n",
    "    ### Backfilling with first not NaN column of z matrix:\n",
    "    ind_valid_index = ser_rolling_z_winsor.first_valid_index()\n",
    "    ser_rolling_z_winsor.loc[ : ind_valid_index] = df_z_matrix.loc[ : ind_valid_index, ind_valid_index]\n",
    "    \n",
    "    return [ser_rolling_z_score, ser_rolling_z_winsor, df_z_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standartized_mri_data(df_model_asset, df_selected_data, date_start, asset_window_min, asset_window_max, arr_winsor_boundary, hdf_file_path):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    ### Base assets determination (resorting by earliest value):\n",
    "    df_model_asset['Asset Date'] = date_first\n",
    "    for (iter_index, asset_code) in df_model_asset['Asset Code'].iteritems():\n",
    "        df_model_asset.loc[iter_index, 'Asset Date'] = df_selected_data[asset_code].dropna().index.min() \n",
    "    df_model_asset.sort_values(['Asset Group', 'Asset Date'], inplace = True)\n",
    "    df_model_asset = df_model_asset.reset_index(drop = True)\n",
    "\n",
    "    ### Initialising loop visibility variables:          \n",
    "#    arr_group_diag_container = []\n",
    "    dict_group_diag_container = {} ### Group z-matrices diagonales container\n",
    "#    arr_group_vector_container = []\n",
    "#    arr_asset_vector_container = []\n",
    "    dict_asset_vector_container = {} ### Asset z-matrices diagonales container\n",
    "#    arr_asset_codes_global = []\n",
    "#    arr_group_codes = []\n",
    "    dict_group_matrix_container = {}\n",
    "    ### Standartizing loop on group level:\n",
    "    for asset_group_name, df_asset_group in df_model_asset.groupby('Asset Group'):\n",
    "        ### Initialising group visibility variables:\n",
    "        print('get_standartized_mri_data: group', asset_group_name, 'standartizing started')\n",
    "        bool_base_asset = True\n",
    "#        arr_asset_matrix_container = []\n",
    "        dict_asset_matrix_container = {} ### Asset matrices collection for group mean matrix calculation\n",
    "#        arr_asset_codes = []\n",
    "        ### Standartizing cycle on asset level with the group:\n",
    "        for (asset_index, asset_code) in df_asset_group['Asset Code'].iteritems():\n",
    "            ### Assignment of base asset data set:\n",
    "            if (bool_base_asset):\n",
    "                bool_base_asset = False\n",
    "                ### Performing z scoring for base asset:\n",
    "                [ser_rolling_z_score_base, ser_rolling_z_winsor_base, df_base_z_matrix] = get_rolling_z_score(df_selected_data[asset_code], \n",
    "                                                                                                              asset_window_min, asset_window_max,\n",
    "                                                                                                              arr_winsor_boundary[0], arr_winsor_boundary[1])\n",
    "                ### Calculating etalon filled quantity before date_start:\n",
    "                int_base_filled = ser_rolling_z_winsor_base[ : date_start].dropna().count()                \n",
    "                ### Defining of standartized values of base asset as diagonal of z matrix (without backfilling):\n",
    "#                arr_asset_vector_container.append(pd.Series(np.copy(np.diag(df_base_z_matrix)), index = df_base_z_matrix.index))\n",
    "                dict_asset_vector_container[asset_code] = pd.Series(list(np.diag(df_base_z_matrix)), index = df_base_z_matrix.index)\n",
    "#                ### Initialising dataset with non np.NaN wages sum for group:\n",
    "#                df_group_weights = pd.DataFrame(np.zeros(df_base_z_matrix.shape), index = df_base_z_matrix.index, columns = df_base_z_matrix.columns)\n",
    "                ### Creating a whole group dataset with multiplying asset matrix to asset weight:\n",
    "#                arr_asset_matrix_container.append(df_base_z_matrix * df_model_asset.at[asset_index, 'Factor Weights'])    \n",
    "                dict_asset_matrix_container[asset_code] = df_base_z_matrix\n",
    "#                df_group_weights = df_group_weights + df_base_z_matrix.notna() * df_model_asset.at[asset_index, 'Factor Weights']\n",
    "#                arr_asset_codes.append(asset_code)\n",
    "#                arr_asset_codes_global.append(asset_code)\n",
    "            ### Normalization of other asset's data sets:                \n",
    "            else:\n",
    "                ### Performing z scoring for asset:                \n",
    "                [ser_asset_z_score_simple, ser_asset_z_score_winsor, df_asset_z_matrix] = get_rolling_z_score(df_selected_data[asset_code], \n",
    "                                                                                                              asset_window_min, asset_window_max, \n",
    "                                                                                                              arr_winsor_boundary[0], arr_winsor_boundary[1])            \n",
    "                ### Calculating asset filled quantity:                \n",
    "                int_asset_filled = ser_asset_z_score_winsor[ : date_start].dropna().count()            \n",
    "                ### Standartizing asset if they do not have enough initial values:\n",
    "                if (int_asset_filled < int_base_filled * 2 / 3):\n",
    "#                    df_asset_start_index = ser_asset_z_score_simple.index.get_loc(ser_asset_z_score_simple.first_valid_index())\n",
    "                    index_asset_start = ser_asset_z_score_simple.first_valid_index()\n",
    "                    ### RenormaLizing asset z matrix with base z matrix data:\n",
    "#                    for end_wnd_index in range(index_asset_start, min(index_asset_start + asset_window_max, ser_asset_z_score_simple.size)):\n",
    "                    for index_asset_end in ser_asset_z_score_simple.index:\n",
    "                        if ((index_asset_end >= index_asset_start) & (index_asset_end <= (index_asset_start + pd.offsets.BusinessDay(asset_window_max)))):\n",
    "#                        ser_base_z_matrix_part = df_base_z_matrix.iloc[max(0, index_asset_start - asset_window_min + 1) : index_asset_end + 1, index_asset_end]\n",
    "                            ser_base_z_part = df_base_z_matrix.loc[index_asset_start - pd.offsets.BusinessDay(asset_window_min) : index_asset_end, index_asset_end]\n",
    "                            df_asset_z_matrix.loc[:, index_asset_end] = df_asset_z_matrix.loc[:, index_asset_end] * ser_base_z_part.std()  + ser_base_z_part.mean()\n",
    "                       \n",
    "                ### Defining of standartized values of asset as diagonale of modified z matrix (without backfilling):\n",
    "#                arr_asset_vector_container.append(pd.Series(np.copy(np.diag(df_asset_z_matrix)), index = df_asset_z_matrix.index))   \n",
    "                dict_asset_vector_container[asset_code] = pd.Series(list(np.diag(df_asset_z_matrix)), index = df_asset_z_matrix.index)\n",
    "                ### Adding asset matrix to a whole group dataset with multiplying asset matrix to asset weight:          \n",
    "#                arr_asset_matrix_container.append(df_asset_z_matrix * df_model_asset.at[asset_index, 'Factor Weights'])  \n",
    "                dict_asset_matrix_container[asset_code] = df_asset_z_matrix\n",
    "#                df_group_weights = df_group_weights + df_asset_z_matrix.notna() * df_model_asset.at[asset_index, 'Factor Weights']                    \n",
    "#                arr_asset_codes.append(asset_code)   \n",
    "#                arr_asset_codes_global.append(asset_code)  \n",
    "            print('get_standartized_mri_data: asset', asset_code, 'in group', asset_group_name, 'standartized successfully')         \n",
    "        ### Calculating z matrix for group from weighted asset matrices:\n",
    "#        df_group_mean = pd.concat(arr_asset_matrix_container, axis = 0, keys = arr_asset_codes, names = ['Asset Code', 'Date'], copy = False)   \n",
    "        df_group_mean = pd.concat(dict_asset_matrix_container, axis = 0, names = ['Asset Code', 'Date'], copy = False)\n",
    "#        print('get_standartized_mri_data: aggregated matrix for group' , asset_group_name, 'builded successfully')    \n",
    "        df_group_mean = df_group_mean.groupby('Date').mean()    \n",
    "#        df_group_mean = df_group_mean.sum(level = 1)\n",
    "#        df_group_mean[df_group_weights > 0] =  df_group_mean[df_group_weights > 0] / df_group_weights[df_group_weights > 0]\n",
    "#        df_group_mean[df_group_weights == 0] = np.NaN\n",
    "#        print('get_standartized_mri_data: mean matrix for group' , asset_group_name, 'builded successfully')         \n",
    "        df_group_mean_z = (df_group_mean - df_group_mean.mean()) / df_group_mean.std()\n",
    "        ### Adding diagonale of group weighted mean z-score matrix to MRI dataset:\n",
    "#        arr_group_diag_container.append(pd.Series(np.copy(np.diag(df_group_mean_z)), index = df_group_mean_z.index))\n",
    "        dict_group_diag_container[asset_group_name] = pd.Series(list(np.diag(df_group_mean_z)), index = df_group_mean_z.index)        \n",
    "        print('get_standartized_mri_data: z-score matrix for group' , asset_group_name, 'mean matrix builded successfully') \n",
    "        ### Saving group matrix to hdf file for further manipulations:\n",
    "#        df_group_to_save = df_group_mean_z.copy()\n",
    "#        df_group_to_save = df_group_to_save.astype(float)\n",
    "#        df_group_to_save.reset_index(inplace = True)\n",
    "#        df_group_to_save.columns = np.arange(len(df_group_to_save.columns))\n",
    "#        df_group_to_save.to_hdf(hdf_file_path, key = asset_group_name, mode = 'a', format = 'fixed')\n",
    "#        arr_group_codes.append(asset_group_name)\n",
    "#        df_group_mean_z = df_group_mean_z.astype(float)\n",
    "        df_group_mean_z.reset_index(inplace = True)\n",
    "        df_group_mean_z.columns = np.arange(len(df_group_mean_z.columns))\n",
    "        df_group_mean_z.to_hdf(hdf_file_path, key = asset_group_name, mode = 'a', format = 'fixed')\n",
    "#        dict_group_matrix_container[asset_group_name] = df_group_mean_z\n",
    "        print('get_standartized_mri_data: z-score matrix for group' , asset_group_name, 'saved to HDF5 file', hdf_file_path, '(object key:', asset_group_name, ')')\n",
    "#        ### Creating data vector of percentiled group's z matrix columns for each group:\n",
    "#        ser_group_z_percentile = pd.Series(np.NaN, index = df_group_mean_z.index) \n",
    "#        ser_group_z_percentile.name = asset_group_name\n",
    "#        for column_index in df_group_mean_z.columns:\n",
    "#            if (column_index >= datetime.strptime(date_start, date_format)):                \n",
    "#                ser_rolling_wnd = df_group_mean_z.loc[(column_index - pd.DateOffset(years = 1) + pd.DateOffset(days = 1)) : column_index, column_index]\n",
    "#                ser_group_z_percentile[column_index] = ((ser_rolling_wnd.rank(method = 'min')[-1] - 1) / ser_rolling_wnd.notna().sum() + \n",
    "#                        ser_rolling_wnd.rank(pct = True, method = 'max')[-1]) / 2                    \n",
    "#        arr_group_vector_container.append(ser_group_z_percentile)\n",
    "    ### Collection of standartized z-scores for all assets:\n",
    "#    df_asset_standartized = pd.concat(arr_asset_vector_container, axis = 0, keys = arr_asset_codes_global, names = ['Asset Code', 'Date'], copy = False)\n",
    "    ser_asset_standartized = pd.concat(dict_asset_vector_container, axis = 0, names = ['Asset', 'Date'], copy = False)    \n",
    "    ser_asset_standartized = ser_asset_standartized.astype(float)\n",
    "    print('get_standartized_mri_data: asset standartized z-score collection builded successfully')\n",
    "    ### Collection of diagonales of group's z matrices for all groups:\n",
    "#    df_group_mean_z_diag = pd.concat(arr_group_diag_container, axis = 0, keys = arr_group_codes, names = ['Group Name', 'Date'], copy = False)\n",
    "    ser_group_mean_z_diag = pd.concat(dict_group_diag_container, axis = 0, names = ['Group', 'Date'], copy = False)    \n",
    "    ser_group_mean_z_diag = ser_group_mean_z_diag.astype(float)     \n",
    "    print('get_standartized_mri_data: data vector collection of diagonales of mean z score matrix for all groups builded successfully')    \n",
    "#    ### Collection of percentiled group's z matrices for all groups:\n",
    "#    df_group_percentiled = pd.concat(arr_group_vector_container, axis = 0, keys = arr_group_codes, names = ['Group Name', 'Date'], copy = False)\n",
    "#    print('get_standartized_mri_data: percentiled data vector collection on base of mean z score matrix for all groups builded successfully')         \n",
    "#    df_mri_groups = pd.concat(dict_group_matrix_container, axis = 0, names = ['Group Name', 'Date'], copy = False)  \n",
    "#    df_mri_mean = df_group_mean.groupby('Date').mean()\n",
    "#    df_mri_mean.stack(dropna = False).to_hdf(hdf_file_path, key = 'mri', mode = 'a', format = 'fixed')\n",
    "#    return [df_asset_standartized, df_group_mean_z_diag, df_group_percentiled]      \n",
    "    return [ser_asset_standartized, ser_group_mean_z_diag] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### STANDARTISING SOURCE DATA FOR MRI CALCUCATION\n",
    "\n",
    "### Standartizing dataset:\n",
    "### Building collection of standartized winsorized z-scores for all assets:\n",
    "### Building collection of group's z matrices diagonales for all groups:\n",
    "### Saving group's z matrices:\n",
    "[ser_standartized_assets, ser_diag_mean_z_groups] = get_standartized_mri_data(df_model_asset, df_selected_data, date_start, \n",
    "                                                                              asset_window_min, asset_window_max, arr_winsor_boundary, \n",
    "                                                                              path_mri_standart_hdf)\n",
    "### Saving results for assets to HDF5 to avoid hard calculations with constant source model and datasets:\n",
    "ser_standartized_assets.to_hdf(path_mri_assets_hdf, key = object_standartized_data_hdf, mode = 'w', format = 'fixed')\n",
    "### Saving results for groups to HDF5 to avoid hard calculations with constant source model and datasets:\n",
    "ser_diag_mean_z_groups.to_hdf(path_mri_groups_hdf, key = object_diag_grouped_hdf, mode = 'w', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_mri_data(df_model_mri, hdf_z_matrix_path, date_start, mri_window_max, ma_max_wnd, arr_winsor_boundary):   \n",
    "    import numpy as np\n",
    "    import pandas as pd   \n",
    "\n",
    "    ### Initialising containers for weighted mean matrix calculation:\n",
    "#    arr_matrix_container = []\n",
    "#    arr_group_codes = []\n",
    "    dict_group_matrix_container = {}\n",
    "    winsor_bottom = arr_winsor_boundary[0]\n",
    "    winsor_top = arr_winsor_boundary[1]    \n",
    "    ### Group aggregating cycle:    \n",
    "    for group_index, ser_group_info in df_model_mri.iterrows():\n",
    "        group_code = ser_group_info['Asset Code']\n",
    "#        group_weight = ser_group_info['Factor Weights']\n",
    "        ### Loading group z score matrix from HDF5 file:\n",
    "        df_group_z_matrix = pd.read_hdf(hdf_z_matrix_path, group_code)\n",
    "        df_group_z_matrix.set_index(0, drop = True, inplace = True)\n",
    "        df_group_z_matrix.columns = df_group_z_matrix.index\n",
    "#        ### Initialising not np.NaN weights sum DataFrame:\n",
    "#        if (group_index == 0):\n",
    "#            df_group_weights = pd.DataFrame(np.zeros(df_group_z_matrix.shape), index = df_group_z_matrix.index, columns = df_group_z_matrix.columns)\n",
    "#        ### Adding weighted matrix to container:\n",
    "#        arr_matrix_container.append(df_group_z_matrix * group_weight)\n",
    "        dict_group_matrix_container[group_index] = df_group_z_matrix\n",
    "#        arr_group_codes.append(group_code)\n",
    "#        # Adding not np.NaN weights to aggregated DataFrame:\n",
    "#        df_group_weights = df_group_weights + df_group_z_matrix.notna() * group_weight\n",
    "        print('aggregate_mri_data: group', group_code, 'z matrix data extracted successfully')\n",
    "    ### Calculating mean matrix for MRI from group matrices:        \n",
    "#    df_MRI_mean = pd.concat(arr_matrix_container, axis = 0, keys = arr_group_codes, names = ['Group Code', 'Date'], copy = False) \n",
    "#    df_MRI_mean = df_MRI_mean.sum(level = 1)\n",
    "#    df_group_weights[df_group_weights == 0] = np.NaN\n",
    "#    df_MRI_mean = df_MRI_mean / df_group_weights   \n",
    "    df_group_mean = pd.concat(dict_group_matrix_container, axis = 0, names = ['Group', 'Date'], copy = False)\n",
    "#    df_group_mean = ser_group_mean.to_frame().unstack(level = -1)\n",
    "#    df_MRI_mean = df_group_mean.groupby('Date').mean()\n",
    "    df_group_mean = df_group_mean.groupby(['Date']).mean()    \n",
    "    print('aggregate_mri_data: groups z matrices data aggregated successfully')    \n",
    "    ### Calculating z matrix for MRI with insorization:    \n",
    "    df_mri_z_score = pd.DataFrame(np.NaN, index = df_group_mean.index, columns = df_group_mean.columns)\n",
    "    for iter_date in df_group_mean.columns:\n",
    "        if (iter_date >= pd.Timestamp(date_start)):\n",
    "            ser_iter_mri = df_group_mean.loc[iter_date - pd.offsets.BusinessDay(mri_window_max) : iter_date, iter_date]\n",
    "            ser_iter_z_score = (ser_iter_mri - ser_iter_mri.mean()) / ser_iter_mri.std()\n",
    "            ### Winsorization process:\n",
    "            bool_to_winsor = True            \n",
    "            while (bool_to_winsor):       \n",
    "                ### Value based winsorization:\n",
    "                ser_iter_z_score.clip(lower = winsor_bottom, upper = winsor_top, inplace = True)\n",
    "                ### Recalculating of z scores:\n",
    "                ser_iter_z_score = (ser_iter_z_score - ser_iter_z_score.mean()) / ser_iter_z_score.std()                \n",
    "                ### Checking for boundaries:\n",
    "                if (ser_iter_z_score[(ser_iter_z_score < winsor_bottom) & (ser_iter_z_score > winsor_top)].size == 0):\n",
    "                    bool_to_winsor = False    \n",
    "            df_mri_z_score.loc[iter_date - pd.offsets.BusinessDay(mri_window_max) : iter_date, iter_date] = ser_iter_z_score.values\n",
    "    ser_mri_z_diag = pd.Series(list(np.diag(df_mri_z_score)), index = df_mri_z_score.index) \n",
    "    ser_mri_z_diag.name = 'MRI-Z'\n",
    "    print('aggregate_mri_data:  MRI z matrix builded successfully')             \n",
    "    ### Calculating z matrix for MRI with insorization:       \n",
    "    ser_mri_released = pd.Series(np.NaN, index = df_mri_z_score.index)\n",
    "    for iter_date in df_mri_z_score.columns:\n",
    "        ser_mri_released.loc[iter_date] = df_mri_z_score.loc[iter_date -  pd.offsets.BusinessDay(ma_max_wnd) : iter_date, iter_date].mean()    \n",
    "    ser_mri_released.name = 'MRI-Z-Winsor-MA5'        \n",
    "    print('aggregate_mri_data: MRI moving average resulting vector builded successfully')     \n",
    "    return [ser_mri_z_diag, ser_mri_released]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate_mri_data: group EQ z matrix data extracted successfully\n",
      "aggregate_mri_data: group FI z matrix data extracted successfully\n",
      "aggregate_mri_data: group FX z matrix data extracted successfully\n",
      "aggregate_mri_data: groups z matrices data aggregated successfully\n",
      "aggregate_mri_data:  MRI z matrix builded successfully\n",
      "aggregate_mri_data: MRI moving average resulting vector builded successfully\n"
     ]
    }
   ],
   "source": [
    "### BUILDING MRI INDEX\n",
    "#[ser_MRI_mean_diag, ser_MRI_released_diag] = aggregate_mri_data(df_model_mri, path_mri_standart_hdf, mri_window_min, mri_window_max, mri_moving_average_window_max)\n",
    "[ser_mri_z_diag, ser_mri_released] = aggregate_mri_data(df_model_mri, path_mri_standart_hdf, date_start, \n",
    "                                                           mri_window_max, mri_moving_average_window_max, arr_winsor_boundary)\n",
    "### Saving results for groups to HDF5 to avoid hard calculations with constant source model and datasets:\n",
    "import tables\n",
    "tables.file._open_files.close_all()\n",
    "ser_mri_z_diag.to_hdf(path_mri_index_hdf, key = object_diag_mri_hdf, mode = 'w', format = 'fixed')\n",
    "ser_mri_released.to_hdf(path_mri_index_hdf, key = object_released_mri_hdf, mode = 'a', format = 'fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ser_standartized_assets.loc[['iv_eem', 'iv_eu', 'iv_jp','iv_rvx', 'iv_uk', 'iv_us'], :].groupby('Date').mean().tail())\n",
    "print(ser_standartized_assets.loc[['fx_chf', 'fx_eur', 'fx_gbp', 'fx_jpy'], :].groupby('Date').mean().tail())\n",
    "print(ser_standartized_assets.loc[['oas_hy', 'oas_em'], :].groupby('Date').mean().tail())\n",
    "print(ser_diag_mean_z_groups['EQ'].tail())\n",
    "print(ser_diag_mean_z_groups['FX'].tail())\n",
    "print(ser_diag_mean_z_groups['FI'].tail())\n",
    "print(ser_mri_mean_diag.tail())\n",
    "print(ser_mri_released.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_standartized_assets = pd.read_hdf(path_mri_assets_hdf, key = object_standartized_data_hdf)\n",
    "ser_diag_mean_z_groups = pd.read_hdf(path_mri_groups_hdf, key = object_diag_grouped_hdf)\n",
    "ser_mri_z_diag = pd.read_hdf(path_mri_index_hdf, key = object_diag_mri_hdf)\n",
    "ser_mri_released = pd.read_hdf(path_mri_index_hdf, key = object_released_mri_hdf)\n",
    "path_test_xlsx = 'Data_Files/Test_Files/mri_common_test.xlsx'\n",
    "with pd.ExcelWriter(path_test_xlsx) as writer:  \n",
    "    ser_standartized_assets.unstack(level = 0).to_excel(writer, sheet_name = 'Assets')\n",
    "    ser_diag_mean_z_groups.unstack(level = 0).to_excel(writer, sheet_name = 'Groups')\n",
    "    ser_mri_z_diag.to_excel(writer, sheet_name = 'Groups mean')\n",
    "    ser_mri_released.to_excel(writer, sheet_name = 'Released MRI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Group</th>\n",
       "      <th>EQ</th>\n",
       "      <th>FI</th>\n",
       "      <th>FX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-02-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-20</th>\n",
       "      <td>-0.001369</td>\n",
       "      <td>-0.293246</td>\n",
       "      <td>-0.642191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-21</th>\n",
       "      <td>-0.195554</td>\n",
       "      <td>-0.312917</td>\n",
       "      <td>-0.686091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-22</th>\n",
       "      <td>-0.176744</td>\n",
       "      <td>-0.312890</td>\n",
       "      <td>-0.716364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-23</th>\n",
       "      <td>-0.161919</td>\n",
       "      <td>-0.287714</td>\n",
       "      <td>-0.743787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-26</th>\n",
       "      <td>-0.247498</td>\n",
       "      <td>-0.291047</td>\n",
       "      <td>-0.757428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-27</th>\n",
       "      <td>-0.307348</td>\n",
       "      <td>-0.269228</td>\n",
       "      <td>-0.749248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-28</th>\n",
       "      <td>-0.289299</td>\n",
       "      <td>-0.289608</td>\n",
       "      <td>-0.748072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-29</th>\n",
       "      <td>-0.311861</td>\n",
       "      <td>-0.306629</td>\n",
       "      <td>-0.706986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-30</th>\n",
       "      <td>-0.330265</td>\n",
       "      <td>-0.306602</td>\n",
       "      <td>-0.721089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-03</th>\n",
       "      <td>-0.588743</td>\n",
       "      <td>-0.343334</td>\n",
       "      <td>-0.716510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-04</th>\n",
       "      <td>-0.294934</td>\n",
       "      <td>-0.309617</td>\n",
       "      <td>-0.652132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-05</th>\n",
       "      <td>-0.182174</td>\n",
       "      <td>-0.309591</td>\n",
       "      <td>-0.654466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-06</th>\n",
       "      <td>0.090058</td>\n",
       "      <td>-0.238818</td>\n",
       "      <td>-0.517192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-07</th>\n",
       "      <td>0.139140</td>\n",
       "      <td>-0.241870</td>\n",
       "      <td>-0.508219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-10</th>\n",
       "      <td>0.207098</td>\n",
       "      <td>-0.221431</td>\n",
       "      <td>-0.419844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-11</th>\n",
       "      <td>0.015191</td>\n",
       "      <td>-0.244206</td>\n",
       "      <td>-0.493525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-12</th>\n",
       "      <td>-0.128223</td>\n",
       "      <td>-0.275509</td>\n",
       "      <td>-0.534821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-13</th>\n",
       "      <td>-0.203158</td>\n",
       "      <td>-0.287089</td>\n",
       "      <td>-0.740203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-14</th>\n",
       "      <td>-0.156187</td>\n",
       "      <td>-0.264261</td>\n",
       "      <td>-0.744306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-17</th>\n",
       "      <td>0.054723</td>\n",
       "      <td>-0.234297</td>\n",
       "      <td>-0.733866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-18</th>\n",
       "      <td>0.163321</td>\n",
       "      <td>-0.184604</td>\n",
       "      <td>-0.710279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-19</th>\n",
       "      <td>0.146642</td>\n",
       "      <td>-0.149183</td>\n",
       "      <td>-0.752246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-20</th>\n",
       "      <td>0.350570</td>\n",
       "      <td>-0.067572</td>\n",
       "      <td>-0.590270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-21</th>\n",
       "      <td>0.459752</td>\n",
       "      <td>-0.044047</td>\n",
       "      <td>-0.576329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>0.459913</td>\n",
       "      <td>-0.044033</td>\n",
       "      <td>-0.576331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>0.460074</td>\n",
       "      <td>-0.044018</td>\n",
       "      <td>-0.576333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>0.460234</td>\n",
       "      <td>-0.044004</td>\n",
       "      <td>-0.576334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>0.460394</td>\n",
       "      <td>-0.043989</td>\n",
       "      <td>-0.576335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>0.460553</td>\n",
       "      <td>-0.043975</td>\n",
       "      <td>-0.576336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>0.460712</td>\n",
       "      <td>-0.043960</td>\n",
       "      <td>-0.576337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7566 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Group             EQ        FI        FX\n",
       "Date                                    \n",
       "1990-01-01       NaN       NaN       NaN\n",
       "1990-01-02       NaN       NaN       NaN\n",
       "1990-01-03       NaN       NaN       NaN\n",
       "1990-01-04       NaN       NaN       NaN\n",
       "1990-01-05       NaN       NaN       NaN\n",
       "1990-01-08       NaN       NaN       NaN\n",
       "1990-01-09       NaN       NaN       NaN\n",
       "1990-01-10       NaN       NaN       NaN\n",
       "1990-01-11       NaN       NaN       NaN\n",
       "1990-01-12       NaN       NaN       NaN\n",
       "1990-01-15       NaN       NaN       NaN\n",
       "1990-01-16       NaN       NaN       NaN\n",
       "1990-01-17       NaN       NaN       NaN\n",
       "1990-01-18       NaN       NaN       NaN\n",
       "1990-01-19       NaN       NaN       NaN\n",
       "1990-01-22       NaN       NaN       NaN\n",
       "1990-01-23       NaN       NaN       NaN\n",
       "1990-01-24       NaN       NaN       NaN\n",
       "1990-01-25       NaN       NaN       NaN\n",
       "1990-01-26       NaN       NaN       NaN\n",
       "1990-01-29       NaN       NaN       NaN\n",
       "1990-01-30       NaN       NaN       NaN\n",
       "1990-01-31       NaN       NaN       NaN\n",
       "1990-02-01       NaN       NaN       NaN\n",
       "1990-02-02       NaN       NaN       NaN\n",
       "1990-02-05       NaN       NaN       NaN\n",
       "1990-02-06       NaN       NaN       NaN\n",
       "1990-02-07       NaN       NaN       NaN\n",
       "1990-02-08       NaN       NaN       NaN\n",
       "1990-02-09       NaN       NaN       NaN\n",
       "...              ...       ...       ...\n",
       "2018-11-20 -0.001369 -0.293246 -0.642191\n",
       "2018-11-21 -0.195554 -0.312917 -0.686091\n",
       "2018-11-22 -0.176744 -0.312890 -0.716364\n",
       "2018-11-23 -0.161919 -0.287714 -0.743787\n",
       "2018-11-26 -0.247498 -0.291047 -0.757428\n",
       "2018-11-27 -0.307348 -0.269228 -0.749248\n",
       "2018-11-28 -0.289299 -0.289608 -0.748072\n",
       "2018-11-29 -0.311861 -0.306629 -0.706986\n",
       "2018-11-30 -0.330265 -0.306602 -0.721089\n",
       "2018-12-03 -0.588743 -0.343334 -0.716510\n",
       "2018-12-04 -0.294934 -0.309617 -0.652132\n",
       "2018-12-05 -0.182174 -0.309591 -0.654466\n",
       "2018-12-06  0.090058 -0.238818 -0.517192\n",
       "2018-12-07  0.139140 -0.241870 -0.508219\n",
       "2018-12-10  0.207098 -0.221431 -0.419844\n",
       "2018-12-11  0.015191 -0.244206 -0.493525\n",
       "2018-12-12 -0.128223 -0.275509 -0.534821\n",
       "2018-12-13 -0.203158 -0.287089 -0.740203\n",
       "2018-12-14 -0.156187 -0.264261 -0.744306\n",
       "2018-12-17  0.054723 -0.234297 -0.733866\n",
       "2018-12-18  0.163321 -0.184604 -0.710279\n",
       "2018-12-19  0.146642 -0.149183 -0.752246\n",
       "2018-12-20  0.350570 -0.067572 -0.590270\n",
       "2018-12-21  0.459752 -0.044047 -0.576329\n",
       "2018-12-24  0.459913 -0.044033 -0.576331\n",
       "2018-12-25  0.460074 -0.044018 -0.576333\n",
       "2018-12-26  0.460234 -0.044004 -0.576334\n",
       "2018-12-27  0.460394 -0.043989 -0.576335\n",
       "2018-12-28  0.460553 -0.043975 -0.576336\n",
       "2018-12-31  0.460712 -0.043960 -0.576337\n",
       "\n",
       "[7566 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
