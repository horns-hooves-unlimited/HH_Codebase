{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing standard modules and date-special modules:\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACTING UNIVERSE DATA FROM GENERAL MS EXCEL SOURCE\n",
    "def get_market_membership_from_excel():\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    path_msci_data = 'Data_Files/Source_Files/sample_data_gaps.xlsx'\n",
    "    tab_monthly = 'monthly_data'    \n",
    "    arr_markets_needed = ['DM', 'FM', 'EM']   \n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM'}\n",
    "    ### Extracting universe data:\n",
    "    df_universe = pd.read_excel(io = path_msci_data, sheet_name = tab_monthly, skiprows = [0, 2], header = 0,\n",
    "                                na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'])\n",
    "    df_universe = df_universe.loc[:, ['dates', 'region', 'ctry']]\n",
    "    df_universe.columns = ['Date', 'Market', 'Code']\n",
    "    df_universe.set_index(['Code', 'Date'], inplace = True)\n",
    "    ser_universe = df_universe.squeeze()\n",
    "    ser_universe.sort_index(level = [0, 1], inplace = True)\n",
    "    ser_universe.replace(dict_markets, inplace = True)\n",
    "    ser_market_membership = ser_universe[ser_universe.isin(arr_markets_needed)]\n",
    "    \n",
    "    return ser_market_membership\n",
    "\n",
    "### EXTRACTING RETURNS DATA FROM GENERAL MS EXCEL SOURCE\n",
    "def get_universe_returns_from_excel():\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    from datetime import date\n",
    "    ### Constants declaring:\n",
    "    path_msci_data = 'Data_Files/Source_Files/sample_data_gaps.xlsx'\n",
    "    tab_daily = 'daily_returns'\n",
    "    date_first = date(1992, 1, 1)\n",
    "    date_last = date(2018, 12, 31)\n",
    "    index_dates = pd.date_range(date_first, date_last, freq = 'B')    \n",
    "    ### Extracting returns data:\n",
    "    df_returns = pd.read_excel(io = path_msci_data, sheet_name = tab_daily, skiprows = [0, 2], header = 0,\n",
    "                               na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                            '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'])\n",
    "    df_returns = df_returns.loc[:, ['dates', 'ctry', 'retusd', 'retloc']]\n",
    "    df_returns.columns = ['Date', 'Code', 'Ret_USD', 'Ret_LOC']\n",
    "    df_returns.set_index(['Code', 'Date'], inplace = True)\n",
    "    df_returns.sort_index(level = [0, 1], inplace = True)\n",
    "    ser_realized_ret_USD = df_returns['Ret_USD'].copy()\n",
    "    ser_realized_ret_LOC = df_returns['Ret_LOC'].copy()\n",
    "    ### Reindexation and forward filling procedure for USD returns:\n",
    "    dict_realized_ret_USD = {}\n",
    "    dict_index_ret_USD = {}\n",
    "    ser_ret_index_USD = pd.Series(np.NaN, index = ser_realized_ret_USD.index)\n",
    "    for iter_country in ser_realized_ret_USD.index.get_level_values(0).unique():\n",
    "        ser_ret_index_USD[iter_country] = (1 + ser_realized_ret_USD[iter_country]).cumprod()\n",
    "        ser_ret_index_USD[iter_country].iloc[0] = 1\n",
    "        ser_ret_index_USD_iter = ser_ret_index_USD[iter_country].reindex(index_dates, method = 'ffill')\n",
    "        ser_ret_index_USD_iter.fillna(method = 'ffill', inplace = True)  \n",
    "        dict_index_ret_USD[iter_country] = ser_ret_index_USD_iter\n",
    "        ser_realized_ret_USD_iter = (ser_ret_index_USD_iter / ser_ret_index_USD_iter.shift(1) - 1)\n",
    "        dict_realized_ret_USD[iter_country] = ser_realized_ret_USD_iter\n",
    "    ser_realized_ret_USD = pd.concat(dict_realized_ret_USD)  \n",
    "    ser_realized_ret_USD.index.names = ['Code', 'Date']\n",
    "    ser_realized_ret_USD.sort_index(level = [0, 1], inplace = True)\n",
    "    ser_index_ret_USD = pd.concat(dict_index_ret_USD)  \n",
    "    ser_index_ret_USD.index.names = ['Code', 'Date']\n",
    "    ser_index_ret_USD.sort_index(level = [0, 1], inplace = True)    \n",
    "    ### Reindexation and forward filling procedure for LOC returns:\n",
    "    dict_realized_ret_LOC = {}\n",
    "    dict_index_ret_LOC = {}    \n",
    "    ser_ret_index_LOC = pd.Series(np.NaN, index = ser_realized_ret_LOC.index)\n",
    "    for iter_country in ser_realized_ret_LOC.index.get_level_values(0).unique():\n",
    "        ser_ret_index_LOC[iter_country] = (1 + ser_realized_ret_LOC[iter_country]).cumprod()\n",
    "        ser_ret_index_LOC[iter_country].iloc[0] = 1   \n",
    "        ser_ret_index_LOC_iter = ser_ret_index_LOC[iter_country].reindex(index_dates, method = 'ffill')\n",
    "        ser_ret_index_LOC_iter.fillna(method = 'ffill', inplace = True)\n",
    "        dict_index_ret_LOC[iter_country] = ser_ret_index_LOC_iter        \n",
    "        ser_realized_ret_LOC_iter = (ser_ret_index_LOC_iter / ser_ret_index_LOC_iter.shift(1) - 1)   \n",
    "        dict_realized_ret_LOC[iter_country] = ser_realized_ret_LOC_iter\n",
    "    ser_realized_ret_LOC = pd.concat(dict_realized_ret_LOC)    \n",
    "    ser_realized_ret_LOC.index.names = ['Code', 'Date']\n",
    "    ser_realized_ret_LOC.sort_index(level = [0, 1], inplace = True)\n",
    "    ser_index_ret_LOC = pd.concat(dict_index_ret_LOC)  \n",
    "    ser_index_ret_LOC.index.names = ['Code', 'Date']\n",
    "    ser_index_ret_LOC.sort_index(level = [0, 1], inplace = True)  \n",
    "    \n",
    "    return [ser_realized_ret_USD, ser_realized_ret_LOC, ser_index_ret_USD, ser_index_ret_LOC]\n",
    "\n",
    "### EXTRACTING IMPLIED VOLATILITY DATA AND VRP FACTOR DATA FROM GENERAL MS EXCEL SOURCE\n",
    "def get_universe_ivol_from_excel():\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    from datetime import date    \n",
    "    ### Constants declaring:\n",
    "    path_msci_data = 'Data_Files/Source_Files/sample_data_gaps.xlsx'\n",
    "    tab_ivol = 'ivol_data'\n",
    "    date_first = date(1992, 1, 1)\n",
    "    date_last = date(2018, 12, 31)\n",
    "    index_dates = pd.date_range(date_first, date_last, freq = 'B')    \n",
    "    ### Extracting ivol data:\n",
    "    df_ivol = pd.read_excel(io = path_msci_data, sheet_name = tab_ivol, skiprows = [0, 2], header = 0,\n",
    "                            na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                         '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'])\n",
    "    df_ivol = df_ivol.loc[:, ['dates', 'ctry', 'ivol3m', 'vrp3m']]\n",
    "    df_ivol.columns = ['Date', 'Code', 'IVol_3m', 'VRP_3m']\n",
    "    df_ivol.set_index(['Code', 'Date'], inplace = True)\n",
    "    df_ivol.sort_index(level = [0, 1], inplace = True)\n",
    "    ser_ivol3m = df_ivol['IVol_3m']\n",
    "    ser_vrp3m = df_ivol['VRP_3m']    \n",
    "    ### Reindexation and forward filling procedure for implied volatlity variables:\n",
    "    dict_ivol3m = {}\n",
    "    for iter_country in ser_ivol3m.index.get_level_values(0).unique():  \n",
    "        ser_ivol_iter = ser_ivol3m[iter_country].reindex(index_dates, method = 'ffill')\n",
    "        ser_ivol_iter.fillna(method = 'ffill', inplace = True)\n",
    "        dict_ivol3m[iter_country] = ser_ivol_iter\n",
    "    ser_ivol3m = pd.concat(dict_ivol3m)    \n",
    "    ser_ivol3m.index.names = ['Code', 'Date']\n",
    "    ser_ivol3m.sort_index(level = [0, 1], inplace = True)\n",
    "    dict_vrp3m = {}\n",
    "    for iter_country in ser_vrp3m.index.get_level_values(0).unique():    \n",
    "        ser_vrp_iter = ser_vrp3m[iter_country].reindex(index_dates, method = 'ffill')\n",
    "        ser_vrp_iter.fillna(method = 'ffill', inplace = True)\n",
    "        dict_vrp3m[iter_country] = ser_vrp_iter\n",
    "    ser_vrp3m = pd.concat(dict_vrp3m)    \n",
    "    ser_vrp3m.index.names = ['Code', 'Date']\n",
    "    ser_vrp3m.sort_index(level = [0, 1], inplace = True)\n",
    "    \n",
    "    return [ser_ivol3m, ser_vrp3m]\n",
    "\n",
    "### EXTRACTING MRI INDEX FROM HDF5 SOURCE\n",
    "def get_gri_from_hdf():\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Constants declaring:\n",
    "    path_gri_index_hdf = 'Data_Files/Source_Files/gri_released_index.h5'\n",
    "    gri_vector_key = 'gri_vector_key'\n",
    "    gri_start_key = 'gri_start_key'  \n",
    "    gri_level_perc_key = 'gri_level_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    gri_momentum_perc_key = 'gri_momentum_perc_key' ### ADDED FOR BETA FACTORS CALCULATION    \n",
    "    ### Extracting GRI:\n",
    "    ser_gri_released = pd.read_hdf(path_gri_index_hdf, gri_vector_key)\n",
    "    ser_gri_start_col = pd.read_hdf(path_gri_index_hdf, gri_start_key)\n",
    "    ser_gri_level_perc = pd.read_hdf(path_gri_index_hdf, gri_level_perc_key) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    ser_gri_momentum_perc = pd.read_hdf(path_gri_index_hdf, gri_momentum_perc_key) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "    return [ser_gri_released, ser_gri_start_col, ser_gri_level_perc, ser_gri_momentum_perc] ### CHANGED FOR BETA FACTORS CALCULATION\n",
    "\n",
    "### EXTRACTING SOURCE DATA FROM MS EXCEL FILES AND SAVING TO HDF FILES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "### Extracting data from xlsx files\n",
    "ser_market_membership = get_market_membership_from_excel()\n",
    "[ser_realized_ret_USD, ser_realized_ret_LOC, ser_index_ret_USD, ser_index_ret_LOC] = get_universe_returns_from_excel()\n",
    "[ser_ivol3m, ser_vrp3m] = get_universe_ivol_from_excel()\n",
    "[ser_gri_released, ser_gri_start_col, ser_gri_level_perc, ser_gri_momentum_perc] = get_gri_from_hdf() ### CHANGED FOR BETA FACTORS CALCULATION\n",
    "ser_gri_released.name = 'GRI'\n",
    "ser_gri_start_col.name = 'GRI'\n",
    "### Declaring constants:\n",
    "path_market_risk_source_hdf = 'Data_Files/Source_Files/market_risk_source.h5'\n",
    "market_membership_key = 'market_membership_key'\n",
    "realized_ret_USD_key = 'realized_ret_USD_key'\n",
    "realized_ret_LOC_key = 'realized_ret_LOC_key'\n",
    "index_ret_USD_key = 'index_ret_USD_key'\n",
    "index_ret_LOC_key = 'index_ret_LOC_key'\n",
    "ivol3m_key = 'ivol3m_key'\n",
    "vrp3m_key = 'vrp3m_key'\n",
    "gri_released_key = 'gri_released_key'\n",
    "gri_start_key = 'gri_start_key'\n",
    "gri_level_perc_key = 'gri_level_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "gri_momentum_perc_key = 'gri_momentum_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "### close files that have been previously opened\n",
    "import tables\n",
    "tables.file._open_files.close_all()\n",
    "### Saving data to hdf5 fixed formatted files:\n",
    "ser_market_membership.to_hdf(path_market_risk_source_hdf, market_membership_key, mode = 'w', format = 'fixed')\n",
    "ser_realized_ret_USD.to_hdf(path_market_risk_source_hdf, realized_ret_USD_key, mode = 'a', format = 'fixed')\n",
    "ser_realized_ret_LOC.to_hdf(path_market_risk_source_hdf, realized_ret_LOC_key, mode = 'a', format = 'fixed')\n",
    "ser_index_ret_USD.to_hdf(path_market_risk_source_hdf, index_ret_USD_key, mode = 'a', format = 'fixed')\n",
    "ser_index_ret_LOC.to_hdf(path_market_risk_source_hdf, index_ret_LOC_key, mode = 'a', format = 'fixed')\n",
    "ser_ivol3m.to_hdf(path_market_risk_source_hdf, ivol3m_key, mode = 'a', format = 'fixed')\n",
    "ser_vrp3m.to_hdf(path_market_risk_source_hdf, vrp3m_key, mode = 'a', format = 'fixed')\n",
    "ser_gri_released.to_hdf(path_market_risk_source_hdf, gri_released_key, mode = 'a', format = 'fixed')\n",
    "ser_gri_start_col.to_hdf(path_market_risk_source_hdf, gri_start_key, mode = 'a', format = 'fixed')\n",
    "ser_gri_level_perc.to_hdf(path_market_risk_source_hdf, gri_level_perc_key, mode = 'a', format = 'fixed') ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_gri_momentum_perc.to_hdf(path_market_risk_source_hdf, gri_momentum_perc_key, mode = 'a', format = 'fixed') ### ADDED FOR BETA FACTORS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACTING PREVIOUSLY SAVED DATA FROM HDF5 FILE\n",
    "### Declaring constants:\n",
    "path_market_risk_source_hdf = 'Data_Files/Source_Files/market_risk_source.h5'\n",
    "market_membership_key = 'market_membership_key'\n",
    "realized_ret_USD_key = 'realized_ret_USD_key'\n",
    "realized_ret_LOC_key = 'realized_ret_LOC_key'\n",
    "index_ret_USD_key = 'index_ret_USD_key'\n",
    "index_ret_LOC_key = 'index_ret_LOC_key'\n",
    "ivol3m_key = 'ivol3m_key'\n",
    "vrp3m_key = 'vrp3m_key'\n",
    "path_gri_index_hdf = 'Data_Files/Source_Files/gri_released_index.h5'\n",
    "gri_released_key = 'gri_released_key'\n",
    "gri_start_key = 'gri_start_key'\n",
    "gri_level_perc_key = 'gri_level_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "gri_momentum_perc_key = 'gri_momentum_perc_key' ### ADDED FOR BETA FACTORS CALCULATION\n",
    "### Exporting data from hdf5 fixed formatted files:\n",
    "ser_market_membership = pd.read_hdf(path_market_risk_source_hdf, market_membership_key)\n",
    "ser_realized_ret_USD = pd.read_hdf(path_market_risk_source_hdf, realized_ret_USD_key)\n",
    "ser_realized_ret_LOC = pd.read_hdf(path_market_risk_source_hdf, realized_ret_LOC_key)\n",
    "ser_index_ret_USD = pd.read_hdf(path_market_risk_source_hdf, index_ret_USD_key)\n",
    "ser_index_ret_LOC = pd.read_hdf(path_market_risk_source_hdf, index_ret_LOC_key)\n",
    "ser_ivol3m = pd.read_hdf(path_market_risk_source_hdf, ivol3m_key)\n",
    "ser_vrp3m = pd.read_hdf(path_market_risk_source_hdf, vrp3m_key)\n",
    "ser_gri_released = pd.read_hdf(path_market_risk_source_hdf, gri_released_key)\n",
    "ser_gri_start_col = pd.read_hdf(path_market_risk_source_hdf, gri_start_key)\n",
    "ser_gri_level_perc = pd.read_hdf(path_market_risk_source_hdf, gri_level_perc_key) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_gri_momentum_perc = pd.read_hdf(path_market_risk_source_hdf, gri_momentum_perc_key) ### ADDED FOR BETA FACTORS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRACTING UNIVERSE DATA FOR A PARTICULAR DATE\n",
    "def get_date_membership(iter_date):\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Defining constants:    \n",
    "    path_market_risk_source_hdf = 'Data_Files/Source_Files/market_risk_source.h5'\n",
    "    market_membership_key = 'market_membership_key'  \n",
    "    ### Preparing data for universe filtering:\n",
    "    if (pd.to_datetime(iter_date) == pd.to_datetime(iter_date - pd.offsets.BusinessMonthEnd(0))):\n",
    "        iter_month_end = iter_date\n",
    "    else: \n",
    "        iter_month_end = pd.to_datetime(iter_date - pd.offsets.BusinessMonthEnd(1))    \n",
    "#    ser_iter_membership = pd.read_hdf(path_market_risk_source_hdf, market_membership_key, where = 'Date = iter_month_end')\n",
    "    ser_iter_membership = ser_market_membership.loc[:, iter_month_end : iter_month_end]\n",
    "    ser_iter_membership.rename(index = {iter_month_end : iter_date}, inplace = True)\n",
    "    \n",
    "    return ser_iter_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHTS GENERATOR\n",
    "def get_exp_weights(window_years = 5, halflife_months = 3):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_year_months = 12\n",
    "    ### Array of regressioon window day numbers descending:\n",
    "    arr_weight_days = np.arange(num_year_work_days * window_years, 0, -1) - 1\n",
    "    ### Creating weights series:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round((num_year_work_days / num_year_months * halflife_months)))\n",
    "    arr_weights = np.exp(math.log(num_period_factor) * arr_weight_days)\n",
    "    ser_weights = pd.Series(arr_weights)        \n",
    "    ser_weights.name = 'Weight'\n",
    "    \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTS TO SERIES BINDER\n",
    "def bind_exp_weights(ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Creating weights series:\n",
    "    if (weighting_kind == 'equal'):\n",
    "        ser_weights = pd.Series(1, index = ser_returns.index)\n",
    "    if (weighting_kind == 'expo'):       \n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]\n",
    "        ser_weights.index = ser_returns.index\n",
    "    if (weighting_kind == 'expo_cond'):\n",
    "        ser_condition = abs(ser_condition - ser_condition.iloc[-1])\n",
    "        ser_condition = ser_condition.sort_values(ascending = False)\n",
    "        ser_weights = get_exp_weights(window_years, halflife_months)[- ser_returns.size : ]\n",
    "        ser_weights = pd.Series(ser_weights.values, ser_condition.index)\n",
    "        ser_weights.sort_index(inplace = True)\n",
    "        ser_weights.name = 'Weight'\n",
    "        \n",
    "    return ser_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL VOLATILITY CALCULATOR\n",
    "def get_expvol_value(ser_returns, ser_weights):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Exponential volatility calculating:\n",
    "    expvol_result = np.NaN\n",
    "    ser_returns = ser_returns.dropna()\n",
    "    index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "    ### Exponential volatility calculating:\n",
    "    expvol_y = ser_returns[index_rolling]\n",
    "    expvol_w = ser_weights[index_rolling]             \n",
    "    expvol_w = expvol_w / expvol_w.sum()\n",
    "    expvol_result = np.sqrt(expvol_w.dot(expvol_y * expvol_y))\n",
    "        \n",
    "    return expvol_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL VOLATILITY SERIES BUILDER\n",
    "def get_expvol_series(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    ### Flattening MSCI changes by logarythm\n",
    "    ser_returns = np.log(1 + ser_returns)\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)\n",
    "    ### Main loop performing:\n",
    "    ser_expvol = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.mean()\n",
    "                if (ser_iter_returns.size > 0):\n",
    "                    if (ser_condition.count() > 0):\n",
    "                        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                    else:\n",
    "                        ser_iter_condition = pd.Series(np.NaN)                     \n",
    "                    ser_iter_weights = bind_exp_weights(ser_iter_returns, weighting_kind, window_years, halflife_months, ser_iter_condition) ## CHANGES: ADDED       \n",
    "                ser_iter_returns.dropna(inplace = True)\n",
    "                if (ser_iter_returns.count() > num_year_work_days // 2):\n",
    "                    expvol_result = get_expvol_value(ser_iter_returns, ser_iter_weights) * np.sqrt(num_year_work_days)\n",
    "                    ser_expvol.loc[iter_country, iter_date] = expvol_result\n",
    "                    \n",
    "    return ser_expvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING BETA CALCULATOR\n",
    "def get_beta_value(ser_returns, ser_weights, ser_factors):\n",
    "    ### Importing standard and stats modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import statsmodels.api as sm\n",
    "    ### Index intersection:\n",
    "    wls_result = np.NaN\n",
    "    ser_returns = ser_returns.dropna()\n",
    "    index_rolling = ser_returns.index.intersection(ser_factors.index)           \n",
    "    ### Regression performing:   \n",
    "    wls_y = ser_returns[index_rolling].values\n",
    "    wls_x = ser_factors[index_rolling].values\n",
    "    wls_x = sm.add_constant(wls_x)\n",
    "    wls_w = ser_weights[index_rolling].values        \n",
    "    wls_model = sm.WLS(wls_y, wls_x, weights = wls_w)            \n",
    "    wls_regression = wls_model.fit()\n",
    "    wls_result = wls_regression.params[1] \n",
    "    \n",
    "    return wls_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING BETA SERIES BUILDER\n",
    "def get_beta_series(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_factors = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    num_month_work_days = 21\n",
    "    ### Main loop performing:\n",
    "    ser_beta = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "                if (ser_iter_returns.count() > num_year_work_days):\n",
    "                    ser_iter_weights = bind_exp_weights(ser_iter_returns, weighting_kind, window_years, halflife_months)\n",
    "                    ser_iter_returns = ser_iter_returns.dropna()\n",
    "                    ser_iter_factors = ser_factors.loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date].dropna()\n",
    "                    index_iter = ser_iter_returns.index.intersection(ser_iter_factors.index)\n",
    "                    ser_iter_returns = ser_iter_returns[index_iter]\n",
    "                    ser_iter_factors = ser_iter_factors[index_iter]\n",
    "                    ser_iter_weights = ser_iter_weights[index_iter]                    \n",
    "                    if (ser_iter_returns.count() > num_year_work_days):                    \n",
    "                        ser_iter_returns = np.log(1 + ser_iter_returns)\n",
    "                        ser_iter_returns = ser_iter_returns - ser_iter_returns.mean()                   \n",
    "                        ser_iter_factors = ser_iter_factors - ser_iter_factors.mean()\n",
    "                        beta_result = get_beta_value(ser_iter_returns, ser_iter_weights, ser_iter_factors)\n",
    "                        ser_beta.loc[iter_country, iter_date] = beta_result\n",
    "                    \n",
    "    return ser_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SKEWNESS CALCULATOR\n",
    "def get_skewness_value(ser_returns):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy.stats as sc    \n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    ### Skewness calculating:\n",
    "    skewness_result = np.nan\n",
    "    ser_returns = ser_returns.dropna()\n",
    "    if (ser_returns.count() > 0):\n",
    "        skewness_result = sc.skew(ser_returns, bias = False)\n",
    "#    skewness_result = sc.skew(ser_returns, bias = False)\n",
    "    return skewness_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SKEWNESS SERIES BUILDER\n",
    "def get_skewness_series(ser_market_membership, ser_returns, window_years = 2):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    ### Main loop performing:\n",
    "    ser_skewness = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]  \n",
    "                if (ser_iter_returns.count() > num_year_work_days // 2):\n",
    "                    ser_iter_returns.dropna(inplace = True)\n",
    "                    ser_iter_returns = ser_iter_returns.iloc[- num_year_work_days * 2 : ]\n",
    "#                    ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * 2 - 1) : iter_date]     \n",
    "                    skewness_result = get_skewness_value(ser_iter_returns)\n",
    "                    ser_skewness.loc[iter_country, iter_date] = skewness_result\n",
    "\n",
    "    return ser_skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING READY TO GO FACTOR, i.e. VRP, SERIES BUILDER (no modifications to raw data), however, \n",
    "### it check the availability of rolling data to replicate some redundant logic in research code\n",
    "def get_market_series(ser_market_membership, ser_returns, window_years = 5):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    ### Main loop performing:\n",
    "    ser_market = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date].dropna()     \n",
    "                if (ser_iter_returns.count() > num_year_work_days // 4):\n",
    "                    ser_market.loc[iter_country, iter_date] = ser_returns.loc[iter_country, iter_date]\n",
    "\n",
    "    return ser_market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE CALCULATOR\n",
    "def get_average_value(ser_returns, ser_weights):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Rolling average calculating:\n",
    "    average_result = np.NaN  \n",
    "    ser_returns = ser_returns.dropna()\n",
    "    index_rolling = ser_returns.index.intersection(ser_weights.index)           \n",
    "    ### Exponential volatility calculating:\n",
    "    average_x = ser_returns[index_rolling]\n",
    "    average_w = ser_weights[index_rolling]                    \n",
    "    average_result = average_x.dot(average_w) / sum(average_w)        \n",
    "        \n",
    "    return average_result  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE SERIES BUILDER\n",
    "def get_average_series(ser_market_membership, ser_returns, weighting_kind = 'equal', window_years = 5, halflife_months = 3, ser_condition = pd.Series(np.NaN)):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Defining constants:\n",
    "    num_year_work_days = 260\n",
    "    ### Initialising delta series:\n",
    "    ser_condition.fillna(method = 'ffill', inplace = True)        \n",
    "    ### Main loop performing:\n",
    "    ser_average = pd.Series(np.NaN, index = ser_market_membership.index)\n",
    "    for iter_country in ser_market_membership.index.get_level_values(0).unique():        \n",
    "        ### Extracting returns data vector for each country/date point:\n",
    "        if (iter_country in ser_returns.index.get_level_values(0).unique()):\n",
    "            for iter_date in ser_market_membership[iter_country].index.get_level_values(0).unique():\n",
    "                ser_iter_returns = ser_returns[iter_country].loc[iter_date - pd.offsets.BusinessDay(num_year_work_days * window_years - 1) : iter_date]\n",
    "                if (ser_iter_returns.size > 0):\n",
    "                    if (ser_condition.count() > 0):\n",
    "                        ser_iter_condition = ser_condition[ser_iter_returns.index]\n",
    "                    else:\n",
    "                        ser_iter_condition = pd.Series(np.NaN)                      \n",
    "                    ser_iter_weights = bind_exp_weights(ser_iter_returns, weighting_kind, window_years, halflife_months, ser_iter_condition)                 \n",
    "                ser_iter_returns = ser_iter_returns - ser_iter_returns.shift(1)        \n",
    "                ser_iter_returns = ser_iter_returns.dropna()[ser_iter_returns != 0]\n",
    "                if (ser_iter_returns.count() > num_year_work_days // 4):\n",
    "                    average_result = get_average_value(ser_iter_returns, ser_iter_weights)\n",
    "                    ser_average.loc[iter_country, iter_date] = average_result\n",
    "    ser_average.sort_index(level = [0, 1], inplace = True)\n",
    "    \n",
    "    return ser_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET SHORT TERM EVENT RISK FACTOR\n",
    "def get_short_term_event_risk_factor(iter_date):\n",
    "    ### Importing standard modules and date-special modules (common for all factors):\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Constants declaring (common for all factors):  \n",
    "    num_year_work_days = 260\n",
    "    window_years = 5   \n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)\n",
    "    ### Impossible to add country filter here:\n",
    "    ser_iter_returns = ser_realized_ret_LOC.loc[:, index_iter_date]\n",
    "    ### Removing effectively missing return observations    \n",
    "    ser_iter_returns.replace(0, np.nan, inplace = True)\n",
    "    ### Factor calculation:\n",
    "    ser_iter_factor = - get_expvol_series(ser_iter_membership, ser_iter_returns, 'expo', window_years = window_years, halflife_months = 1)\n",
    "            \n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET LOW VOLATILITY ANOMALY FACTOR\n",
    "def get_low_vol_factor(iter_date):\n",
    "    ### Importing standard modules and date-special modules (common for all factors):\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Constants declaring (common for all factors):  \n",
    "    num_year_work_days = 260\n",
    "    window_years = 5       \n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)\n",
    "    ### Impossible to add country filter here:    \n",
    "    ser_iter_returns = ser_realized_ret_LOC.loc[:, index_iter_date]\n",
    "    ### Removing effectively missing return observations    \n",
    "    ser_iter_returns.replace(0, np.nan, inplace = True)    \n",
    "    ### Factor calculation:\n",
    "    ser_expvol24m = get_expvol_series(ser_iter_membership, ser_iter_returns, 'expo', window_years = window_years, halflife_months = 24)\n",
    "    ser_lowvol_base = 1 / (ser_expvol24m * ser_expvol24m)\n",
    "    ser_lowvol_base.replace([np.inf, -np.inf], np.nan, inplace = True)    \n",
    "    ### the following lines are not necessary if factor gets standardized downstream (cross-sectional rescaling on the full universe)\n",
    "    ser_lowvol_base = ser_lowvol_base.swaplevel()\n",
    "    ser_lowvol_base.sort_index(inplace = True)\n",
    "    ser_lowvol = pd.Series(np.NaN, index = ser_lowvol_base.index)\n",
    "    for iter_date in ser_lowvol.index.get_level_values(0).unique():  \n",
    "        ser_lowvol[iter_date] = (ser_lowvol_base[iter_date] / ser_lowvol_base[iter_date].sum())\n",
    "    ser_lowvol = ser_lowvol.swaplevel()\n",
    "    ser_iter_factor = ser_lowvol.sort_index()\n",
    "            \n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET VOLATILITY SURPRISE FACTOR\n",
    "def get_vol_surprise_factor(iter_date):\n",
    "    ### Importing standard modules and date-special modules (common for all factors):\n",
    "    import numpy as np\n",
    "    import pandas as pd   \n",
    "    from datetime import date\n",
    "    ### Global parameters declaring:\n",
    "    date_start = date(1993, 12, 31)     \n",
    "    ### Constants declaring (common for all factors):  \n",
    "    num_year_work_days = 260\n",
    "    window_years = 5        \n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)\n",
    "    ser_iter_returns = ser_realized_ret_LOC.loc[:, index_iter_date]\n",
    "    ### Removing effectively missing return observations    \n",
    "    ser_iter_returns.replace(0, np.nan, inplace = True)    \n",
    "    ser_gri_added = pd.concat([ser_gri_start_col[date_start].loc[ : date_start - pd.offsets.BusinessDay()], ser_gri_released])\n",
    "    ser_iter_condition = ser_gri_added[index_iter_date]\n",
    "    ### Factor calculation:\n",
    "    ser_expvol1m = get_expvol_series(ser_iter_membership, ser_iter_returns, 'expo', window_years = window_years, halflife_months = 1)\n",
    "    ser_expvol1m_cond = get_expvol_series(ser_iter_membership, ser_iter_returns, 'expo_cond', window_years = window_years, halflife_months = 1, \n",
    "                                          ser_condition = ser_iter_condition)\n",
    "    ser_expvol1m_surp = -np.log(ser_expvol1m / ser_expvol1m_cond)\n",
    "    ser_iter_factor = ser_expvol1m_surp \n",
    "    \n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET TAIL RISK FACTOR\n",
    "def get_tail_risk_factor(iter_date):\n",
    "    ### Importing standard modules and date-special modules (common for all factors):\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Constants declaring (common for all factors):  \n",
    "    num_year_work_days = 260\n",
    "    window_years = 5       \n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)\n",
    "    ser_iter_returns = ser_realized_ret_LOC.loc[:, index_iter_date]\n",
    "    ### Removing effectively missing return observations\n",
    "    ser_iter_returns.replace(0, np.nan, inplace = True)    \n",
    "    ### Factor calculation:\n",
    "    ser_iter_factor = - get_skewness_series(ser_iter_membership, ser_iter_returns, window_years = window_years) \n",
    "    \n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET VRP FACTOR\n",
    "def get_vrp_factor(iter_date):\n",
    "    ### Importing standard modules and date-special modules (common for all factors):\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Constants declaring (common for all factors):  \n",
    "    num_year_work_days = 260\n",
    "    window_years = 5    \n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)\n",
    "    ser_iter_vrp = ser_vrp3m.loc[:, index_iter_date]\n",
    "    ### Factor calculation:\n",
    "    ### This factor does not need a 5-year rolling dataset, used to reconcile with some data checks performed in research code   \n",
    "    ser_iter_factor = get_market_series(ser_iter_membership, ser_iter_vrp, window_years = window_years) \n",
    "    \n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET IMPLIED VOLATILITY SHORT TERM MOMENTUM FACTOR\n",
    "def get_ivol_mom_1m_factor(iter_date):\n",
    "    ### Importing standard modules and date-special modules (common for all factors):\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Constants declaring (common for all factors):  \n",
    "    num_year_work_days = 260\n",
    "    window_years = 5     \n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)\n",
    "    ser_iter_ivol = ser_ivol3m.loc[:, index_iter_date]\n",
    "    ### Factor calculation:\n",
    "    ser_iter_factor = get_average_series(ser_iter_membership, ser_iter_ivol, 'expo', window_years = window_years, halflife_months = 1) \n",
    "\n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GET IMPLIED VOLATILITY SHORT TERM MOMENTUM FACTOR\n",
    "def get_ivol_mom_12m_factor(iter_date):\n",
    "    ### Importing standard modules and date-special modules (common for all factors):\n",
    "    import numpy as np\n",
    "    import pandas as pd    \n",
    "    ### Constants declaring (common for all factors):  \n",
    "    num_year_work_days = 260\n",
    "    window_years = 5       \n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    ser_iter_membership = get_date_membership(iter_date)\n",
    "    ser_iter_ivol = ser_ivol3m.loc[:, index_iter_date]\n",
    "    ### Factor calculation:\n",
    "    ser_iter_factor = get_average_series(ser_iter_membership, ser_iter_ivol, 'expo', window_years = window_years, halflife_months = 12) \n",
    "    \n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADDED FOR BETA FACTORS CALCULATION\n",
    "    \n",
    "### GET BETA FACTOR\n",
    "def get_beta_factor(iter_date):\n",
    "    ### Importing standard modules and date-special modules (common for all factors):\n",
    "    import numpy as np\n",
    "    import pandas as pd \n",
    "    from datetime import date\n",
    "    ### Global parameters declaring:\n",
    "    date_start = date(1993, 12, 31)    \n",
    "    ### Constants declaring (common for all factors):  \n",
    "    num_year_work_days = 260\n",
    "    num_month_work_days = 21\n",
    "    window_years = 5 \n",
    "    ### Data preparing (common for all factors):\n",
    "    index_iter_date = pd.date_range(end = iter_date, periods = num_year_work_days * window_years, freq = 'B')\n",
    "    index_iter_date_plus = pd.date_range(end = iter_date, periods = num_year_work_days * window_years + num_month_work_days, freq = 'B')    \n",
    "    ser_iter_membership = get_date_membership(iter_date)\n",
    "    ### Impossible to add country filter here:\n",
    "    ser_iter_returns = ser_monthly_ret_USD.loc[:, index_iter_date]\n",
    "    ### Removing effectively missing return observations    \n",
    "    ser_iter_returns.replace(0, np.nan, inplace = True)\n",
    "    ### Factor calculation:\n",
    "    ser_gri_added = pd.concat([ser_gri_start_col[date_start].loc[ : date_start - pd.offsets.BusinessDay()], ser_gri_released])\n",
    "    ser_iter_gri = ser_gri_released[index_iter_date_plus]\n",
    "    ser_iter_gri = ser_iter_gri - ser_iter_gri.shift(num_month_work_days)\n",
    "    ser_iter_gri = ser_iter_gri[index_iter_date] \n",
    "    ser_iter_factor = get_beta_series(ser_iter_membership, ser_iter_returns, 'equal', window_years = window_years, halflife_months = 24, ser_factors = ser_iter_gri)\n",
    "            \n",
    "    return ser_iter_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR SHORT TERM EVENT RISK FACTOR\n",
    "#date_range_test = pd.date_range(start = '2010-10-25', periods = 6)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 160]\n",
    "arr_short_term_event_risk_factor = []\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    arr_short_term_event_risk_factor.append(get_short_term_event_risk_factor(iter_date))\n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "ser_short_term_event_risk_factor = pd.concat(arr_short_term_event_risk_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_short_term_event_risk_factor - AR 29-Dec-2006: -0.21489979042709245\n",
      "ser_short_term_event_risk_factor - US 29-Dec-2006: -0.07838774324887841\n",
      "ser_short_term_event_risk_factor - cross-sectional mean min: -0.6654721997843369\n",
      "ser_short_term_event_risk_factor - cross-sectional mean mean: -0.213007981528966\n",
      "ser_short_term_event_risk_factor - cross-sectional mean max: -0.11613865884310284\n",
      "ser_short_term_event_risk_factor - cross-sectional mean stdev: 0.07230243438728112\n",
      "ser_short_term_event_risk_factor - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### SHORT TERM EVENT RISK FACTOR TESTING:\n",
    "print('ser_short_term_event_risk_factor - AR 29-Dec-2006:', ser_short_term_event_risk_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_short_term_event_risk_factor - US 29-Dec-2006:', ser_short_term_event_risk_factor.loc['US' , '2006-12-29'])\n",
    "ser_short_term_event_risk_factor_mean = pd.Series(np.NaN, index = ser_short_term_event_risk_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_short_term_event_risk_factor_mean.index:  \n",
    "    ser_short_term_event_risk_factor_mean[iter_date] = ser_short_term_event_risk_factor.loc[:, iter_date].mean()\n",
    "ser_short_term_event_risk_factor_mean.sort_index(inplace = True)\n",
    "print('ser_short_term_event_risk_factor - cross-sectional mean min:', ser_short_term_event_risk_factor_mean.min())\n",
    "print('ser_short_term_event_risk_factor - cross-sectional mean mean:', ser_short_term_event_risk_factor_mean.mean())\n",
    "print('ser_short_term_event_risk_factor - cross-sectional mean max:', ser_short_term_event_risk_factor_mean.max())\n",
    "print('ser_short_term_event_risk_factor - cross-sectional mean stdev:', ser_short_term_event_risk_factor_mean.std())\n",
    "print('ser_short_term_event_risk_factor - cross-sectional mean mean:', ser_short_term_event_risk_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR LOW VOLATILITY ANOMALY FACTOR\n",
    "#date_range_test = pd.date_range(start = '2010-10-25', periods = 6)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 160]\n",
    "arr_low_vol_factor = []\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    arr_low_vol_factor.append(get_low_vol_factor(iter_date))\n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_low_vol_factor = pd.concat(arr_low_vol_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_low_vol_factor - AR 29-Dec-2006: 0.007339857068326828\n",
      "ser_low_vol_factor - US 29-Dec-2006: 0.03595279045940818\n",
      "ser_low_vol_factor - cross-sectional mean min: 0.02040816326530611\n",
      "ser_low_vol_factor - cross-sectional mean mean: 0.02255744652417695\n",
      "ser_low_vol_factor - cross-sectional mean max: 0.04545454545454546\n",
      "ser_low_vol_factor - cross-sectional mean stdev: 0.006440980492665572\n",
      "ser_low_vol_factor - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### LOW VOLATILITY ANOMALY FACTOR TESTING:\n",
    "print('ser_low_vol_factor - AR 29-Dec-2006:', ser_low_vol_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_low_vol_factor - US 29-Dec-2006:', ser_low_vol_factor.loc['US' , '2006-12-29'])\n",
    "ser_low_vol_factor_mean = pd.Series(np.NaN, index = ser_low_vol_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_low_vol_factor_mean.index:  \n",
    "    ser_low_vol_factor_mean[iter_date] = ser_low_vol_factor.loc[:, iter_date].mean()\n",
    "ser_low_vol_factor_mean.sort_index(inplace = True)\n",
    "print('ser_low_vol_factor - cross-sectional mean min:', ser_low_vol_factor_mean.min())\n",
    "print('ser_low_vol_factor - cross-sectional mean mean:', ser_low_vol_factor_mean.mean())\n",
    "print('ser_low_vol_factor - cross-sectional mean max:', ser_low_vol_factor_mean.max())\n",
    "print('ser_low_vol_factor - cross-sectional mean stdev:', ser_low_vol_factor_mean.std())\n",
    "print('ser_low_vol_factor - cross-sectional mean mean:', ser_low_vol_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR VOLATILITY SURPRISE FACTOR\n",
    "#date_range_test = pd.date_range(start = '2010-10-25', periods = 6)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 156]\n",
    "arr_vol_surprise_factor = []\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    arr_vol_surprise_factor.append(get_vol_surprise_factor(iter_date))\n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_vol_surprise_factor = pd.concat(arr_vol_surprise_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_vol_surprise_factor - AR 29-Dec-2006: -0.019338598670127435\n",
      "ser_vol_surprise_factor - US 29-Dec-2006: 0.04195253755895069\n",
      "ser_vol_surprise_factor - cross-sectional mean min: -0.04373786858119822\n",
      "ser_vol_surprise_factor - cross-sectional mean mean: -0.04373786858119822\n",
      "ser_vol_surprise_factor - cross-sectional mean max: -0.04373786858119822\n",
      "ser_vol_surprise_factor - cross-sectional mean stdev: nan\n",
      "ser_vol_surprise_factor - cross-sectional mean mean: 1\n"
     ]
    }
   ],
   "source": [
    "### VOLATILITY SURPRISE FACTOR TESTING:\n",
    "print('ser_vol_surprise_factor - AR 29-Dec-2006:', ser_vol_surprise_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_vol_surprise_factor - US 29-Dec-2006:', ser_vol_surprise_factor.loc['US' , '2006-12-29'])\n",
    "ser_vol_surprise_factor_mean = pd.Series(np.NaN, index = ser_vol_surprise_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_vol_surprise_factor_mean.index:  \n",
    "    ser_vol_surprise_factor_mean[iter_date] = ser_vol_surprise_factor.loc[:, iter_date].mean()\n",
    "ser_vol_surprise_factor_mean.sort_index(inplace = True)\n",
    "print('ser_vol_surprise_factor - cross-sectional mean min:', ser_vol_surprise_factor_mean.min())\n",
    "print('ser_vol_surprise_factor - cross-sectional mean mean:', ser_vol_surprise_factor_mean.mean())\n",
    "print('ser_vol_surprise_factor - cross-sectional mean max:', ser_vol_surprise_factor_mean.max())\n",
    "print('ser_vol_surprise_factor - cross-sectional mean stdev:', ser_vol_surprise_factor_mean.std())\n",
    "print('ser_vol_surprise_factor - cross-sectional mean mean:', ser_vol_surprise_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR TAIL RISK FACTOR\n",
    "#date_range_test = pd.date_range(start = '2010-10-25', periods = 6)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 156]\n",
    "arr_tail_risk_factor = []\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    arr_tail_risk_factor.append(get_tail_risk_factor(iter_date))\n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_tail_risk_factor = pd.concat(arr_tail_risk_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_tail_risk_factor - AR 29-Dec-2006: -0.01861014783183245\n",
      "ser_tail_risk_factor - US 29-Dec-2006: -0.060239949254639255\n",
      "ser_tail_risk_factor - cross-sectional mean min: -0.3221148968296049\n",
      "ser_tail_risk_factor - cross-sectional mean mean: 0.0909717780143243\n",
      "ser_tail_risk_factor - cross-sectional mean max: 0.5167322540605308\n",
      "ser_tail_risk_factor - cross-sectional mean stdev: 0.16315908350625816\n",
      "ser_tail_risk_factor - cross-sectional mean mean: 234\n"
     ]
    }
   ],
   "source": [
    "### TAIL RISK FACTOR TESTING:\n",
    "print('ser_tail_risk_factor - AR 29-Dec-2006:', ser_tail_risk_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_tail_risk_factor - US 29-Dec-2006:', ser_tail_risk_factor.loc['US' , '2006-12-29'])\n",
    "ser_tail_risk_factor_mean = pd.Series(np.NaN, index = ser_tail_risk_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_tail_risk_factor_mean.index:  \n",
    "    ser_tail_risk_factor_mean[iter_date] = ser_tail_risk_factor.loc[:, iter_date].mean()\n",
    "ser_tail_risk_factor_mean.sort_index(inplace = True)\n",
    "print('ser_tail_risk_factor - cross-sectional mean min:', ser_tail_risk_factor_mean.min())\n",
    "print('ser_tail_risk_factor - cross-sectional mean mean:', ser_tail_risk_factor_mean.mean())\n",
    "print('ser_tail_risk_factor - cross-sectional mean max:', ser_tail_risk_factor_mean.max())\n",
    "print('ser_tail_risk_factor - cross-sectional mean stdev:', ser_tail_risk_factor_mean.std())\n",
    "print('ser_tail_risk_factor - cross-sectional mean mean:', ser_tail_risk_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR VRP FACTOR\n",
    "#date_range_test = pd.date_range(start = '2010-10-25', periods = 6)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 160]\n",
    "arr_vrp_factor = []\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    arr_vrp_factor.append(get_vrp_factor(iter_date))\n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_vrp_factor = pd.concat(arr_vrp_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_vrp_factor - AR 29-Dec-2006: 0.00163\n",
      "ser_vrp_factor - US 29-Dec-2006: -0.004396\n",
      "ser_vrp_factor - cross-sectional mean min: -0.023387673469387766\n",
      "ser_vrp_factor - cross-sectional mean mean: 0.002256157878726162\n",
      "ser_vrp_factor - cross-sectional mean max: 0.036017938775510204\n",
      "ser_vrp_factor - cross-sectional mean stdev: 0.00794579999880406\n",
      "ser_vrp_factor - cross-sectional mean mean: 237\n"
     ]
    }
   ],
   "source": [
    "### VRP FACTOR TESTING:\n",
    "print('ser_vrp_factor - AR 29-Dec-2006:', ser_vrp_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_vrp_factor - US 29-Dec-2006:', ser_vrp_factor.loc['US' , '2006-12-29'])\n",
    "ser_vrp_factor_mean = pd.Series(np.NaN, index = ser_vrp_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_vrp_factor_mean.index:  \n",
    "    ser_vrp_factor_mean[iter_date] = ser_vrp_factor.loc[:, iter_date].mean()\n",
    "ser_vrp_factor_mean.sort_index(inplace = True)\n",
    "print('ser_vrp_factor - cross-sectional mean min:', ser_vrp_factor_mean.min())\n",
    "print('ser_vrp_factor - cross-sectional mean mean:', ser_vrp_factor_mean.mean())\n",
    "print('ser_vrp_factor - cross-sectional mean max:', ser_vrp_factor_mean.max())\n",
    "print('ser_vrp_factor - cross-sectional mean stdev:', ser_vrp_factor_mean.std())\n",
    "print('ser_vrp_factor - cross-sectional mean mean:', ser_vrp_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR IVOL MOMENTUM 1M FACTOR\n",
    "#date_range_test = pd.date_range(start = '2010-10-25', periods = 6)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 160]\n",
    "arr_ivol_mom_1m_factor = []\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    arr_ivol_mom_1m_factor.append(get_ivol_mom_1m_factor(iter_date))\n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_ivol_mom_1m_factor = pd.concat(arr_ivol_mom_1m_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_ivol_mom_1m_factor - AR 29-Dec-2006: -5.728833933486413e-05\n",
      "ser_ivol_mom_1m_factor - US 29-Dec-2006: -0.0001643642124898838\n",
      "ser_ivol_mom_1m_factor - cross-sectional mean min: -0.0007719988881806511\n",
      "ser_ivol_mom_1m_factor - cross-sectional mean mean: 6.00820444231159e-05\n",
      "ser_ivol_mom_1m_factor - cross-sectional mean max: 0.0011461061607235408\n",
      "ser_ivol_mom_1m_factor - cross-sectional mean stdev: 0.0002536959457065397\n",
      "ser_ivol_mom_1m_factor - cross-sectional mean mean: 237\n"
     ]
    }
   ],
   "source": [
    "### IVOL MOMENTUM 1M FACTOR TESTING:\n",
    "print('ser_ivol_mom_1m_factor - AR 29-Dec-2006:', ser_ivol_mom_1m_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_ivol_mom_1m_factor - US 29-Dec-2006:', ser_ivol_mom_1m_factor.loc['US' , '2006-12-29'])\n",
    "ser_ivol_mom_1m_factor_mean = pd.Series(np.NaN, index = ser_ivol_mom_1m_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_ivol_mom_1m_factor_mean.index:  \n",
    "    ser_ivol_mom_1m_factor_mean[iter_date] = ser_ivol_mom_1m_factor.loc[:, iter_date].mean()\n",
    "ser_ivol_mom_1m_factor_mean.sort_index(inplace = True)\n",
    "print('ser_ivol_mom_1m_factor - cross-sectional mean min:', ser_ivol_mom_1m_factor_mean.min())\n",
    "print('ser_ivol_mom_1m_factor - cross-sectional mean mean:', ser_ivol_mom_1m_factor_mean.mean())\n",
    "print('ser_ivol_mom_1m_factor - cross-sectional mean max:', ser_ivol_mom_1m_factor_mean.max())\n",
    "print('ser_ivol_mom_1m_factor - cross-sectional mean stdev:', ser_ivol_mom_1m_factor_mean.std())\n",
    "print('ser_ivol_mom_1m_factor - cross-sectional mean mean:', ser_ivol_mom_1m_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR IVOL MOMENTUM 12M FACTOR\n",
    "#date_range_test = pd.date_range(start = '2010-10-25', periods = 6)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 160]\n",
    "arr_ivol_mom_12m_factor = []\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    arr_ivol_mom_12m_factor.append(get_ivol_mom_12m_factor(iter_date))\n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_ivol_mom_12m_factor = pd.concat(arr_ivol_mom_12m_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_ivol_mom_12m_factor - AR 29-Dec-2006: 2.4071504646865075e-05\n",
      "ser_ivol_mom_12m_factor - US 29-Dec-2006: -1.281313465985619e-05\n",
      "ser_ivol_mom_12m_factor - cross-sectional mean min: -8.265832098596306e-05\n",
      "ser_ivol_mom_12m_factor - cross-sectional mean mean: 5.48849915627575e-06\n",
      "ser_ivol_mom_12m_factor - cross-sectional mean max: 9.861384692768506e-05\n",
      "ser_ivol_mom_12m_factor - cross-sectional mean stdev: 2.4997621734106227e-05\n",
      "ser_ivol_mom_12m_factor - cross-sectional mean mean: 237\n"
     ]
    }
   ],
   "source": [
    "### IVOL MOMENTUM 12M FACTOR TESTING:\n",
    "print('ser_ivol_mom_12m_factor - AR 29-Dec-2006:', ser_ivol_mom_12m_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_ivol_mom_12m_factor - US 29-Dec-2006:', ser_ivol_mom_12m_factor.loc['US' , '2006-12-29'])\n",
    "ser_ivol_mom_12m_factor_mean = pd.Series(np.NaN, index = ser_ivol_mom_12m_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_ivol_mom_12m_factor_mean.index:  \n",
    "    ser_ivol_mom_12m_factor_mean[iter_date] = ser_ivol_mom_12m_factor.loc[:, iter_date].mean()\n",
    "ser_ivol_mom_12m_factor_mean.sort_index(inplace = True)\n",
    "print('ser_ivol_mom_12m_factor - cross-sectional mean min:', ser_ivol_mom_12m_factor_mean.min())\n",
    "print('ser_ivol_mom_12m_factor - cross-sectional mean mean:', ser_ivol_mom_12m_factor_mean.mean())\n",
    "print('ser_ivol_mom_12m_factor - cross-sectional mean max:', ser_ivol_mom_12m_factor_mean.max())\n",
    "print('ser_ivol_mom_12m_factor - cross-sectional mean stdev:', ser_ivol_mom_12m_factor_mean.std())\n",
    "print('ser_ivol_mom_12m_factor - cross-sectional mean mean:', ser_ivol_mom_12m_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR BETA FACTOR\n",
    "def get_returns_from_index(ser_index, ma_wnd, day_period):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd     \n",
    "    ### Moving average:\n",
    "    dict_index_ma = {}\n",
    "    for iter_country in ser_index.index.get_level_values(0).unique():\n",
    "        ser_index_ma_iter = ser_index[iter_country].rolling(window = ma_wnd, win_type = None).mean()\n",
    "        dict_index_ma[iter_country] = ser_index_ma_iter\n",
    "    ser_index_ma = pd.concat(dict_index_ma)\n",
    "    ### Monthly returns:\n",
    "    dict_period_ret = {}\n",
    "    for iter_country in ser_index_ma.index.get_level_values(0).unique():\n",
    "        ser_period_ret_iter = (ser_index_ma[iter_country] / ser_index_ma[iter_country].shift(day_period) - 1)\n",
    "        dict_period_ret[iter_country] = ser_period_ret_iter\n",
    "    ser_period_ret = pd.concat(dict_period_ret)    \n",
    "    \n",
    "    return ser_period_ret\n",
    "\n",
    "\n",
    "path_market_risk_source_hdf = 'Data_Files/Source_Files/market_risk_source.h5'\n",
    "index_ret_USD_key = 'index_ret_USD_key'\n",
    "monthly_ret_key = 'monthly_ret_key'\n",
    "ser_index_ret_USD = pd.read_hdf(path_market_risk_source_hdf, index_ret_USD_key)\n",
    "ser_monthly_ret_USD = get_returns_from_index(ser_index_ret_USD, ma_wnd = 5, day_period = 21)\n",
    "ser_monthly_ret_USD.to_hdf(path_market_risk_source_hdf, monthly_ret_key, mode = 'a', format = 'table')\n",
    "market_membership_key = 'market_membership_key'\n",
    "ser_market_membership = pd.read_hdf(path_market_risk_source_hdf, market_membership_key)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 156]\n",
    "arr_beta_factor = []\n",
    "iter_counter = 0\n",
    "for iter_date in date_range_test:\n",
    "    iter_counter = iter_counter + 1\n",
    "    arr_beta_factor.append(get_beta_factor(iter_date))\n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_beta_factor = pd.concat(arr_beta_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_beta_factor - AR 29-Dec-2006: -0.03799388234480591\n",
      "ser_beta_factor - US 29-Dec-2006: -0.058970323931999996\n",
      "ser_beta_factor - cross-sectional mean min: -0.13326304502959702\n",
      "ser_beta_factor - cross-sectional mean mean: -0.08160695129341232\n",
      "ser_beta_factor - cross-sectional mean max: -0.015540779141930442\n",
      "ser_beta_factor - cross-sectional mean stdev: 0.023424263557680902\n",
      "ser_beta_factor - cross-sectional mean mean: 227\n"
     ]
    }
   ],
   "source": [
    "### BETA FACTOR TESTING:\n",
    "print('ser_beta_factor - AR 29-Dec-2006:', ser_beta_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_beta_factor - US 29-Dec-2006:', ser_beta_factor.loc['US' , '2006-12-29'])\n",
    "ser_beta_factor_mean = pd.Series(np.NaN, index = ser_beta_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_beta_factor_mean.index:  \n",
    "    ser_beta_factor_mean[iter_date] = ser_beta_factor.loc[:, iter_date].mean()\n",
    "ser_beta_factor_mean.sort_index(inplace = True)\n",
    "print('ser_beta_factor - cross-sectional mean min:', ser_beta_factor_mean.min())\n",
    "print('ser_beta_factor - cross-sectional mean mean:', ser_beta_factor_mean.mean())\n",
    "print('ser_beta_factor - cross-sectional mean max:', ser_beta_factor_mean.max())\n",
    "print('ser_beta_factor - cross-sectional mean stdev:', ser_beta_factor_mean.std())\n",
    "print('ser_beta_factor - cross-sectional mean mean:', ser_beta_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR BETA LEVEL PERCENTILE FACTOR\n",
    "def get_returns_from_index(ser_index, ma_wnd, day_period):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd     \n",
    "    ### Moving average:\n",
    "    dict_index_ma = {}\n",
    "    for iter_country in ser_index.index.get_level_values(0).unique():\n",
    "        ser_index_ma_iter = ser_index[iter_country].rolling(window = ma_wnd, win_type = None).mean()\n",
    "        dict_index_ma[iter_country] = ser_index_ma_iter\n",
    "    ser_index_ma = pd.concat(dict_index_ma)\n",
    "    ### Monthly returns:\n",
    "    dict_period_ret = {}\n",
    "    for iter_country in ser_index_ma.index.get_level_values(0).unique():\n",
    "        ser_period_ret_iter = (ser_index_ma[iter_country] / ser_index_ma[iter_country].shift(day_period) - 1)\n",
    "        dict_period_ret[iter_country] = ser_period_ret_iter\n",
    "    ser_period_ret = pd.concat(dict_period_ret)    \n",
    "    \n",
    "    return ser_period_ret\n",
    "\n",
    "\n",
    "path_market_risk_source_hdf = 'Data_Files/Source_Files/market_risk_source.h5'\n",
    "index_ret_USD_key = 'index_ret_USD_key'\n",
    "monthly_ret_key = 'monthly_ret_key'\n",
    "gri_level_perc_key = 'gri_level_perc_key'\n",
    "ser_index_ret_USD = pd.read_hdf(path_market_risk_source_hdf, index_ret_USD_key)\n",
    "ser_monthly_ret_USD = get_returns_from_index(ser_index_ret_USD, ma_wnd = 5, day_period = 21)\n",
    "ser_monthly_ret_USD.to_hdf(path_market_risk_source_hdf, monthly_ret_key, mode = 'a', format = 'table')\n",
    "ser_perc_gri = pd.read_hdf(path_market_risk_source_hdf, gri_level_perc_key)\n",
    "### For testing purposes:\n",
    "#ser_perc_gri = round(ser_perc_gri, 2)\n",
    "market_membership_key = 'market_membership_key'\n",
    "ser_market_membership = pd.read_hdf(path_market_risk_source_hdf, market_membership_key)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 156]\n",
    "arr_beta_factor = []\n",
    "iter_counter = 0\n",
    "tumbler_to_minus = 0.60\n",
    "tumbler_to_plus = 0.40\n",
    "for iter_date in date_range_test:\n",
    "    ### Sign defining:\n",
    "    ser_beta_signs = pd.Series(np.NaN, index = ser_perc_gri[ : iter_date].index)\n",
    "    ser_beta_signs.iloc[0] = 1\n",
    "    for signs_date in ser_beta_signs.index:\n",
    "        if ser_beta_signs.index.get_loc(signs_date) > 0:\n",
    "            if (ser_beta_signs.loc[signs_date - pd.offsets.BusinessDay()] == 1):\n",
    "                if (ser_perc_gri[signs_date] > tumbler_to_minus):\n",
    "                    ser_beta_signs.loc[signs_date] = -1\n",
    "                else:\n",
    "                    ser_beta_signs.loc[signs_date] = 1\n",
    "            else:\n",
    "                if (ser_perc_gri[signs_date] < tumbler_to_plus):\n",
    "                    ser_beta_signs.loc[signs_date] = 1\n",
    "                else:\n",
    "                    ser_beta_signs.loc[signs_date] = -1             \n",
    "    \n",
    "    arr_beta_factor.append(get_beta_factor(iter_date) * ser_beta_signs[iter_date])\n",
    "    \n",
    "    iter_counter = iter_counter + 1    \n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_beta_factor = pd.concat(arr_beta_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_beta_level_perc_factor - AR 29-Dec-2006: -0.03799388234480591\n",
      "ser_beta_level_perc_factor - US 29-Dec-2006: -0.058970323931999996\n",
      "ser_beta_level_perc_factor - cross-sectional mean min: -0.13326304502959702\n",
      "ser_beta_level_perc_factor - cross-sectional mean mean: -0.0120127690149135\n",
      "ser_beta_level_perc_factor - cross-sectional mean max: 0.12980528767580887\n",
      "ser_beta_level_perc_factor - cross-sectional mean stdev: 0.08421943453343116\n",
      "ser_beta_level_perc_factor - cross-sectional mean mean: 227\n"
     ]
    }
   ],
   "source": [
    "### BETA LEVEL PERCENTILE FACTOR TESTING:\n",
    "print('ser_beta_level_perc_factor - AR 29-Dec-2006:', ser_beta_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_beta_level_perc_factor - US 29-Dec-2006:', ser_beta_factor.loc['US' , '2006-12-29'])\n",
    "ser_beta_factor_mean = pd.Series(np.NaN, index = ser_beta_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_beta_factor_mean.index:\n",
    "    ser_beta_factor_mean[iter_date] = ser_beta_factor.loc[:, iter_date].mean()\n",
    "ser_beta_factor_mean.sort_index(inplace = True)\n",
    "print('ser_beta_level_perc_factor - cross-sectional mean min:', ser_beta_factor_mean.min())\n",
    "print('ser_beta_level_perc_factor - cross-sectional mean mean:', ser_beta_factor_mean.mean())\n",
    "print('ser_beta_level_perc_factor - cross-sectional mean max:', ser_beta_factor_mean.max())\n",
    "print('ser_beta_level_perc_factor - cross-sectional mean stdev:', ser_beta_factor_mean.std())\n",
    "print('ser_beta_level_perc_factor - cross-sectional mean mean:', ser_beta_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_beta_signs_level = ser_beta_signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOPER FOR BETA MOMENTUM PERCENTILE FACTOR\n",
    "def get_returns_from_index(ser_index, ma_wnd, day_period):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd     \n",
    "    ### Moving average:\n",
    "    dict_index_ma = {}\n",
    "    for iter_country in ser_index.index.get_level_values(0).unique():\n",
    "        ser_index_ma_iter = ser_index[iter_country].rolling(window = ma_wnd, win_type = None).mean()\n",
    "        dict_index_ma[iter_country] = ser_index_ma_iter\n",
    "    ser_index_ma = pd.concat(dict_index_ma)\n",
    "    ### Monthly returns:\n",
    "    dict_period_ret = {}\n",
    "    for iter_country in ser_index_ma.index.get_level_values(0).unique():\n",
    "        ser_period_ret_iter = (ser_index_ma[iter_country] / ser_index_ma[iter_country].shift(day_period) - 1)\n",
    "        dict_period_ret[iter_country] = ser_period_ret_iter\n",
    "    ser_period_ret = pd.concat(dict_period_ret)    \n",
    "    \n",
    "    return ser_period_ret\n",
    "\n",
    "\n",
    "path_market_risk_source_hdf = 'Data_Files/Source_Files/market_risk_source.h5'\n",
    "index_ret_USD_key = 'index_ret_USD_key'\n",
    "monthly_ret_key = 'monthly_ret_key'\n",
    "gri_momentum_perc_key = 'gri_momentum_perc_key'\n",
    "ser_index_ret_USD = pd.read_hdf(path_market_risk_source_hdf, index_ret_USD_key)\n",
    "ser_monthly_ret_USD = get_returns_from_index(ser_index_ret_USD, ma_wnd = 5, day_period = 21)\n",
    "ser_monthly_ret_USD.to_hdf(path_market_risk_source_hdf, monthly_ret_key, mode = 'a', format = 'table')\n",
    "ser_perc_gri = pd.read_hdf(path_market_risk_source_hdf, gri_momentum_perc_key)\n",
    "### For testing purposes:\n",
    "#ser_perc_gri = round(ser_perc_gri, 2)\n",
    "market_membership_key = 'market_membership_key'\n",
    "ser_market_membership = pd.read_hdf(path_market_risk_source_hdf, market_membership_key)\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "#date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()[155 : 156]\n",
    "arr_beta_factor = []\n",
    "iter_counter = 0\n",
    "tumbler_to_minus = 0.60\n",
    "tumbler_to_plus = 0.40 \n",
    "for iter_date in date_range_test:        \n",
    "    ### Sign defining:\n",
    "    ser_beta_signs = pd.Series(np.NaN, index = ser_perc_gri[ : iter_date].index)\n",
    "    ser_beta_signs.iloc[0] = 1\n",
    "    for signs_date in ser_beta_signs.index:\n",
    "        if ser_beta_signs.index.get_loc(signs_date) > 0:\n",
    "            if (ser_beta_signs.loc[signs_date - pd.offsets.BusinessDay()] == 1):\n",
    "                if (ser_perc_gri[signs_date] > tumbler_to_minus):\n",
    "                    ser_beta_signs.loc[signs_date] = -1\n",
    "                else:\n",
    "                    ser_beta_signs.loc[signs_date] = 1\n",
    "            else:\n",
    "                if (ser_perc_gri[signs_date] < tumbler_to_plus):\n",
    "                    ser_beta_signs.loc[signs_date] = 1\n",
    "                else:\n",
    "                    ser_beta_signs.loc[signs_date] = -1             \n",
    "    \n",
    "    arr_beta_factor.append(get_beta_factor(iter_date) * ser_beta_signs[iter_date])\n",
    "    \n",
    "    iter_counter = iter_counter + 1    \n",
    "    if ((iter_counter // 10) == (iter_counter / 10)):\n",
    "        print('Progress printout', iter_counter, '/', iter_date)\n",
    "        \n",
    "ser_beta_factor = pd.concat(arr_beta_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser_beta_momentum_perc_factor - AR 29-Dec-2006: -0.03799388234480591\n",
      "ser_beta_momentum_perc_factor - US 29-Dec-2006: -0.058970323931999996\n",
      "ser_beta_momentum_perc_factor - cross-sectional mean min: -0.13326304502959702\n",
      "ser_beta_momentum_perc_factor - cross-sectional mean mean: -0.020065067085754482\n",
      "ser_beta_momentum_perc_factor - cross-sectional mean max: 0.12980528767580887\n",
      "ser_beta_momentum_perc_factor - cross-sectional mean stdev: 0.0826648046856053\n",
      "ser_beta_momentum_perc_factor - cross-sectional mean mean: 227\n"
     ]
    }
   ],
   "source": [
    "### BETA LEVEL PERCENTILE FACTOR TESTING:\n",
    "print('ser_beta_momentum_perc_factor - AR 29-Dec-2006:', ser_beta_factor.loc['AR' , '2006-12-29'])\n",
    "print('ser_beta_momentum_perc_factor - US 29-Dec-2006:', ser_beta_factor.loc['US' , '2006-12-29'])\n",
    "ser_beta_factor_mean = pd.Series(np.NaN, index = ser_beta_factor.index.get_level_values(1).unique())\n",
    "for iter_date in ser_beta_factor_mean.index:\n",
    "    ser_beta_factor_mean[iter_date] = ser_beta_factor.loc[:, iter_date].mean()\n",
    "ser_beta_factor_mean.sort_index(inplace = True)\n",
    "print('ser_beta_momentum_perc_factor - cross-sectional mean min:', ser_beta_factor_mean.min())\n",
    "print('ser_beta_momentum_perc_factor - cross-sectional mean mean:', ser_beta_factor_mean.mean())\n",
    "print('ser_beta_momentum_perc_factor - cross-sectional mean max:', ser_beta_factor_mean.max())\n",
    "print('ser_beta_momentum_perc_factor - cross-sectional mean stdev:', ser_beta_factor_mean.std())\n",
    "print('ser_beta_momentum_perc_factor - cross-sectional mean mean:', ser_beta_factor_mean.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_beta_signs_momentum = ser_beta_signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PERCENTILE FACTORS TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_beta_signs_level = ser_beta_signs_level[ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()]\n",
    "ser_beta_signs_momentum = ser_beta_signs_momentum[ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Data_Files/Test_Files/sign_testing_runner.xlsx') as sign_writer:\n",
    "    ser_beta_signs_level.to_excel(sign_writer, sheet_name = 'Level - Runner')\n",
    "    ser_beta_signs_momentum.to_excel(sign_writer, sheet_name = 'Momentum - Runner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_gri_level_perc = pd.read_hdf(path_market_risk_source_hdf, gri_level_perc_key) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "ser_gri_momentum_perc = pd.read_hdf(path_market_risk_source_hdf, gri_momentum_perc_key) ### ADDED FOR BETA FACTORS CALCULATION\n",
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range_test = ser_market_membership.sort_index(level = 1).index.get_level_values(1).unique()\n",
    "with pd.ExcelWriter('Data_Files/Test_Files/perc_testing_runner.xlsx') as sign_writer:\n",
    "    ser_gri_level_perc[date_range_test].to_excel(sign_writer, sheet_name = 'Level - Runner')\n",
    "    ser_gri_momentum_perc[date_range_test].to_excel(sign_writer, sheet_name = 'Momentum - Runner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
