{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a661548e-05e3-40e6-8065-cef732ce41ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RETURNS MOMENTUM FACTORS DAILY GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7b8b70-a122-49a3-94f0-22df027aad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "from datetime import date, datetime\n",
    "from sklearn.linear_model import LinearRegression ### For Residual Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ce81ac9-3ccb-446e-8e5a-8fea3d51d36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version:  3.7.4\n",
      "numpy version:  1.17.2\n",
      "pandas version:  0.25.3\n"
     ]
    }
   ],
   "source": [
    "### VERSION CONTROL\n",
    "\n",
    "from platform import python_version\n",
    "### Expected Python Version:  3.7.4\n",
    "print('python version: ', python_version())\n",
    "### Expected NumPy Version:  1.17.2\n",
    "print('numpy version: ', np.__version__)\n",
    "### Expected Pandas version:  0.25.3\n",
    "print('pandas version: ', pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cb224cf-7a5c-4317-a765-259de640dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS INITIALIZATION\n",
    "\n",
    "### Common constants:\n",
    "All = slice(None)\n",
    "### General daily-mode and ranges initialization:\n",
    "str_date_factor_start = '2004-12-31' ### End date for efficacy measures\n",
    "str_date_factor_end = '2023-04-28' ### End date for efficacy measures\n",
    "idx_test_monthly_range = pd.date_range(str_date_factor_start, str_date_factor_end, freq = 'BM') ### Range for source data filtering\n",
    "idx_test_daily_range = pd.date_range(str_date_factor_start, str_date_factor_end, freq = 'B') ### Range for source data filtering\n",
    "### Results saving paths:\n",
    "str_wa_basic_path = 'Data_Files/Test_Files/ret_mom_wa_basic.csv'\n",
    "str_wa_ranks_path = 'Data_Files/Test_Files/ret_mom_wa_ranks.csv'\n",
    "str_seas_q_path = 'Data_Files/Test_Files/ret_mom_seas_q.csv'\n",
    "str_seas_y_3mo_path = 'Data_Files/Test_Files/ret_mom_seas_y_3mo.csv'\n",
    "str_wa_residual_path = 'Data_Files/Test_Files/ret_mom_wa_residual.csv'\n",
    "str_wa_dyn_lb_ind_path = 'Data_Files/Test_Files/ret_mom_wa_dyn_lb_ind.csv'\n",
    "str_wa_dyn_lb_reg_path = 'Data_Files/Test_Files/ret_mom_wa_dyn_lb_reg.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ee6111-6172-4727-889d-bbc4fe9978e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = math.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0351cd27-ba78-4782-8e1b-6e0d991c5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = None, int_min_count = 0, bool_normalize = True):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if (ser_weight is None):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                if bool_normalize:\n",
    "                    num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "                else:\n",
    "                    num_result = np.nansum(list_data * list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae02d1a-2b53-4ed3-b061-ceba6b03717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING AVERAGE OF RETURNS CALCULATION (TO IMPLEMENT ADDITIONAL CONTROL FILTERS)\n",
    "\n",
    "def weighted_average_grouped(ser_country, ser_weight = None):\n",
    "    ### Minimum number of observations:\n",
    "    int_min_number = int(260 / 2)\n",
    "    ### Last values control interval length:\n",
    "    int_last_control = 10\n",
    "    ### Implementing control filters and performing weighted average calulation:\n",
    "    if ((ser_country.count() >= int_min_number) & (ser_country[-int_last_control : ].count() > 0)):\n",
    "        flo_wa = weighted_average(ser_country.droplevel('Country'), ser_weight)\n",
    "    else:\n",
    "        flo_wa = np.NaN\n",
    "    return flo_wa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf9ee33-e865-498e-ab84-f062058157eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES\n",
    "\n",
    "def multistep_standartize(ser_data_source, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):  \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_data_source.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_data_source.dropna()\n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        if reuse_outliers:\n",
    "            ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        else:\n",
    "            ### Saving to result and excluding from further calculations truncated values:             \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)      \n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):\n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_data_source.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63bf0f14-f544-485c-af78-348d361ac3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING STANDARTIZE -> RECOVER -> NEGATIVE RETURNS CLIP TRANSFORMATION\n",
    "\n",
    "def get_country_vector(ser_source_raw, idx_source_range):\n",
    "    ### Drop level to avoid stack/unstack manipulations:\n",
    "    if (len(ser_source_raw.index.names) == 2):\n",
    "        ser_source_raw = ser_source_raw.droplevel(-1)       \n",
    "    ### Source vector cleaning:\n",
    "    ser_source_raw = ser_source_raw.replace([np.inf, -np.inf], np.NaN).replace(0.0, np.NaN)\n",
    "    ### Source vector standartizing:\n",
    "    ser_source_std, list_mean, list_std = multistep_standartize(ser_source_raw, [7.5, 6.0], full_result = True)\n",
    "    ### Source vector recovering:\n",
    "    ser_source_rec = (ser_source_std * list_std[1] + list_mean[1]) * list_std[0] + list_mean[0]\n",
    "    ### Negative values clipping:\n",
    "    ser_source_rec.loc[ser_source_rec < -1.0] = -0.9\n",
    "    ### Source vector to rates:\n",
    "    ser_source_rate = (1 + ser_source_rec.fillna(0.0)).cumprod()\n",
    "    ### Rates vector reindexing:\n",
    "    ser_source_rate = ser_source_rate.reindex(idx_source_range)\n",
    "    ### Rates vector forward filling:\n",
    "    ser_source_rate = ser_source_rate.ffill()\n",
    "    ### Rates vector to returns:\n",
    "    ser_source_res = ser_source_rate.diff() / ser_source_rate.shift()\n",
    "    ser_source_res = ser_source_res.replace(0.0, np.NaN)\n",
    "    ### Result output:    \n",
    "    ser_source_res.index.names = ['Date']\n",
    "    ser_source_res.name = 'Returns'\n",
    "    return ser_source_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc056e4-5945-45b3-bec8-42f8833f5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO REPLACE WITH SQL REQUEST: DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE (TO BE IGNORED IN PRODUCT CODE)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(engine = 'openpyxl', io = str_path_universe, sheet_name = 'Switchers', header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc44c99b-32cc-42b3-a699-d4156cf0739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO REPLACE WITH SQL REQUEST: REGIONAL INFO EXTRACTION\n",
    "\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### ISON membership history in daily frequency:\n",
    "ser_ison_membership_daily = ison_membership_converting(str_path_universe, pd.to_datetime(str_date_factor_end), True, 60)\n",
    "ser_ison_membership_daily.index.names = ['Date', 'Country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8296e4c-d938-4566-8fe2-0737f5cc9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO REPLACE WITH SQL REQUEST: RETURNS DATASETS LOADING\n",
    "\n",
    "### Regions list to extract Market Returns from common file:\n",
    "list_regions = ['DM', 'EM', 'FM']\n",
    "### NA for MS Excel / CSV files:\n",
    "list_na_excel_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "                        '#N/A Requesting Data...', '#N/A Invalid Security', '#N/A Field Not Applicable', '---']\n",
    "### Sample data source:\n",
    "str_path_returns = 'Data_Files/Source_Files/msci_sample.csv'\n",
    "### Sample returns data loading:\n",
    "df_sample_ret = pd.read_csv(str_path_returns, header = 0, parse_dates = None, index_col = None, na_values = list_na_excel_values, keep_default_na = False, sep = ';')\n",
    "df_sample_ret['Date'] = pd.to_datetime(df_sample_ret['Date'], format = '%d.%m.%Y')\n",
    "df_sample_ret = df_sample_ret.set_index('Date').sort_index()\n",
    "### Regional returns extraction::\n",
    "ser_region_ret_raw = df_sample_ret[list_regions].stack().sort_index()\n",
    "ser_region_ret_raw.index.set_names('Market', 1, inplace = True)\n",
    "ser_region_ret_raw.name = 'Market_Returns'\n",
    "### Country returns extraction:\n",
    "ser_country_ret_raw = df_sample_ret[list(col for col in df_sample_ret.columns if (col not in list_regions))].stack().sort_index()\n",
    "ser_country_ret_raw.index.set_names('Country', 1, inplace = True)\n",
    "ser_country_ret_raw.name = 'Returns'\n",
    "### Country returns clipping:\n",
    "ser_country_ret_raw = ser_country_ret_raw.groupby('Date').transform(lambda ser_date: ser_date.clip(lower = ser_date.quantile(0.02), upper = ser_date.quantile(0.98)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83009116-e22a-4f05-9ec5-59c9d8228e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING BASIC WEIGHTED AVERAGE MOMENTUM FACTOR CALCULATING FUNCTION\n",
    "\n",
    "#def get_wa_basic_factor(iter_date):\n",
    "iter_date = pd.to_datetime('2015-09-30')\n",
    "if True:     \n",
    "    ### Length of interval to transform raw returns vector:\n",
    "    int_stand_win = int(260 * 18 / 12)\n",
    "    ### Length of returns series to exponential weighted average calculation:\n",
    "    int_factor_win = int(260 * 12 / 12)\n",
    "    ### Length of half-life period of exponential weights list:\n",
    "    int_half_life = int(260 * 9 / 12)\n",
    "    ### Start date to load raw data:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1)\n",
    "    ### Start date to extract returns to exponential weighted average calculation:    \n",
    "    date_factor_start = iter_date - pd.tseries.offsets.BDay(int_factor_win - 1)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    list_countries = ser_ison_membership_daily[iter_date].dropna().index.get_level_values('Country').unique()\n",
    "    ser_iter_ret_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, list_countries]    \n",
    "    ### Date range to reindex raw data:       \n",
    "    idx_iter_stand_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Date range to extract returns to exponential weighted average calculation:    \n",
    "    idx_iter_factor_range = pd.date_range(start = date_factor_start, end = iter_date, freq = 'B')     \n",
    "    ### Raw data transformation:\n",
    "    ser_iter_ret_trans = ser_iter_ret_raw.groupby('Country').apply(get_country_vector, idx_iter_stand_range).swaplevel().sort_index()\n",
    "    ### Dataset for factor calculation extracting:\n",
    "    ser_iter_ret_cut = ser_iter_ret_trans.loc[idx_iter_factor_range, All] \n",
    "    ### Exponential weights series preparation:\n",
    "    list_weight = list(map(lambda iter_num: exp_weight_single(int_half_life, iter_num), range(int_factor_win)))[::-1]    \n",
    "    ser_iter_weight = pd.Series(list_weight, index = idx_iter_factor_range)\n",
    "    ser_iter_weight.index.name = 'Date'\n",
    "    ser_iter_weight.name = 'Weight'\n",
    "    ### Basic Weighted Average Factor calculation by applying of exponential weighted average to ln(1 + ret) series:\n",
    "    ser_iter_factor = np.log(1 + ser_iter_ret_cut).groupby('Country').apply(weighted_average_grouped, ser_iter_weight)\n",
    "    ser_iter_factor = np.exp(ser_iter_factor) - 1\n",
    "#    ### Add to csv file (should be substituted by SQL query):\n",
    "#    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "#    ser_iter_factor_csv.to_csv(str_wa_basic_path, mode = 'a', header = not os.path.exists(str_wa_basic_path), sep = ';')\n",
    "#    ### Results output:\n",
    "#    return ser_iter_factor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09816c57-aec7-4417-accc-29fd91331266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE MOMENTUM OF RANKS FACTOR CALCULATING FUNCTION\n",
    "\n",
    "#def get_wa_ranks_factor(iter_date):\n",
    "iter_date = pd.to_datetime('2020-12-31')\n",
    "if True:    \n",
    "    ### Length of interval to transform raw returns vector:\n",
    "    int_stand_win = int(260 * 18 / 12)\n",
    "    ### Length of returns series to calculate factor:    \n",
    "    int_factor_win = int(260 * 12 / 12)\n",
    "    ### Length of half-life period of exponential weights list:    \n",
    "    int_half_life = int(260 * 9 / 12)\n",
    "    ### Start date to load raw data:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1)\n",
    "    ### Start date to extract returns to exponential weighted average calculation:     \n",
    "    date_factor_start = iter_date - pd.tseries.offsets.BDay(int_factor_win - 1)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    list_countries = ser_ison_membership_daily[iter_date].dropna().index.get_level_values('Country').unique()\n",
    "    ser_iter_ret_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, list_countries]    \n",
    "    ### Date range to reindex raw data:\n",
    "    idx_iter_stand_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Date range to extract returns to exponential weighted average calculation:     \n",
    "    idx_iter_factor_range = pd.date_range(start = date_factor_start, end = iter_date, freq = 'B')     \n",
    "    ### Raw data transformation:\n",
    "    ser_iter_ret_trans = ser_iter_ret_raw.groupby('Country').apply(get_country_vector, idx_iter_stand_range).swaplevel().sort_index()\n",
    "    ### Dataset for factor calculation extracting:\n",
    "    ser_iter_ret_cut = ser_iter_ret_trans.loc[idx_iter_factor_range, All] \n",
    "    ### Exponential weights series preparation:\n",
    "    list_weight = list(map(lambda iter_num: exp_weight_single(int_half_life, iter_num), range(int_factor_win)))[::-1]    \n",
    "    ser_iter_weight = pd.Series(list_weight, index = idx_iter_factor_range)\n",
    "    ser_iter_weight.index.name = 'Date'\n",
    "    ser_iter_weight.name = 'Weight'\n",
    "    ### Adding regional info to returns series (status on iter_date):\n",
    "    df_iter_ret_cut = ser_iter_ret_cut.to_frame().join(ser_ison_membership_daily[iter_date])\n",
    "    ### Calculation of percentiled ranks within each region:\n",
    "    ser_iter_ret_rank = df_iter_ret_cut.groupby(['Date', 'Market']).rank(pct = True).squeeze()\n",
    "    ### Ranks Weighted Average Factor calculation by applying of exponential weighted average to ranks:\n",
    "    ser_iter_factor = ser_iter_ret_rank.groupby('Country').apply(weighted_average_grouped, ser_iter_weight)\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_wa_ranks_path, mode = 'a', header = not os.path.exists(str_wa_ranks_path), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfc5b1b4-865a-422a-8a7e-c90af9e1bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING LAST YEAR MONTH QUARTERLY SEASONALITY AVERAGE FACTOR CALCULATING FUNCTION\n",
    "\n",
    "def get_seas_q_factor(iter_date):  \n",
    "#iter_date = pd.to_datetime('2020-12-15')\n",
    "#if True:        \n",
    "    ### Length of interval to transform raw returns vector:\n",
    "    int_stand_win = int(260 * 24 / 12)\n",
    "    ### Start date to load raw data:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    list_countries = ser_ison_membership_daily[iter_date].dropna().index.get_level_values('Country').unique()\n",
    "    ser_iter_ret_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, list_countries]    \n",
    "    ### Date range to reindex raw data:    \n",
    "    idx_iter_stand_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Raw data transformation:\n",
    "    ser_iter_ret_trans = ser_iter_ret_raw.groupby('Country').apply(get_country_vector, idx_iter_stand_range).swaplevel().sort_index()\n",
    "    ### Container to collect seasonality intervals:\n",
    "    list_ranges = []\n",
    "    ### Looping over periods we choose to be responsible for seasonality:\n",
    "    for iter_period in range(12, 0, -3):\n",
    "        ### Creating of business daily data range to represent particular period:\n",
    "        idx_iter_seasonal =  pd.date_range(start = iter_date + pd.DateOffset(days = 1) - pd.DateOffset(months = iter_period), \n",
    "                                           end = iter_date - pd.DateOffset(months = (iter_period - 1)))\n",
    "        ### Add the part of returns series to collection:\n",
    "        list_ranges.append(ser_iter_ret_trans.loc[idx_iter_seasonal])\n",
    "    ### Transform collection of periods to one series:\n",
    "    ser_iter_seasonal = pd.concat(list_ranges)\n",
    "    ### Last Year Monthly Seasonality Factor calculation:\n",
    "    ser_iter_factor = np.log(1 + ser_iter_seasonal).groupby('Country').mean()\n",
    "    ser_iter_factor = np.exp(ser_iter_factor) - 1\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_seas_q_path, mode = 'a', header = not os.path.exists(str_seas_q_path), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e16c2dc4-54ca-4812-be7b-118ba8928cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING LAST 3 YEARS QUARTER ANNUAL SEASONALITY AVERAGE FACTOR CALCULATING FUNCTION\n",
    "\n",
    "def get_seas_y_3mo_factor(iter_date):\n",
    "#iter_date = pd.to_datetime('2020-12-15')\n",
    "#if True:      \n",
    "    ### Length of interval to transform raw returns vector:\n",
    "    int_stand_win = int(260 * 60 / 12)\n",
    "    ### Start date to load raw data:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1)\n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    list_countries = ser_ison_membership_daily[iter_date].dropna().index.get_level_values('Country').unique()\n",
    "    ser_iter_ret_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, list_countries]   \n",
    "    ### Date range to reindex raw data:        \n",
    "    idx_iter_stand_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Raw data transformation:\n",
    "    ser_iter_ret_trans = ser_iter_ret_raw.groupby('Country').apply(get_country_vector, idx_iter_stand_range).swaplevel().sort_index()\n",
    "    ### Container to collect seasonality intervals:\n",
    "    list_ranges = []\n",
    "    ### Looping over periods we choose to be responsible for seasonality:\n",
    "    for iter_period in range(36, 0, -12):\n",
    "        ### Creating of business daily data range to represent particular period:\n",
    "        idx_iter_seasonal =  pd.date_range(start = iter_date + pd.DateOffset(days = 1) - pd.DateOffset(months = iter_period), \n",
    "                                           end = iter_date - pd.DateOffset(months = (iter_period - 3)))\n",
    "        ### Add the part of returns series to collection:\n",
    "        list_ranges.append(ser_iter_ret_trans.loc[idx_iter_seasonal])\n",
    "    ### Transform collection of periods to one series:\n",
    "    ser_iter_seasonal = pd.concat(list_ranges)\n",
    "    ### Last Year Monthly Seasonality Factor calculation:\n",
    "    ser_iter_factor = np.log(1 + ser_iter_seasonal).groupby('Country').mean()\n",
    "    ser_iter_factor = np.exp(ser_iter_factor) - 1\n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_seas_y_3mo_path, mode = 'a', header = not os.path.exists(str_seas_y_3mo_path), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edd6729b-a9f0-4ca2-be42-666c29cad4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE OF RESIDUAL RETURNS MOMENTUM FACTOR CALCULATING FUNCTION\n",
    "\n",
    "#def get_wa_residual_factor(iter_date):\n",
    "iter_date = pd.to_datetime('2021-12-31')\n",
    "if True:                \n",
    "    ### Defining internal function to perform market returns regression on country returns:\n",
    "    def perform_regression(df_country):\n",
    "        ### Minimum number of observations:\n",
    "        int_min_number = 260\n",
    "        ### Last values control interval length:\n",
    "        int_last_control = 10      \n",
    "        ### Dummy returns:\n",
    "        (flo_intercept, flo_beta) = (np.NaN, np.NaN)\n",
    "        ### Conditional regression performing:\n",
    "        df_country = df_country.dropna()\n",
    "        if ((df_country['Returns'].count() >= int_min_number) & (df_country['Returns'][-int_last_control :].count() > 0)):\n",
    "            ### Activation of regression engine:\n",
    "            reg_on_market.fit(df_country[['Market_Returns']], df_country[['Returns']])\n",
    "            ### Beta extraction:\n",
    "            flo_beta = reg_on_market.coef_[0][0]\n",
    "            ### Interception extraction:\n",
    "            flo_intercept = reg_on_market.intercept_[0]\n",
    "        ### Results output:\n",
    "        return (flo_intercept, flo_beta)    \n",
    "    ### Length of interval to transform raw returns vector:    \n",
    "    int_stand_win = int(260 * 84 / 12) \n",
    "    ### Length of returns series to calculate factor:    \n",
    "    int_factor_win = int(260 * 12 / 12)\n",
    "    ### Length of half-life period of exponential weights list:    \n",
    "    int_half_life = int(260 * 9 / 12)\n",
    "    ### Length of business month (measured in business days):\n",
    "    int_bmonth_len = 21\n",
    "    ### Length of interval to perform regression (measured in months):\n",
    "    int_regression_month = 60    \n",
    "    ### Start date to load raw data:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1) \n",
    "    ### Start date to extract returns to exponential weighted average calculation:     \n",
    "    date_factor_start = iter_date - pd.tseries.offsets.BDay(int_factor_win - 1)\n",
    "    ### Datasource of country returns for particular date (should be substituted by SQL query):\n",
    "    list_countries = ser_ison_membership_daily[iter_date].dropna().index.get_level_values('Country').unique()\n",
    "    ser_iter_ret_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, list_countries]   \n",
    "    ### Datasource of regional returns for particular date (should be substituted by SQL query):    \n",
    "    ser_iter_region_raw = ser_region_ret_raw.loc[date_stand_start : iter_date, All]\n",
    "    ### Date range to reindex raw data:    \n",
    "    idx_iter_stand_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Date range to extract returns to exponential weighted average calculation:     \n",
    "    idx_iter_factor_range = pd.date_range(start = date_factor_start, end = iter_date, freq = 'B')     \n",
    "    ### Raw country returns data transformation:\n",
    "    ser_iter_ret_trans = ser_iter_ret_raw.groupby('Country').apply(get_country_vector, idx_iter_stand_range).swaplevel().sort_index()\n",
    "    ### Raw regional returns data transformation:    \n",
    "    ser_iter_region_trans = ser_iter_region_raw.groupby('Market').apply(get_country_vector, idx_iter_stand_range).swaplevel().sort_index()\n",
    "    ### Country cumulative monthly returns calculation:\n",
    "    ser_iter_ret_cumprod = (1 + ser_iter_ret_trans.fillna(0.0)).groupby('Country').cumprod()\n",
    "    ser_iter_ret_monthly = (ser_iter_ret_cumprod / ser_iter_ret_cumprod.groupby('Country').shift(int_bmonth_len)) - 1\n",
    "    ### Fill monthly returns with NaN if last daily returns is NaN:\n",
    "    ser_iter_ret_monthly.loc[ser_iter_ret_trans.isna()] = np.NaN\n",
    "    ### Regional cumulative monthly returns calculation:\n",
    "    ser_iter_region_cumprod = (1 + ser_iter_region_trans.fillna(0.0)).groupby('Market').cumprod()    \n",
    "    ser_iter_region_monthly = (ser_iter_region_cumprod / ser_iter_region_cumprod.groupby('Market').shift(int_bmonth_len)) - 1 \n",
    "    ### Fill monthly returns with NaN if last daily returns is NaN:\n",
    "    ser_iter_region_monthly.loc[ser_iter_region_trans.isna()] = np.NaN    \n",
    "    ### Dataframe to perform regressions on base of country returns series:\n",
    "    ### 1) Adding regional info to returns series (status on each series date)\n",
    "    ### 2) Adding regional returns\n",
    "    df_iter_ret_monthly = ser_iter_ret_monthly.to_frame().join(ser_ison_membership_daily).dropna(subset = ['Market']).set_index('Market', append = True)\\\n",
    "                                .reset_index('Country').sort_index().join(ser_iter_region_monthly).droplevel('Market').set_index('Country', append = True).sort_index()\n",
    "    ### Choosing dates to one per month regression applying:\n",
    "#    idx_iter_regress_point = pd.date_range(end = iter_date, freq = pd.DateOffset(months = 1), periods = 12)\n",
    "    idx_iter_regress_point = pd.date_range(end = iter_date, freq = pd.tseries.offsets.BDay(int_bmonth_len), periods = 12)\n",
    "    ### Initialization of regression engine:\n",
    "    reg_on_market = LinearRegression()\n",
    "    ### Container to collect regression coefficients:\n",
    "    list_params = []\n",
    "    ### Looping over choosed dates:\n",
    "    for iter_point in idx_iter_regress_point:\n",
    "        ### Shifting calendar date to business date:\n",
    "        iter_b_point = iter_point + pd.tseries.offsets.BDay(0)\n",
    "        ### Extracting interval to perform regression:\n",
    "        df_point_to_regress = df_iter_ret_monthly.loc[iter_b_point + pd.DateOffset(days = 1) - pd.DateOffset(months = int_regression_month) : iter_b_point]\n",
    "        ### Performing of regression (pair of Intercept & Beta as a result):\n",
    "        ser_point_params = df_point_to_regress.groupby('Country').apply(perform_regression)\n",
    "        ### Separating coefficient vectors to individual columns:\n",
    "        df_point_params = pd.DataFrame(ser_point_params.to_list(), columns = ['Intercept', 'Beta'], index = ser_point_params.index)\n",
    "        ### Adding parameters to collection:\n",
    "        list_params.append(pd.concat({iter_b_point: df_point_params}, names = ['Date']))\n",
    "    ### Transforming parameters collection to dataframe:\n",
    "    df_iter_params = pd.concat(list_params).sort_index() \n",
    "    ### Dataframe to calculate residual returns:\n",
    "    ### 1) Adding regional info to returns series (status on each series date)\n",
    "    ### 2) Adding regional returns\n",
    "    df_iter_ret_daily = ser_iter_ret_trans.to_frame().join(ser_ison_membership_daily).dropna(subset = ['Market']).set_index('Market', append = True)\\\n",
    "                                .reset_index('Country').sort_index().join(ser_iter_region_trans).droplevel('Market').set_index('Country', append = True).sort_index()\n",
    "    ### Extracting country returns to calculate residuals:\n",
    "    df_iter_residual = df_iter_ret_daily.loc[(idx_iter_factor_range, All), :].fillna(-1000.0)\n",
    "    ### Adding parameters to calculate residuals and performing backfill to convert monthly data to daily:\n",
    "    df_iter_residual = df_iter_residual.join(df_iter_params).groupby('Country').bfill().replace({-1000.0 : np.NaN})\n",
    "    ### Residual returns calculation:\n",
    "    ser_iter_ret_cut = df_iter_residual['Returns'] - (df_iter_residual['Intercept'] / int_bmonth_len + df_iter_residual['Beta'] * df_iter_residual['Market_Returns'])\n",
    "    ser_iter_ret_cut.name = 'Residual'\n",
    "    ### Exponential weights series preparation:\n",
    "    list_weight = list(map(lambda iter_num: exp_weight_single(int_half_life, iter_num), range(int_factor_win)))[::-1]    \n",
    "    ser_iter_weight = pd.Series(list_weight, index = idx_iter_factor_range)\n",
    "    ser_iter_weight.index.name = 'Date'\n",
    "    ser_iter_weight.name = 'Weight'\n",
    "    ### Residual Weighted Average Factor calculation:\n",
    "    ser_iter_factor = np.log(1 + ser_iter_ret_cut).groupby('Country').apply(weighted_average_grouped, ser_iter_weight)\n",
    "    ser_iter_factor = np.exp(ser_iter_factor) - 1\n",
    "#    ### Add to csv file (should be substituted by SQL query):\n",
    "#    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "#    ser_iter_factor_csv.to_csv(str_wa_residual_path, mode = 'a', header = not os.path.exists(str_wa_residual_path), sep = ';')\n",
    "#    ### Results output:\n",
    "#    return ser_iter_factor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf3f839c-4e30-49c1-86da-e6b4282ac339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007696190351087706"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_iter_factor.abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2392388-37ca-41dd-a076-059c24e38b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING DYNAMIC LOOK BACK WITH COMPARISION BY COUNTRY WEIGHTED AVERAGE FACTOR CALCULATING FUNCTION\n",
    "\n",
    "def get_wa_dyn_lb_ind_factor(iter_date):\n",
    "#iter_date = pd.to_datetime('2010-12-15')\n",
    "#if True:            \n",
    "    ### Length of interval to transform raw returns vector:    \n",
    "    int_stand_win = int(260 * 72 / 12)\n",
    "    ### Length of interval to calculate variance:\n",
    "    int_var_win = int(260 * 60 / 12)\n",
    "    ### Bounds to clip lookback length (measured by months):\n",
    "    int_dyn_win_min = 2\n",
    "    int_dyn_win_max = 24    \n",
    "    ### Start date to load raw data:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1)\n",
    "    ### Start date to calculate of variance:\n",
    "    date_var_start = iter_date - pd.tseries.offsets.BDay(int_var_win - 1)    \n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    list_countries = ser_ison_membership_daily[iter_date].dropna().index.get_level_values('Country').unique()\n",
    "    ser_iter_ret_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, list_countries]   \n",
    "    ### Date range to reindex raw data:\n",
    "    idx_iter_stand_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Date range to calculate variance:    \n",
    "    idx_iter_var_range = pd.date_range(start = date_var_start, end = iter_date, freq = 'B')     \n",
    "    ### Raw data transformation:\n",
    "    ser_iter_ret_trans = ser_iter_ret_raw.groupby('Country').apply(get_country_vector, idx_iter_stand_range).swaplevel().sort_index()\n",
    "    ### Dataset for variance calculation extracting:\n",
    "    ser_iter_ret_cut = ser_iter_ret_trans.loc[idx_iter_var_range, All]\n",
    "    ### Annualized variance calculation by country:\n",
    "    ser_iter_ret_squared = ser_iter_ret_cut ** 2\n",
    "    ser_iter_var_ann = ser_iter_ret_squared.groupby('Country').mean() * 260\n",
    "    ser_iter_var_ann.name = 'Var_Annualized'\n",
    "    ### Cumulative sum of squared returns:\n",
    "    ser_iter_ret_cumsum = ser_iter_ret_squared.groupby('Country', group_keys = False).apply(lambda ser_country: ser_country.sort_index(ascending = False).cumsum())\\\n",
    "                                              .sort_index()  \n",
    "    ser_iter_ret_cumsum.name = 'Var_CumSum'\n",
    "    ### Searching for the Intersection:\n",
    "    df_iter_ret_cumsum = ser_iter_ret_cumsum.to_frame().join(ser_iter_var_ann)\n",
    "    df_iter_intersection = df_iter_ret_cumsum[df_iter_ret_cumsum['Var_CumSum'] >= df_iter_ret_cumsum['Var_Annualized']].groupby('Country', group_keys = False)\\\n",
    "                                                                                                                       .apply(lambda df_country: df_country.iloc[-1 :])\n",
    "    ser_iter_dyn_win = df_iter_intersection.reset_index('Date')['Date']\n",
    "    ### Clipping outlied dates:\n",
    "    ser_iter_dyn_win[ser_iter_dyn_win > (iter_date - pd.DateOffset(months = int_dyn_win_min))] = \\\n",
    "                            iter_date - pd.DateOffset(months = int_dyn_win_min) + pd.tseries.offsets.BDay(0)\n",
    "    ser_iter_dyn_win[ser_iter_dyn_win < (iter_date - pd.DateOffset(months = int_dyn_win_max))] = \\\n",
    "                            iter_date - pd.DateOffset(months = int_dyn_win_max) + pd.tseries.offsets.BDay(0)\n",
    "    ### Filtering short datasets:\n",
    "    ser_iter_dyn_win = ser_iter_dyn_win[ser_iter_ret_cut.groupby('Country').count() >= 260]\n",
    "    ### By country dynamic weighted average calculation:\n",
    "    dict_wa_dyn_lb = {}\n",
    "    for iter_country in ser_iter_dyn_win.index:\n",
    "        ### Dynamic dataset extraction:\n",
    "        ser_iter_ret_country = ser_iter_ret_cut.loc[ser_iter_dyn_win[iter_country] : , iter_country]\n",
    "        int_iter_win = len(ser_iter_ret_country)\n",
    "        int_iter_half_life = int_iter_win // 2\n",
    "        ### Exponential weights series preparation:\n",
    "        list_weight = list(map(lambda iter_num: exp_weight_single(int_iter_half_life, iter_num), range(int_iter_win)))[::-1]    \n",
    "        ser_iter_weight = pd.Series(list_weight, index = ser_iter_ret_country.index)\n",
    "        ser_iter_weight.index.name = 'Date'\n",
    "        ser_iter_weight.name = 'Weight'\n",
    "        ### Exponential weighted average calculation:\n",
    "        dict_wa_dyn_lb[iter_country] = weighted_average(ser_iter_ret_country, ser_iter_weight, bool_normalize = True)\n",
    "    ### Transforming collection to series:\n",
    "    ser_iter_factor = pd.Series(dict_wa_dyn_lb)\n",
    "    ser_iter_factor.name = 'Factor'\n",
    "    ser_iter_factor.index.names = ['Country']    \n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_wa_dyn_lb_ind_path, mode = 'a', header = not os.path.exists(str_wa_dyn_lb_ind_path), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60a177b6-8f85-462c-ab8c-4201e6f7e1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING DYNAMIC LOOK BACK WITH COMPARISION BY REGION WEIGHTED AVERAGE FACTOR CALCULATING FUNCTION\n",
    "\n",
    "def get_wa_dyn_lb_reg_factor(iter_date):\n",
    "#iter_date = pd.to_datetime('2010-12-31')\n",
    "#if True:       \n",
    "    ### Length of interval to transform raw returns vector:\n",
    "    int_stand_win = int(260 * 72 / 12)\n",
    "    ### Length of interval to calculate variance:\n",
    "    int_var_win = int(260 * 60 / 12)    \n",
    "    ### Bounds to clip lookback length (measured by months):    \n",
    "    int_dyn_win_min = 2\n",
    "    int_dyn_win_max = 24    \n",
    "    ### Start date to load raw data:\n",
    "    date_stand_start = iter_date - pd.tseries.offsets.BDay(int_stand_win - 1)\n",
    "    ### Start date to calculate of variance:    \n",
    "    date_var_start = iter_date - pd.tseries.offsets.BDay(int_var_win - 1)    \n",
    "    ### Datasource for particular date (should be substituted by SQL query):\n",
    "    list_countries = ser_ison_membership_daily[iter_date].dropna().index.get_level_values('Country').unique()\n",
    "    ser_iter_ret_raw = ser_country_ret_raw.loc[date_stand_start : iter_date, list_countries]   \n",
    "    ser_iter_region_raw = ser_region_ret_raw.loc[date_stand_start : iter_date, All] \n",
    "    ### Date range to reindex raw data:    \n",
    "    idx_iter_stand_range = pd.date_range(start = date_stand_start, end = iter_date, freq = 'B')\n",
    "    ### Date range to calculate variance:    \n",
    "    idx_iter_var_range = pd.date_range(start = date_var_start, end = iter_date, freq = 'B')     \n",
    "    ### Raw data transformation:\n",
    "    ser_iter_ret_trans = ser_iter_ret_raw.groupby('Country').apply(get_country_vector, idx_iter_stand_range).swaplevel().sort_index()\n",
    "    ser_iter_region_trans = ser_iter_region_raw.groupby('Market').apply(get_country_vector, idx_iter_stand_range).swaplevel().sort_index()    \n",
    "    ### Dataset for variance calculation extracting:\n",
    "    ser_iter_ret_cut = ser_iter_ret_trans.loc[idx_iter_var_range, All]\n",
    "    ser_iter_region_cut = ser_iter_region_trans.loc[idx_iter_var_range, All]\n",
    "    ### Dataframe to calculate residual returns:\n",
    "    ### 1) Adding regional info to returns series (status on each series date)\n",
    "    ### 2) Adding regional returns\n",
    "    df_iter_ret_bench = ser_iter_ret_cut.to_frame().join(ser_ison_membership_daily).dropna(subset = ['Market']).set_index('Market', append = True)\\\n",
    "                                    .reset_index('Country').sort_index().join(ser_iter_region_cut).droplevel('Market').set_index('Country', append = True).sort_index()\n",
    "    ### Annualized variance calculation by country:\n",
    "    ser_iter_ret_squared = df_iter_ret_bench['Market_Returns'] ** 2\n",
    "    ser_iter_var_ann = ser_iter_ret_squared.groupby('Country').mean() * 260\n",
    "    ser_iter_var_ann.name = 'Var_Annualized'\n",
    "    ### Cumulative sum of squared returns:\n",
    "    ser_iter_ret_cumsum = ser_iter_ret_squared.groupby('Country', group_keys = False).apply(lambda ser_country: ser_country.sort_index(ascending = False).cumsum())\\\n",
    "                                              .sort_index()  \n",
    "    ser_iter_ret_cumsum.name = 'Var_CumSum'\n",
    "    ### Searching for the Intersection:\n",
    "    df_iter_ret_cumsum = ser_iter_ret_cumsum.to_frame().join(ser_iter_var_ann)\n",
    "    df_iter_intersection = df_iter_ret_cumsum[df_iter_ret_cumsum['Var_CumSum'] >= df_iter_ret_cumsum['Var_Annualized']].groupby('Country', group_keys = False)\\\n",
    "                                                                                                                       .apply(lambda df_country: df_country.iloc[-1 :])\n",
    "    ser_iter_dyn_win = df_iter_intersection.reset_index('Date')['Date']\n",
    "    ### Clipping outlied dates:\n",
    "    ser_iter_dyn_win[ser_iter_dyn_win > (iter_date - pd.DateOffset(months = int_dyn_win_min))] = \\\n",
    "                            iter_date - pd.DateOffset(months = int_dyn_win_min) + pd.tseries.offsets.BDay(0)\n",
    "    ser_iter_dyn_win[ser_iter_dyn_win < (iter_date - pd.DateOffset(months = int_dyn_win_max))] = \\\n",
    "                            iter_date - pd.DateOffset(months = int_dyn_win_max) + pd.tseries.offsets.BDay(0)\n",
    "    ### Filtering short datasets:\n",
    "    ser_iter_dyn_win = ser_iter_dyn_win[ser_iter_ret_cut.groupby('Country').count() >= 260]\n",
    "    ### By country dynamic weighted average calculation:\n",
    "    dict_wa_dyn_lb = {}\n",
    "    for iter_country in ser_iter_dyn_win.index:\n",
    "        ### Dynamic dataset extraction:\n",
    "        ser_iter_ret_country = ser_iter_ret_cut.loc[ser_iter_dyn_win[iter_country] : , iter_country]\n",
    "        int_iter_win = len(ser_iter_ret_country)\n",
    "        int_iter_half_life = int_iter_win // 2\n",
    "        ### Exponential weights series preparation:\n",
    "        list_weight = list(map(lambda iter_num: exp_weight_single(int_iter_half_life, iter_num), range(int_iter_win)))[::-1]    \n",
    "        ser_iter_weight = pd.Series(list_weight, index = ser_iter_ret_country.index)\n",
    "        ser_iter_weight.index.name = 'Date'\n",
    "        ser_iter_weight.name = 'Weight'\n",
    "        ### Exponential weighted average calculation:\n",
    "        dict_wa_dyn_lb[iter_country] = weighted_average(ser_iter_ret_country, ser_iter_weight, bool_normalize = True)    \n",
    "    ### Transforming collection to series:        \n",
    "    ser_iter_factor = pd.Series(dict_wa_dyn_lb)\n",
    "    ser_iter_factor.name = 'Factor'\n",
    "    ser_iter_factor.index.names = ['Country']    \n",
    "    ### Add to csv file (should be substituted by SQL query):\n",
    "    ser_iter_factor_csv = pd.concat({iter_date: ser_iter_factor}, names = ['Date'])\n",
    "    ser_iter_factor_csv.to_csv(str_wa_dyn_lb_reg_path, mode = 'a', header = not os.path.exists(str_wa_dyn_lb_reg_path), sep = ';')\n",
    "    ### Results output:\n",
    "    return ser_iter_factor    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c85593cd-ff87-484e-b321-85459d67db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2023-05-15 09:41:28.826060\n",
      "Counter marker: 10 / 25\n",
      "Time interval since last marker: 0:00:40.408796\n",
      "Average interval for single date: 0:00:04.040880\n",
      "Counter marker: 20 / 25\n",
      "Time interval since last marker: 0:00:42.456839\n",
      "Average interval for single date: 0:00:04.245684\n",
      "Finish time: 2023-05-15 09:43:13.961169\n",
      "Full interval: 0:01:45.135109\n",
      "Average interval for single date: 0:00:04.205404\n"
     ]
    }
   ],
   "source": [
    "### TESTING: PERFORMING FACTOR FOR DATE RANGE\n",
    "\n",
    "### Removing csv files before loop running:\n",
    "if (os.path.exists(str_wa_basic_path)):\n",
    "    os.remove(str_wa_basic_path)\n",
    "if (os.path.exists(str_wa_ranks_path)):\n",
    "    os.remove(str_wa_ranks_path)    \n",
    "if (os.path.exists(str_seas_q_path)):\n",
    "    os.remove(str_seas_q_path)\n",
    "if (os.path.exists(str_seas_y_3mo_path)):\n",
    "    os.remove(str_seas_y_3mo_path)    \n",
    "if (os.path.exists(str_wa_residual_path)):\n",
    "    os.remove(str_wa_residual_path)\n",
    "if (os.path.exists(str_wa_dyn_lb_ind_path)):\n",
    "    os.remove(str_wa_dyn_lb_ind_path)    \n",
    "if (os.path.exists(str_wa_dyn_lb_reg_path)):\n",
    "    os.remove(str_wa_dyn_lb_reg_path)      \n",
    "### Local testing parameters:\n",
    "int_interval = 10 ### Interval of progress displaying\n",
    "date_start = datetime.utcnow() ### Start time of calculations\n",
    "date_control = datetime.utcnow() ### Control time to display\n",
    "idx_test_date_range = idx_test_monthly_range[-25 : ] # idx_test_monthly_range # idx_test_daily_range # \n",
    "### Test performing:\n",
    "print('Start time:', date_start)\n",
    "for iter_num, iter_date in enumerate(idx_test_date_range):\n",
    "    ### Progress printing:\n",
    "    if not (divmod(iter_num, int_interval)[1]):\n",
    "        if iter_num:\n",
    "            print('Counter marker:', iter_num, '/', len(idx_test_date_range))\n",
    "            timedelta_interval = datetime.utcnow() - date_control\n",
    "            print('Time interval since last marker:', datetime.utcnow() - date_control)            \n",
    "            print('Average interval for single date:', str(timedelta_interval / int_interval))\n",
    "        date_control = datetime.utcnow()\n",
    "        \n",
    "    ### Basic Weighted Average Momentum factor calculating:\n",
    "    ser_wa_basic_factor = get_wa_basic_factor(iter_date)  \n",
    "    ### Weighted Average of Ranks Momentum factor calculating:\n",
    "    ser_wa_ranks_factor = get_wa_ranks_factor(iter_date) \n",
    "    ### Last Year Monthly Seasonality factor calculating:\n",
    "    ser_seas_q_factor = get_seas_q_factor(iter_date) \n",
    "    ### Last 3 Years Quarterly Seasonality factor calculating:\n",
    "    ser_seas_y_3mo_factor = get_seas_y_3mo_factor(iter_date) \n",
    "    ### Weighted Average of Residual Returns Momentum factor calculating:\n",
    "    ser_wa_residual_factor = get_wa_residual_factor(iter_date) \n",
    "    ### Weighted Average with Dynamic LookBack by Country Momentum factor calculating:\n",
    "    ser_wa_dyn_lb_ind_factor = get_wa_dyn_lb_ind_factor(iter_date) \n",
    "    ### Weighted Average with Dynamic LookBack by Region Momentum factor calculating:\n",
    "    ser_wa_dyn_lb_reg_factor = get_wa_dyn_lb_reg_factor(iter_date)   \n",
    "\n",
    "date_finish = datetime.utcnow()\n",
    "### Overall statistics printing:\n",
    "print('Finish time:', date_finish)\n",
    "print('Full interval:', date_finish - date_start)\n",
    "print('Average interval for single date:', str((date_finish - date_start) / len(idx_test_date_range)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d278664a-b141-4e26-9a00-e13f2bf37032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
