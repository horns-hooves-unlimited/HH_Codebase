{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EER FACTOR DAILY GENERATOR (SHOULD BE IGNORED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULES IMPORT (PART OF THE PRODUCT CODE)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL INITIALIZATION (SHOULD BE IGNORED)\n",
    "\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Bloomberg structured data extraction parameters:\n",
    "str_path_bb_hdf = 'Data_Files/Source_Files/Bloomberg_prepared.h5'\n",
    "str_key_fx_demeaned = 'bb_fx_demeaned'\n",
    "str_key_reer = 'bb_reer'\n",
    "str_key_neer = 'bb_neer'\n",
    "str_key_reer_sourced = 'bb_reer_sourced'\n",
    "str_key_neer_sourced = 'bb_neer_sourced'\n",
    "str_key_xcra = 'bb_xcra'\n",
    "### General parameters:\n",
    "str_measure_date_start = '1996-08-01' # Start date for efficacy measures\n",
    "str_measure_date_end = '2020-08-31' # End date for efficacy measures\n",
    "idx_measure_date_range = pd.date_range(str_measure_date_start, str_measure_date_end, freq = 'BM')\n",
    "str_source_date_start = '1992-01-01' # Start date for source vectors\n",
    "idx_source_date_range = pd.date_range(str_source_date_start, str_measure_date_end, freq = 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL INITIALIZATION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "### Filtering parameters:\n",
    "list_ison = ['DM', 'EM', 'FM']\n",
    "list_filter = ['DM', 'EM', 'FM']\n",
    "list_countries_to_exclude = ['VE'] # Countries not to play the game\n",
    "### Scaling parameter:\n",
    "int_concept_divider = 1000 # Divider to equalize concepts and GDP scales\n",
    "### Standartization parameters:\n",
    "flo_elem_similarity = 5 * (10 ** (-8)) ### THA mean ones excluding boundary\n",
    "flo_tha_ratio = 0.9 ### THA progression ratio\n",
    "int_tha_length = 24 ### THA horizon length\n",
    "list_truncate = [2.5, 2.0] # Standartization boundaries\n",
    "bool_within_market = True # Standartization way\n",
    "### FX parameters:\n",
    "list_extreme_boundaries = [-0.5, 2.0]\n",
    "### Interaction variable options:\n",
    "int_concept_lag = 3 ### Lag in months for GDP like concepts, months\n",
    "int_concept_min = 0.0 # Minimal value to compare with log(1 + EXPORT/GDP)\n",
    "int_concept_max = 0.3 # Maximal value to compare with log(1 + EXPORT/GDP)\n",
    "int_eer_fill_limit = 260 * 50 # Days for forward fill NEER and REER inside country vectors\n",
    "### Momentum calculation options\n",
    "int_mom_length = 5 # Years of momentum vector\n",
    "dict_mom_min = {} # minimal values number for momentum factor calculation, days:\n",
    "dict_mom_min['LONG_TERM'] = int(260 * 2.5)\n",
    "dict_mom_min['SHORT_TERM'] = 260 // 4\n",
    "dict_mom_hl = {} # Half-life period for momentum factor, months:\n",
    "dict_mom_hl['LONG_TERM'] = 24\n",
    "dict_mom_hl['SHORT_TERM'] = 3\n",
    "### Factors options:\n",
    "dict_combinations = {}\n",
    "dict_combinations['LONG_TERM_EER'] = ('LONG_TERM', 'REER')\n",
    "dict_combinations['SHORT_TERM_MIXED'] = ('SHORT_TERM', 'NEER')\n",
    "dict_combinations['LONG_TERM_EXPORT'] = ('LONG_TERM', 'EXPORT')\n",
    "### Factor averaging weights:\n",
    "dict_factors_weights = {}\n",
    "dict_factors_weights['LONG_TERM_EER'] = 1.0\n",
    "dict_factors_weights['SHORT_TERM_MIXED'] = 1.0\n",
    "dict_factors_weights['LONG_TERM_EXPORT'] = 0.75 # 1.0\n",
    "### Factors signs:\n",
    "dict_factors_signs = {}\n",
    "dict_factors_signs['LONG_TERM_EER'] = -1.0\n",
    "dict_factors_signs['SHORT_TERM_MIXED'] = -1.0\n",
    "dict_factors_signs['LONG_TERM_EXPORT'] = 1.0\n",
    "### Work periods:\n",
    "ser_work_periods = pd.Series(1 , index = pd.MultiIndex.from_product([['Year', 'Month'], ['Y', 'M', 'D']], names = ['Period', 'Frequency']))\n",
    "ser_work_periods['Year', 'M'] = 12\n",
    "ser_work_periods['Year', 'D'] = 260\n",
    "ser_work_periods['Month', 'Y'] = 0\n",
    "ser_work_periods['Month', 'D'] = 22\n",
    "flo_exp_weight_month = ser_work_periods['Year', 'D'] / ser_work_periods['Year', 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING GEOMETRICAL WEIGHT (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def geom_weight_single(flo_ratio, flo_factor = 1, num_element = 0):\n",
    "    ### Results output:\n",
    "    return flo_factor * (flo_ratio ** num_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEAN MOMENTUM FUNCTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def rolling_cond_weighted_mean(ser_country_matrix, ser_full_source, int_mean_win, int_mean_min, list_weight = False, ser_full_cond = False):\n",
    "    ### Defining conditional average calculator:\n",
    "    def conditional_average(ser_source, list_weight, int_min_count = 0, ser_condition = False):\n",
    "        ### Weight setting\n",
    "        ser_weight = pd.Series(list_weight[ : len(ser_source.index)], ser_source.index)\n",
    "        ### If we have condition we should resort the weight array:\n",
    "        if not isinstance(ser_condition, bool):\n",
    "            ser_condition_sorted = pd.Series(ser_condition.sort_values().index, ser_condition.index)\n",
    "            ser_condition_sorted.name = 'Condition'\n",
    "            ser_weight = pd.concat([ser_weight, ser_condition_sorted], axis = 1).reset_index(drop = True).set_index('Condition').squeeze().sort_index()            \n",
    "        ### Results output:\n",
    "        return weighted_average(ser_source, ser_weight, int_min_count)    \n",
    "    ### Country saving:\n",
    "    str_country = ser_country_matrix.index[0][1]\n",
    "    ### Checking for country presence in source vector:\n",
    "    if (str_country in ser_full_source.index.get_level_values(1)):\n",
    "        ### Filtering country vector from source:\n",
    "        ser_country_source = ser_full_source.loc[All, str_country]\n",
    "        if not isinstance(ser_full_cond, bool):\n",
    "            ser_country_cond = ser_full_cond.loc[All, str_country]\n",
    "        ### Looping over matrix index dates:\n",
    "        for iter_bm_date in ser_country_matrix.index.get_level_values(0):\n",
    "            try:\n",
    "                ### Defining monthend date number in source country vector:\n",
    "                int_idx_num = ser_country_source.index.get_loc(iter_bm_date)\n",
    "                ### Creating vectors for numerator and denominator means calculation:\n",
    "                ser_rolled_source = ser_country_source.iloc[max((int_idx_num - int_mean_win + 1), 0) : int_idx_num + 1]\n",
    "                if not isinstance(ser_full_cond, bool):\n",
    "                    ser_rolled_cond = ser_country_cond.loc[ser_rolled_source.index]\n",
    "                else:\n",
    "                    ser_rolled_cond = False\n",
    "                ### Action for MatLab compatibility:\n",
    "                ser_rolled_source.iloc[0] = np.NaN\n",
    "                ### Simple mean calculation:\n",
    "                if isinstance(list_weight, bool):\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = weighted_average(ser_rolled_source, False, int_mean_min)\n",
    "                else:\n",
    "                    ### Weighted mean calculation:\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = conditional_average(ser_rolled_source, list_weight, int_mean_min, ser_rolled_cond)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    ### Resulting vector output:\n",
    "    return ser_country_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def multistep_standartize(ser_data_source, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):  \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_data_source.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_data_source.dropna()\n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        if reuse_outliers:\n",
    "            ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        else:\n",
    "            ### Saving to result and excluding from further calculations truncated values:             \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):\n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_data_source.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR CROSS-SECTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def ison_standartize(ser_to_manage, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False, within_market = False):\n",
    "    ### Multi-step standartizing:\n",
    "    if (within_market):\n",
    "    ### Within market standartizing:\n",
    "        ser_result = ser_to_manage.groupby(by = 'Market', group_keys = False).apply(multistep_standartize, arr_truncate, ser_weight, \n",
    "                                                                                                  reuse_outliers, center_result, full_result)\n",
    "    else:\n",
    "    ### Full universe standartizing:\n",
    "        ser_result = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result)\n",
    "    ### Results output:\n",
    "    return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP THA STANDARTIZATION BY MARKET FOR CROSS-SECTION (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def tha_standartize(ser_to_manage, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):\n",
    "    ### Multi-step standartizing:\n",
    "    (ser_reversed, list_mean, list_std) = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result = True)\n",
    "    for iter_num in range(len(arr_truncate))[::-1]:\n",
    "        ser_reversed = (ser_reversed * list_std[iter_num] + list_mean[iter_num])\n",
    "    ser_demeaned = ser_reversed.groupby('Market').apply(lambda ser_region: ser_region - ser_region.mean())\n",
    "    ser_stand_z = multistep_standartize(ser_demeaned, arr_truncate, ser_weight, reuse_outliers, center_result, full_result = False)\n",
    "    ### Results output:\n",
    "    return ser_stand_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING UNIVERSAL AUTOCORRELATION FOR DATE-COUNTRY-UNIVERSE SERIES (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def vector_autocorr(ser_source, int_shift):\n",
    "    ### Defining adding full universe for each date:\n",
    "    def universe_reindex(iter_group, idx_universe):\n",
    "        df_iter_result = iter_group.unstack('Date').reindex(idx_universe).sort_index().stack('Date', dropna = False)\n",
    "        ### Results output:\n",
    "        return df_iter_result   \n",
    "    ### Defining adding full date range for each country and date index shifting:\n",
    "    def date_reindex(iter_group, idx_date_range, num_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-int_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result   \n",
    "    ### Defining by date correrlation function:\n",
    "    def corr_by_date(iter_group):\n",
    "        num_iter_corr = iter_group['Corr_factor_minus'].corr(iter_group['Corr_factor_plus'])\n",
    "        ### Results output:\n",
    "        return num_iter_corr       \n",
    "    ### Preparing expanded universe for autocorrelation performing:\n",
    "    idx_date_range = ser_source.index.get_level_values(0).unique()\n",
    "    idx_universe = ser_source.index.get_level_values(1).unique()\n",
    "    ser_source_full = ser_source.to_frame().reset_index('Market', drop = True).groupby('Date', group_keys = False).apply(universe_reindex, idx_universe)\\\n",
    "                                .swaplevel().squeeze()\n",
    "    ### Autocorrelation preparing:\n",
    "    ser_source_plus = ser_source_full.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[1 : ])\\\n",
    "                                     .sort_index(level = ['Date', 'Country'])\n",
    "    ser_source_minus = ser_source_full.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[: -1])\\\n",
    "                                      .sort_index(level = ['Date', 'Country'])\n",
    "    ### Artificial series combining for indexes synchronization:        \n",
    "    ser_source_plus_shifted = ser_source_plus.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, int_shift)\n",
    "    df_to_corr = pd.concat([ser_source_minus, ser_source_plus_shifted], axis = 1)\n",
    "    df_to_corr.columns = ['Corr_factor_minus', 'Corr_factor_plus']\n",
    "    ser_autocorr_vector = df_to_corr.groupby('Date').apply(corr_by_date).shift(int_shift)\n",
    "    ### Results output:\n",
    "    return ser_autocorr_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK (PART OF THE PRODUCT CODE)\n",
    "\n",
    "def single_factor_standartize(ser_factor, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, within_market = False, \n",
    "                              flag_tha = False, flo_similarity = 5 * (10 ** (-8))):\n",
    "    ### Local constants:\n",
    "    dict_tha_pow = {}\n",
    "    dict_tha_pow['monthly'] = 1\n",
    "    dict_tha_pow['quarterly'] = 1 / 3\n",
    "    dict_tha_pow['annual'] = 1 / 12\n",
    "    ### Weights preparing:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "        ser_weight.name = 'Weight'\n",
    "    ### Multi-step standartizing:        \n",
    "    df_factor = ser_factor.to_frame().join(ser_weight, how = 'left')\n",
    "    df_factor.columns = ['Factor', 'Weight']\n",
    "    ### Time-horizon adjusted standartization:  \n",
    "    if (flag_tha):\n",
    "        ### Z-scored vector calculating:       \n",
    "        ser_stand_z = df_factor.groupby('Date', group_keys = False)\\\n",
    "                               .apply(lambda iter_df: tha_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False))\n",
    "        ### Results output:\n",
    "        ser_stand_z.name = ser_factor.name\n",
    "        ### Autocorrelation vector calculating:\n",
    "        ser_autocorr_vector = ser_stand_z.groupby('Market').apply(vector_autocorr, 1)\n",
    "        ser_autocorr_vector.name = 'Autocorr'\n",
    "        ser_autocorr_cum_mean = ser_autocorr_vector.loc[np.abs(ser_autocorr_vector - 1) > flo_similarity].groupby('Market', group_keys = False).expanding().mean()\n",
    "        ### THA-coeficcient calculating:\n",
    "        ser_tha_coeff = ser_autocorr_cum_mean.transform(lambda iter_mean: max(iter_mean, 0.0) ** dict_tha_pow[flag_tha])\n",
    "        ser_tha_coeff = ser_tha_coeff.transform(lambda iter_mean: \n",
    "                                                sum(map(lambda iter_num: geom_weight_single(flo_tha_ratio * iter_mean, 1, iter_num), range(int_tha_length))) / 2)\n",
    "        ser_tha_coeff = ser_tha_coeff.swaplevel()\n",
    "        ser_tha_coeff = ser_tha_coeff.unstack('Market').reindex(ser_stand_z.index.levels[0]).stack('Market', dropna = False).sort_index(level = ['Date', 'Market'])        \n",
    "        ### THA-adjusted z-score calculating:\n",
    "#        ser_stand_s = (ser_stand_z * ser_tha_coeff)\n",
    "        ### Artifical filling values for first date of region appearance (not to loose observations):\n",
    "        ser_stand_s = (ser_stand_z * ser_tha_coeff.fillna(0.5))        \n",
    "        ser_stand_s = ser_stand_s[ser_stand_s.index.dropna()].reorder_levels(['Date', 'Country', 'Market']).sort_index()\n",
    "        ### Standart deviation for THA-adjusted z-score calculating:\n",
    "        ser_region_std = ser_stand_s.groupby(['Date', 'Market']).std()\n",
    "        ser_universe_std = ser_stand_s.groupby(['Date']).std()\n",
    "        ser_universe_std = pd.concat([ser_universe_std], keys = ['Overall'], names = ['Market']).swaplevel()\n",
    "        ser_std = pd.concat([ser_region_std, ser_universe_std], axis = 0).sort_index()\n",
    "        ### Results output:\n",
    "        return (ser_stand_s, ser_stand_z, ser_autocorr_vector, ser_tha_coeff, ser_std)\n",
    "    ### Simple standartization:    \n",
    "    else:    \n",
    "        ser_result = df_factor.groupby('Date', group_keys = False)\\\n",
    "                     .apply(lambda iter_df: ison_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False, within_market))\n",
    "        ### Results output:\n",
    "        ser_result.name = ser_factor.name\n",
    "        return ser_result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE (SHOULD BE ADOPTED OR IGNORED)\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(io = str_path_universe, sheet_name = 0, header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: BLOOMBERG STRUCTURED DATA & ISON MEMBERSHIP LOADING (SHOULD BE ADOPTED)\n",
    "\n",
    "### FX Rates demeaned:\n",
    "ser_fx_rate_demeaned = pd.read_hdf(str_path_bb_hdf, key = str_key_fx_demeaned)\n",
    "### Real effective rates:\n",
    "ser_reer = pd.read_hdf(str_path_bb_hdf, key = str_key_reer)\n",
    "### Nominal effective rates:\n",
    "ser_neer = pd.read_hdf(str_path_bb_hdf, key = str_key_neer)\n",
    "### Country export & GDP:\n",
    "df_xcra_filled = pd.read_hdf(str_path_bb_hdf, key = str_key_xcra)\n",
    "ser_export = df_xcra_filled['Exports']\n",
    "ser_gdp = df_xcra_filled['GDP']\n",
    "### End-of-business-month ISON membership:\n",
    "ser_ison = ison_membership_converting(str_path_universe, datetime.strptime(str_measure_date_end, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: BLOOMBERG STRUCTURED DATA LOADING (SHOULD BE ADOPTED OR IGNORED)\n",
    "\n",
    "### EER with sources for each country (for composite Short-Term factor source constructing):\n",
    "ser_reer_sourced = pd.read_hdf(str_path_bb_hdf, key = str_key_reer_sourced)\n",
    "ser_neer_sourced = pd.read_hdf(str_path_bb_hdf, key = str_key_neer_sourced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FACTOR SOURCE DATA PREPARING (SHOULD BE ADOPTED)\n",
    "\n",
    "### Effective exchange rates options preparing:\n",
    "dict_ser_eer = {}\n",
    "### Sources forward filling and reindexing:\n",
    "ser_reer_source = ser_reer.unstack('Country').reindex(idx_source_date_range).ffill(limit = int_eer_fill_limit).stack('Country').sort_index()\n",
    "ser_neer_source = ser_neer.unstack('Country').reindex(idx_source_date_range).ffill(limit = int_eer_fill_limit).stack('Country').sort_index()\n",
    "ser_fx_source = ser_fx_rate_demeaned.unstack('Country').reindex(idx_source_date_range).ffill(limit = int_eer_fill_limit).stack('Country').sort_index()\n",
    "ser_export_source = ser_export.unstack('Country').reindex(idx_source_date_range).ffill(limit = int_eer_fill_limit).stack('Country').sort_index()\n",
    "### REER source saving:\n",
    "dict_ser_eer['REER'] = ser_reer_source\n",
    "### Selecting all ISON countries:\n",
    "set_ison = set(ser_ison.dropna().index.get_level_values('Country').unique())\n",
    "### Selecting all REER countries:\n",
    "set_reer_all = set(ser_reer.dropna().index.get_level_values('Country').unique())\n",
    "### Selecting all NEER countries:\n",
    "set_neer_all = set(ser_neer.dropna().index.get_level_values('Country').unique())\n",
    "### Selecting countries, where REER has monthly frequency:\n",
    "set_reer_monthly = set(ser_reer_sourced.loc[All, All, ['IMF', 'BIS']].index.get_level_values(1).unique())\n",
    "### Defining countries from REER to participate in NEER source:\n",
    "set_reer_st = set_reer_all - set_reer_monthly\n",
    "### Defining countries from NEER to participate in NEER source:\n",
    "ser_neer_st = set_reer_monthly & set_neer_all\n",
    "### Defining rest of countries to participate in NEER source from FX rates:\n",
    "set_fx_st = set_ison - (set_reer_st | ser_neer_st)\n",
    "### Converting sets to lists:\n",
    "list_reer_st = sorted(list(set_reer_st))\n",
    "list_neer_st = sorted(list(ser_neer_st))\n",
    "list_fx_st = sorted(list(set_fx_st))\n",
    "### NEER source saving:\n",
    "dict_ser_eer['NEER'] = pd.concat([ser_reer_source.loc[All, list_reer_st], ser_neer_source.loc[All, list_neer_st], ser_fx_source.loc[All, list_fx_st]]).sort_index()\n",
    "### EXPORT source saving:\n",
    "dict_ser_eer['EXPORT'] = ser_export_source    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INTERACTION VARIABLE PREPARING (SHOULD BE ADOPTED)\n",
    "\n",
    "### Concepts options preparing:\n",
    "dict_ser_concept = {}\n",
    "### Concept data shifting:\n",
    "df_xcra_shifted = df_xcra_filled.groupby('Country').shift(int_concept_lag)\n",
    "### Concepts calculating:\n",
    "ser_open_concept = ser_export / ser_gdp\n",
    "### XCRA concepts scaling:\n",
    "ser_open_concept = ser_open_concept / int_concept_divider\n",
    "### XCRA concepts adjusting:\n",
    "ser_open_concept.loc[ser_open_concept <= -1] = -0.99\n",
    "ser_open_concept = np.maximum(int_concept_min, (np.minimum(int_concept_max, np.log(1 + ser_open_concept))))      \n",
    "### Concept series renaming:\n",
    "ser_open_concept.name = 'Multiplicator'\n",
    "dict_ser_concept['EXP_GDP_rate'] = ser_open_concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        Country  Market\n",
       "1996-08-30  AT       DM        0.246902\n",
       "            AU       DM        1.843528\n",
       "            BE       DM       -0.061191\n",
       "            CA       DM       -1.689743\n",
       "            CH       DM        0.615481\n",
       "            DE       DM        0.016261\n",
       "            DK       DM        0.030603\n",
       "            ES       DM       -0.960748\n",
       "            FI       DM       -0.260437\n",
       "            FR       DM        0.005586\n",
       "            GB       DM       -0.888181\n",
       "            HK       DM       -0.024373\n",
       "            IE       DM       -0.119403\n",
       "            IT       DM       -0.301307\n",
       "            JP       DM       -2.009767\n",
       "            NL       DM        0.027876\n",
       "            NO       DM       -0.078223\n",
       "            NZ       DM        1.989312\n",
       "            SE       DM        1.392938\n",
       "            SG       DM        0.652140\n",
       "            US       DM       -0.427254\n",
       "Name: Factor_standartized_standartized, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "df_factor = ser_iter_factor.to_frame().assign(**{'Weight': 1})\n",
    "\n",
    "#df_factor.groupby('Date', group_keys = False)\\\n",
    "#                               .apply(lambda iter_df: tha_standartize(iter_df['Factor'], list_truncate, iter_df['Weight'], False, True, False)).transpose()\n",
    "tha_standartize(df_factor['Factor'], list_truncate, df_factor['Weight'], False, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Factor</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>Market</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"21\" valign=\"top\">1996-08-30</td>\n",
       "      <td>AT</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AU</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>BE</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CA</td>\n",
       "      <td>DM</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>CH</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DE</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>DK</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ES</td>\n",
       "      <td>DM</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FI</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FR</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>GB</td>\n",
       "      <td>DM</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>HK</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>IE</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>IT</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>JP</td>\n",
       "      <td>DM</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NL</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NO</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NZ</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SE</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>SG</td>\n",
       "      <td>DM</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>US</td>\n",
       "      <td>DM</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Factor  Weight\n",
       "Date       Country Market                  \n",
       "1996-08-30 AT      DM      0.000045       1\n",
       "           AU      DM      0.000165       1\n",
       "           BE      DM      0.000022       1\n",
       "           CA      DM     -0.000101       1\n",
       "           CH      DM      0.000072       1\n",
       "           DE      DM      0.000027       1\n",
       "           DK      DM      0.000029       1\n",
       "           ES      DM     -0.000046       1\n",
       "           FI      DM      0.000007       1\n",
       "           FR      DM      0.000027       1\n",
       "           GB      DM     -0.000040       1\n",
       "           HK      DM      0.000024       1\n",
       "           IE      DM      0.000017       1\n",
       "           IT      DM      0.000004       1\n",
       "           JP      DM     -0.000250       1\n",
       "           NL      DM      0.000028       1\n",
       "           NO      DM      0.000020       1\n",
       "           NZ      DM      0.000206       1\n",
       "           SE      DM      0.000131       1\n",
       "           SG      DM      0.000075       1\n",
       "           US      DM     -0.000006       1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: STANDALONE FACTORS CALCULATING LOOP (SHOULD BE ADOPTED)\n",
    "\n",
    "for iter_date in idx_measure_date_range[: 1]:\n",
    "    ### Container for standalone factors:\n",
    "    dict_date_factor_hdf = {}\n",
    "    ### Factors looping:\n",
    "    for iter_factor in dict_combinations.keys():\n",
    "        ### Parameters loading:\n",
    "        iter_term = dict_combinations[iter_factor][0]\n",
    "        iter_eer = dict_combinations[iter_factor][1]\n",
    "        ### Iteration data loading:\n",
    "        ser_iter_concept = ser_open_concept.loc[iter_date, All]\n",
    "        ### Factor matrix creating:\n",
    "        ser_iter_factor = pd.Series(index = pd.MultiIndex.from_product([[iter_date], ser_ison.index.get_level_values(1).unique()])).sort_index()\n",
    "        ser_iter_factor.index.set_names(['Date', 'Country'], inplace = True)                \n",
    "        ### Mean factor calculating:    \n",
    "        ser_iter_eer = dict_ser_eer[iter_eer]\n",
    "        ser_iter_delta = ser_iter_eer.groupby('Country').diff() / ser_iter_eer.groupby('Country').shift()   \n",
    "        ser_iter_delta = ser_iter_delta.replace([np.inf, -np.inf], np.NaN)\n",
    "        ### Extremum FX returns zeroing in case of shotr-treem factor:\n",
    "        if (iter_factor == 'SHORT_TERM_MIXED'):\n",
    "            ser_iter_delta.loc[All, list_fx_st] = ser_iter_delta.loc[All, list_fx_st]\\\n",
    "                                                  .where(((ser_iter_delta >= list_extreme_boundaries[0]) & (ser_iter_delta <= list_extreme_boundaries[1])), 0.0)\n",
    "        ### Momentum parameters:\n",
    "        int_mom_hl = dict_mom_hl[iter_term] * flo_exp_weight_month\n",
    "        int_mom_win = int_mom_length * ser_work_periods['Year', 'D']\n",
    "        int_mom_min = dict_mom_min[iter_term]\n",
    "        ### Weights array:\n",
    "        list_weight = list(map(lambda iter_num: exp_weight_single(int_mom_hl, iter_num), range(int_mom_win)))[::-1]\n",
    "        ### Momentum factor calculation:\n",
    "        ser_iter_factor = ser_iter_factor.groupby('Country').transform(rolling_cond_weighted_mean, ser_iter_delta, int_mom_win, int_mom_min, list_weight, False)\n",
    "        ### Factor ISONing:\n",
    "        ser_iter_factor = ser_iter_factor.to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "        ser_iter_factor.name = 'Factor'\n",
    "        ### Concept multiplicator ISONing:\n",
    "        ser_iter_concept = ser_iter_concept.to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "        ### Regions clearing:\n",
    "        ser_iter_factor = ser_iter_factor.loc[All, All, list_ison]\n",
    "        ser_iter_concept = ser_iter_concept.loc[All, All, list_ison]\n",
    "        ### Countries filtering:\n",
    "        ser_iter_factor = ser_iter_factor.drop(list_countries_to_exclude, level = 'Country') \n",
    "        ser_iter_concept = ser_iter_concept.drop(list_countries_to_exclude, level = 'Country')\n",
    "        ### Factor and Multiplicator standartizing (Multiplicator shifting), multiplying and restandartizing:\n",
    "        ser_iter_factor_std = dict_factors_signs[iter_factor] \\\n",
    "                              * single_factor_standartize(ser_iter_factor, list_truncate, within_market = bool_within_market, flag_tha = 'monthly')[0]    \n",
    "        ser_iter_factor_std.name = 'Factor'  \n",
    "        ser_iter_multiplied = ser_iter_factor_std * ser_iter_concept        \n",
    "        ### Preliminary results saving:\n",
    "        dict_date_factor_hdf[iter_factor] = ser_iter_multiplied      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
