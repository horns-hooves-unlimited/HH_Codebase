{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EER FACTORS CREATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as ss\n",
    "import math     \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "    \n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Bloomberg structured data extraction parameters:\n",
    "str_path_bb_hdf = 'Data_Files/Source_Files/Bloomberg_prepared.h5'\n",
    "str_key_ret_daily = 'bb_ret_daily'\n",
    "str_key_ret_monthly = 'bb_ret_monthly'\n",
    "str_key_mmr = 'bb_mmr'\n",
    "str_key_fx_country = 'bb_fx_country'\n",
    "str_key_fx_demeaned = 'bb_fx_demeaned'\n",
    "str_key_fx_currency = 'bb_fx_currency'\n",
    "str_key_mcap = 'bb_mcap'\n",
    "str_key_reer = 'bb_reer'\n",
    "str_key_neer = 'bb_neer'\n",
    "str_key_reer_sourced = 'bb_reer_sourced'\n",
    "str_key_neer_sourced = 'bb_neer_sourced'\n",
    "str_key_xcra = 'bb_xcra'\n",
    "### NEER usage scheme:\n",
    "bool_neer_raw = False\n",
    "### Standartization parameters:\n",
    "flo_elem_similarity = 5 * (10 ** (-8)) ### THA mean ones excluding boundary\n",
    "flo_tha_ratio = 0.9 ### THA progression ratio\n",
    "int_tha_length = 24 ### THA horizon length\n",
    "list_truncate = [2.5, 2.0] # Standartization boundaries\n",
    "bool_within_market = True # Standartization way\n",
    "### Factors parameters:\n",
    "str_measure_date_start = '1996-08-01' # Start date for efficacy measures\n",
    "str_measure_date_end = '2020-08-31' # End date for efficacy measures\n",
    "idx_measure_date_range = pd.date_range(str_measure_date_start, str_measure_date_end, freq = 'BM')\n",
    "str_source_date_start = '1992-01-01' # Start date for source vectors\n",
    "idx_source_date_range = pd.date_range(str_source_date_start, str_measure_date_end, freq = 'B')\n",
    "list_ison = ['DM', 'EM', 'FM']\n",
    "list_filter = ['DM', 'EM', 'FM']\n",
    "list_countries_to_exclude = ['VE'] # Countries not to play the game\n",
    "flo_returns_similarity = 0.0025 # Selecting countries with currencies bound to the USD\n",
    "flo_returns_completeness = 1 / 3\n",
    "int_concept_lag = 3 ### Lag in months for GDP like concepts, months\n",
    "int_concept_divider = 1000 # Divider to equalize concepts and GDP scales\n",
    "int_concept_min = 0.0 # Minimal value to compare with log(1 + EXPORT/GDP)\n",
    "int_concept_max = 0.3 # Maximal value to compare with log(1 + EXPORT/GDP)\n",
    "int_eer_fill_limit = 260 * 50 # Days for forward fill NEER and REER inside country vectors ### For product version we need value = 66, days\n",
    "int_regress_win = 60 # Regression window length for alternative sensitivity concept\n",
    "int_regress_hl = 3 # Half-life period for alternative sensitivity concept, months\n",
    "int_factor_addendum = 2.5 # list_truncate[0] # Factor scaler\n",
    "dict_numer_ma_win = {} # Moving average window length for factors numerators, days\n",
    "dict_numer_ma_win['LONG_TERM'] = round(260 / 12 * 3)\n",
    "dict_numer_ma_win['SHORT_TERM'] = round(260 / 12 / 2)\n",
    "dict_denom_ma_win = {} # Moving average window length for factors denomenators, days\n",
    "dict_denom_ma_win['LONG_TERM'] = round(260 * 5)\n",
    "dict_denom_ma_win['SHORT_TERM'] = round(260 / 12 * 6)\n",
    "dict_numer_ma_min = {} # Moving average minimal notna count for factors numerators, days\n",
    "dict_numer_ma_min['LONG_TERM'] = round(1)\n",
    "dict_numer_ma_min['SHORT_TERM'] = round(1)\n",
    "dict_denom_ma_min = {} # Moving average minimal notna count for factors denominators, days\n",
    "dict_denom_ma_min['LONG_TERM'] = dict_denom_ma_win['LONG_TERM'] // 2\n",
    "dict_denom_ma_min['SHORT_TERM'] = dict_denom_ma_win['SHORT_TERM'] // 2\n",
    "int_mom_length = 5 # Years of momentum vector\n",
    "dict_mom_min = {} # minimal values number for momentum factor calculation, days:\n",
    "dict_mom_min['LONG_TERM'] = int(260 * 2.5)\n",
    "dict_mom_min['SHORT_TERM'] = 260 // 4\n",
    "dict_mom_hl = {} # Half-life period for momentum factor, months:\n",
    "dict_mom_hl['LONG_TERM'] = 24\n",
    "dict_mom_hl['SHORT_TERM'] = 3\n",
    "### Factors options:\n",
    "dict_combinations = {}\n",
    "dict_combinations['LONG_TERM_EER'] = ('LONG_TERM', 'MOMENTUM', 'EXP_GDP_rate', 'REER', 'HEDGED')\n",
    "dict_combinations['SHORT_TERM_MIXED'] = ('SHORT_TERM', 'MOMENTUM', 'EXP_GDP_rate', 'NEER', 'HEDGED')\n",
    "dict_combinations['LONG_TERM_EXPORT'] = ('LONG_TERM', 'MOMENTUM', 'EXP_GDP_rate', 'EXPORT', 'HEDGED')\n",
    "dict_combinations['COMBO_DOUBLE'] = ('DOUBLE', 'COMBO', 'COMBO', 'COMBO', 'COMBO')\n",
    "dict_combinations['COMBO_TRIPLE'] = ('TRIPLE', 'COMBO', 'COMBO', 'COMBO', 'COMBO')\n",
    "### Factor averaging weights:\n",
    "dict_factors_weights = {}\n",
    "dict_factors_weights['LONG_TERM_EER'] = 1.0\n",
    "dict_factors_weights['SHORT_TERM_MIXED'] = 1.0\n",
    "dict_factors_weights['LONG_TERM_EXPORT'] = 0.75 # 1.0\n",
    "### Factors signs:\n",
    "dict_factors_signs = {}\n",
    "dict_factors_signs['LONG_TERM_EER'] = -1.0\n",
    "dict_factors_signs['SHORT_TERM_MIXED'] = -1.0\n",
    "dict_factors_signs['LONG_TERM_EXPORT'] = 1.0\n",
    "### FX Factor parameters:\n",
    "int_short_diff = 21 * 3\n",
    "list_extreme_boundaries = [-0.5, 2.0]\n",
    "### Work periods:\n",
    "ser_work_periods = pd.Series(1 , index = pd.MultiIndex.from_product([['Year', 'Month'], ['Y', 'M', 'D']], names = ['Period', 'Frequency']))\n",
    "ser_work_periods['Year', 'M'] = 12\n",
    "ser_work_periods['Year', 'D'] = 260\n",
    "ser_work_periods['Month', 'Y'] = 0\n",
    "ser_work_periods['Month', 'D'] = 22\n",
    "flo_exp_weight_month = ser_work_periods['Year', 'D'] / ser_work_periods['Year', 'M']\n",
    "### Transitional results parameters:\n",
    "str_path_trans_hdf = 'Data_Files/Test_Files/EER_factors_transitional.h5'\n",
    "str_key_trans_ret = 'trans_ret'\n",
    "str_key_trans_mcap = 'trans_mcap'\n",
    "str_key_trans_factor = 'trans_factor'\n",
    "### Measures parameters:\n",
    "list_measure = ['fmb_weighted', 'qtl4'] # Efficacy measures list\n",
    "list_back_period = [99, 10, 5] # Look back periods\n",
    "int_horizon = 12 # Measure stats horizon\n",
    "### \n",
    "str_path_efficacy_hdf = 'Data_Files/Test_Files/EER_factors_stats.h5'\n",
    "str_path_vectors_hdf = 'Data_Files/Test_Files/EER_factors_vectors.h5'\n",
    "str_key_efficacy = 'fmb_weight'\n",
    "str_path_efficacy_xlsx = 'Data_Files/Test_Files/EER_factors_stats.xlsx'\n",
    "str_path_vectors_xlsx = 'Data_Files/Test_Files/EER_factors_vectors.xlsx'\n",
    "str_path_factors_xlsx = 'Data_Files/Test_Files/EER_factors_source.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING DATE/COUNTRY DATA VECTOR DESCRIBER FUNCTION\n",
    "\n",
    "def date_country_vector_describer(ser_data, ser_ison):\n",
    "    ### ISON countries set:\n",
    "    set_ison_countries = set(ser_ison.index.get_level_values(1).unique())\n",
    "    ### Vector countries set:    \n",
    "    set_vector_countries = set(ser_data.dropna().index.get_level_values(1).unique())    \n",
    "    ### Vector completeness:\n",
    "    print('Data vector name: {}'.format(ser_data.name))\n",
    "    print('Data vector completeness: {:.2%}'.format(ser_data.count() / len(ser_data.index))) \n",
    "    print('ISON countries completeness: {:.2%} ({} / {})'.format(len(set_vector_countries.intersection(set_ison_countries)) / len(set_ison_countries),\n",
    "                                                                 len(set_vector_countries.intersection(set_ison_countries)),\n",
    "                                                                 len(set_ison_countries)))\n",
    "    print('Absent ISON countries: [{}]'.format(str(', '.join(sorted(list(set_ison_countries - set_vector_countries))))))\n",
    "    ### ISON Universe binding (if needed):\n",
    "    if not ('Market' in ser_data.index.names):\n",
    "        ser_data = ser_data.to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\\\n",
    "                                      .loc[All, All, ['DM', 'EM', 'FM']].sort_index(level = ['Date', 'Country'])\n",
    "    ### Dates for heatmap x-axis labeles:\n",
    "    list_idx_dates = ser_data.index.get_level_values('Date').unique()\n",
    "    ### Dates reindexation (adding NaN values for absent observations):\n",
    "    ser_region_data = ser_data.loc[All, All, ['DM', 'EM', 'FM']].droplevel('Market').unstack('Country').reindex(list_idx_dates).stack('Country', dropna = False)      \n",
    "    ### Countries number for heatmap height defining:\n",
    "    int_fig_height = len(ser_region_data.index.get_level_values('Country').unique())    \n",
    "    ### Adding shade column for future heatmap striping:\n",
    "    list_countries = list(ser_region_data.index.get_level_values('Country').unique())\n",
    "    dict_countries = dict(zip(list_countries, map(lambda iter_num: iter_num % 2 + 2, range(len(list_countries)))))\n",
    "    df_region_shades = ser_region_data.to_frame().assign(Shade = list(map(dict_countries.get, ser_region_data.index.get_level_values('Country'))))\n",
    "    df_region_shades.columns = ['Data', 'Shade']\n",
    "    ### Heatmap drawing:\n",
    "    fig_heatmap = plt.figure(figsize = (15, int_fig_height // 5))\n",
    "    df_region_data = (df_region_shades['Data'] / df_region_shades['Data'] * df_region_shades['Shade']).unstack('Date').sort_index()\n",
    "    df_region_data.columns = df_region_data.columns.strftime('%d-%m-%Y')\n",
    "    ax_heatmap = sns.heatmap(df_region_data, cbar = False, annot = False, cmap = 'binary', xticklabels = 'auto', yticklabels = True, \n",
    "                             vmin = 0.0, vmax = 6.0)\n",
    "    ax_heatmap.set_title('ISON Universe')    \n",
    "    ### Visualizer heatmap plotting:        \n",
    "    for str_region_code, ser_region_data in ser_data.groupby('Market'):\n",
    "        ### Dates reindexation (adding NaN values for absent observations):\n",
    "        ser_region_data = ser_region_data.droplevel('Market').unstack('Country').reindex(list_idx_dates).stack('Country', dropna = False)   \n",
    "        ### Countries number for heatmap height defining:        \n",
    "        int_fig_height = len(ser_region_data.index.get_level_values('Country').unique())\n",
    "        ### Adding shade column for future heatmap striping:\n",
    "        list_countries = list(ser_region_data.index.get_level_values('Country').unique())\n",
    "        dict_countries = dict(zip(list_countries, map(lambda iter_num: iter_num % 2 + 2, range(len(list_countries)))))\n",
    "        df_region_shades = ser_region_data.to_frame().assign(Shade = list(map(dict_countries.get, ser_region_data.index.get_level_values('Country'))))\n",
    "        df_region_shades.columns = ['Data', 'Shade']\n",
    "        ### Heatmap drawing:\n",
    "        fig_heatmap = plt.figure(figsize = (15, int_fig_height // 5))\n",
    "        df_region_data = (df_region_shades['Data'] / df_region_shades['Data'] * df_region_shades['Shade']).unstack('Date').sort_index()\n",
    "        df_region_data.columns = df_region_data.columns.strftime('%d-%m-%Y')\n",
    "        ax_heatmap = sns.heatmap(df_region_data, cbar = False, annot = False, cmap = 'binary', xticklabels = 'auto', yticklabels = True, \n",
    "                                 vmin = 0.0, vmax = 6.0)\n",
    "        ax_heatmap.set_title(str_region_code)\n",
    "    ### Plots showing:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEAN MOMENTUM FUNCTION (TO CALCULATE FACTOR ONLY FOR MONTHENDS):\n",
    "\n",
    "def rolling_cond_mean_momentum(ser_country_matrix, ser_full_source, int_numer_win, int_numer_min, int_denom_win, int_denom_min):\n",
    "    ### Country saving:\n",
    "    str_country = ser_country_matrix.index[0][1]\n",
    "    ### Checking for country presence in source vector:\n",
    "    if (str_country in ser_full_source.index.get_level_values(1)):\n",
    "        ### Filtering country vector from source:\n",
    "        ser_country_source = ser_full_source.loc[All, str_country]\n",
    "        ### Looping over matrix index dates:\n",
    "        for iter_bm_date in ser_country_matrix.index.get_level_values(0):\n",
    "            try:\n",
    "                ### Defining monthend date number in source country vector:\n",
    "                int_idx_num = ser_country_source.index.get_loc(iter_bm_date)\n",
    "                ### Creating vectors for numerator and denominator means calculation:\n",
    "                ser_rolled_numer = -ser_country_source.iloc[max((int_idx_num - int_numer_win + 1), 0) : int_idx_num + 1]        \n",
    "                ser_rolled_denom = -ser_country_source.iloc[max((int_idx_num - int_denom_win + 1), 0) : int_idx_num + 1]\n",
    "                ### Checking for minimal data presence:\n",
    "                if ((ser_rolled_numer.count() >= int_numer_min) & (ser_rolled_denom.count() >= int_denom_min)):\n",
    "                    ### Mena momentum value calculation:\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = np.log(ser_rolled_numer.mean() / ser_rolled_denom.mean())\n",
    "            except KeyError:\n",
    "                pass\n",
    "    ### Resulting vector output:\n",
    "    return ser_country_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING GEOMETRICAL WEIGHT\n",
    "\n",
    "def geom_weight_single(flo_ratio, flo_factor = 1, num_element = 0):\n",
    "    ### Results output:\n",
    "    return flo_factor * (flo_ratio ** num_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEAN MOMENTUM FUNCTION (TO CALCULATE FACTOR ONLY FOR MONTHENDS)\n",
    "\n",
    "def rolling_cond_weighted_mean(ser_country_matrix, ser_full_source, int_mean_win, int_mean_min, list_weight = False, ser_full_cond = False):\n",
    "    ### Defining conditional average calculator:\n",
    "    def conditional_average(ser_source, list_weight, int_min_count = 0, ser_condition = False):\n",
    "        ### Weight setting\n",
    "        ser_weight = pd.Series(list_weight[ : len(ser_source.index)], ser_source.index)\n",
    "        ### If we have condition we should resort the weight array:\n",
    "        if not isinstance(ser_condition, bool):\n",
    "            ser_condition_sorted = pd.Series(ser_condition.sort_values().index, ser_condition.index)\n",
    "            ser_condition_sorted.name = 'Condition'\n",
    "            ser_weight = pd.concat([ser_weight, ser_condition_sorted], axis = 1).reset_index(drop = True).set_index('Condition').squeeze().sort_index()            \n",
    "        ### Results output:\n",
    "        return weighted_average(ser_source, ser_weight, int_min_count)    \n",
    "    ### Country saving:\n",
    "    str_country = ser_country_matrix.index[0][1]\n",
    "    ### Checking for country presence in source vector:\n",
    "    if (str_country in ser_full_source.index.get_level_values(1)):\n",
    "        ### Filtering country vector from source:\n",
    "        ser_country_source = ser_full_source.loc[All, str_country]\n",
    "        if not isinstance(ser_full_cond, bool):\n",
    "            ser_country_cond = ser_full_cond.loc[All, str_country]\n",
    "        ### Looping over matrix index dates:\n",
    "        for iter_bm_date in ser_country_matrix.index.get_level_values(0):\n",
    "            try:\n",
    "                ### Defining monthend date number in source country vector:\n",
    "                int_idx_num = ser_country_source.index.get_loc(iter_bm_date)\n",
    "                ### Creating vectors for numerator and denominator means calculation:\n",
    "                ser_rolled_source = ser_country_source.iloc[max((int_idx_num - int_mean_win + 1), 0) : int_idx_num + 1]\n",
    "                if not isinstance(ser_full_cond, bool):\n",
    "                    ser_rolled_cond = ser_country_cond.loc[ser_rolled_source.index]\n",
    "                else:\n",
    "                    ser_rolled_cond = False\n",
    "                ### Action for MatLab compatibility:\n",
    "                ser_rolled_source.iloc[0] = np.NaN\n",
    "                ### Simple mean calculation:\n",
    "                if isinstance(list_weight, bool):\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = weighted_average(ser_rolled_source, False, int_mean_min)\n",
    "                else:\n",
    "                    ### Weighted mean calculation:\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = conditional_average(ser_rolled_source, list_weight, int_mean_min, ser_rolled_cond)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    ### Resulting vector output:\n",
    "    return ser_country_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES\n",
    "\n",
    "def multistep_standartize(ser_data_source, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):  \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_data_source.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_data_source.dropna()\n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        if reuse_outliers:\n",
    "            ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        else:\n",
    "            ### Saving to result and excluding from further calculations truncated values:             \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):\n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_data_source.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR CROSS-SECTION\n",
    "\n",
    "def ison_standartize(ser_to_manage, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False, within_market = False):\n",
    "    ### Multi-step standartizing:\n",
    "    if (within_market):\n",
    "    ### Within market standartizing:\n",
    "        ser_result = ser_to_manage.groupby(by = 'Market', group_keys = False).apply(multistep_standartize, arr_truncate, ser_weight, \n",
    "                                                                                                  reuse_outliers, center_result, full_result)\n",
    "    else:\n",
    "    ### Full universe standartizing:\n",
    "        ser_result = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result)\n",
    "    ### Results output:\n",
    "    return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP THA STANDARTIZATION BY MARKET FOR CROSS-SECTION\n",
    "\n",
    "def tha_standartize(ser_to_manage, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):\n",
    "    ### Multi-step standartizing:\n",
    "    (ser_reversed, list_mean, list_std) = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result = True)\n",
    "    for iter_num in range(len(arr_truncate))[::-1]:\n",
    "        ser_reversed = (ser_reversed * list_std[iter_num] + list_mean[iter_num])\n",
    "    ser_demeaned = ser_reversed.groupby('Market').apply(lambda ser_region: ser_region - ser_region.mean())\n",
    "    ser_stand_z = multistep_standartize(ser_demeaned, arr_truncate, ser_weight, reuse_outliers, center_result, full_result = False)\n",
    "    ### Results output:\n",
    "    return ser_stand_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING UNIVERSAL AUTOCORRELATION FOR DATE-COUNTRY-UNIVERSE SERIES\n",
    "\n",
    "def vector_autocorr(ser_source, int_shift):\n",
    "    ### Defining adding full universe for each date:\n",
    "    def universe_reindex(iter_group, idx_universe):\n",
    "        df_iter_result = iter_group.unstack('Date').reindex(idx_universe).sort_index().stack('Date', dropna = False)\n",
    "        ### Results output:\n",
    "        return df_iter_result   \n",
    "    ### Defining adding full date range for each country and date index shifting:\n",
    "    def date_reindex(iter_group, idx_date_range, num_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-int_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result   \n",
    "    ### Defining by date correrlation function:\n",
    "    def corr_by_date(iter_group):\n",
    "        num_iter_corr = iter_group['Corr_factor_minus'].corr(iter_group['Corr_factor_plus'])\n",
    "        ### Results output:\n",
    "        return num_iter_corr       \n",
    "    ### Preparing expanded universe for autocorrelation performing:\n",
    "    idx_date_range = ser_source.index.get_level_values(0).unique()\n",
    "    idx_universe = ser_source.index.get_level_values(1).unique()\n",
    "    ser_source_full = ser_source.to_frame().reset_index('Market', drop = True).groupby('Date', group_keys = False).apply(universe_reindex, idx_universe)\\\n",
    "                                .swaplevel().squeeze()\n",
    "    ### Autocorrelation preparing:\n",
    "    ser_source_plus = ser_source_full.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[1 : ])\\\n",
    "                                     .sort_index(level = ['Date', 'Country'])\n",
    "    ser_source_minus = ser_source_full.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[: -1])\\\n",
    "                                      .sort_index(level = ['Date', 'Country'])\n",
    "    ### Artificial series combining for indexes synchronization:        \n",
    "    ser_source_plus_shifted = ser_source_plus.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, int_shift)\n",
    "    df_to_corr = pd.concat([ser_source_minus, ser_source_plus_shifted], axis = 1)\n",
    "    df_to_corr.columns = ['Corr_factor_minus', 'Corr_factor_plus']\n",
    "    ser_autocorr_vector = df_to_corr.groupby('Date').apply(corr_by_date).shift(int_shift)\n",
    "    ### Results output:\n",
    "    return ser_autocorr_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK\n",
    "\n",
    "def single_factor_standartize(ser_factor, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, within_market = False, \n",
    "                              flag_tha = False, flo_similarity = 5 * (10 ** (-8))):\n",
    "    ### Local constants:\n",
    "    dict_tha_pow = {}\n",
    "    dict_tha_pow['monthly'] = 1\n",
    "    dict_tha_pow['quarterly'] = 1 / 3\n",
    "    dict_tha_pow['annual'] = 1 / 12\n",
    "    ### Weights preparing:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "        ser_weight.name = 'Weight'\n",
    "    ### Multi-step standartizing:        \n",
    "    df_factor = ser_factor.to_frame().join(ser_weight, how = 'left')\n",
    "    df_factor.columns = ['Factor', 'Weight']\n",
    "    ### Time-horizon adjusted standartization:\n",
    "    if (flag_tha):\n",
    "        ### Z-scored vector calculating:       \n",
    "        ser_stand_z = df_factor.groupby('Date', group_keys = False)\\\n",
    "                               .apply(lambda iter_df: tha_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False))\n",
    "        ### Results output:\n",
    "        ser_stand_z.name = ser_factor.name\n",
    "        ### Autocorrelation vector calculating:\n",
    "        ser_autocorr_vector = ser_stand_z.groupby('Market').apply(vector_autocorr, 1)\n",
    "        ser_autocorr_vector.name = 'Autocorr'\n",
    "        ser_autocorr_cum_mean = ser_autocorr_vector.loc[np.abs(ser_autocorr_vector - 1) > flo_similarity].groupby('Market', group_keys = False).expanding().mean()\n",
    "        ### THA-coeficcient calculating:\n",
    "        ser_tha_coeff = ser_autocorr_cum_mean.transform(lambda iter_mean: max(iter_mean, 0.0) ** dict_tha_pow[flag_tha])\n",
    "        ser_tha_coeff = ser_tha_coeff.transform(lambda iter_mean: \n",
    "                                                sum(map(lambda iter_num: geom_weight_single(flo_tha_ratio * iter_mean, 1, iter_num), range(int_tha_length))) / 2)\n",
    "        ser_tha_coeff = ser_tha_coeff.swaplevel()\n",
    "        ser_tha_coeff = ser_tha_coeff.unstack('Market').reindex(ser_stand_z.index.levels[0]).stack('Market', dropna = False).sort_index(level = ['Date', 'Market'])        \n",
    "        ### THA-adjusted z-score calculating:\n",
    "#        ser_stand_s = (ser_stand_z * ser_tha_coeff)\n",
    "        ### Artifical filling values for first date of region appearance (not to loose observations):\n",
    "        ser_stand_s = (ser_stand_z * ser_tha_coeff.fillna(0.5))        \n",
    "        ser_stand_s = ser_stand_s[ser_stand_s.index.dropna()].reorder_levels(['Date', 'Country', 'Market']).sort_index()\n",
    "        ### Standart deviation for THA-adjusted z-score calculating:\n",
    "        ser_region_std = ser_stand_s.groupby(['Date', 'Market']).std()\n",
    "        ser_universe_std = ser_stand_s.groupby(['Date']).std()\n",
    "        ser_universe_std = pd.concat([ser_universe_std], keys = ['Overall'], names = ['Market']).swaplevel()\n",
    "        ser_std = pd.concat([ser_region_std, ser_universe_std], axis = 0).sort_index()\n",
    "        ### Results output:\n",
    "        return (ser_stand_s, ser_stand_z, ser_autocorr_vector, ser_tha_coeff, ser_std)\n",
    "    ### Simple standartization:    \n",
    "    else:    \n",
    "        ser_result = df_factor.groupby('Date', group_keys = False)\\\n",
    "                     .apply(lambda iter_df: ison_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False, within_market))\n",
    "        ### Results output:\n",
    "        ser_result.name = ser_factor.name\n",
    "        return ser_result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING GROUP MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK FOR MULTIPLE FACTORS\n",
    "\n",
    "def multi_factor_standartize(df_factor, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, within_market = False, \n",
    "                             flag_tha = False, flo_similarity = 5 * (10 ** (-8))):\n",
    "    ### Time-horizon adjusted standartization:\n",
    "    if (flag_tha):\n",
    "        dict_standartized = {}\n",
    "        dict_standartized_tha = {}        \n",
    "        dict_autocorr_vector = {}\n",
    "        dict_tha_coeff = {}\n",
    "        dict_std = {}\n",
    "        ### Single factor standartizing:\n",
    "        for iter_factor in df_factor.columns:\n",
    "            (dict_standartized_tha[iter_factor], dict_standartized[iter_factor], dict_autocorr_vector[iter_factor], dict_tha_coeff[iter_factor], dict_std[iter_factor]) = \\\n",
    "            single_factor_standartize(df_factor[iter_factor], arr_truncate, ser_weight, reuse_outliers, center_result, within_market, flag_tha, flo_similarity)\n",
    "        ### Concatenating to dataframe:\n",
    "        tup_result = (pd.concat(dict_standartized_tha, axis = 1), pd.concat(dict_standartized, axis = 1), \n",
    "                      pd.concat(dict_autocorr_vector, axis = 1), pd.concat(dict_tha_coeff, axis = 1), pd.concat(dict_std, axis = 1))\n",
    "        ### Results output:\n",
    "        return tup_result\n",
    "    ### Simple standartization:    \n",
    "    else:\n",
    "        dict_standartized = {}\n",
    "        ### Single factor standartizing:\n",
    "        for iter_factor in df_factor.columns:\n",
    "            dict_standartized[iter_factor] = single_factor_standartize(df_factor[iter_factor], arr_truncate, ser_weight, \n",
    "                                                                       reuse_outliers, center_result, within_market, flag_tha, flo_similarity)\n",
    "        ### Concatenating to dataframe:\n",
    "        df_result = pd.concat(dict_standartized, axis = 1)\n",
    "        ### Results output:\n",
    "        return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EFFICACY MEASURES FOR SINGLE FACTOR\n",
    "\n",
    "def single_factor_multiple_efficacy_measures(ser_factor, ser_return, ser_weight, arr_measure, return_shift = 0, arr_truncate = [2.5, 2.0]):\n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    dict_measure = {}\n",
    "    set_std_needed = {'fmb_std_eqw', 'fmb_std_weighted'}\n",
    "    num_precision = 5 # For quintile bins rounding and borders controlling\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining MatLab style prctile function:\n",
    "    def prctile(ser_to_perc, p):\n",
    "        ### Sorted list preparing:\n",
    "        list_to_perc = ser_to_perc.dropna().values\n",
    "        list_sorted = np.sort(list_to_perc)\n",
    "        ### Length calculating:\n",
    "        num_len = len(list_to_perc)    \n",
    "        ### Prctile calculating:\n",
    "        num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "        ### Results output:\n",
    "        return num_result\n",
    "    ### Defining quintile bins distribution:\n",
    "    def quintile_distribution(ser_iter_group, num_bins = 5, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True):\n",
    "        ### Bins preparing:\n",
    "        list_bin = list(np.arange(0, 100, 100 / num_bins))[1 : ]\n",
    "        list_bin = [round(iter_element / 100, 2) for iter_element in list_bin]\n",
    "        list_bin = [prctile(ser_iter_group, iter_element) for iter_element in list_bin]\n",
    "        list_bin = [ser_iter_group.min() - abs(ser_iter_group.min())] + list_bin + [ser_iter_group.max() + abs(ser_iter_group.max())]\n",
    "        list_bin = [round(iter_element, num_precision) for iter_element in list_bin]\n",
    "        if bool_populate_last:\n",
    "            list_bin[-2] -= 10 ** (-num_precision)\n",
    "        ### Bins distribution:\n",
    "        ser_iter_distribution = pd.cut(ser_iter_group, bins = list_bin, labels = range(num_bins), \n",
    "                                       right = bool_right_included, include_lowest = bool_include_lowest, precision = num_precision)\n",
    "        ### Results output:\n",
    "        return ser_iter_distribution    \n",
    "    ### Defining get_measure group level function:\n",
    "    def get_measure(df_to_measure, iter_measure):\n",
    "        ### Checking data sufficiency:\n",
    "        if (len(df_to_measure.dropna().index) > 1):           \n",
    "            ### Measure calculating:\n",
    "            if (iter_measure == 'ic_spearman'):\n",
    "                ### Spearmen information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values\n",
    "                num_result = ss.spearmanr(list_factor, list_return, nan_policy = 'omit').correlation\n",
    "            if (iter_measure == 'ic_pearson'):\n",
    "                ### Pearson information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values                \n",
    "                num_result = ss.pearsonr(list_factor, list_return)[0]\n",
    "            if (iter_measure == 'fmb_eqw'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (equal weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return']].dropna()['Return'].values\n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values\n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_std_eqw'):             \n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()['Return'].values                \n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_std_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]                 \n",
    "            if (iter_measure == 'fmb_std_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values                \n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_std_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]  \n",
    "            if (iter_measure == 'clp'):\n",
    "                ### Constant leverage portfolio signed normalized multiplication sum:                \n",
    "                ser_clp_weighted = df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Factor']\n",
    "                ser_clp_weighted = ser_clp_weighted * df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Weight'].transform(np.sqrt)\n",
    "                ser_clp_weighted.loc[ser_clp_weighted < 0] = -ser_clp_weighted / ser_clp_weighted[ser_clp_weighted < 0].sum()\n",
    "                ser_clp_weighted.loc[ser_clp_weighted > 0] = ser_clp_weighted / ser_clp_weighted[ser_clp_weighted > 0].sum()\n",
    "                num_result = (ser_clp_weighted * df_to_measure['Return']).sum()\n",
    "            if ('qtl' in iter_measure):\n",
    "                ### Interquntile range:\n",
    "                num_bins = int(iter_measure.split('qtl')[1])   \n",
    "                df_to_measure = df_to_measure[['Factor', 'Return', 'Constant']].dropna()\n",
    "                df_to_measure['Return'] = df_to_measure['Return'] - df_to_measure['Return'].mean()\n",
    "                df_to_measure['Factor'] = df_to_measure['Factor'].round(num_precision)\n",
    "                ### Distribution factor values between quintile bins:\n",
    "                ser_qtl_bins = quintile_distribution(df_to_measure['Factor'], num_bins, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True)\n",
    "                ser_qtl_bins.name = 'Bin'\n",
    "                ### Mean return for each bin calculating:\n",
    "                df_to_measure = df_to_measure.join(ser_qtl_bins)\n",
    "                df_qtl_rets = df_to_measure.loc[(All), ['Return', 'Bin']]\n",
    "                df_qtl_rets.set_index('Bin', append = True, inplace = True)\n",
    "                ser_qtl_rets = df_qtl_rets.unstack('Bin').mean(axis = 0).droplevel(0).squeeze()\n",
    "                num_result = ser_qtl_rets.iloc[-1] - ser_qtl_rets.iloc[0]                                 \n",
    "        else:                          \n",
    "            num_result = np.NaN\n",
    "        ### Preparing results: \n",
    "        return num_result\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    if (ser_weight.count() == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ser_weight = ser_weight.reset_index('Market', drop = True)\n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted, ser_weight], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return', 'Weight']\n",
    "    df_to_measure['Constant'] = 1\n",
    "    if set_std_needed.intersection(set(arr_measure)):\n",
    "        ser_factor_std = df_to_measure.dropna()['Factor'].groupby('Date').apply(ison_standartize, arr_truncate = arr_truncate, within_market = False)\n",
    "        df_to_measure['Factor_std'] = ser_factor_std.reindex(df_to_measure.index)\n",
    "    ### Looping efficacy measures for calculating measures timeseries:\n",
    "    for iter_measure in arr_measure:\n",
    "        dict_measure[iter_measure] = df_to_measure.groupby('Date').apply(get_measure, iter_measure = iter_measure)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_measure, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEASURE STATISTICS CALCULATOR\n",
    "\n",
    "def measure_stats(df_measures, arr_back_period = [99]):\n",
    "    ### Declaring local constants & variables:\n",
    "    dict_stats = {}\n",
    "    ### Stats calculating:\n",
    "    for iter_measure in df_measures.columns:\n",
    "        dict_period = {}\n",
    "        for iter_back_period in arr_back_period:\n",
    "            ser_iter_measure = df_measures[iter_measure].dropna()\n",
    "            idx_iter_range = pd.date_range(end = ser_iter_measure.index[-1], periods = iter_back_period * 12, freq = 'BM')\n",
    "            ser_iter_measure = ser_iter_measure[idx_iter_range]            \n",
    "            ser_iter_stats = pd.Series()\n",
    "            ser_iter_stats['count'] = ser_iter_measure.count()\n",
    "            ser_iter_stats['min'] = ser_iter_measure.min()\n",
    "            ser_iter_stats['max'] = ser_iter_measure.max()        \n",
    "            ser_iter_stats['mean'] = ser_iter_measure.mean()\n",
    "            ser_iter_stats['std'] = ser_iter_measure.std()\n",
    "            ser_iter_stats['median'] = ser_iter_measure.median()        \n",
    "            ser_iter_stats['perc_25'] = ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['perc_75'] = ser_iter_measure.quantile(0.75, 'midpoint')\n",
    "            ser_iter_stats['iq_range'] = ser_iter_measure.quantile(0.75, 'midpoint') - ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['mean_abs'] = ser_iter_measure.abs().mean()\n",
    "            ser_iter_stats['t_stat'] = (ser_iter_measure.mean() / ser_iter_measure.std()) * np.sqrt(ser_iter_measure.count())  \n",
    "            dict_period[iter_back_period] = ser_iter_stats\n",
    "        dict_stats[iter_measure] = pd.concat(dict_period, axis = 1)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_stats, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SPECIAL CLP STATS\n",
    "\n",
    "def special_clp_stats(ser_factor, ser_return, ser_weight, return_shift = 0):\n",
    "    ### Declaring local constants & variables:    \n",
    "    dict_clp_stats = {}\n",
    "    list_bin_labels = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining function for constant leverage portfolio normalizing:\n",
    "    def get_normalized_clp(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Constant leverage portfolio signed normalized:\n",
    "            ser_clp_weighted.loc[ser_clp_weighted < 0] = -ser_clp_weighted / ser_clp_weighted[ser_clp_weighted < 0].sum()\n",
    "            ser_clp_weighted.loc[ser_clp_weighted > 0] = ser_clp_weighted / ser_clp_weighted[ser_clp_weighted > 0].sum()\n",
    "            ser_result = ser_clp_weighted.copy()\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index)\n",
    "        ### Results output:\n",
    "        return ser_result\n",
    "    ### Defining function for constant leverage portfolio normalizing:\n",
    "    def get_normalized_factor(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Factor signed normalized:\n",
    "            ser_factor_normalized = df_to_measure['Factor']\n",
    "            ser_factor_normalized.loc[ser_factor_normalized < 0] = -ser_factor_normalized / ser_factor_normalized[ser_factor_normalized < 0].sum()\n",
    "            ser_factor_normalized.loc[ser_factor_normalized > 0] = ser_factor_normalized / ser_factor_normalized[ser_factor_normalized > 0].sum()\n",
    "            ser_result = ser_factor_normalized\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index) \n",
    "        ### Results output:\n",
    "        return ser_result            \n",
    "    ### Defining function for returns for constant leverage portfolio:\n",
    "    def get_normalized_return(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Normalized return:  \n",
    "            ser_result = df_to_measure['Return']\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index)            \n",
    "        ### Results output:\n",
    "        return ser_result  \n",
    "    ### Defining MatLab style prctile function:\n",
    "    def prctile(ser_to_perc, p):\n",
    "        ### Sorted list preparing:\n",
    "        list_to_perc = ser_to_perc.dropna().values\n",
    "        list_sorted = np.sort(list_to_perc)\n",
    "        ### Length calculating:\n",
    "        num_len = len(list_to_perc)    \n",
    "        ### Prctile calculating:\n",
    "        num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "        ### Results output:\n",
    "        return num_result\n",
    "    ### Defining quintile bins distribution:\n",
    "    def quintile_distribution(ser_iter_group, num_bins = 5, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True):\n",
    "        ### Bins preparing:\n",
    "        list_bin = list(np.arange(0, 100, 100 / num_bins))[1 : ]\n",
    "        list_bin = [round(iter_element / 100, 2) for iter_element in list_bin]\n",
    "        list_bin = [prctile(ser_iter_group, iter_element) for iter_element in list_bin]\n",
    "        list_bin = [ser_iter_group.min() - abs(ser_iter_group.min())] + list_bin + [ser_iter_group.max() + abs(ser_iter_group.max())]\n",
    "        list_bin = [round(iter_element, num_precision) for iter_element in list_bin]\n",
    "        if bool_populate_last:\n",
    "            list_bin[-2] -= 10 ** (-num_precision)\n",
    "        ### Bins distribution:\n",
    "        ser_iter_distribution = pd.cut(ser_iter_group, bins = list_bin, labels = range(num_bins), \n",
    "                                       right = bool_right_included, include_lowest = bool_include_lowest, precision = num_precision)\n",
    "        ### Results output:\n",
    "        return ser_iter_distribution \n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    if (ser_weight.count() == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ser_weight = ser_weight.reset_index('Market', drop = True)      \n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted, ser_weight], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return', 'Weight']\n",
    "    ser_clp_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_clp)\n",
    "    ser_factor_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_factor)\n",
    "    ser_return_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_return)\n",
    "    ### CLP stats calculating:\n",
    "    dict_clp_stats['Average Bias'] = ser_factor_normalized.groupby('Country').mean()\n",
    "    dict_clp_stats['Weights Sum'] = ser_clp_normalized.groupby('Country').sum()\n",
    "    dict_clp_stats['Average Return'] = ser_return_normalized.groupby('Country').mean()   \n",
    "    dict_clp_stats['Static Contribution'] = dict_clp_stats['Weights Sum'] * dict_clp_stats['Average Return']\n",
    "    dict_clp_stats['Total Contribution'] = (ser_clp_normalized * ser_return_normalized).groupby('Country').sum()\n",
    "    dict_clp_stats['Dynamic Contribution'] = dict_clp_stats['Total Contribution'] - dict_clp_stats['Static Contribution'] \n",
    "    ### CLP active weights calculating:\n",
    "    ser_clp_delta = ser_clp_normalized.unstack('Date').stack('Date', dropna = False).swaplevel().sort_index(level = ['Date', 'Country'])\n",
    "    ser_clp_delta = ser_clp_delta.fillna(0)\n",
    "    num_clp_mean = ser_clp_delta.groupby('Country').mean().abs().sum()\n",
    "    ser_clp_delta = ser_clp_delta.groupby('Country').apply(lambda iter_group: iter_group - iter_group.mean())\n",
    "    num_clp_delta = (ser_clp_delta.abs().groupby('Date').sum() / (ser_clp_delta.abs().groupby('Date').sum() + num_clp_mean)).mean()\n",
    "    df_clp_stats = pd.concat(dict_clp_stats, axis = 1).reindex(df_to_measure.index.get_level_values('Country').unique()).sort_index()\n",
    "    ### Preparing sum:\n",
    "    df_clp_sum = pd.DataFrame([[np.NaN, np.NaN, np.NaN, df_clp_stats['Static Contribution'].sum(), \n",
    "                               df_clp_stats['Total Contribution'].sum(), df_clp_stats['Dynamic Contribution'].sum()]], \n",
    "                              index = ['Sum'], columns = df_clp_stats.columns)\n",
    "    ### Preparing expected based on active weights:\n",
    "    df_clp_expected = pd.DataFrame([[np.NaN, np.NaN, np.NaN, \n",
    "                                     df_clp_stats['Total Contribution'].sum() * (1 - num_clp_delta), np.NaN, df_clp_stats['Total Contribution'].sum() * num_clp_delta]], \n",
    "                              index = ['Expected based on active weights =>'], columns = df_clp_stats.columns)    \n",
    "    ### Adding totals:\n",
    "    df_clp_stats = pd.concat([df_clp_stats, df_clp_sum, df_clp_expected], axis = 0, join = 'inner')\n",
    "    ### CLP Bias calculating:   \n",
    "    ser_clp_quintile = ser_clp_normalized.groupby('Date', group_keys = False).apply(quartile_distribution)\n",
    "    df_clp_bias = ser_clp_quintile.to_frame()  \n",
    "    df_clp_bias.columns = ['Bin']\n",
    "    df_clp_bias['Quintile'] = 1\n",
    "    df_clp_bias = df_clp_bias.set_index('Bin', append = True).unstack('Bin').fillna(0).droplevel(level = 0, axis = 1)\n",
    "    df_clp_bias.columns = list(df_clp_bias.columns)  \n",
    "    df_clp_bias = df_clp_bias[list_bin_labels]\n",
    "    df_clp_bias = df_clp_bias.groupby('Country').mean()\n",
    "    df_clp_bias.loc[:, 'Q5 - Q1'] = df_clp_bias['Q5'] - df_clp_bias['Q1']   \n",
    "    df_clp_bias = df_clp_bias.reindex(df_to_measure.index.get_level_values('Country').unique()).sort_index()\n",
    "    df_clp_bias = df_clp_bias\n",
    "    ### Output results:\n",
    "    return (df_clp_stats, df_clp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SPECIAL QTL STATS\n",
    "\n",
    "def special_qtl_stats(ser_factor, ser_return, return_shift = 0, num_bins = 10):\n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    num_precision = 5 # For quintile bins rounding and borders controlling\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining MatLab style prctile function:\n",
    "    def prctile(ser_to_perc, p):\n",
    "        ### Sorted list preparing:\n",
    "        list_to_perc = ser_to_perc.dropna().values\n",
    "        list_sorted = np.sort(list_to_perc)\n",
    "        ### Length calculating:\n",
    "        num_len = len(list_to_perc)    \n",
    "        ### Prctile calculating:\n",
    "        num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "        ### Results output:\n",
    "        return num_result\n",
    "    ### Defining quintile bins distribution:\n",
    "    def quintile_distribution(ser_iter_group, num_bins = 5, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True):\n",
    "        ### Bins preparing:\n",
    "        list_bin = list(np.arange(0, 100, 100 / num_bins))[1 : ]\n",
    "        list_bin = [round(iter_element / 100, 2) for iter_element in list_bin]\n",
    "        list_bin = [prctile(ser_iter_group, iter_element) for iter_element in list_bin]\n",
    "        list_bin = [ser_iter_group.min() - abs(ser_iter_group.min())] + list_bin + [ser_iter_group.max() + abs(ser_iter_group.max())]\n",
    "        list_bin = [round(iter_element, num_precision) for iter_element in list_bin]\n",
    "        if bool_populate_last:\n",
    "            list_bin[-2] -= 10 ** (-num_precision)\n",
    "        ### Bins distribution:\n",
    "        ser_iter_distribution = pd.cut(ser_iter_group, bins = list_bin, labels = range(num_bins), \n",
    "                                       right = bool_right_included, include_lowest = bool_include_lowest, precision = num_precision)\n",
    "        ### Results output:\n",
    "        return ser_iter_distribution    \n",
    "\n",
    "    ### Defining qts stats generator:\n",
    "    def get_qtl_stats(df_to_measure):\n",
    "        df_to_measure = df_to_measure[['Factor', 'Return', 'Constant']].dropna()\n",
    "        if len(df_to_measure.index):\n",
    "            df_to_measure['Return'] = df_to_measure['Return'] - df_to_measure['Return'].mean()\n",
    "            df_to_measure['Factor'] = df_to_measure['Factor'].round(num_precision)\n",
    "            ### Distribution factor values between quintile bins:\n",
    "            ser_qtl_bins = quintile_distribution(df_to_measure['Factor'], num_bins, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True)\n",
    "            ser_qtl_bins.name = 'Bin'\n",
    "            df_to_measure = df_to_measure.join(ser_qtl_bins)        \n",
    "            ### Bins distribution:\n",
    "            df_qtl_bins = df_to_measure.loc[(All), ['Constant', 'Bin']]\n",
    "            df_qtl_bins.set_index('Bin', append = True, inplace = True)\n",
    "            ser_qtl_bins = df_qtl_bins.unstack('Bin').sum(axis = 0).droplevel(0).squeeze()\n",
    "            ### Return mean for each bin:\n",
    "            df_qtl_rets = df_to_measure.loc[(All), ['Return', 'Bin']]\n",
    "            df_qtl_rets.set_index('Bin', append = True, inplace = True)\n",
    "            ser_qtl_rets = df_qtl_rets.unstack('Bin').mean(axis = 0).droplevel(0).squeeze()\n",
    "        else:\n",
    "            ser_qtl_bins = pd.Series(np.NaN, index = range(num_bins))\n",
    "            ser_qtl_rets = pd.Series(np.NaN, index = range(num_bins))            \n",
    "        ### Results output:\n",
    "        ser_qtl_bins.name = 'Distribution'                \n",
    "        ser_qtl_rets.name = 'Mean return'\n",
    "        return pd.concat([ser_qtl_bins, ser_qtl_rets], axis = 0)      \n",
    "        \n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return']\n",
    "    df_to_measure['Constant'] = 1\n",
    "    df_bins_stats = df_to_measure.groupby('Date', group_keys = False).apply(get_qtl_stats)\n",
    "    ### Output results:\n",
    "    df_bins_distribution = df_bins_stats.iloc[All, : num_bins]\n",
    "    df_bins_return_mean = df_bins_stats.iloc[All, num_bins :]    \n",
    "    return (df_bins_distribution, df_bins_return_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SINGLE EFFICACY MEASURE FOR MULTIPLE FACTORS\n",
    "    \n",
    "def multiple_factor_single_efficacy_measure_stats(df_factors, ser_return, ser_weight, str_measure, num_back_period = 99, \n",
    "                                                  num_horizon = 12, list_region_xmo = ['DM', 'EM', 'FM']): \n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    list_months = [1, 2, 3, 6, 9 ,12]\n",
    "    list_range = [iter_month - 1 for iter_month in list_months if iter_month <= num_horizon]\n",
    "    ### Defining full universe expanding for date:\n",
    "    def universe_reindex(iter_group, idx_universe):\n",
    "        df_iter_result = iter_group.unstack('Date').reindex(idx_universe).sort_index().stack('Date', dropna = False)\n",
    "        ### Results output:\n",
    "        return df_iter_result   \n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result   \n",
    "    ### Defining by date correrlation function:\n",
    "    def corr_by_date(iter_group):\n",
    "        num_iter_corr = iter_group['Corr_factor_minus'].corr(iter_group['Corr_factor_plus'])\n",
    "        ### Results output:\n",
    "        return num_iter_corr       \n",
    "    ### Preparing expanded universe for autocorrelation performing:\n",
    "    df_factors_region = df_factors.loc[(All, All, list_region_xmo), :]\n",
    "    idx_date_range = df_factors_region.index.get_level_values(0).unique()\n",
    "    idx_universe = df_factors_region.index.get_level_values(1).unique()\n",
    "    df_factors_full = df_factors_region.reset_index('Market', drop = True).groupby('Date', group_keys = False).apply(universe_reindex, idx_universe).swaplevel()\n",
    "    ### Factors looping:\n",
    "    dict_factors_measures = {} ### Container for all factor measure stats\n",
    "    dict_factors_vectors = {} ### Container for all factor measure vectors\n",
    "    dict_factors_autocorr = {} ### Container for autocorrelation results\n",
    "    for iter_factor in df_factors.columns:\n",
    "        ### Shifts looping for factors measures stats:\n",
    "        ### Stats calculation:\n",
    "        dict_factor_stats = {} ### Container for iterated factor stats\n",
    "        dict_factor_vectors = {} ### Container for iterated factor measure vectors\n",
    "        for iter_shift in list_range:            \n",
    "            df_factor_filtered = df_factors_region[iter_factor]\n",
    "            df_iter_shift_measure = single_factor_multiple_efficacy_measures(df_factor_filtered, ser_return, ser_weight, [str_measure], iter_shift, list_truncate)\n",
    "            df_iter_shift_stats = measure_stats(df_iter_shift_measure, [num_back_period])\n",
    "            dict_factor_stats[iter_shift] = df_iter_shift_stats.loc[['mean', 't_stat'], (str_measure, num_back_period)]\n",
    "            dict_factor_vectors[iter_shift] = df_iter_shift_measure\n",
    "        ### Aggegating factor measure stats:\n",
    "        df_iter_factor_stats = pd.concat(dict_factor_stats, axis = 1)\n",
    "        df_iter_factor_stats.columns = df_iter_factor_stats.columns + 1        \n",
    "        dict_factors_measures[iter_factor] = df_iter_factor_stats\n",
    "        ### Aggegating factor measure vectors:        \n",
    "        df_iter_factor_vectors = pd.concat(dict_factor_vectors, axis = 1)    \n",
    "        df_iter_factor_vectors.columns = df_iter_factor_vectors.columns.droplevel(1) + 1\n",
    "        dict_factors_vectors[iter_factor] = df_iter_factor_vectors        \n",
    "        ### Autocorrelation calculation:\n",
    "        ser_iter_factor = df_factors_full[iter_factor]\n",
    "        ser_iter_factor_plus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[1 : ]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ser_iter_factor_minus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[: -1]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ### Artificial series combining for indices synchronization:        \n",
    "        ser_iter_factor_plus_shifted = ser_iter_factor_plus.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, 1)\n",
    "        df_iter_factor_to_corr = pd.concat([ser_iter_factor_minus, ser_iter_factor_plus_shifted], axis = 1)\n",
    "        df_iter_factor_to_corr.columns = ['Corr_factor_minus', 'Corr_factor_plus']\n",
    "        dict_factors_autocorr[iter_factor] = pd.Series(df_iter_factor_to_corr.groupby('Date').apply(corr_by_date).mean(), index = ['autocorr'])\n",
    "    ### Results output:\n",
    "    df_factors_measures_stats = pd.concat(dict_factors_measures, axis = 0)\n",
    "    df_factors_autocorr =  pd.concat(dict_factors_autocorr, axis = 1).transpose()\n",
    "    df_factors_vectors = pd.concat(dict_factors_vectors, axis = 0)\n",
    "    df_factors_coeff = df_factors_measures_stats.loc[(All, 'mean'), All].reset_index(1, drop = True)    \n",
    "    df_factors_coeff.columns = [('coeff_' + str(iter_column)) for iter_column in df_factors_coeff.columns]\n",
    "    df_factors_t_stat = df_factors_measures_stats.loc[(All, 't_stat'), All].reset_index(1, drop = True)    \n",
    "    df_factors_t_stat.columns = [('t_' + str(iter_column)) for iter_column in df_factors_t_stat.columns]\n",
    "    df_factors_result = pd.concat([df_factors_autocorr, df_factors_coeff, df_factors_t_stat], axis = 1)    \n",
    "    return (df_factors_result, df_factors_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(io = str_path_universe, sheet_name = 0, header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: CONSTANTS INITIALIZATION\n",
    "\n",
    "str_test_date = '2008-02-29'\n",
    "list_test_country = ['BH', 'OM'] # All\n",
    "str_test_region = All # 'EM'\n",
    "str_test_eer = 'NEER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: BLOOMBERG STRUCTURED DATA & ISON MEMBERSHIP EXTRACTION (NO PRELIMINARY DATA USING)\n",
    "\n",
    "ser_returns = pd.read_hdf(str_path_bb_hdf, key = str_key_ret_monthly)\n",
    "ser_mmr = pd.read_hdf(str_path_bb_hdf, key = str_key_mmr)\n",
    "ser_fx_country = pd.read_hdf(str_path_bb_hdf, key = str_key_fx_country)\n",
    "ser_fx_rate_demeaned = pd.read_hdf(str_path_bb_hdf, key = str_key_fx_demeaned)\n",
    "ser_mcap = pd.read_hdf(str_path_bb_hdf, key = str_key_mcap)\n",
    "ser_reer = pd.read_hdf(str_path_bb_hdf, key = str_key_reer)\n",
    "ser_neer = pd.read_hdf(str_path_bb_hdf, key = str_key_neer)\n",
    "ser_reer_sourced = pd.read_hdf(str_path_bb_hdf, key = str_key_reer_sourced)\n",
    "ser_neer_sourced = pd.read_hdf(str_path_bb_hdf, key = str_key_neer_sourced)\n",
    "df_xcra_filled = pd.read_hdf(str_path_bb_hdf, key = str_key_xcra)\n",
    "ser_ison = ison_membership_converting(str_path_universe, datetime.strptime(str_measure_date_end, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: DATA PREPARING (NO PRELIMINARY DATA USING)\n",
    "\n",
    "### List of countries with de-facto equal returns (to impact on hedged returns calculating)\n",
    "ser_ret_similarity_test = ser_returns.unstack('Currency').groupby('Country').apply(lambda df_country: (df_country['LOC'] - df_country['USD']).abs().mean())\n",
    "set_ret_usd_only = set(ser_ret_similarity_test.loc[ser_ret_similarity_test < flo_returns_similarity].index)\n",
    "### List of countries with unsufficient data quantity:\n",
    "ser_ret_completeness_test = ser_returns.groupby('Country').apply(lambda ser_country: ser_country.count() / len(ser_country.index))\n",
    "set_not_complete = set(ser_ret_completeness_test.loc[ser_ret_completeness_test < flo_returns_completeness].index)\n",
    "### Filtering uncomplete countries:\n",
    "ser_returns.loc[All, All, set_not_complete] = np.NaN\n",
    "### Returns options preparing:\n",
    "dict_ser_ret = {}\n",
    "### Returns in local currency:\n",
    "dict_ser_ret['LOC'] = ser_returns.loc['LOC', All, All].droplevel(0)\n",
    "### Returns in USD:\n",
    "dict_ser_ret['USD'] = ser_returns.loc['USD', All, All].droplevel(0)\n",
    "### Hedged returns in local currency:\n",
    "dict_ser_hedged = {}\n",
    "### Filling data for countries with no MMR data:\n",
    "set_ison_countries = set(dict_ser_ret['LOC'].index.get_level_values(1).unique())\n",
    "set_mmr_countries = set(ser_mmr.index.get_level_values(1).unique())\n",
    "set_no_mmr_countries = (set_ison_countries - set_mmr_countries) | set_ret_usd_only\n",
    "set_to_hedge_countries = set_mmr_countries - set_no_mmr_countries\n",
    "dict_ser_hedged['No_MMR'] = dict_ser_ret['USD'].loc[All, set_no_mmr_countries]\n",
    "### Money Market rates shifting forward:\n",
    "ser_mmr_shifted = ser_mmr.groupby('Country').shift(1)\n",
    "### Filling data for other countries:\n",
    "df_ser_hedged = pd.DataFrame()\n",
    "df_ser_hedged['Returns LOC'] = dict_ser_ret['LOC'].loc[All, set_to_hedge_countries]\n",
    "df_ser_hedged = df_ser_hedged.join(ser_mmr_shifted, how = 'left')\n",
    "df_ser_hedged.columns = ['Returns LOC', 'MMR LOC']\n",
    "dict_ser_hedged['MMR_Based'] = df_ser_hedged.groupby('Country', group_keys = False)\\\n",
    "                               .apply(lambda df_country: (1 + df_country['Returns LOC']) * (1 + ser_mmr_shifted.loc[All, 'US'] / 12) / (1 + df_country['MMR LOC'] / 12) - 1)\n",
    "#dict_ser_hedged['MMR_Based'] = df_ser_hedged.groupby('Country', group_keys = False)\\\n",
    "#                               .apply(lambda df_country: (1 + df_country['Returns LOC']) * (((1 + ser_mmr.loc[All, 'US']) / (1 + df_country['MMR LOC'])) ** (1 /12)) - 1)\n",
    "### Aggregating hedged returns:\n",
    "dict_ser_ret['HEDGED'] = pd.concat(dict_ser_hedged).droplevel(0).sort_index()\n",
    "### Effective exchange rates options preparing:\n",
    "dict_ser_eer = {}\n",
    "### Sources forward filling and reindexing:\n",
    "ser_reer_source = ser_reer.unstack('Country').reindex(idx_source_date_range).ffill(limit = int_eer_fill_limit).stack('Country').sort_index()\n",
    "ser_neer_source = ser_neer.unstack('Country').reindex(idx_source_date_range).ffill(limit = int_eer_fill_limit).stack('Country').sort_index()\n",
    "ser_fx_source = ser_fx_rate_demeaned.unstack('Country').reindex(idx_source_date_range).ffill(limit = int_eer_fill_limit).stack('Country').sort_index()\n",
    "ser_export_source = df_xcra_filled['Exports'].unstack('Country').reindex(idx_source_date_range).ffill(limit = int_eer_fill_limit).stack('Country').sort_index()\n",
    "dict_ser_eer['REER'] = ser_reer_source\n",
    "if bool_neer_raw:\n",
    "    ### Simple NEER usage:\n",
    "    dict_ser_eer['NEER'] = ser_neer_source\n",
    "else:\n",
    "    ### Alternative NEER usage:\n",
    "    ### Selecting all ISON countries:\n",
    "    set_ison = set(ser_ison.dropna().index.get_level_values('Country').unique())\n",
    "    ### Selecting all REER countries:\n",
    "    set_reer_all = set(ser_reer.dropna().index.get_level_values('Country').unique())\n",
    "    ### Selecting all NEER countries:\n",
    "    set_neer_all = set(ser_neer.dropna().index.get_level_values('Country').unique())\n",
    "    ### Selecting countries, where REER has monthly frequency:\n",
    "    set_reer_monthly = set(ser_reer_sourced.loc[All, All, ['IMF', 'BIS']].index.get_level_values(1).unique())\n",
    "    ### Defining countries from REER to participate in NEER source:\n",
    "    set_reer_st = set_reer_all - set_reer_monthly\n",
    "    ### Defining countries from NEER to participate in NEER source:\n",
    "    ser_neer_st = set_reer_monthly & set_neer_all\n",
    "    ### Defining rest of countries to participate in NEER source from FX rates:\n",
    "    set_fx_st = set_ison - (set_reer_st | ser_neer_st)\n",
    "    ### Converting sets to lists:\n",
    "    list_reer_st = sorted(list(set_reer_st))\n",
    "    list_neer_st = sorted(list(ser_neer_st))\n",
    "    list_fx_st = sorted(list(set_fx_st))\n",
    "    dict_ser_eer['NEER'] = pd.concat([ser_reer_source.loc[All, list_reer_st], ser_neer_source.loc[All, list_neer_st], ser_fx_source.loc[All, list_fx_st]]).sort_index()\n",
    "dict_ser_eer['EXPORT'] = ser_export_source    \n",
    "### Concepts options preparing:\n",
    "dict_ser_concept = {}\n",
    "### XCRA concept data shifting:\n",
    "df_xcra_shifted = df_xcra_filled.groupby('Country').shift(int_concept_lag)\n",
    "### XCRA concepts calculating:\n",
    "#dict_ser_concept['EXPIMP_GDP_rate'] = (df_xcra_shifted['Imports'] + df_xcra_shifted['Exports']) / df_xcra_shifted['GDP']\n",
    "dict_ser_concept['EXP_GDP_rate'] = df_xcra_shifted['Exports'] / df_xcra_shifted['GDP']\n",
    "#dict_ser_concept['CA_GDP_rate'] = df_xcra_shifted['Current Account'] / df_xcra_shifted['GDP']\n",
    "### XCRA concepts adjusting:\n",
    "for iter_concept in dict_ser_concept:\n",
    "    dict_ser_concept[iter_concept] = dict_ser_concept[iter_concept] / int_concept_divider\n",
    "### XCRA concepts adjusting:\n",
    "for iter_concept in dict_ser_concept:\n",
    "    dict_ser_concept[iter_concept].loc[dict_ser_concept[iter_concept] <= -1] = -0.99\n",
    "    dict_ser_concept[iter_concept] = np.maximum(int_concept_min, (np.minimum(int_concept_max, np.log(1 + dict_ser_concept[iter_concept]))))      \n",
    "### Neutral concept adding:\n",
    "dict_ser_concept['NO_CONCEPT'] = pd.Series(1, index = dict_ser_concept['EXP_GDP_rate'].index)\n",
    "### Concept series renaming:\n",
    "for iter_concept in dict_ser_concept:\n",
    "    dict_ser_concept[iter_concept].name = 'Multiplicator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: SOURCE DISTRIBUTION EXPORT:\n",
    "\n",
    "#pd.concat([pd.Series('REER', index = list_reer_st), pd.Series('NEER', index = list_neer_st), pd.Series('FX', index = list_fx_st)], axis = 0).sort_index()\\\n",
    "#  .to_excel('Data_Files/Test_Files/ST_EER_Sources.xlsx', merge_cells = False)\n",
    "\n",
    "#ser_reer_sourced.loc['2020-08-31', All, All].droplevel(0).reset_index('Source').drop('EER', axis = 1).squeeze()\\\n",
    "#  .to_excel('Data_Files/Test_Files/LT_EER_Sources.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3544357469015003"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTING: MINIMAL VALUE CONTROL:\n",
    "\n",
    "ser_concept_test = df_xcra_shifted['Exports'] / df_xcra_shifted['GDP'] / int_concept_divider\n",
    "ser_logged_test = np.minimum(int_concept_max, np.log(1 + ser_concept_test))\n",
    "ser_logged_test[ser_logged_test == int_concept_max].count() / ser_logged_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG_TERM / MOMENTUM / EXP_GDP_rate / REER / HEDGED\n",
      "SHORT_TERM / MOMENTUM / EXP_GDP_rate / NEER / HEDGED\n",
      "LONG_TERM / MOMENTUM / EXP_GDP_rate / EXPORT / HEDGED\n"
     ]
    }
   ],
   "source": [
    "### MAIN SCRIPT: FACTORS CALCULATING (NO PRELIMINARY DATA USING)\n",
    "\n",
    "### Defining the way to mount the concept multiplicator to the basis factor:\n",
    "def value_to_rank(ser_group, int_scale):\n",
    "    ser_result = np.maximum(0, (ser_group - ser_group.min()) / (ser_group.max() - ser_group.min()) * int_scale) + 1\n",
    "    return ser_result\n",
    "### Containers for preliminary data:\n",
    "dict_trans_ret_hdf = {}\n",
    "dict_trans_mcap_hdf = {}\n",
    "dict_trans_factor_hdf = {}\n",
    "dict_test_factor_raw = {}\n",
    "dict_test_factor_std = {}\n",
    "### Factors looping:\n",
    "for iter_factor in list(dict_combinations.keys())[ : -2]: # ['SHORT_TERM_MIXED']: # \n",
    "    ### Parameters loading:\n",
    "    iter_term = dict_combinations[iter_factor][0]\n",
    "    iter_algo = dict_combinations[iter_factor][1]\n",
    "    iter_concept = dict_combinations[iter_factor][2]\n",
    "    iter_eer = dict_combinations[iter_factor][3]\n",
    "    iter_ret = dict_combinations[iter_factor][4]    \n",
    "    print(f'{iter_term} / {iter_algo} / {iter_concept} / {iter_eer} / {iter_ret}')                \n",
    "    ### Iteration data loading:\n",
    "    ser_iter_ret = dict_ser_ret[iter_ret]\n",
    "    ser_iter_concept = dict_ser_concept[iter_concept]\n",
    "    ser_iter_eer = dict_ser_eer[iter_eer]\n",
    "    ### Factor matrix creating:\n",
    "    ser_iter_factor = pd.Series(index = pd.MultiIndex.from_product([idx_measure_date_range, ser_ison.index.get_level_values(1).unique()])).sort_index()\n",
    "    ser_iter_factor.index.set_names(['Date', 'Country'], inplace = True)                \n",
    "    ### Mean factor calculating:\n",
    "    if (iter_algo == 'MEAN'):\n",
    "        ### Mean momentum parameters:\n",
    "        int_iter_numer_win = dict_numer_ma_win[iter_term]\n",
    "        int_iter_denom_win = dict_denom_ma_win[iter_term]\n",
    "        int_iter_numer_min = dict_numer_ma_min[iter_term]\n",
    "        int_iter_denom_min = dict_denom_ma_min[iter_term]                    \n",
    "        ### Mean factor calculation:\n",
    "        ser_iter_factor = -ser_iter_factor.groupby('Country').transform(rolling_cond_mean_momentum, ser_iter_eer, \n",
    "                                                                        int_iter_numer_win, int_iter_numer_min, int_iter_denom_win, int_iter_denom_min)\n",
    "    else:       \n",
    "        ser_iter_delta = ser_iter_eer.groupby('Country').diff() / ser_iter_eer.groupby('Country').shift()   \n",
    "        ser_iter_delta = ser_iter_delta.replace([np.inf, -np.inf], np.NaN)\n",
    "        ### Extremum FX returns zeroing in case of shotr-treem factor:\n",
    "        if (iter_factor == 'SHORT_TERM_MIXED'):\n",
    "            ser_iter_delta.loc[All, list_fx_st] = ser_iter_delta.loc[All, list_fx_st]\\\n",
    "                                                            .where(((ser_iter_delta >= list_extreme_boundaries[0]) & (ser_iter_delta <= list_extreme_boundaries[1])), 0.0)\n",
    "        ### Momentum parameters:\n",
    "        int_mom_hl = dict_mom_hl[iter_term] * flo_exp_weight_month\n",
    "        int_mom_win = int_mom_length * ser_work_periods['Year', 'D']\n",
    "        int_mom_min = dict_mom_min[iter_term]\n",
    "        ### Weights array:\n",
    "        list_weight = list(map(lambda iter_num: exp_weight_single(int_mom_hl, iter_num), range(int_mom_win)))[::-1]\n",
    "        ### Momentum factor calculation:\n",
    "        ser_iter_factor = ser_iter_factor.groupby('Country').transform(rolling_cond_weighted_mean, ser_iter_delta, int_mom_win, int_mom_min, list_weight, False)\n",
    "        ### Factor ISONing:\n",
    "        ser_iter_factor = ser_iter_factor.to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "        ser_iter_factor.name = 'Factor'\n",
    "    ### Returns shifting and ISONing:\n",
    "    ser_iter_ret = ser_iter_ret.groupby('Country').shift(periods = -1).to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "    ### Concept multiplicator ISONing:\n",
    "    ser_iter_concept = ser_iter_concept.to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "    ### Regions clearing:\n",
    "    ser_iter_ret = ser_iter_ret.loc[idx_measure_date_range, All, list_ison]\n",
    "    ser_iter_mcap = ser_mcap.loc[idx_measure_date_range, All, list_ison]\n",
    "    ser_iter_factor = ser_iter_factor.loc[idx_measure_date_range, All, list_ison]\n",
    "    ser_iter_concept = ser_iter_concept.loc[idx_measure_date_range, All, list_ison]\n",
    "    ### Countries filtering:\n",
    "    ser_iter_ret = ser_iter_ret.drop(list_countries_to_exclude, level = 'Country')\n",
    "    ser_iter_mcap = ser_iter_mcap.drop(list_countries_to_exclude, level = 'Country')\n",
    "    ser_iter_factor = ser_iter_factor.drop(list_countries_to_exclude, level = 'Country') \n",
    "    ser_iter_concept = ser_iter_concept.drop(list_countries_to_exclude, level = 'Country')\n",
    "    ### Factor and Multiplicator standartizing (Multiplicator shifting), multiplying and restandartizing:\n",
    "    dict_test_factor_raw[iter_factor] = ser_iter_factor\n",
    "    dict_test_factor_std[iter_factor] = dict_factors_signs[iter_factor] * single_factor_standartize(ser_iter_factor, list_truncate, within_market = bool_within_market)\n",
    "    ser_iter_factor_std = dict_factors_signs[iter_factor] \\\n",
    "                          * single_factor_standartize(ser_iter_factor, list_truncate, within_market = bool_within_market, flag_tha = 'monthly')[0]    \n",
    "    ser_iter_factor_std.name = 'Factor'  \n",
    "    if (iter_concept != 'NO_CONCEPT'):\n",
    "#        ser_iter_concept_std = single_factor_standartize(ser_iter_concept, list_truncate, within_market = bool_within_market)\n",
    "#        ser_iter_multiplied = ser_iter_factor_std * ser_iter_concept_std.groupby(['Date', 'Market']).transform(value_to_rank, 2 * list_truncate[-1])\n",
    "        ser_iter_multiplied = ser_iter_factor_std * ser_iter_concept        \n",
    "    ### Preliminary results saving:\n",
    "    str_iter_key = '__'.join([iter_term, iter_algo, iter_concept, iter_eer, iter_ret])\n",
    "    dict_trans_ret_hdf[str_iter_key] = ser_iter_ret\n",
    "    dict_trans_mcap_hdf[str_iter_key] = ser_iter_mcap\n",
    "    dict_trans_factor_hdf[str_iter_key] = ser_iter_multiplied                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            Country\n",
       "1994-01-05  AR        -0.001018\n",
       "            AU         0.001362\n",
       "            BG        -0.021246\n",
       "            BR         0.004516\n",
       "            CA        -0.002680\n",
       "                         ...   \n",
       "2020-08-31  UG        -0.019174\n",
       "            US        -0.000927\n",
       "            VN        -0.010954\n",
       "            ZA        -0.008353\n",
       "            ZM        -0.007076\n",
       "Length: 513969, dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTING: CONTROL OF EXTREMUM VALUES CLEARING:\n",
    "\n",
    "#idx_test = ser_iter_delta.loc[All, list_reer_st].loc[(ser_iter_delta < list_extreme_boundaries[0]) | (ser_iter_delta > list_extreme_boundaries[1])].index\n",
    "#print(ser_iter_delta.loc[idx_test])\n",
    "#ser_iter_delta.loc[All, list_reer_st] = ser_iter_delta.loc[All, list_reer_st]\\\n",
    "#                                                      .where(((ser_iter_delta >= list_extreme_boundaries[0]) & (ser_iter_delta <= list_extreme_boundaries[1])), 0.0)\n",
    "#print(ser_iter_delta.loc[idx_test])\n",
    "#ser_iter_delta[ser_iter_delta != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: STANDALONE FACTORS EXPORT\n",
    "\n",
    "#pd.concat(dict_test_factor_raw, axis = 1).to_excel('Data_Files/Test_Files/Revision_EER_factors.xlsx', merge_cells = False)\n",
    "pd.concat(dict_test_factor_std, axis = 1).to_excel('Data_Files/Test_Files/Revision_EER_factors.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: COMBINED FACTORS CALCULATION\n",
    "\n",
    "### Factor weghtings initializing:\n",
    "dict_weighted_factor = {}\n",
    "### Multiplying factors by averaging weights:\n",
    "for iter_factor in list(dict_combinations.keys())[ : -2]: \n",
    "    str_iter_key = '__'.join(dict_combinations[iter_factor])\n",
    "    dict_weighted_factor[iter_factor] = dict_trans_factor_hdf[str_iter_key] * dict_factors_weights[iter_factor]\n",
    "### Taking first factor name to load Returns and Market Caps\n",
    "str_standalone_key = list(dict_trans_factor_hdf.keys())[0]\n",
    "### Aggregated factors looping:\n",
    "for iter_factor in list(dict_combinations.keys())[-2 : ]:\n",
    "    ### Key constructing:\n",
    "    str_iter_key = '__'.join([iter_factor.split('_')[-1]] + ['COMBO'] * 4)\n",
    "    dict_trans_ret_hdf[str_iter_key] = dict_trans_ret_hdf[str_standalone_key]\n",
    "    dict_trans_mcap_hdf[str_iter_key] = dict_trans_mcap_hdf[str_standalone_key]\n",
    "    \n",
    "    ### Combo factor calculating:\n",
    "    if (iter_factor.split('_')[-1] == 'DOUBLE'):\n",
    "        ser_combo_factor = pd.concat([dict_weighted_factor['LONG_TERM_EER'], dict_weighted_factor['SHORT_TERM_MIXED']], axis = 1).mean(axis = 1)\n",
    "    if (iter_factor.split('_')[-1] == 'TRIPLE'):\n",
    "        ser_combo_factor = pd.concat(dict_weighted_factor, axis = 1).mean(axis = 1)\n",
    "    ### Combo factor standartizing:\n",
    "    dict_trans_factor_hdf[str_iter_key] = single_factor_standartize(ser_combo_factor, list_truncate, within_market = bool_within_market)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: COMBINED FACTORS EXPORT\n",
    "\n",
    "#pd.concat(dict_trans_factor_hdf, axis = 1).to_excel('Data_Files/Test_Files/Revision_EER_factors.xlsx', merge_cells = False)\n",
    "#pd.concat(dict_trans_factor_hdf, axis = 1).to_excel('Data_Files/Test_Files/Revision_EER_factors.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: SAVING TRANSITIONAL RESULTS (NO PRELIMINARY DATA USING)\n",
    "\n",
    "for iter_key in dict_trans_factor_hdf:\n",
    "    dict_trans_ret_hdf[iter_key].to_hdf(str_path_trans_hdf, key = str_key_trans_ret + '__' + iter_key, mode = 'a')\n",
    "    dict_trans_mcap_hdf[iter_key].to_hdf(str_path_trans_hdf, key = str_key_trans_mcap + '__' + iter_key, mode = 'a')\n",
    "    dict_trans_factor_hdf[iter_key].to_hdf(str_path_trans_hdf, key = str_key_trans_factor + '__' + iter_key, mode = 'a')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG_TERM / MOMENTUM / EXP_GDP_rate / REER / HEDGED\n",
      "SHORT_TERM / MOMENTUM / EXP_GDP_rate / NEER / HEDGED\n",
      "LONG_TERM / MOMENTUM / EXP_GDP_rate / EXPORT / HEDGED\n",
      "DOUBLE / COMBO / COMBO / COMBO / COMBO\n",
      "TRIPLE / COMBO / COMBO / COMBO / COMBO\n"
     ]
    }
   ],
   "source": [
    "### TESTING: FACTOR SAVING:\n",
    "\n",
    "dict_trans_factor_hdf = {}\n",
    "### Single factors looping:\n",
    "for iter_factor in dict_combinations.keys(): # ['LONG_TERM']: # ['SHORT_TERM']: # \n",
    "    ### Parameters loading:\n",
    "    iter_term = dict_combinations[iter_factor][0]\n",
    "    iter_algo = dict_combinations[iter_factor][1]\n",
    "    iter_concept = dict_combinations[iter_factor][2]\n",
    "    iter_eer = dict_combinations[iter_factor][3]\n",
    "    iter_ret = dict_combinations[iter_factor][4]    \n",
    "    print(f'{iter_term} / {iter_algo} / {iter_concept} / {iter_eer} / {iter_ret}') \n",
    "    str_iter_key = '__'.join([iter_term, iter_algo, iter_concept, iter_eer, iter_ret])\n",
    "    dict_trans_factor_hdf[str_iter_key] = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_factor + '__' + str_iter_key) \n",
    "### Data aggregating:\n",
    "df_factor_values = pd.concat(dict_trans_factor_hdf).to_frame()\n",
    "df_factor_values.index.rename('KEY', level = 0, inplace = True)\n",
    "df_factor_values.reset_index('KEY', inplace = True)\n",
    "df_factor_values['Algorythm'] = df_factor_values['KEY'].str.split('__').str[0]\n",
    "df_factor_values['Factor'] = df_factor_values['KEY'].str.split('__').str[1]\n",
    "df_factor_values['Returns'] = df_factor_values['KEY'].str.split('__').str[2]\n",
    "df_factor_values['Multiplicator'] = df_factor_values['KEY'].str.split('__').str[3]\n",
    "df_factor_values['EER'] = df_factor_values['KEY'].str.split('__').str[4]\n",
    "ser_factor_values = df_factor_values.set_index(['Algorythm', 'Factor', 'Returns', 'Multiplicator', 'EER'], append = True).drop('KEY', axis = 1).squeeze()\n",
    "ser_factor_values.name = 'Factor_Value'\n",
    "ser_factor_values.to_excel(str_path_factors_xlsx, merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__REER__HEDGED__DM & EM & FM\n",
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__REER__HEDGED__DM & EM\n",
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__REER__HEDGED__DM\n",
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__REER__HEDGED__EM\n",
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__REER__HEDGED__FM\n",
      "SHORT_TERM__MOMENTUM__EXP_GDP_rate__NEER__HEDGED__DM & EM & FM\n",
      "SHORT_TERM__MOMENTUM__EXP_GDP_rate__NEER__HEDGED__DM & EM\n",
      "SHORT_TERM__MOMENTUM__EXP_GDP_rate__NEER__HEDGED__DM\n",
      "SHORT_TERM__MOMENTUM__EXP_GDP_rate__NEER__HEDGED__EM\n",
      "SHORT_TERM__MOMENTUM__EXP_GDP_rate__NEER__HEDGED__FM\n",
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__EXPORT__HEDGED__DM & EM & FM\n",
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__EXPORT__HEDGED__DM & EM\n",
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__EXPORT__HEDGED__DM\n",
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__EXPORT__HEDGED__EM\n",
      "LONG_TERM__MOMENTUM__EXP_GDP_rate__EXPORT__HEDGED__FM\n",
      "DOUBLE__COMBO__COMBO__COMBO__COMBO__DM & EM & FM\n",
      "DOUBLE__COMBO__COMBO__COMBO__COMBO__DM & EM\n",
      "DOUBLE__COMBO__COMBO__COMBO__COMBO__DM\n",
      "DOUBLE__COMBO__COMBO__COMBO__COMBO__EM\n",
      "DOUBLE__COMBO__COMBO__COMBO__COMBO__FM\n",
      "TRIPLE__COMBO__COMBO__COMBO__COMBO__DM & EM & FM\n",
      "TRIPLE__COMBO__COMBO__COMBO__COMBO__DM & EM\n",
      "TRIPLE__COMBO__COMBO__COMBO__COMBO__DM\n",
      "TRIPLE__COMBO__COMBO__COMBO__COMBO__EM\n",
      "TRIPLE__COMBO__COMBO__COMBO__COMBO__FM\n"
     ]
    }
   ],
   "source": [
    "### FACTORS PERFORMING (PRELIMINARY DATA USED)\n",
    "\n",
    "### Results container:\n",
    "dict_measure_vectors = {}\n",
    "dict_measure_stats = {}\n",
    "### Algorythms looping:\n",
    "### Factors looping:\n",
    "for iter_factor in dict_combinations.keys(): # ['LONG_TERM']: # ['SHORT_TERM']: # \n",
    "    ### Parameters loading:\n",
    "    iter_term = dict_combinations[iter_factor][0]\n",
    "    iter_algo = dict_combinations[iter_factor][1]\n",
    "    iter_concept = dict_combinations[iter_factor][2]\n",
    "    iter_eer = dict_combinations[iter_factor][3]\n",
    "    iter_ret = dict_combinations[iter_factor][4] \n",
    "    ### Regions looping:\n",
    "    for iter_region in [['DM', 'EM', 'FM'], ['DM', 'EM'], ['DM'], ['EM'], ['FM']]: # [['DM', 'EM'], ['DM'], ['EM']]\n",
    "        ### Transitional data loading, filtering and concatenating:\n",
    "        str_iter_key = '__'.join([iter_term, iter_algo, iter_concept, iter_eer, iter_ret]) \n",
    "        str_iter_res = str_iter_key + '__' + ' & '.join(iter_region)\n",
    "        print(str_iter_res)\n",
    "        ser_iter_ret = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_ret + '__' + str_iter_key).loc[All, All, list_filter]\n",
    "        ser_iter_mcap = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_mcap + '__' + str_iter_key).loc[All, All, list_filter]\n",
    "        ser_iter_factor = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_factor + '__' + str_iter_key).loc[All, All, list_filter]\n",
    "        ser_iter_factor.name = str_iter_res\n",
    "        ### Measure calculating:\n",
    "        dict_measure_stats[str_iter_res], dict_measure_vectors[str_iter_res] = multiple_factor_single_efficacy_measure_stats(ser_iter_factor.to_frame(), \n",
    "                                                                                                                             ser_iter_ret * 100, \n",
    "                                                                                                                             ser_iter_mcap,\n",
    "                                                                                                                             list_measure[0], \n",
    "                                                                                                                             list_back_period[0], \n",
    "                                                                                                                             int_horizon, \n",
    "                                                                                                                             iter_region)      \n",
    "### Results preparing (stats):\n",
    "df_factor_measure_stats = pd.concat(dict_measure_stats).droplevel(0)\n",
    "df_factor_measure_stats.index.name = 'KEY'\n",
    "df_factor_measure_stats.reset_index(inplace = True)\n",
    "df_factor_measure_stats['Term'] = df_factor_measure_stats['KEY'].str.split('__').str[0]\n",
    "df_factor_measure_stats['Algorythm'] = df_factor_measure_stats['KEY'].str.split('__').str[1]\n",
    "df_factor_measure_stats['Interaction'] = df_factor_measure_stats['KEY'].str.split('__').str[2]\n",
    "df_factor_measure_stats['Source'] = df_factor_measure_stats['KEY'].str.split('__').str[3]\n",
    "df_factor_measure_stats['Returns'] = df_factor_measure_stats['KEY'].str.split('__').str[4]\n",
    "df_factor_measure_stats['Region'] = df_factor_measure_stats['KEY'].str.split('__').str[5]\n",
    "df_factor_measure_stats = df_factor_measure_stats.set_index(['Term', 'Algorythm', 'Interaction', 'Source', 'Returns', 'Region',]).drop('KEY', axis = 1)\n",
    "df_factor_measure_stats.to_hdf(str_path_efficacy_hdf, key = str_key_efficacy, mode = 'w')\n",
    "### Results preparing (vectors):\n",
    "df_measure_vectors = pd.concat(dict_measure_vectors).droplevel(0)\n",
    "df_measure_vectors.index.names = ['KEY', 'Date']\n",
    "df_measure_vectors.reset_index(inplace = True)\n",
    "df_measure_vectors['Term'] = df_measure_vectors['KEY'].str.split('__').str[0]\n",
    "df_measure_vectors['Algorythm'] = df_measure_vectors['KEY'].str.split('__').str[1]\n",
    "df_measure_vectors['Interaction'] = df_measure_vectors['KEY'].str.split('__').str[2]\n",
    "df_measure_vectors['Source'] = df_measure_vectors['KEY'].str.split('__').str[3]\n",
    "df_measure_vectors['Returns'] = df_measure_vectors['KEY'].str.split('__').str[4]\n",
    "df_measure_vectors['Region'] = df_measure_vectors['KEY'].str.split('__').str[5]\n",
    "df_measure_vectors = df_measure_vectors.set_index(['Term', 'Algorythm', 'Interaction', 'Source', 'Returns', 'Region', 'Date']).drop('KEY', axis = 1)\n",
    "df_measure_vectors.to_hdf(str_path_vectors_hdf, key = str_key_efficacy, mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: FACTORS EXTRACTING (PRELIMINARY DATA USED)\n",
    "\n",
    "df_factor_measure_vectors = pd.read_hdf(str_path_vectors_hdf, key = str_key_efficacy)\n",
    "df_factor_measure_vectors.to_excel(str_path_vectors_xlsx, merge_cells = False)\n",
    "df_factor_measure_stats = pd.read_hdf(str_path_efficacy_hdf, key = str_key_efficacy)\n",
    "df_factor_measure_stats.to_excel(str_path_efficacy_xlsx, merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: DATA PREPARATION\n",
    "\n",
    "iter_term = 'SHORT_TERM' # 'LONG_TERM' # 'COMBO' # \n",
    "iter_algo = 'MOMENTUM' # 'COMBO' # 'MEAN' # \n",
    "iter_concept = 'EXP_GDP_rate' # 'COMBO' # 'NO_CONCEPT' # \n",
    "iter_eer = 'NEER' # 'REER' # 'COMBO' # \n",
    "iter_ret = 'HEDGED' # 'COMBO' # 'LOC' # 'USD' #\n",
    "iter_region = ['DM', 'EM', 'FM'] # ['DM'] # ['EM'] # ['FM'] #\n",
    "### Transitional data loading, filtering and concatenating:\n",
    "str_iter_key = '__'.join([iter_term, iter_algo, iter_concept, iter_eer, iter_ret]) \n",
    "print(str_iter_key)\n",
    "ser_iter_ret = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_ret + '__' + str_iter_key).loc[All, All, iter_region]\n",
    "ser_iter_ret.name = 'Returns'\n",
    "ser_iter_mcap = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_mcap + '__' + str_iter_key).loc[All, All, iter_region]\n",
    "ser_iter_mcap.name = 'Market Cap'\n",
    "ser_iter_factor = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_factor + '__' + str_iter_key).loc[All, All, iter_region]\n",
    "ser_iter_factor.name = 'Factor'\n",
    "### Converting to MatLab regions notification:\n",
    "#dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM'}\n",
    "#dict_regions = dict(zip(dict_markets.values(), dict_markets.keys()))\n",
    "#ser_iter_ret = ser_iter_ret.reset_index('Market').replace(dict_regions).set_index('Market', append = True).squeeze()\n",
    "#ser_iter_mcap = ser_iter_mcap.reset_index('Market').replace(dict_regions).set_index('Market', append = True).squeeze()\n",
    "#ser_iter_factor = ser_iter_factor.reset_index('Market').replace(dict_regions).set_index('Market', append = True).squeeze()\n",
    "#ser_iter_ret.to_excel('Data_Files/Test_Files/Example_EER_Returns.xlsx', merge_cells = False)\n",
    "#ser_iter_mcap.to_excel('Data_Files/Test_Files/Example_EER_Market_Caps.xlsx', merge_cells = False)\n",
    "#ser_iter_factor.to_excel('Data_Files/Test_Files/Example_EER_Factor.xlsx', merge_cells = False)\n",
    "### Multiple factors\n",
    "#ser_mean_factor = ser_iter_factor\n",
    "#ser_mom_factor = ser_iter_factor\n",
    "#df_factors = pd.concat([ser_mean_factor,  ser_mom_factor], axis = 1)\n",
    "#df_factors.columns = ['Mean', 'Momentum']\n",
    "#df_factors.to_excel('Data_Files/Test_Files/Example_EER_Factors.xlsx', merge_cells = False)\n",
    "#df_factors = ser_iter_ret.to_frame().join(ser_iter_mcap).join(df_factors).loc[All, ['Mean', 'Momentum']]\n",
    "### Multiple returns:\n",
    "#idx_data_short = pd.date_range('2000-01-01', '2000-04-01', freq = 'BM')\n",
    "#dict_returns = {}\n",
    "#for iter_ret in ['LOC', 'USD', 'HEDGED']:\n",
    "#    ser_iter_ret = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_ret + '__' + str_iter_key).loc[All, All, iter_region]\n",
    "#    ser_iter_ret.name = iter_ret\n",
    "#    dict_returns[iter_ret] = ser_iter_ret\n",
    "#df_returns = pd.concat(dict_ret)    \n",
    "#df_returns.index.names = ['Type'] + df_returns.index.names[1 : ]\n",
    "\n",
    "#df_returns.loc[All, idx_data_short, All, All].to_excel('Data_Files/Test_Files/Example_EER_Returns_DF.xlsx', merge_cells = False)\n",
    "#ser_iter_factor.name = 'Factor LT Mean No Concept'\n",
    "#ser_iter_factor.loc[idx_data_short, All, All].to_excel('Data_Files/Test_Files/Example_EER_Factor_Filtered.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
