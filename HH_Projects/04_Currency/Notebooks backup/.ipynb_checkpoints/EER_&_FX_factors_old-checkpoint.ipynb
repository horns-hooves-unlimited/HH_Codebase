{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BLOOMBERG EFFECTIVE EXCHANGE RATE SOURCES COMPARING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "### Factors sources:\n",
    "str_path_bb_eer_source = 'Data_Files/Source_Files/Bloomberg_EER.xlsx'\n",
    "str_path_bb_fx_source = 'Data_Files/Source_Files/Bloomberg_FX.xlsx'\n",
    "dict_eer_sources = {'JPM REER B C D': 'REER 01-JPM', 'CTG REER B D': 'REER 02-CTG', 'IMF REER B M': 'REER 03-IMF', 'BIS REER B M': 'REER 04-BIS',\n",
    "                    'JPM NEER B D': 'NEER 01-JPM', 'CTG REER B D (2)': 'NEER 02-CTG', 'BIS NEER B D': 'NEER 03-BIS'}\n",
    "### Factors parameters:\n",
    "list_truncate = [2.5, 2.0]\n",
    "int_numer_ma_win = 21 * 3\n",
    "int_denom_ma_win = 252 * 5\n",
    "int_short_diff = 21 * 3\n",
    "### Basics for efficacy measures calculating:\n",
    "path_market_cap = 'Data_Files/Source_Files/Market_Cap.h5'\n",
    "key_market_cap = 'mcap'\n",
    "path_return = 'Data_Files/Source_Files/Returns_Integrated.h5'\n",
    "key_return = 'returns'\n",
    "### Export parameters:\n",
    "str_path_neer_export = 'Data_Files/Test_Files/Saved_NEER.xlsx'\n",
    "str_path_reer_export = 'Data_Files/Test_Files/Saved_REER.xlsx'\n",
    "str_path_fx_export = 'Data_Files/Test_Files/Saved_FX.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE\n",
    "\n",
    "def weighted_average(ser_to_manage, ser_weight = pd.Series()):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if (len(ser_weight.index) == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_to_manage.index)\n",
    "        ser_weight.name = 'Weight'\n",
    "    ### Clearing and docking vectors:\n",
    "    df_to_manage = ser_to_manage.to_frame().join(ser_weight, how = 'left').dropna()\n",
    "    df_to_manage.columns = ['Data', 'Weight']\n",
    "    ### Result calculating:\n",
    "    num_result = np.NaN\n",
    "    if (len(df_to_manage.index) > 0):\n",
    "        num_result = df_to_manage['Data'].dot(df_to_manage['Weight']) / sum(df_to_manage['Weight'])      \n",
    "    \n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES\n",
    "\n",
    "def multistep_standartize(ser_to_manage, arr_truncate, ser_weight = pd.Series(), reuse_outliers = False, center_result = True, full_result = False):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd      \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if (len(ser_weight.index) == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_to_manage.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_to_manage.dropna().copy() \n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Clearing and docking vectors:        \n",
    "        index_iter = ser_data_iter.index.intersection(ser_weight_iter.index)\n",
    "        ser_data_iter = ser_data_iter[index_iter]\n",
    "        ser_weight_iter = ser_weight_iter[index_iter] \n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        if not (reuse_outliers):\n",
    "            ### Saving to result and excluding from further calculations truncated values:     \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):      \n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_to_manage.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR SEPARATE SERIES\n",
    "\n",
    "def ison_standartize(ser_to_manage, arr_truncate, ser_weight = pd.Series(), reuse_outliers = False, center_result = True, full_result = False, within_market = False):\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Multi-step standartizing:\n",
    "    if (within_market):\n",
    "    ### Within market standartizing:\n",
    "        ser_result = ser_to_manage.groupby(by = 'Market', group_keys = False).apply(multistep_standartize, arr_truncate, ser_weight, \n",
    "                                                                                    reuse_outliers, center_result, full_result)\n",
    "    else:\n",
    "    ### Full universe standartizing:\n",
    "        ser_result = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result)\n",
    "    ### Results output:\n",
    "    return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK\n",
    "\n",
    "def single_factor_standartize(ser_factor, arr_truncate, ser_weight = pd.Series(), reuse_outliers = False, center_result = True, within_market = False):\n",
    "    ### Defining by date standartizing function:\n",
    "    def by_date_standartize(df_date, arr_truncate, reuse_outliers, center_result, within_market):\n",
    "        ### ISON standartizing:\n",
    "        ser_date = ison_standartize(df_date['Factor'], arr_truncate, df_date['Weight'], reuse_outliers, center_result, False, within_market)\n",
    "        ser_date = ser_date.reindex(df_date.index)\n",
    "        ### Result output:\n",
    "        return ser_date\n",
    "    ### Importing standard modules and date-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Weights preparing:\n",
    "    if (len(ser_weight.index) == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Multi-step standartizing:        \n",
    "    df_factor = ser_factor.to_frame().join(ser_weight, how = 'left')\n",
    "    df_factor.columns = ['Factor', 'Weight']\n",
    "#    ser_result = df_factor.groupby('Date').apply\\\n",
    "#    (lambda iter_df: ison_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False, within_market))\n",
    "    ser_result = df_factor.groupby('Date', group_keys = False).apply(by_date_standartize, arr_truncate, reuse_outliers, center_result, within_market)\n",
    "    ### Results output:\n",
    "    ser_result.name = ser_factor.name\n",
    "    return ser_result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING GROUP MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK FOR MULTIPLE FACTORS\n",
    "\n",
    "def multi_factor_standartize(df_factor, arr_truncate, ser_weight = pd.Series(), reuse_outliers = False, center_result = True, within_market = False):\n",
    "    \n",
    "    dict_standartized = {}\n",
    "    ### Single factor standartizing:\n",
    "    for iter_factor in df_factor.columns:\n",
    "        dict_standartized[iter_factor] = single_factor_standartize(df_factor[iter_factor], arr_truncate, ser_weight, \n",
    "                                                                   reuse_outliers, center_result, within_market)\n",
    "    ### Concatenating to dataframe:\n",
    "    df_result = pd.concat(dict_standartized, axis = 1)\n",
    "    ### Results output:\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Importing standard modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math     \n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EFFICACY MEASURES FOR SINGLE FACTOR\n",
    "\n",
    "def single_factor_multiple_efficacy_measures(ser_factor, ser_return, ser_weight, arr_measure, return_shift = 0, arr_truncate = [2.5, 2.0]):\n",
    "    ### Importing standard modules and data-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import statsmodels.api as sm\n",
    "    from scipy import stats as ss\n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    dict_measure = {}\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining get_measure group level function:\n",
    "    def get_measure(df_to_measure, iter_measure):\n",
    "        ### Checking data sufficiency:\n",
    "        if (len(df_to_measure.dropna().index) > 1):           \n",
    "            ### Measure calculating:\n",
    "            if (iter_measure == 'ic_spearman'):\n",
    "                ### Spearmen information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values\n",
    "                num_result = ss.spearmanr(list_factor, list_return, nan_policy = 'omit').correlation\n",
    "            if (iter_measure == 'ic_pearson'):\n",
    "                ### Pearson information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values                \n",
    "                num_result = ss.pearsonr(list_factor, list_return)[0]\n",
    "            if (iter_measure == 'fmb_eqw'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (equal weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return']].dropna()['Return'].values\n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values\n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_std_eqw'):             \n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()['Return'].values                \n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_std_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]                 \n",
    "            if (iter_measure == 'fmb_std_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values                \n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_std_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]  \n",
    "            if (iter_measure == 'clp'):\n",
    "                ### Constant leverage portfolio signed normalized multiplication sum:                \n",
    "                ser_clp_weighted = df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Factor']\n",
    "                ser_clp_weighted = ser_clp_weighted * df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Weight'].transform(np.sqrt)\n",
    "                ser_clp_weighted.loc[ser_clp_weighted < 0] = -ser_clp_weighted / ser_clp_weighted[ser_clp_weighted < 0].sum()\n",
    "                ser_clp_weighted.loc[ser_clp_weighted > 0] = ser_clp_weighted / ser_clp_weighted[ser_clp_weighted > 0].sum()\n",
    "                num_result = (ser_clp_weighted * df_to_measure['Return']).sum()\n",
    "                                  \n",
    "        else:                          \n",
    "            num_result = np.NaN\n",
    "        ### Preparing results: \n",
    "        return num_result\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    if (ser_weight.count() == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ser_weight = ser_weight.reset_index('Market', drop = True)\n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted, ser_weight], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return', 'Weight']\n",
    "    df_to_measure['Constant'] = 1\n",
    "    ser_factor_std = df_to_measure.dropna()['Factor'].groupby('Date').apply(ison_standartize, arr_truncate = arr_truncate, within_market = False)\n",
    "    df_to_measure['Factor_std'] = ser_factor_std.reindex(df_to_measure.index)\n",
    "    ### Looping efficacy measures for calculating measures timeseries:\n",
    "    for iter_measure in arr_measure:\n",
    "        dict_measure[iter_measure] = df_to_measure.groupby('Date').apply(get_measure, iter_measure = iter_measure)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_measure, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEASURE STATISTICS CALCULATOR\n",
    "\n",
    "def measure_stats(df_measures, arr_back_period = [99]):\n",
    "    ### Importing standard modules and data-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Declaring local constants & variables:\n",
    "    dict_stats = {}\n",
    "    ### Stats calculating:\n",
    "    for iter_measure in df_measures.columns:\n",
    "        dict_period = {}\n",
    "        for iter_back_period in arr_back_period:\n",
    "            ser_iter_measure = df_measures[iter_measure].dropna()\n",
    "            idx_iter_range = pd.date_range(end = ser_iter_measure.index[-1], periods = iter_back_period * 12, freq = 'BM')\n",
    "            ser_iter_measure = ser_iter_measure[idx_iter_range]            \n",
    "            ser_iter_stats = pd.Series()\n",
    "            ser_iter_stats['count'] = ser_iter_measure.count()\n",
    "            ser_iter_stats['min'] = ser_iter_measure.min()\n",
    "            ser_iter_stats['max'] = ser_iter_measure.max()        \n",
    "            ser_iter_stats['mean'] = ser_iter_measure.mean()\n",
    "            ser_iter_stats['std'] = ser_iter_measure.std()\n",
    "            ser_iter_stats['median'] = ser_iter_measure.median()        \n",
    "            ser_iter_stats['perc_25'] = ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['perc_75'] = ser_iter_measure.quantile(0.75, 'midpoint')\n",
    "            ser_iter_stats['iq_range'] = ser_iter_measure.quantile(0.75, 'midpoint') - ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['mean_abs'] = ser_iter_measure.abs().mean()\n",
    "            ser_iter_stats['t_stat'] = (ser_iter_measure.mean() / ser_iter_measure.std()) * np.sqrt(ser_iter_measure.count())  \n",
    "            dict_period[iter_back_period] = ser_iter_stats\n",
    "        dict_stats[iter_measure] = pd.concat(dict_period, axis = 1)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_stats, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SPECIAL CLP STATS\n",
    "\n",
    "def special_clp_stats(ser_factor, ser_return, ser_weight, return_shift = 0):\n",
    "    ### Importing standard modules and data-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Declaring local constants & variables:    \n",
    "    dict_clp_stats = {}\n",
    "    list_bin_labels = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining function for constant leverage portfolio normalizing:\n",
    "    def get_normalized_clp(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Constant leverage portfolio signed normalized:\n",
    "            ser_clp_weighted.loc[ser_clp_weighted < 0] = -ser_clp_weighted / ser_clp_weighted[ser_clp_weighted < 0].sum()\n",
    "            ser_clp_weighted.loc[ser_clp_weighted > 0] = ser_clp_weighted / ser_clp_weighted[ser_clp_weighted > 0].sum()\n",
    "            ser_result = ser_clp_weighted.copy()\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index)\n",
    "        ### Results output:\n",
    "        return ser_result\n",
    "    ### Defining function for constant leverage portfolio normalizing:\n",
    "    def get_normalized_factor(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Factor signed normalized:\n",
    "            ser_factor_normalized = df_to_measure['Factor']\n",
    "            ser_factor_normalized.loc[ser_factor_normalized < 0] = -ser_factor_normalized / ser_factor_normalized[ser_factor_normalized < 0].sum()\n",
    "            ser_factor_normalized.loc[ser_factor_normalized > 0] = ser_factor_normalized / ser_factor_normalized[ser_factor_normalized > 0].sum()\n",
    "            ser_result = ser_factor_normalized\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index) \n",
    "        ### Results output:\n",
    "        return ser_result            \n",
    "    ### Defining function for returns for constant leverage portfolio:\n",
    "    def get_normalized_return(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Normalized return:  \n",
    "            ser_result = df_to_measure['Return']\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index)            \n",
    "        ### Results output:\n",
    "        return ser_result  \n",
    "    ### Defining MatLab style prctile function:\n",
    "    def prctile(ser_to_perc, p):\n",
    "        ### Sorted list preparing:\n",
    "        list_to_perc = ser_to_perc.dropna().values\n",
    "        list_sorted = np.sort(list_to_perc)\n",
    "        ### Length calculating:\n",
    "        num_len = len(list_to_perc)    \n",
    "        ### Prctile calculating:\n",
    "        num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "        ### Results output:\n",
    "        return num_result\n",
    "    ### Defining quintile bins distribution:\n",
    "    def quartile_distribution(ser_iter_group):\n",
    "        ### Bins preparing:\n",
    "        list_bin = list(np.arange(0, 100, 20))[1 : ]\n",
    "        list_bin = [round(iter_element / 100, 2) for iter_element in list_bin]\n",
    "        list_bin = [prctile(ser_iter_group, iter_element) for iter_element in list_bin]\n",
    "        list_bin = [ser_iter_group.min() - abs(ser_iter_group.min())] + list_bin + [ser_iter_group.max() + abs(ser_iter_group.max())]\n",
    "        ### Bins distribution:\n",
    "        ser_iter_distribution = pd.cut(ser_iter_group, bins = list_bin, labels = list_bin_labels)    \n",
    "        ### Results output:\n",
    "        return ser_iter_distribution   \n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    if (ser_weight.count() == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ser_weight = ser_weight.reset_index('Market', drop = True)      \n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted, ser_weight], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return', 'Weight']\n",
    "    ser_clp_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_clp)\n",
    "    ser_factor_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_factor)\n",
    "    ser_return_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_return)\n",
    "    ### CLP stats calculating:\n",
    "    dict_clp_stats['Average Bias'] = ser_factor_normalized.groupby('Country').mean()\n",
    "#    dict_clp_stats['Temp'] = pd.Series(np.NaN, index = dict_clp_stats['Average Bias'].index)\n",
    "    dict_clp_stats['Weights Sum'] = ser_clp_normalized.groupby('Country').sum()\n",
    "    dict_clp_stats['Average Return'] = ser_return_normalized.groupby('Country').mean()   \n",
    "    dict_clp_stats['Static Contribution'] = dict_clp_stats['Weights Sum'] * dict_clp_stats['Average Return']\n",
    "    dict_clp_stats['Total Contribution'] = (ser_clp_normalized * ser_return_normalized).groupby('Country').sum()\n",
    "    dict_clp_stats['Dynamic Contribution'] = dict_clp_stats['Total Contribution'] - dict_clp_stats['Static Contribution'] \n",
    "    ### CLP active weights calculating:\n",
    "    ser_clp_delta = ser_clp_normalized.unstack('Date').stack('Date', dropna = False).swaplevel().sort_index(level = ['Date', 'Country'])\n",
    "    ser_clp_delta = ser_clp_delta.fillna(0)\n",
    "    num_clp_mean = ser_clp_delta.groupby('Country').mean().abs().sum()\n",
    "    ser_clp_delta = ser_clp_delta.groupby('Country').apply(lambda iter_group: iter_group - iter_group.mean())\n",
    "    num_clp_delta = (ser_clp_delta.abs().groupby('Date').sum() / (ser_clp_delta.abs().groupby('Date').sum() + num_clp_mean)).mean()\n",
    "    df_clp_stats = pd.concat(dict_clp_stats, axis = 1).reindex(df_to_measure.index.get_level_values('Country').unique()).sort_index()\n",
    "    ### Preparing sum:\n",
    "    df_clp_sum = pd.DataFrame([[np.NaN, np.NaN, np.NaN, df_clp_stats['Static Contribution'].sum(), \n",
    "                               df_clp_stats['Total Contribution'].sum(), df_clp_stats['Dynamic Contribution'].sum()]], \n",
    "                              index = ['Sum'], columns = df_clp_stats.columns)\n",
    "    ### Preparing expected based on active weights:\n",
    "    df_clp_expected = pd.DataFrame([[np.NaN, np.NaN, np.NaN, \n",
    "                                     df_clp_stats['Total Contribution'].sum() * (1 - num_clp_delta), np.NaN, df_clp_stats['Total Contribution'].sum() * num_clp_delta]], \n",
    "                              index = ['Expected based on active weights =>'], columns = df_clp_stats.columns)    \n",
    "    ### Adding totals:\n",
    "    df_clp_stats = pd.concat([df_clp_stats, df_clp_sum, df_clp_expected], axis = 0, join = 'inner')\n",
    "    ### CLP Bias calculating:   \n",
    "    ser_clp_quintile = ser_clp_normalized.groupby('Date', group_keys = False).apply(quartile_distribution)\n",
    "    df_clp_bias = ser_clp_quintile.to_frame()  \n",
    "    df_clp_bias.columns = ['Bin']\n",
    "    df_clp_bias['Quintile'] = 1\n",
    "    df_clp_bias = df_clp_bias.set_index('Bin', append = True).unstack('Bin').fillna(0).droplevel(level = 0, axis = 1)\n",
    "    df_clp_bias.columns = list(df_clp_bias.columns)  \n",
    "    df_clp_bias = df_clp_bias[list_bin_labels]\n",
    "    df_clp_bias = df_clp_bias.groupby('Country').mean()\n",
    "    df_clp_bias.loc[:, 'Q5 - Q1'] = df_clp_bias['Q5'] - df_clp_bias['Q1']   \n",
    "    df_clp_bias = df_clp_bias.reindex(df_to_measure.index.get_level_values('Country').unique()).sort_index()\n",
    "    df_clp_bias = df_clp_bias\n",
    "    ### Output results:\n",
    "    return (df_clp_stats, df_clp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SINGLE EFFICACY MEASURE FOR MULTIPLE FACTORS\n",
    "    \n",
    "def multiple_factor_single_efficacy_measure_stats(df_factors, ser_return, ser_weight, str_measure, num_back_period = 99, \n",
    "                                                  num_horizon = 12, list_region_xmo = ['DM', 'EM', 'FM']): \n",
    "    ### Importing standard modules and data-special modules:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    list_months = [1, 2, 3, 6, 9 ,12]\n",
    "    ### Defining full universe expanding for date:\n",
    "    def universe_reindex(iter_group, idx_universe):\n",
    "        df_iter_result = iter_group.unstack('Date').reindex(idx_universe).sort_index().stack('Date', dropna = False)\n",
    "        ### Results output:\n",
    "        return df_iter_result   \n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result   \n",
    "    ### Defining by date correrlation function:\n",
    "    def corr_by_date(iter_group):\n",
    "        num_iter_corr = iter_group['Corr_factor_minus'].corr(iter_group['Corr_factor_plus'])\n",
    "        ### Results output:\n",
    "        return num_iter_corr       \n",
    "    ### Preparing expanded universe for autocorrelation performing:\n",
    "    df_factors_region = df_factors.loc[(All, All, list_region_xmo), :]\n",
    "    idx_date_range = df_factors_region.index.get_level_values(0).unique()\n",
    "    idx_universe = df_factors_region.index.get_level_values(1).unique()\n",
    "    df_factors_full = df_factors_region.reset_index('Market', drop = True).groupby('Date', group_keys = False).apply(universe_reindex, idx_universe).swaplevel()\n",
    "    ### Factors looping:\n",
    "    dict_factors_measures = {} ### Container for all factor stats\n",
    "    dict_factors_autocorr = {} ### Container for autocorrelation results\n",
    "    for iter_factor in df_factors.columns:\n",
    "        ### Shifts looping for factors measures stats:\n",
    "        ### Stats calculation:\n",
    "        dict_factor_stats = {} ### Container for iterated factor stats\n",
    "        for iter_shift in range(num_horizon):\n",
    "#            df_factor_filtered = df_factors[iter_factor].loc[All, All, list_region_xmo]\n",
    "            df_factor_filtered = df_factors_region[iter_factor]\n",
    "            df_iter_shift_measure = single_factor_multiple_efficacy_measures(df_factor_filtered, ser_return, ser_weight, [str_measure], iter_shift, list_truncate)\n",
    "            df_iter_shift_stats = measure_stats(df_iter_shift_measure, [num_back_period])\n",
    "            dict_factor_stats[iter_shift] = df_iter_shift_stats.loc[['mean', 't_stat'], (str_measure, num_back_period)]\n",
    "        df_iter_factor_stats = pd.concat(dict_factor_stats, axis = 1)\n",
    "        df_iter_factor_stats.columns = df_iter_factor_stats.columns + 1\n",
    "        dict_factors_measures[iter_factor] = df_iter_factor_stats\n",
    "        ### Autocorrelation calculation:\n",
    "        ser_iter_factor = df_factors_full[iter_factor]\n",
    "        ser_iter_factor_plus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[1 : ]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ser_iter_factor_minus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[: -1]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ### Artificial series combining for indexes synchronization:        \n",
    "        ser_iter_factor_plus_shifted = ser_iter_factor_plus.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, 1)\n",
    "        df_iter_factor_to_corr = pd.concat([ser_iter_factor_minus, ser_iter_factor_plus_shifted], axis = 1)\n",
    "        df_iter_factor_to_corr.columns = ['Corr_factor_minus', 'Corr_factor_plus']\n",
    "        dict_factors_autocorr[iter_factor] = pd.Series(df_iter_factor_to_corr.groupby('Date').apply(corr_by_date).mean(), index = ['autocorr'])\n",
    "    ### Results output:\n",
    "    df_factors_measures_stats = pd.concat(dict_factors_measures, axis = 0)\n",
    "    df_factors_autocorr =  pd.concat(dict_factors_autocorr, axis = 1).transpose()\n",
    "    df_factors_coeff = df_factors_measures_stats.loc[(All, 'mean'), list_months].reset_index(1, drop = True)\n",
    "    df_factors_coeff.columns = [('coeff_' + str(iter_column)) for iter_column in df_factors_coeff.columns]\n",
    "    df_factors_t_stat = df_factors_measures_stats.loc[(All, 't_stat'), list_months].reset_index(1, drop = True)\n",
    "    df_factors_t_stat.columns = [('t_' + str(iter_column)) for iter_column in df_factors_t_stat.columns]\n",
    "    df_factors_result = pd.concat([df_factors_autocorr, df_factors_coeff, df_factors_t_stat], axis = 1)    \n",
    "    return (df_factors_result, df_factors_measures_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM GENERAL MS EXCEL SOURCE\n",
    "\n",
    "def get_market_membership_from_excel(convert_to_daily = False):\n",
    "    ### Importing standard modules and date-special modules:    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    ### Reindexing function declaring:\n",
    "    def reindex_month_ends(iter_group):\n",
    "        iter_range = pd.date_range(iter_group.first_valid_index(), iter_group.last_valid_index(), freq = 'BM')\n",
    "        iter_result = iter_group.reindex(iter_range)\n",
    "        return iter_result    \n",
    "    ### Declaring local constants & variables:\n",
    "    path_msci = 'Data_Files/Source_Files/sample_universe.xlsx' ### Path for membership source     \n",
    "    tab_monthly = 'universe_joined'    \n",
    "    arr_markets_needed = ['DM', 'FM', 'EM']   \n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM'}\n",
    "    no_slice = slice(None)\n",
    "    ### Extracting universe data:\n",
    "    df_universe = pd.read_excel(io = path_msci, sheet_name = tab_monthly, skiprows = [0, 2], header = 0, parse_dates = True, \n",
    "                                na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    df_universe = df_universe.loc[no_slice, ['dates', 'region', 'ctry']]\n",
    "    df_universe.columns = ['Date', 'Market', 'Country']\n",
    "    df_universe.set_index(['Date', 'Country'], inplace = True)\n",
    "    ser_universe = df_universe.squeeze()\n",
    "    ser_universe.sort_index(level = [0, 1], inplace = True)\n",
    "    ser_universe.replace(dict_markets, inplace = True)\n",
    "    ser_market_membership = ser_universe[ser_universe.isin(arr_markets_needed)]\n",
    "    ### Reindexing to show absent monthes for future daily resampling: \n",
    "    if (convert_to_daily):\n",
    "        ser_market_membership = ser_market_membership.groupby('Country').apply(lambda iter_group: reindex_month_ends(iter_group.droplevel(1)))\n",
    "        ser_market_membership.index.names = ['Country', 'Date']\n",
    "        ser_market_membership = ser_market_membership.swaplevel()\n",
    "        ser_market_membership = ser_market_membership.reset_index('Country').groupby('Country').resample('B').ffill().drop('Country', axis = 1).squeeze()\n",
    "        ser_market_membership = ser_market_membership.swaplevel().sort_index(level = ['Country', 'Date'])\n",
    "        \n",
    "    return ser_market_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EER: LONG-TERM FACTOR CONSTRUCTION\n",
    "\n",
    "### NEER long factor preparing:\n",
    "ser_neer_data = ser_eer_data.loc['NEER', ['BB', 'JPM'], All, All].droplevel('Type')\n",
    "df_neer_long = ser_neer_data.groupby(['Source', 'Country']).transform(lambda ser_group: ser_group.rolling(int_numer_ma_win, int_numer_ma_win // 2).mean() / \\\n",
    "                                                                                    ser_group.rolling(int_denom_ma_win, int_denom_ma_win // 2).mean()).unstack('Source')\n",
    "### NEER long factor adding ISON index:\n",
    "df_neer_long = df_neer_long.join(ser_ison_membership.dropna(), how = 'inner').set_index('Market', append = True).sort_index(level = ['Date', 'Country', 'Market'])\n",
    "### NEER long factor standartizing for each source:\n",
    "df_neer_long = multi_factor_standartize(df_neer_long, list_truncate, within_market = True).droplevel('Market')\n",
    "### NEER long factors comparing:\n",
    "df_neer_long.groupby('Date').corr().loc[(All, 'JPM'), 'BB'].droplevel(1).plot(figsize = (15, 5), title = 'NEER : JPM vs BB Long-term Factors x-sect correlation') \n",
    "plt.show()\n",
    "### Combining to combined NEER long factor:\n",
    "ser_neer_long = df_neer_long['JPM'].combine_first(df_neer_long['BB'])\n",
    "ser_neer_long.name = 'NEER'\n",
    "### REER long factor preparing:\n",
    "ser_reer_data = ser_eer_data.loc['REER', ['CTG', 'JPM'], All, All].droplevel('Type')\n",
    "df_reer_long = ser_reer_data.groupby(['Source', 'Country']).transform(lambda ser_group: ser_group.rolling(int_numer_ma_win, int_numer_ma_win // 2).mean() / \\\n",
    "                                                                                    ser_group.rolling(int_denom_ma_win, int_denom_ma_win // 2).mean()).unstack('Source')\n",
    "### REER long factor adding ISON index:\n",
    "df_reer_long = df_reer_long.join(ser_ison_membership.dropna(), how = 'inner').set_index('Market', append = True).sort_index(level = ['Date', 'Country', 'Market'])\n",
    "### REER long factor standartizing for each source:\n",
    "df_reer_long = multi_factor_standartize(df_reer_long, list_truncate, within_market = True).droplevel('Market')\n",
    "### REER long factors conmparing:\n",
    "df_reer_long.groupby('Date').corr().loc[(All, 'JPM'), 'CTG'].droplevel(1).plot(figsize = (15, 5), title = 'REER : JPM vs CTG Long-term Factors x-sect correlation') \n",
    "plt.show()\n",
    "### Combining to combined REER long factor:\n",
    "ser_reer_long = df_reer_long['JPM'].combine_first(df_reer_long['CTG'])\n",
    "ser_reer_long.name = 'REER'\n",
    "### EER JPM only long factors comparing:\n",
    "df_jpm_long = pd.concat([df_neer_long['JPM'], df_reer_long['JPM']], join = 'outer', axis = 1)\n",
    "df_jpm_long.columns = ['NEER', 'REER']\n",
    "df_jpm_long.groupby('Date').corr().loc[(All, 'REER'), 'NEER'].droplevel(1).plot(figsize = (15, 5), title = 'REER vs NEER JPM only Long-term Factors x-sect correlation') \n",
    "plt.show()\n",
    "### EER combined long factors comparing:\n",
    "df_factor_long = pd.concat([ser_neer_long, ser_reer_long], join = 'outer', axis = 1)\n",
    "df_factor_long.groupby('Date').corr().loc[(All, 'REER'), 'NEER'].droplevel(1).plot(figsize = (15, 5), title = 'REER vs NEER combined Long-term Factors x-sect correlation') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EER: SHORT-TERM FACTOR CONSTRUCTION\n",
    "\n",
    "### Forward filling the gaps:\n",
    "ser_eer_filled = ser_eer_data.groupby(['Type', 'Source', 'Country']).ffill()\n",
    "### NEER short factor preparing:\n",
    "ser_neer_data = ser_eer_filled.loc['NEER', ['BB', 'JPM'], All, All].droplevel('Type')\n",
    "ser_neer_short = ser_neer_data.groupby(['Source', 'Country']).diff(int_short_diff)\n",
    "ser_neer_shifted = ser_neer_data.groupby(['Source', 'Country']).shift(int_short_diff)\n",
    "ser_neer_short = ser_neer_short / ser_neer_shifted\n",
    "df_neer_short = ser_neer_short.unstack('Source')\n",
    "### NEER short factor adding ISON index:\n",
    "df_neer_short = df_neer_short.join(ser_ison_membership.dropna(), how = 'inner').set_index('Market', append = True).sort_index(level = ['Date', 'Country', 'Market'])\n",
    "### NEER short factor standartizing for each source:\n",
    "df_neer_short = multi_factor_standartize(df_neer_short, list_truncate, within_market = True).droplevel('Market')\n",
    "### NEER short factor comparing:\n",
    "df_neer_short.groupby('Date').corr().loc[(All, 'JPM'), 'BB'].droplevel(1).plot(figsize = (15, 5), title = 'NEER : JPM vs BB Short-term Factors x-sect correlation') \n",
    "plt.show()\n",
    "### Combining to combined NEER short factor:\n",
    "ser_neer_short = df_neer_short['JPM'].combine_first(df_neer_short['BB'])\n",
    "ser_neer_short.name = 'NEER'\n",
    "### REER short factor preparing:\n",
    "ser_reer_data = ser_eer_filled.loc['REER', ['CTG', 'JPM'], All, All].droplevel('Type')\n",
    "ser_reer_short = ser_reer_data.groupby(['Source', 'Country']).diff(int_short_diff)\n",
    "ser_reer_shifted = ser_reer_data.groupby(['Source', 'Country']).shift(int_short_diff)\n",
    "ser_reer_short = ser_reer_short / ser_reer_shifted\n",
    "df_reer_short = ser_reer_short.unstack('Source')\n",
    "### REER short factor adding ISON index:\n",
    "df_reer_short = df_reer_short.join(ser_ison_membership.dropna(), how = 'inner').set_index('Market', append = True).sort_index(level = ['Date', 'Country', 'Market'])\n",
    "### REER short factor standartizing for each source:\n",
    "df_reer_short = multi_factor_standartize(df_reer_short, list_truncate, within_market = True).droplevel('Market')\n",
    "### REER short factors comparing:\n",
    "df_reer_short.groupby('Date').corr().loc[(All, 'JPM'), 'CTG'].droplevel(1).plot(figsize = (15, 5), title = 'REER : JPM vs CTG Short-term Factors x-sect correlation') \n",
    "plt.show()\n",
    "### Combining to combined REER short factor:\n",
    "ser_reer_short = df_reer_short['JPM'].combine_first(df_reer_short['CTG'])\n",
    "ser_reer_short.name = 'REER'\n",
    "### EER JPM only short factors comparing:\n",
    "df_jpm_short = pd.concat([df_neer_short['JPM'], df_reer_short['JPM']], join = 'outer', axis = 1)\n",
    "df_jpm_short.columns = ['NEER', 'REER']\n",
    "df_jpm_short.groupby('Date').corr().loc[(All, 'REER'), 'NEER'].droplevel(1).plot(figsize = (15, 5), title = 'REER vs NEER JPM only Short-term Factors x-sect correlation') \n",
    "plt.show()\n",
    "### EER combined short factors comparing:\n",
    "df_factor_short = pd.concat([ser_neer_short, ser_reer_short], join = 'outer', axis = 1)\n",
    "df_factor_short.groupby('Date').corr().loc[(All, 'REER'), 'NEER'].droplevel(1).plot(figsize = (15, 5), \n",
    "                                                                                    title = 'REER vs NEER combined Short-term Factors x-sect correlation') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FACTORS FOR EFFICACY MEASURES PREPARING (NO TRANSITIONAL RESULTS USAGE)\n",
    "\n",
    "### NEER long factor preparing:\n",
    "ser_neer_data = ser_eer_data.loc['NEER', ['BB', 'JPM'], All, All].droplevel('Type')\n",
    "df_neer_long = ser_neer_data.groupby(['Source', 'Country']).transform(lambda ser_group: ser_group.rolling(int_numer_ma_win, int_numer_ma_win // 2).mean() / \\\n",
    "                                                                                    ser_group.rolling(int_denom_ma_win, int_denom_ma_win // 2).mean()).unstack('Source')\n",
    "### NEER long factor adding ISON index:\n",
    "df_neer_long = df_neer_long.join(ser_ison_membership, how = 'left').set_index('Market', append = True)\n",
    "### Combining to combined NEER long factor:\n",
    "ser_neer_long = df_neer_long['JPM'].combine_first(df_neer_long['BB'])\n",
    "ser_neer_long.name = 'NEER long'\n",
    "### REER long factor preparing:\n",
    "ser_reer_data = ser_eer_data.loc['REER', ['CTG', 'JPM'], All, All].droplevel('Type')\n",
    "df_reer_long = ser_reer_data.groupby(['Source', 'Country']).transform(lambda ser_group: ser_group.rolling(int_numer_ma_win, int_numer_ma_win // 2).mean() / \\\n",
    "                                                                                    ser_group.rolling(int_denom_ma_win, int_denom_ma_win // 2).mean()).unstack('Source')\n",
    "### REER long factor adding ISON index:\n",
    "df_reer_long = df_reer_long.join(ser_ison_membership, how = 'left').set_index('Market', append = True)\n",
    "### Combining to combined REER long factor:\n",
    "ser_reer_long = df_reer_long['JPM'].combine_first(df_reer_long['CTG'])\n",
    "ser_reer_long.name = 'REER long'\n",
    "### EER forward filling the gaps for the short factor:\n",
    "ser_eer_filled = ser_eer_data.groupby(['Type', 'Source', 'Country']).ffill()\n",
    "### NEER short factor preparing:\n",
    "ser_neer_data = ser_eer_filled.loc['NEER', ['BB', 'JPM'], All, All].droplevel('Type')\n",
    "ser_neer_short = ser_neer_data.groupby(['Source', 'Country']).diff(int_short_diff)\n",
    "ser_neer_shifted = ser_neer_data.groupby(['Source', 'Country']).shift(int_short_diff)\n",
    "ser_neer_short = ser_neer_short / ser_neer_shifted\n",
    "df_neer_short = ser_neer_short.unstack('Source')\n",
    "### NEER short factor adding ISON index:\n",
    "df_neer_short = df_neer_short.join(ser_ison_membership, how = 'left').set_index('Market', append = True)\n",
    "### Combining to combined NEER short factor:\n",
    "ser_neer_short = df_neer_short['JPM'].combine_first(df_neer_short['BB'])\n",
    "ser_neer_short.name = 'NEER short'\n",
    "### REER short factor preparing:\n",
    "ser_reer_data = ser_eer_filled.loc['REER', ['CTG', 'JPM'], All, All].droplevel('Type')\n",
    "ser_reer_short = ser_reer_data.groupby(['Source', 'Country']).diff(int_short_diff)\n",
    "ser_reer_shifted = ser_reer_data.groupby(['Source', 'Country']).shift(int_short_diff)\n",
    "ser_reer_short = ser_reer_short / ser_reer_shifted\n",
    "df_reer_short = ser_reer_short.unstack('Source')\n",
    "### REER short factor adding ISON index:\n",
    "df_reer_short = df_reer_short.join(ser_ison_membership, how = 'left').set_index('Market', append = True)\n",
    "### Combining to combined REER short factor:\n",
    "ser_reer_short = df_reer_short['JPM'].combine_first(df_reer_short['CTG'])\n",
    "ser_reer_short.name = 'REER short'\n",
    "### FX forward filling the gaps:\n",
    "ser_fx_filled = ser_fx_data.groupby('Country').ffill()\n",
    "### FX short factor constructing:\n",
    "ser_fx_short = ser_fx_filled.groupby('Country').diff(int_short_diff)\n",
    "ser_fx_shifted = ser_fx_filled.groupby('Country').shift(int_short_diff)\n",
    "ser_fx_short = ser_fx_short / ser_fx_shifted\n",
    "### FX short factor adding ISON index:\n",
    "ser_fx_short = ser_fx_short.to_frame().join(ser_ison_membership.dropna(), how = 'left').set_index('Market', append = True).squeeze()\n",
    "ser_fx_short.name = 'FX short'\n",
    "### Factor datasets joining:\n",
    "df_factors = pd.concat([ser_neer_long, ser_reer_long, ser_neer_short, ser_reer_short, ser_fx_short], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MEASURES CALCULATIONS PREPARING (NO TRANSITIONAL RESULTS USAGE)\n",
    "\n",
    "### Datasets loading\n",
    "ser_mcaps = pd.read_hdf(path_market_cap, key_market_cap)\n",
    "ser_returns = pd.read_hdf(path_return, key_return)\n",
    "### Parameters:\n",
    "list_region = ['DM', 'EM', 'FM'] # ['DM'] # ['DM', 'EM'] ### Regions list\n",
    "list_countries_to_exclude = ['VE'] # ['GR', 'UA', 'VE'] ### Countries not to play the game\n",
    "bool_within_market = True # Standartization way\n",
    "str_date_start = '1995-01-01' # Start date to filter returns, market caps and factors\n",
    "idx_date_range = pd.date_range(str_date_start, periods = 999, freq = 'BM')\n",
    "### Measures to calculate:\n",
    "list_measures = ['ic_pearson', 'ic_spearman', 'fmb_eqw', 'fmb_weighted', 'fmb_std_eqw', 'fmb_std_weighted', 'clp']\n",
    "### Measures preparing parameters:\n",
    "list_truncate = [2.5, 2.0]\n",
    "list_back_period = [99, 10, 5]\n",
    "### Using transitional results:\n",
    "path_transitional_results = 'Data_Files/Test_Files/Factor_transitionals.h5'\n",
    "key_factors = 'factors'\n",
    "key_return = 'returns'\n",
    "key_mcap = 'mcaps'\n",
    "### Returns shifting:\n",
    "ser_returns_prepared = ser_returns.groupby('Country').shift(periods = -1)\n",
    "### Region and date clearing:\n",
    "ser_returns_prepared = ser_returns_prepared.loc[idx_date_range, All, list_region]\n",
    "ser_mcaps_prepared = ser_mcaps.loc[idx_date_range, All, list_region]\n",
    "df_factors_prepared = df_factors.loc[(idx_date_range, All, list_region), All]\n",
    "### Countries filtering:\n",
    "ser_returns_prepared = ser_returns_prepared.drop(list_countries_to_exclude, level = 'Country')\n",
    "ser_mcaps_prepared = ser_mcaps_prepared.drop(list_countries_to_exclude, level = 'Country')\n",
    "df_factors_prepared = df_factors_prepared.drop(list_countries_to_exclude, level = 'Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MULTI FACTORS STANDARTIZING (NO TRANSITIONAL RESULTS USAGE)\n",
    "\n",
    "df_factors_std = multi_factor_standartize(df_factors_prepared, list_truncate, within_market = bool_within_market)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STANDARTIZED FACTORS FILTERING (NO TRANSITIONAL RESULTS USAGE)\n",
    "\n",
    "list_region_filter = ['DM', 'EM', 'FM'] #['DM'] ### Regions filter\n",
    "df_factors_filtered = df_factors_std.loc[(All, All, list_region_filter), :]\n",
    "ser_returns_filtered = ser_returns_prepared.loc[All, All, list_region_filter]\n",
    "ser_mcaps_filtered = ser_mcaps_prepared.loc[All, All, list_region_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVING TRANSITIONAL RESULTS\n",
    "\n",
    "df_factors_filtered.to_hdf(path_transitional_results, key_factors, mode = 'w')\n",
    "ser_returns_filtered.to_hdf(path_transitional_results, key_return)\n",
    "ser_mcaps_filtered.to_hdf(path_transitional_results, key_mcap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MEASURES CALCULATIONS PREPARING (TRANSITIONAL RESULTS USAGE)\n",
    "\n",
    "### Parameters:\n",
    "list_region = ['DM', 'EM', 'FM'] # ['DM'] # ['DM', 'EM'] ### Regions list\n",
    "list_countries_to_exclude = ['VE'] # ['GR', 'UA', 'VE'] ### Countries not to play the game\n",
    "bool_within_market = True # Standartization way\n",
    "str_date_start = '1995-01-01' # Start date to filter returns, market caps and factors\n",
    "idx_date_range = pd.date_range(str_date_start, periods = 999, freq = 'BM')\n",
    "### Measures to calculate:\n",
    "list_measures = ['ic_pearson', 'ic_spearman', 'fmb_eqw', 'fmb_weighted', 'fmb_std_eqw', 'fmb_std_weighted', 'clp']\n",
    "### Measures preparing parameters:\n",
    "list_truncate = [2.5, 2.0]\n",
    "list_back_periods = [99, 10, 5]\n",
    "int_horizon = 12\n",
    "list_region_xmo = ['DM']\n",
    "### Using transitional results:\n",
    "path_transitional_results = 'Data_Files/Test_Files/Factor_transitionals.h5'\n",
    "key_factors = 'factors'\n",
    "key_return = 'returns'\n",
    "key_mcap = 'mcaps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATASETS LOADING & ADDITIONAL FILTERING\n",
    "\n",
    "### Datasets loading:\n",
    "df_factors = pd.read_hdf(path_transitional_results, key_factors)\n",
    "ser_returns = pd.read_hdf(path_transitional_results, key_return)\n",
    "ser_mcaps = pd.read_hdf(path_transitional_results, key_mcap)\n",
    "### Datasets filtering:\n",
    "list_region_filter = ['DM', 'EM', 'FM'] #['DM'] ### Regions filter\n",
    "df_factors_filtered = df_factors.loc[(All, All, list_region_filter), :]\n",
    "ser_returns_filtered = ser_returns.loc[All, All, list_region_filter]\n",
    "ser_mcaps_filtered = ser_mcaps.loc[All, All, list_region_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMPARING WITH MATLAB: PERFORMANCE TEST\n",
    "\n",
    "### Multiple measures calculating:\n",
    "dict_s_f_m_m = {}\n",
    "for iter_factor in df_factors_filtered.columns:\n",
    "    dict_s_f_m_m[iter_factor] = single_factor_multiple_efficacy_measures(df_factors_filtered[iter_factor], ser_returns_filtered, ser_mcaps_filtered, \n",
    "                                 list_measures, return_shift = 0, arr_truncate = list_truncate)\n",
    "### Measure stats calculating:\n",
    "dict_measure_stats = {}\n",
    "for iter_factor in dict_s_f_m_m:\n",
    "    dict_measure_stats[iter_factor] = measure_stats(dict_s_f_m_m[iter_factor], list_back_periods)\n",
    "### CLP stats calculating:\n",
    "dict_clp_stats = {}\n",
    "dict_clp_bias = {}\n",
    "for iter_factor in df_factors_filtered.columns:\n",
    "    (dict_clp_stats[iter_factor], dict_clp_bias[iter_factor]) = \\\n",
    "    special_clp_stats(df_factors_filtered[iter_factor], ser_returns_filtered, ser_mcaps_filtered, return_shift = 0)\n",
    "### Multiple factors calculating:\n",
    "dict_m_f_s_m_result = {}\n",
    "dict_m_f_s_m_stats = {}\n",
    "for iter_measure in list_measures:\n",
    "    (dict_m_f_s_m_result[iter_measure], dict_m_f_s_m_stats[iter_measure]) = multiple_factor_single_efficacy_measure_stats(df_factors_filtered, ser_returns_filtered, \n",
    "                                                                            ser_mcaps_filtered, iter_measure, list_back_periods[0], int_horizon)\n",
    "    print(iter_measure)   \n",
    "### Multiple factors calculating (xmo):\n",
    "dict_m_f_s_m_xmo_result = {}\n",
    "dict_m_f_s_m_xmo_stats = {}\n",
    "for iter_measure in list_measures:\n",
    "    (dict_m_f_s_m_xmo_result[iter_measure], dict_m_f_s_m_xmo_stats[iter_measure]) = multiple_factor_single_efficacy_measure_stats(df_factors_filtered, \n",
    "                                                                                    ser_returns_filtered, ser_mcaps_filtered, iter_measure, list_back_periods[0], \n",
    "                                                                                    int_horizon, list_region_xmo)\n",
    "    print(iter_measure)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVING TO EXCEL\n",
    "\n",
    "### Source datasets for efficacy measures saving:\n",
    "str_path_efficacy_source_export = 'Data_Files/Test_Files/Efficacy_Source.xlsx'\n",
    "with pd.ExcelWriter(str_path_efficacy_source_export) as excel_writer_source:\n",
    "    df_factors_filtered.to_excel(excel_writer_source, sheet_name = 'Factors', merge_cells = False)\n",
    "    ser_returns_filtered.to_excel(excel_writer_source, sheet_name = 'Returns shifted', merge_cells = False)\n",
    "    ser_mcaps_filtered.to_excel(excel_writer_source, sheet_name = 'Market Caps', merge_cells = False) \n",
    "### Multiple measures for single factor saving:\n",
    "str_path_s_f_m_m_export = 'Data_Files/Test_Files/Multiple_Measures.xlsx'\n",
    "with pd.ExcelWriter(str_path_s_f_m_m_export) as excel_writer_m_m:\n",
    "    for iter_factor in dict_s_f_m_m:    \n",
    "        dict_s_f_m_m[iter_factor].to_excel(excel_writer_m_m, sheet_name = iter_factor, merge_cells = False)\n",
    "### Stats for multiple measures for single factor saving:\n",
    "str_path_measure_stats_export = 'Data_Files/Test_Files/Multiple_Measures_Stats.xlsx'\n",
    "with pd.ExcelWriter(str_path_measure_stats_export) as excel_writer_stats:\n",
    "    for iter_factor in dict_s_f_m_m:    \n",
    "        dict_measure_stats[iter_factor].transpose().to_excel(excel_writer_stats, sheet_name = iter_factor, merge_cells = False)       \n",
    "### CLP stats and bias:\n",
    "str_path_clp_stats_export = 'Data_Files/Test_Files/CLP_Stats.xlsx'\n",
    "with pd.ExcelWriter(str_path_clp_stats_export) as excel_writer_CLP_stats:\n",
    "    for iter_factor in dict_clp_stats:    \n",
    "        dict_clp_stats[iter_factor].to_excel(excel_writer_CLP_stats, sheet_name = iter_factor, merge_cells = False)\n",
    "str_path_clp_bias_export = 'Data_Files/Test_Files/CLP_Bias.xlsx'\n",
    "with pd.ExcelWriter(str_path_clp_bias_export) as excel_writer_CLP_bias:\n",
    "    for iter_factor in dict_clp_bias:    \n",
    "        dict_clp_bias[iter_factor].to_excel(excel_writer_CLP_bias, sheet_name = iter_factor, merge_cells = False)\n",
    "### Single measure for multiple factors result saving:\n",
    "str_path_m_f_s_m_result_export = 'Data_Files/Test_Files/Single_Measure_Result.xlsx'\n",
    "with pd.ExcelWriter(str_path_m_f_s_m_result_export) as excel_writer_s_m_result:\n",
    "    for iter_measure in dict_m_f_s_m_result:    \n",
    "        dict_m_f_s_m_result[iter_measure].to_excel(excel_writer_s_m_result, sheet_name = iter_measure, merge_cells = False) \n",
    "### Single measure for multiple factors stats saving:        \n",
    "str_path_m_f_s_m_stats_export = 'Data_Files/Test_Files/Single_Measure_Stats.xlsx'\n",
    "with pd.ExcelWriter(str_path_m_f_s_m_stats_export) as excel_writer_s_m_stats:\n",
    "    for iter_measure in dict_m_f_s_m_stats:    \n",
    "        dict_m_f_s_m_stats[iter_measure].to_excel(excel_writer_s_m_stats, sheet_name = iter_measure, merge_cells = False)  \n",
    "### Single measure for multiple factors result (xmo) saving:\n",
    "str_path_m_f_s_m_xmo_result_export = 'Data_Files/Test_Files/Single_Measure_xmo_Result.xlsx'\n",
    "with pd.ExcelWriter(str_path_m_f_s_m_xmo_result_export) as excel_writer_s_m_xmo_result:\n",
    "    for iter_measure in dict_m_f_s_m_xmo_result:    \n",
    "        dict_m_f_s_m_xmo_result[iter_measure].to_excel(excel_writer_s_m_xmo_result, sheet_name = iter_measure, merge_cells = False) \n",
    "### Single measure for multiple factors stats (xmo) saving:        \n",
    "str_path_m_f_s_m_xmo_stats_export = 'Data_Files/Test_Files/Single_Measure_xmo_Stats.xlsx'\n",
    "with pd.ExcelWriter(str_path_m_f_s_m_xmo_stats_export) as excel_writer_s_m_xmo_stats:\n",
    "    for iter_measure in dict_m_f_s_m_xmo_stats:    \n",
    "        dict_m_f_s_m_xmo_stats[iter_measure].to_excel(excel_writer_s_m_xmo_stats, sheet_name = iter_measure, merge_cells = False)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "ser_ison_membership = get_market_membership_from_excel()\n",
    "ser_ison_membership.groupby('Country').apply(lambda iter_country: iter_country.last_valid_index()[0]).to_excel('Data_Files/Test_Files/ISON_Dates.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
