{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THA STANDARTIZE PLAYGROUND RESEARCH MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODULES IMPORT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INTERNAL PARAMETERS INITIALIZATION\n",
    "\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Bloomberg structured data extraction keys:\n",
    "str_path_bb_hdf = 'Data_Files/Source_Files/Bloomberg_prepared.h5'\n",
    "str_key_reer_sourced = 'bb_reer_sourced'\n",
    "### General daily-mone ranges parameters:\n",
    "str_source_date_start = '1992-01-01' ### Start date for source vectors\n",
    "str_measure_date_start = '1996-08-01' ### Start date for efficacy measures\n",
    "str_measure_date_end = '2020-08-31' ### End date for efficacy measures\n",
    "idx_source_date_range = pd.date_range(str_source_date_start, str_measure_date_end, freq = 'B') ### Range for source data filtering\n",
    "idx_factor_date_range = pd.date_range(str_source_date_start, str_measure_date_end, freq = 'BM') ### Range for factor data filtering\n",
    "idx_measure_date_range = pd.date_range(str_measure_date_start, str_measure_date_end, freq = 'BM') ### Range for measures calculation\n",
    "### Results saving:\n",
    "str_reer_tha_xlsx = 'Data_Files/Test_Files/research_mode_tha_reer_std.xlsx'\n",
    "str_reer_raw_xlsx = 'Data_Files/Test_Files/research_mode_tha_reer_raw.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL PARAMETERS INITIALIZATION\n",
    "\n",
    "### Common constants:\n",
    "All = slice(None)\n",
    "\n",
    "### Standartization parameters:\n",
    "list_truncate = [2.5, 2.0] ### Standartization boundaries\n",
    "bool_within_market = True ### Standartization option\n",
    "flo_tha_ratio = 0.9 ### THA progression ratio\n",
    "int_tha_length = 24 ### THA horizon length\n",
    "\n",
    "### ISON filtering options:\n",
    "list_ison = ['DM', 'EM', 'FM'] ### Regions filter to drop NaN region values\n",
    "list_filter = ['DM', 'EM', 'FM'] ### Additional regions filter\n",
    "list_countries_to_exclude = ['VE'] ### Countries not to play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING GEOMETRICAL WEIGHT\n",
    "\n",
    "def geom_weight_single(flo_ratio, flo_factor = 1, num_element = 0):\n",
    "    ### Results output:\n",
    "    return flo_factor * (flo_ratio ** num_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEAN MOMENTUM FUNCTION\n",
    "\n",
    "def rolling_cond_weighted_mean(ser_country_matrix, ser_full_source, int_mean_win, int_mean_min, list_weight = False, ser_full_cond = False):\n",
    "    ### Defining conditional average calculator:\n",
    "    def conditional_average(ser_source, list_weight, int_min_count = 0, ser_condition = False):\n",
    "        ### Weight setting\n",
    "        ser_weight = pd.Series(list_weight[ : len(ser_source.index)], ser_source.index)\n",
    "        ### If we have condition we should resort the weight array:\n",
    "        if not isinstance(ser_condition, bool):\n",
    "            ser_condition_sorted = pd.Series(ser_condition.sort_values().index, ser_condition.index)\n",
    "            ser_condition_sorted.name = 'Condition'\n",
    "            ser_weight = pd.concat([ser_weight, ser_condition_sorted], axis = 1).reset_index(drop = True).set_index('Condition').squeeze().sort_index()            \n",
    "        ### Results output:\n",
    "        return weighted_average(ser_source, ser_weight, int_min_count)    \n",
    "    ### Country saving:\n",
    "    str_country = ser_country_matrix.index[0][1]\n",
    "    ### Checking for country presence in source vector:\n",
    "    if (str_country in ser_full_source.index.get_level_values(1)):\n",
    "        ### Filtering country vector from source:\n",
    "        ser_country_source = ser_full_source.loc[All, str_country]\n",
    "        if not isinstance(ser_full_cond, bool):\n",
    "            ser_country_cond = ser_full_cond.loc[All, str_country]\n",
    "        ### Looping over matrix index dates:\n",
    "        for iter_bm_date in ser_country_matrix.index.get_level_values(0):\n",
    "            try:\n",
    "                ### Defining monthend date number in source country vector:\n",
    "                int_idx_num = ser_country_source.index.get_loc(iter_bm_date)\n",
    "                ### Creating vectors for numerator and denominator means calculation:\n",
    "                ser_rolled_source = ser_country_source.iloc[max((int_idx_num - int_mean_win + 1), 0) : int_idx_num + 1]\n",
    "                if not isinstance(ser_full_cond, bool):\n",
    "                    ser_rolled_cond = ser_country_cond.loc[ser_rolled_source.index]\n",
    "                else:\n",
    "                    ser_rolled_cond = False\n",
    "                ### Action for MatLab compatibility:\n",
    "                ser_rolled_source.iloc[0] = np.NaN\n",
    "                ### Simple mean calculation:\n",
    "                if isinstance(list_weight, bool):\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = weighted_average(ser_rolled_source, False, int_mean_min)\n",
    "                else:\n",
    "                    ### Weighted mean calculation:\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = conditional_average(ser_rolled_source, list_weight, int_mean_min, ser_rolled_cond)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    ### Resulting vector output:\n",
    "    return ser_country_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES\n",
    "\n",
    "def multistep_standartize(ser_data_source, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):  \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_data_source.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_data_source.dropna()\n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        if reuse_outliers:\n",
    "            ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        else:\n",
    "            ### Saving to result and excluding from further calculations truncated values:             \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):\n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_data_source.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR CROSS-SECTION\n",
    "\n",
    "def ison_standartize(ser_to_manage, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False, within_market = False):\n",
    "    ### Multi-step standartizing:\n",
    "    if (within_market):\n",
    "    ### Within market standartizing:\n",
    "        ser_result = ser_to_manage.groupby(by = 'Market', group_keys = False).apply(multistep_standartize, arr_truncate, ser_weight, \n",
    "                                                                                                  reuse_outliers, center_result, full_result)\n",
    "    else:\n",
    "    ### Full universe standartizing:\n",
    "        ser_result = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result)\n",
    "    ### Results output:\n",
    "    return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP THA STANDARTIZATION BY MARKET FOR CROSS-SECTION\n",
    "\n",
    "def tha_standartize(ser_to_manage, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):\n",
    "    ### Multi-step standartizing and recovering data (just for outliers winsorizing only):\n",
    "    (ser_reversed, list_mean, list_std) = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result = True)\n",
    "    for iter_num in range(len(arr_truncate))[::-1]:\n",
    "        ser_reversed = (ser_reversed * list_std[iter_num] + list_mean[iter_num])\n",
    "    ### By market demeaning for winsorized data vector:\n",
    "    ser_demeaned = ser_reversed.groupby('Market').apply(lambda ser_region: ser_region - ser_region.mean())\n",
    "    ### Winsorized & demeaned by market data vector standartizing:\n",
    "    ser_stand_z = multistep_standartize(ser_demeaned, arr_truncate, ser_weight, reuse_outliers, center_result, full_result = False)\n",
    "    ### Results output:\n",
    "    return ser_stand_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING UNIVERSAL AUTOCORRELATION FOR DATE-COUNTRY-UNIVERSE SERIES\n",
    "\n",
    "def vector_autocorr(ser_source, int_shift):\n",
    "    ### Defining adding full universe for each date:\n",
    "    def universe_reindex(iter_group, idx_universe):\n",
    "        df_iter_result = iter_group.unstack('Date').reindex(idx_universe).sort_index().stack('Date', dropna = False)\n",
    "        ### Results output:\n",
    "        return df_iter_result   \n",
    "    ### Defining adding full date range for each country and date index shifting:\n",
    "    def date_reindex(iter_group, idx_date_range, num_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-int_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result   \n",
    "    ### Defining by date correrlation function:\n",
    "    def corr_by_date(iter_group):\n",
    "        num_iter_corr = iter_group['Corr_factor_minus'].corr(iter_group['Corr_factor_plus'])\n",
    "        ### Results output:\n",
    "        return num_iter_corr       \n",
    "    ### Preparing expanded universe for autocorrelation performing:\n",
    "    idx_date_range = ser_source.index.get_level_values(0).unique()\n",
    "    idx_universe = ser_source.index.get_level_values(1).unique()\n",
    "    ser_source_full = ser_source.to_frame().reset_index('Market', drop = True).groupby('Date', group_keys = False).apply(universe_reindex, idx_universe)\\\n",
    "                                .swaplevel().squeeze()\n",
    "    ### Autocorrelation preparing:\n",
    "    ser_source_plus = ser_source_full.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[1 : ])\\\n",
    "                                     .sort_index(level = ['Date', 'Country'])\n",
    "    ser_source_minus = ser_source_full.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[: -1])\\\n",
    "                                      .sort_index(level = ['Date', 'Country'])\n",
    "    ### Artificial series combining for indexes synchronization:        \n",
    "    ser_source_plus_shifted = ser_source_plus.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, int_shift)\n",
    "    df_to_corr = pd.concat([ser_source_minus, ser_source_plus_shifted], axis = 1)\n",
    "    df_to_corr.columns = ['Corr_factor_minus', 'Corr_factor_plus']\n",
    "    ser_autocorr_vector = df_to_corr.groupby('Date').apply(corr_by_date).shift(int_shift)\n",
    "    ### Results output:\n",
    "    return ser_autocorr_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK\n",
    "\n",
    "def single_factor_standartize(ser_factor, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, within_market = False, \n",
    "                              flag_tha = False, flo_similarity = 5 * (10 ** (-8))):\n",
    "    ### Local constants:\n",
    "    dict_tha_pow = {}\n",
    "    dict_tha_pow['monthly'] = 1\n",
    "    dict_tha_pow['quarterly'] = 1 / 3\n",
    "    dict_tha_pow['annual'] = 1 / 12\n",
    "    ### Weights preparing:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "        ser_weight.name = 'Weight'\n",
    "    ### Multi-step standartizing:        \n",
    "    df_factor = ser_factor.to_frame().join(ser_weight, how = 'left')\n",
    "    df_factor.columns = ['Factor', 'Weight']\n",
    "    ### Time-horizon adjusted standartization:\n",
    "    if (flag_tha):\n",
    "        ### Demeaned by region z-scored vector calculating:       \n",
    "        ser_stand_z = df_factor.groupby('Date', group_keys = False)\\\n",
    "                               .apply(lambda iter_df: tha_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False))\n",
    "        ### Results output:\n",
    "        ser_stand_z.name = ser_factor.name\n",
    "        ### Autocorrelation vector calculating:\n",
    "        ser_autocorr_vector = ser_stand_z.groupby('Market').apply(vector_autocorr, 1)\n",
    "        ser_autocorr_vector.name = 'Autocorr'\n",
    "        if (flag_tha == 'monthly'):\n",
    "            ser_autocorr_cum_mean = ser_autocorr_vector.groupby('Market', group_keys = False).expanding().mean()\n",
    "        else:\n",
    "            ser_autocorr_cum_mean = ser_autocorr_vector.loc[np.abs(ser_autocorr_vector - 1) > flo_similarity].groupby('Market', group_keys = False).expanding().mean()\n",
    "        ### THA-coeficcient calculating:\n",
    "        ser_tha_coeff = ser_autocorr_cum_mean.transform(lambda iter_mean: max(iter_mean, 0.0) ** dict_tha_pow[flag_tha])\n",
    "        ser_tha_coeff = ser_tha_coeff.transform(lambda iter_mean: \n",
    "                                                sum(map(lambda iter_num: geom_weight_single(flo_tha_ratio * iter_mean, 1, iter_num), range(int_tha_length))) / 2)\n",
    "        ser_tha_coeff = ser_tha_coeff.swaplevel()\n",
    "        ser_tha_coeff = ser_tha_coeff.unstack('Market').reindex(ser_stand_z.index.levels[0]).stack('Market', dropna = False).sort_index(level = ['Date', 'Market'])        \n",
    "        ### THA-adjusted z-score calculating:\n",
    "#        ser_stand_s = (ser_stand_z * ser_tha_coeff)\n",
    "        ### Artifical filling values for first date of region appearance (not to loose observations):\n",
    "        ser_stand_s = (ser_stand_z * ser_tha_coeff.fillna(2.0)) ### Result of geometric progression sum with assumption that correlation is equal to 0.55\n",
    "        ser_stand_s = ser_stand_s[ser_stand_s.index.dropna()].reorder_levels(['Date', 'Country', 'Market']).sort_index()\n",
    "        ### Standart deviation for THA-adjusted z-score calculating:\n",
    "        ser_region_std = ser_stand_s.groupby(['Date', 'Market']).std()\n",
    "        ser_universe_std = ser_stand_s.groupby(['Date']).std()\n",
    "        ser_universe_std = pd.concat([ser_universe_std], keys = ['Overall'], names = ['Market']).swaplevel()\n",
    "        ser_std = pd.concat([ser_region_std, ser_universe_std], axis = 0).sort_index()\n",
    "        ### Results output:\n",
    "        return (ser_stand_s, ser_stand_z, ser_autocorr_vector, ser_tha_coeff, ser_std)\n",
    "    ### Simple standartization:    \n",
    "    else:    \n",
    "        ser_result = df_factor.groupby('Date', group_keys = False)\\\n",
    "                     .apply(lambda iter_df: ison_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False, within_market))\n",
    "        ### Results output:\n",
    "        ser_result.name = ser_factor.name\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end, bool_daily = False, int_backfill_months = 0):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(io = str_path_universe, sheet_name = 0, header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index() \n",
    "    ### Expanding membership for primary regions members by backfilling:\n",
    "    if int_backfill_months:\n",
    "        ### List of regions:\n",
    "        list_region = list(ser_res_universe.dropna().unique())\n",
    "        ### Initialising of collection of series with backfilled data for each region:\n",
    "        list_ison_backfill = []\n",
    "        ### Regions looping:\n",
    "        for iter_region in list_region:\n",
    "            ### Defining start of region date:\n",
    "            date_first_valid = ser_res_universe.loc[ser_res_universe == iter_region].first_valid_index()[0]\n",
    "            ### Creating dates index to backfilling:\n",
    "            idx_date_backfill = pd.date_range(end = date_first_valid, periods = int_backfill_months + 1, freq = 'BM')[: -1]\n",
    "            ### Creating primary countries index to backfilling:            \n",
    "            idx_region_backfill = ser_res_universe.loc[ser_res_universe == iter_region].loc[date_first_valid, All].index.get_level_values('Country')\n",
    "            ### Creating full index:\n",
    "            idx_ison_backfill = pd.MultiIndex.from_product([idx_date_backfill, idx_region_backfill])\n",
    "            ### Series with backfilled data:\n",
    "            list_ison_backfill.append(pd.Series(iter_region, index = idx_ison_backfill))\n",
    "        ### Combination of backfilled series and original ISON data:    \n",
    "        ser_res_universe = ser_res_universe.combine_first(pd.concat(list_ison_backfill, axis = 0)).sort_index()  \n",
    "        ser_res_universe.index.names = ['Date', 'Country']\n",
    "    ### Converting to daily frequency:\n",
    "    if bool_daily:\n",
    "        ser_res_universe = ser_res_universe.reset_index('Country').groupby('Country').resample('B').ffill()['Market'].swaplevel().sort_index()    \n",
    "    ### Results output:\n",
    "    ser_res_universe.name = 'Market'\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA LOADING AND CONVERTING\n",
    "\n",
    "ser_reer = pd.read_hdf(str_path_bb_hdf, key = str_key_reer_sourced) ### Real Effective Exchange Rate to use as a source in the Short-Term factor\n",
    "int_eer_fill_limit = 66\n",
    "ser_reer_source = ser_reer.droplevel('Source').unstack('Country').reindex(idx_source_date_range).ffill(limit = int_eer_fill_limit).stack('Country').sort_index()\n",
    "ser_reer_source.index.names = ['Date', 'Country']\n",
    "\n",
    "ser_ison = ison_membership_converting(str_path_universe, datetime.strptime(str_measure_date_end, '%Y-%m-%d')) ### ISON universe, end-of-business-months vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: STANDALONE REER FACTOR CALCULATING\n",
    "\n",
    "### Factor matrix creating:\n",
    "ser_reer_factor = pd.Series(index = pd.MultiIndex.from_product([idx_factor_date_range, ser_ison.index.get_level_values(1).unique()])).sort_index()\n",
    "ser_reer_factor.index.set_names(['Date', 'Country'], inplace = True)                \n",
    "### Source performing:\n",
    "ser_reer_delta = ser_reer_source.groupby('Country').diff() / ser_reer_source.groupby('Country').shift()   \n",
    "ser_reer_delta = ser_reer_delta.replace([np.inf, -np.inf], np.NaN)\n",
    "### Momentum parameters:\n",
    "int_mom_hl = 520 ### Without rounding here\n",
    "int_mom_win = 1300\n",
    "int_mom_min = 520\n",
    "### Weights array:\n",
    "list_weight = list(map(lambda iter_num: exp_weight_single(int_mom_hl, iter_num), range(int_mom_win)))[::-1]\n",
    "### Momentum factor calculation:\n",
    "ser_reer_factor = ser_reer_factor.groupby('Country').transform(rolling_cond_weighted_mean, ser_reer_delta, int_mom_win, int_mom_min, list_weight, False)\n",
    "### Factor ISONing:\n",
    "ser_reer_factor = ser_reer_factor.to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "ser_reer_factor.name = 'Factor'               \n",
    "### Regions clearing:\n",
    "ser_reer_factor = ser_reer_factor.loc[idx_factor_date_range, All, list_ison]\n",
    "### Countries filtering:\n",
    "ser_reer_factor = ser_reer_factor.drop(list_countries_to_exclude, level = 'Country') \n",
    "### Standalone factor standartizing:\n",
    "#ser_reer_factor_std = -single_factor_standartize(ser_reer_factor, list_truncate, within_market = bool_within_market)\n",
    "#ser_reer_factor_std.name = 'Factor' \n",
    "#tup_reer_factor_tha = single_factor_standartize(ser_reer_factor_std, list_truncate, within_market = bool_within_market, flag_tha = 'monthly')\n",
    "tup_reer_factor_tha = single_factor_standartize(-ser_reer_factor, list_truncate, within_market = bool_within_market, flag_tha = 'monthly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        Country  Market  Market\n",
       "1994-01-31  AT       DM      DM        0.000056\n",
       "            AU       DM      DM             NaN\n",
       "            BE       DM      DM       -0.000009\n",
       "            CA       DM      DM             NaN\n",
       "            CH       DM      DM             NaN\n",
       "                                         ...   \n",
       "2020-08-31  UG       FM      FM        0.000069\n",
       "            US       DM      DM        0.000025\n",
       "            VN       FM      FM        0.000047\n",
       "            ZA       EM      EM       -0.000126\n",
       "            ZM       FM      FM       -0.000382\n",
       "Name: Factor, Length: 19621, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "#ser_reer_tha = tup_reer_factor_tha[0]\n",
    "#ser_reer_tha\n",
    "#ser_reer_tha.to_excel(str_reer_tha_xlsx, merge_cells = False)\n",
    "ser_reer_factor.to_excel(str_reer_raw_xlsx, merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
