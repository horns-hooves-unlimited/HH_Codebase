{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EER FACTORS REVISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as ss\n",
    "import math     \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "    \n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "### Universe path:\n",
    "str_path_universe = 'Data_Files/Source_Files/acadian_universe.xlsx'\n",
    "### Bloomberg structured data extraction parameters:\n",
    "str_path_bb_hdf = 'Data_Files/Source_Files/Bloomberg_prepared.h5'\n",
    "str_key_ret_daily = 'bb_ret_daily'\n",
    "str_key_ret_monthly = 'bb_ret_monthly'\n",
    "str_key_mmr = 'bb_mmr'\n",
    "str_key_fx_country = 'bb_fx_country'\n",
    "str_key_fx_currency = 'bb_fx_currency'\n",
    "str_key_mcap = 'bb_mcap'\n",
    "str_key_reer = 'bb_reer'\n",
    "str_key_neer = 'bb_neer'\n",
    "str_key_reer_sourced = 'bb_reer_sourced'\n",
    "str_key_neer_sourced = 'bb_neer_sourced'\n",
    "str_key_xcra = 'bb_xcra'\n",
    "### NEER usage scheme:\n",
    "bool_NEER_raw = False\n",
    "### Standartization parameters:\n",
    "list_truncate = [2.5, 2.0] # Standartization boundaries\n",
    "bool_within_market = True # Standartization way\n",
    "### Factors parameters:\n",
    "str_date_start = '1996-08-01' # Start date for efficacy measures\n",
    "str_date_end = '2020-08-31' # End date for efficacy measures\n",
    "#str_date_end = '2020-06-30' # End date for efficacy measures\n",
    "idx_date_range = pd.date_range(str_date_start, str_date_end, freq = 'BM')\n",
    "list_ison = ['DM', 'EM', 'FM']\n",
    "list_filter = ['DM', 'EM', 'FM']\n",
    "list_countries_to_exclude = ['VE'] # Countries not to play the game\n",
    "flo_returns_similarity = 0.0025 # Selecting countries with currencies bound to the USD\n",
    "flo_returns_completeness = 1 / 3\n",
    "int_concept_lag = 3 ### Lag in months for GDP like concepts, months\n",
    "int_concept_divider = 1000 # Divider to equalize concepts and GDP scales\n",
    "int_concept_min = 0.0 # Minimal value to compare with log(1 + EXPORT/GDP)\n",
    "int_concept_max = 0.3 # Maximal value to compare with log(1 + EXPORT/GDP)\n",
    "int_eer_fill_limit = 260 * 50 # Days for forward fill NEER and REER inside country vectors ### For product version we need value = 66, days\n",
    "int_factor_addendum = 2.5 # list_truncate[0] # Factor scaler\n",
    "int_mom_length = 5 # Years of momentum vector\n",
    "int_mom_min_win = 260 // 4 # Days\n",
    "dict_mom_hl = {} # Half-life period for momentum factor, months:\n",
    "dict_mom_hl['LONG_TERM'] = 30\n",
    "dict_mom_hl['SHORT_TERM'] = 3\n",
    "### Factors options:\n",
    "dict_combinations = {}\n",
    "dict_combinations['LONG_TERM'] = ('MOMENTUM', 'EXP_GDP_rate', 'REER', 'HEDGED')\n",
    "dict_combinations['SHORT_TERM'] = ('MOMENTUM', 'EXP_GDP_rate', 'NEER', 'HEDGED')\n",
    "dict_combinations['COMBO'] = ('COMBO', 'COMBO', 'COMBO', 'COMBO')\n",
    "### Work periods:\n",
    "ser_work_periods = pd.Series(1 , index = pd.MultiIndex.from_product([['Year', 'Month'], ['Y', 'M', 'D']], names = ['Period', 'Frequency']))\n",
    "ser_work_periods['Year', 'M'] = 12\n",
    "ser_work_periods['Year', 'D'] = 260\n",
    "ser_work_periods['Month', 'Y'] = 0\n",
    "ser_work_periods['Month', 'D'] = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEAN MOMENTUM FUNCTION (TO CALCULATE FACTOR ONLY FOR MONTHENDS):\n",
    "\n",
    "def rolling_cond_mean_momentum(ser_country_matrix, ser_full_source, int_numer_win, int_numer_min, int_denom_win, int_denom_min):\n",
    "    ### Country saving:\n",
    "    str_country = ser_country_matrix.index[0][1]\n",
    "    ### Checking for country presence in source vector:\n",
    "    if (str_country in ser_full_source.index.get_level_values(1)):\n",
    "        ### Filtering country vector from source:\n",
    "        ser_country_source = ser_full_source.loc[All, str_country]\n",
    "        ### Looping over matrix index dates:\n",
    "        for iter_bm_date in ser_country_matrix.index.get_level_values(0):\n",
    "            try:\n",
    "                ### Defining monthend date number in source country vector:\n",
    "                int_idx_num = ser_country_source.index.get_loc(iter_bm_date)\n",
    "                ### Creating vectors for numerator and denominator means calculation:\n",
    "                ser_rolled_numer = -ser_country_source.iloc[max((int_idx_num - int_numer_win + 1), 0) : int_idx_num + 1]        \n",
    "                ser_rolled_denom = -ser_country_source.iloc[max((int_idx_num - int_denom_win + 1), 0) : int_idx_num + 1]\n",
    "                ### Checking for minimal data presence:\n",
    "                if ((ser_rolled_numer.count() >= int_numer_min) & (ser_rolled_denom.count() >= int_denom_min)):\n",
    "                    ### Mena momentum value calculation:\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = np.log(ser_rolled_numer.mean() / ser_rolled_denom.mean())\n",
    "            except KeyError:\n",
    "                pass\n",
    "    ### Resulting vector output:\n",
    "    return ser_country_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geom_weight_single(flo_ratio, flo_factor = 1, num_element = 0):\n",
    "    ### Results output:\n",
    "    return flo_factor * (flo_ratio ** num_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False, int_min_count = 0):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if (ser_data.count() > int_min_count):       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEAN MOMENTUM FUNCTION (TO CALCULATE FACTOR ONLY FOR MONTHENDS):\n",
    "\n",
    "def rolling_cond_weighted_mean(ser_country_matrix, ser_full_source, int_mean_win, int_mean_min, list_weight = False, ser_full_cond = False):\n",
    "    ### Defining conditional average calculator:\n",
    "    def conditional_average(ser_source, list_weight, int_min_count = 0, ser_condition = False):\n",
    "        ### Weights setting:\n",
    "        ser_weight = pd.Series(list_weight[ : len(ser_source.index)], ser_source.index)\n",
    "        ### If we have condition we should resort the weight array:\n",
    "        if not isinstance(ser_condition, bool):\n",
    "            ser_condition_sorted = pd.Series(ser_condition.sort_values().index, ser_condition.index)\n",
    "            ser_condition_sorted.name = 'Condition'\n",
    "            ser_weight = pd.concat([ser_weight, ser_condition_sorted], axis = 1).reset_index(drop = True).set_index('Condition').squeeze().sort_index()            \n",
    "        ### Results output:\n",
    "        return weighted_average(ser_source, ser_weight, int_min_count)    \n",
    "    ### Country saving:\n",
    "    str_country = ser_country_matrix.index[0][1]\n",
    "    ### Checking for country presence in source vector:\n",
    "    if (str_country in ser_full_source.index.get_level_values(1)):\n",
    "        ### Filtering country vector from source:\n",
    "        ser_country_source = ser_full_source.loc[All, str_country]\n",
    "        if not isinstance(ser_full_cond, bool):\n",
    "            ser_country_cond = ser_full_cond.loc[All, str_country]\n",
    "        ### Looping over matrix index dates:\n",
    "        for iter_bm_date in ser_country_matrix.index.get_level_values(0):\n",
    "            try:\n",
    "                ### Defining monthend date number in source country vector:\n",
    "                int_idx_num = ser_country_source.index.get_loc(iter_bm_date)\n",
    "                ### Creating vectors for numerator and denominator means calculation:\n",
    "                ser_rolled_source = ser_country_source.iloc[max((int_idx_num - int_mean_win + 1), 0) : int_idx_num + 1]\n",
    "                if not isinstance(ser_full_cond, bool):\n",
    "                    ser_rolled_cond = ser_country_cond.loc[ser_rolled_source.index]\n",
    "                else:\n",
    "                    ser_rolled_cond = False\n",
    "                ### Simple mean calculation:\n",
    "                if isinstance(list_weight, bool):\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = weighted_average(ser_rolled_source, False, int_mean_min)\n",
    "                else:\n",
    "                    ### Weighted mean calculation:\n",
    "                    ser_country_matrix.loc[iter_bm_date, str_country] = conditional_average(ser_rolled_source, list_weight, int_mean_min, ser_rolled_cond)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    ### Resulting vector output:\n",
    "    return ser_country_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES\n",
    "\n",
    "def multistep_standartize(ser_data_source, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):  \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_data_source.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_data_source.dropna()\n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        if reuse_outliers:\n",
    "            ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        else:\n",
    "            ### Saving to result and excluding from further calculations truncated values:             \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):\n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_data_source.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR CROSS-SECTION\n",
    "\n",
    "def ison_standartize(ser_to_manage, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False, within_market = False):\n",
    "    ### Multi-step standartizing:\n",
    "    if (within_market):\n",
    "    ### Within market standartizing:\n",
    "        ser_result = ser_to_manage.groupby(by = 'Market', group_keys = False).apply(multistep_standartize, arr_truncate, ser_weight, \n",
    "                                                                                                  reuse_outliers, center_result, full_result)\n",
    "    else:\n",
    "    ### Full universe standartizing:\n",
    "        ser_result = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result)\n",
    "    ### Results output:\n",
    "    return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK\n",
    "\n",
    "def single_factor_standartize(ser_factor, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, within_market = False, \n",
    "                              flag_tha = False, flo_similarity = 5 * (10 ** (-8))):\n",
    "    ### Local constants:\n",
    "    dict_tha_pow = {}\n",
    "    dict_tha_pow['monthly'] = 1\n",
    "    dict_tha_pow['quarterly'] = 1 / 3\n",
    "    dict_tha_pow['annual'] = 1 / 12\n",
    "    ### Weights preparing:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "        ser_weight.name = 'Weight'\n",
    "    ### Multi-step standartizing:        \n",
    "    df_factor = ser_factor.to_frame().join(ser_weight, how = 'left')\n",
    "    df_factor.columns = ['Factor', 'Weight']\n",
    "    ### Time-horizon adjusted standartization:\n",
    "    if (flag_tha):\n",
    "        ### Z-scored vector calculating:\n",
    "        ser_stand_z = df_factor.groupby('Date', group_keys = False)\\\n",
    "                               .apply(lambda iter_df: tha_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False))\n",
    "        ### Results output:\n",
    "        ser_stand_z.name = ser_factor.name\n",
    "        ### Autocorrelation vector calculating:\n",
    "        ser_autocorr_vector = ser_stand_z.groupby('Market').apply(vector_autocorr, 1)\n",
    "        ser_autocorr_vector.name = 'Autocorr'\n",
    "        ser_autocorr_cum_mean = ser_autocorr_vector.loc[np.abs(ser_autocorr_vector - 1) > flo_similarity].groupby('Market', group_keys = False).expanding().mean()\n",
    "        ### THA-coeficcient calculating:\n",
    "        ser_tha_coeff = ser_autocorr_cum_mean.transform(lambda iter_mean: max(iter_mean, 0.0) ** dict_tha_pow[flag_tha])\n",
    "        ser_tha_coeff = ser_tha_coeff.transform(lambda iter_mean: \n",
    "                                                sum(map(lambda iter_num: geom_weight_single(flo_tha_ratio * iter_mean, 1, iter_num), range(int_tha_length))) / 2)\n",
    "        ser_tha_coeff = ser_tha_coeff.swaplevel()\n",
    "        ser_tha_coeff = ser_tha_coeff.unstack('Market').reindex(ser_stand_z.index.levels[0]).stack('Market', dropna = False).sort_index(level = ['Date', 'Market'])        \n",
    "        ### THA-adjusted z-score calculating:\n",
    "#        ser_stand_s = (ser_stand_z * ser_tha_coeff)\n",
    "        ### Artifical filling values for first date of region appearance (not to loose observations):\n",
    "        ser_stand_s = (ser_stand_z * ser_tha_coeff.fillna(0.5))        \n",
    "        ser_stand_s = ser_stand_s[ser_stand_s.index.dropna()].reorder_levels(['Date', 'Country', 'Market']).sort_index()\n",
    "        ### Standart deviation for THA-adjusted z-score calculating:\n",
    "        ser_region_std = ser_stand_s.groupby(['Date', 'Market']).std()\n",
    "        ser_universe_std = ser_stand_s.groupby(['Date']).std()\n",
    "        ser_universe_std = pd.concat([ser_universe_std], keys = ['Overall'], names = ['Market']).swaplevel()\n",
    "        ser_std = pd.concat([ser_region_std, ser_universe_std], axis = 0).sort_index()\n",
    "        ### Results output:\n",
    "        return (ser_stand_s, ser_stand_z, ser_autocorr_vector, ser_tha_coeff, ser_std)\n",
    "    ### Simple standartization:    \n",
    "    else:    \n",
    "        ser_result = df_factor.groupby('Date', group_keys = False).apply\\\n",
    "                     (lambda iter_df: ison_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False, within_market))\n",
    "        ### Results output:\n",
    "        ser_result.name = ser_factor.name\n",
    "        return ser_result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM MS EXCEL SOURCE\n",
    "\n",
    "def ison_membership_converting(str_path_universe, date_end):\n",
    "    ### Defining business-month-end reindexation on country level:\n",
    "    def country_modify(ser_raw_country, date_end):\n",
    "        ser_res_country = ser_raw_country.droplevel(0).resample('MS').last().resample('BM').last()\n",
    "        range_country = pd.date_range(ser_res_country.index[0], date_end, freq = 'BM')\n",
    "        return ser_res_country.reindex(range_country).ffill()\n",
    "    ### Markets encoding table:\n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM', 0: np.NaN}     \n",
    "    ### Loading source file:\n",
    "    df_raw_universe = pd.read_excel(io = str_path_universe, sheet_name = 0, header = 0, parse_dates = True, index_col = [0, 1],\n",
    "                                 na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    ### Converting source file:\n",
    "    df_raw_universe.index.names = ['Country', 'Date']\n",
    "    ser_raw_universe = df_raw_universe['Region']\n",
    "    ser_raw_universe.fillna(0, inplace = True)\n",
    "    ser_raw_universe.name = 'Market'\n",
    "    ### By country reindexation and translation:\n",
    "    ser_res_universe = ser_raw_universe.groupby('Country').apply(country_modify, date_end)\n",
    "    ser_res_universe.index.names = ['Country', 'Date']\n",
    "    ser_res_universe = ser_res_universe.replace(dict_markets).reorder_levels([1, 0]).sort_index()    \n",
    "    ### Results output:\n",
    "    return ser_res_universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: BLOOMBERG STRUCTURED DATA & ISON MEMBERSHIP EXTRACTION (NO PRELIMINARY DATA USING)\n",
    "\n",
    "ser_returns = pd.read_hdf(str_path_bb_hdf, key = str_key_ret_monthly)\n",
    "ser_mmr = pd.read_hdf(str_path_bb_hdf, key = str_key_mmr)\n",
    "ser_fx_country = pd.read_hdf(str_path_bb_hdf, key = str_key_fx_country)\n",
    "ser_mcap = pd.read_hdf(str_path_bb_hdf, key = str_key_mcap)\n",
    "ser_reer = pd.read_hdf(str_path_bb_hdf, key = str_key_reer)\n",
    "ser_neer = pd.read_hdf(str_path_bb_hdf, key = str_key_neer)\n",
    "ser_reer_sourced = pd.read_hdf(str_path_bb_hdf, key = str_key_reer_sourced)\n",
    "ser_neer_sourced = pd.read_hdf(str_path_bb_hdf, key = str_key_neer_sourced)\n",
    "df_xcra_filled = pd.read_hdf(str_path_bb_hdf, key = str_key_xcra)\n",
    "ser_ison = ison_membership_converting(str_path_universe, datetime.strptime(str_date_end, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: DATA PREPARING (NO PRELIMINARY DATA USING)\n",
    "\n",
    "### List of countries with de-facto equal returns (to impact on hedged returns calculating)\n",
    "ser_ret_similarity_test = ser_returns.unstack('Currency').groupby('Country').apply(lambda df_country: (df_country['LOC'] - df_country['USD']).abs().mean())\n",
    "set_ret_usd_only = set(ser_ret_similarity_test.loc[ser_ret_similarity_test < flo_returns_similarity].index)\n",
    "### List of countries with unsufficient data quantity:\n",
    "ser_ret_completeness_test = ser_returns.groupby('Country').apply(lambda ser_country: ser_country.count() / len(ser_country.index))\n",
    "set_not_complete = set(ser_ret_completeness_test.loc[ser_ret_completeness_test < flo_returns_completeness].index)\n",
    "### Filtering uncomplete countries:\n",
    "ser_returns.loc[All, All, set_not_complete] = np.NaN\n",
    "### Returns options preparing:\n",
    "dict_ser_ret = {}\n",
    "### Returns in local currency:\n",
    "dict_ser_ret['LOC'] = ser_returns.loc['LOC', All, All].droplevel(0)\n",
    "### Returns in USD:\n",
    "dict_ser_ret['USD'] = ser_returns.loc['USD', All, All].droplevel(0)\n",
    "### Hedged returns in local currency:\n",
    "dict_ser_hedged = {}\n",
    "### Filling data for countries with no MMR data:\n",
    "set_ison_countries = set(dict_ser_ret['LOC'].index.get_level_values(1).unique())\n",
    "set_mmr_countries = set(ser_mmr.index.get_level_values(1).unique())\n",
    "set_no_mmr_countries = (set_ison_countries - set_mmr_countries) | set_ret_usd_only\n",
    "set_to_hedge_countries = set_mmr_countries - set_no_mmr_countries\n",
    "dict_ser_hedged['No_MMR'] = dict_ser_ret['USD'].loc[All, set_no_mmr_countries]\n",
    "### Money Market rates shifting forward:\n",
    "ser_mmr_shifted = ser_mmr.groupby('Country').shift(1)\n",
    "### Filling data for other countries:\n",
    "df_ser_hedged = pd.DataFrame()\n",
    "df_ser_hedged['Returns LOC'] = dict_ser_ret['LOC'].loc[All, set_to_hedge_countries]\n",
    "df_ser_hedged = df_ser_hedged.join(ser_mmr_shifted, how = 'left')\n",
    "df_ser_hedged.columns = ['Returns LOC', 'MMR LOC']\n",
    "dict_ser_hedged['MMR_Based'] = df_ser_hedged.groupby('Country', group_keys = False)\\\n",
    "                               .apply(lambda df_country: (1 + df_country['Returns LOC']) * (1 + ser_mmr_shifted.loc[All, 'US'] / 12) / (1 + df_country['MMR LOC'] / 12) - 1)\n",
    "#dict_ser_hedged['MMR_Based'] = df_ser_hedged.groupby('Country', group_keys = False)\\\n",
    "#                               .apply(lambda df_country: (1 + df_country['Returns LOC']) * (((1 + ser_mmr.loc[All, 'US']) / (1 + df_country['MMR LOC'])) ** (1 /12)) - 1)\n",
    "### Aggregating hedged returns:\n",
    "dict_ser_ret['HEDGED'] = pd.concat(dict_ser_hedged).droplevel(0).sort_index()\n",
    "### Effective exchange rates options preparing:\n",
    "dict_ser_eer = {}\n",
    "dict_ser_eer['REER'] = ser_reer.groupby('Country').ffill(limit = int_eer_fill_limit)\n",
    "if bool_NEER_raw:\n",
    "    ### Simple NEER usage:\n",
    "    dict_ser_eer['NEER'] = ser_neer.groupby('Country').ffill(limit = int_eer_fill_limit)\n",
    "else:\n",
    "    ### Alternative NEER usage:\n",
    "    set_REER_monthly = set(ser_reer_sourced.loc[All, All, ['IMF', 'BIS']].index.get_level_values(1).unique())\n",
    "    set_REER_all = set(ser_reer.index.get_level_values(1).unique())\n",
    "    list_from_REER = sorted(list(set_REER_all - set_REER_monthly))\n",
    "    list_from_NEER = sorted(list(set_REER_monthly))\n",
    "    dict_ser_eer['NEER'] = pd.concat([ser_reer.loc[All, list_from_REER], ser_neer.loc[All, list_from_NEER]], axis = 0).groupby('Country').ffill(limit = int_eer_fill_limit)\n",
    "#dict_ser_eer['FX'] = ser_fx_country.groupby('Country').ffill(limit = int_eer_fill_limit)\n",
    "### Concepts options preparing:\n",
    "dict_ser_concept = {}\n",
    "### XCRA concept data shifting:\n",
    "df_xcra_shifted = df_xcra_filled.groupby('Country').shift(int_concept_lag)\n",
    "### XCRA concepts calculating:\n",
    "#dict_ser_concept['EXPIMP_GDP_rate'] = (df_xcra_shifted['Imports'] + df_xcra_shifted['Exports']) / df_xcra_shifted['GDP']\n",
    "dict_ser_concept['EXP_GDP_rate'] = df_xcra_shifted['Exports'] / df_xcra_shifted['GDP']\n",
    "#dict_ser_concept['CA_GDP_rate'] = df_xcra_shifted['Current Account'] / df_xcra_shifted['GDP']\n",
    "### XCRA concepts adjusting:\n",
    "for iter_concept in dict_ser_concept:\n",
    "    dict_ser_concept[iter_concept] = dict_ser_concept[iter_concept] / int_concept_divider\n",
    "### XCRA concepts adjusting:\n",
    "for iter_concept in dict_ser_concept:\n",
    "    dict_ser_concept[iter_concept].loc[dict_ser_concept[iter_concept] <= -1] = -0.99\n",
    "    dict_ser_concept[iter_concept] = np.maximum(int_concept_min, (np.minimum(int_concept_max, np.log(1 + dict_ser_concept[iter_concept]))))\n",
    "#    dict_ser_concept[iter_concept] = np.log(1 + dict_ser_concept[iter_concept])             \n",
    "### Neutral concept adding:\n",
    "dict_ser_concept['NO_CONCEPT'] = pd.Series(1, index = dict_ser_concept['EXP_GDP_rate'].index)\n",
    "### Concept series renaming:\n",
    "for iter_concept in dict_ser_concept:\n",
    "    dict_ser_concept[iter_concept].name = 'Multiplicator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3544357469015003"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTING: MINIMAL VALUE CONTROL:\n",
    "\n",
    "ser_concept_test = df_xcra_shifted['Exports'] / df_xcra_shifted['GDP'] / int_concept_divider\n",
    "ser_logged_test = np.minimum(int_concept_max, np.log(1 + ser_concept_test))\n",
    "ser_logged_test[ser_logged_test == int_concept_max].count() / ser_logged_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: RECONCILIATION WITH THE ORIGINAL SOURCE \n",
    "\n",
    "str_test_date = '1999-05-31'\n",
    "str_test_factor = 'LONG_TERM'\n",
    "list_test_country = ['RU'] # All # ['BR'] #\n",
    "str_test_eer = 'REER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTING: CALCULATION PREPARING\n",
    "\n",
    "### Part of canvas matrix:\n",
    "ser_test_matrix = pd.Series(index = pd.MultiIndex.from_product([idx_date_range, ser_ison.index.get_level_values(1).unique()])).loc[str_test_date, list_test_country, All]\n",
    "### EER delta:\n",
    "ser_test_delta = dict_ser_eer[str_test_eer].groupby('Country').diff()\n",
    "ser_test_delta = ser_test_delta / dict_ser_eer[str_test_eer].groupby('Country').shift()\n",
    "### Momentum parameters:\n",
    "int_mom_hl = round(24 * 260 / 12) # = 660\n",
    "int_mom_win = 5 * 260 # = 1300\n",
    "int_mom_min = 260 // 4 # = 65\n",
    "### Weights array:\n",
    "list_weight = list(map(lambda iter_num: exp_weight_single(int_mom_hl, iter_num), range(int_mom_win)))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00027089566205617974"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TESTING: CALCULATION MODELLING\n",
    "\n",
    "# rolling_cond_weighted_mean(ser_test_matrix, -ser_test_delta, int_mom_win, int_mom_min, list_weight, False)\n",
    "\n",
    "### Test parameters converting to function internal parameters:\n",
    "ser_full_source = ser_test_delta\n",
    "str_country = list_test_country[0]\n",
    "iter_bm_date = str_test_date\n",
    "int_mean_win = int_mom_win\n",
    "int_mean_min = int_mom_min\n",
    "### Filtering country vector from source:\n",
    "ser_country_source = ser_full_source.loc[All, str_country]\n",
    "### Defining monthend date number in source country vector:\n",
    "int_idx_num = ser_country_source.index.get_loc(iter_bm_date)\n",
    "### Creating vectors for numerator and denominator means calculation:\n",
    "ser_rolled_source = ser_country_source.iloc[max((int_idx_num - int_mean_win + 1), 0) : int_idx_num + 1]\n",
    "ser_rolled_source.iloc[0] = np.NaN\n",
    "### Test parameters converting to function internal parameters:\n",
    "ser_source = ser_rolled_source\n",
    "int_min_count = int_mean_min\n",
    "### Weights setting:\n",
    "ser_weight = pd.Series(list_weight[ : len(ser_source.index)], ser_source.index)\n",
    "### Test parameters converting to function internal parameters:\n",
    "ser_data = ser_source\n",
    "### Weights filtering:\n",
    "list_weight = ser_weight[ser_data.dropna().index].values\n",
    "### Data filtering:\n",
    "list_data = ser_data.dropna().values\n",
    "### Weighted average calculating:\n",
    "num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING: REVISED DATA SAVING\n",
    "\n",
    "dict_ser_eer[str_test_eer].loc[All, list_test_country].to_excel('Data_Files/Test_Files/Revision_EER_Level_All.xlsx', merge_cells = False)\n",
    "ser_test_delta.loc[All, list_test_country].to_excel('Data_Files/Test_Files/Revision_EER_Ret_All.xlsx', merge_cells = False)\n",
    "ser_weight.to_excel('Data_Files/Test_Files/Revision_EER_Weights_1300.xlsx', merge_cells = False)\n",
    "ser_rolled_source.to_excel('Data_Files/Test_Files/Revision_EER_Ret_1300.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOMENTUM / LONG_TERM / HEDGED / EXP_GDP_rate / REER\n"
     ]
    }
   ],
   "source": [
    "### MAIN SCRIPT: FACTORS CALCULATING (NO PRELIMINARY DATA USING)\n",
    "\n",
    "### Containers for preliminary data:\n",
    "dict_trans_factor_hdf = {}\n",
    "### Factors looping:\n",
    "iter_factor = str_test_factor\n",
    "### Parameters loading:\n",
    "iter_algo = dict_combinations[iter_factor][0]\n",
    "iter_concept = dict_combinations[iter_factor][1]\n",
    "iter_eer = dict_combinations[iter_factor][2]\n",
    "iter_ret = dict_combinations[iter_factor][3]    \n",
    "print(f'{iter_algo} / {iter_factor} / {iter_ret} / {iter_concept} / {iter_eer}')                \n",
    "### Iteration data loading:\n",
    "ser_iter_eer = dict_ser_eer[iter_eer]\n",
    "### Factor matrix creating:\n",
    "ser_iter_factor = pd.Series(index = pd.MultiIndex.from_product([idx_date_range, ser_ison.index.get_level_values(1).unique()])).sort_index()\n",
    "ser_iter_factor.index.set_names(['Date', 'Country'], inplace = True)                \n",
    "### Momentum factor data preparing:\n",
    "ser_iter_delta = ser_iter_eer.groupby('Country').diff().dropna()\n",
    "### Momentum parameters:\n",
    "int_mom_hl = dict_mom_hl[iter_factor] * ser_work_periods['Month', 'D']\n",
    "int_mom_win = int_mom_length * ser_work_periods['Year', 'D']\n",
    "int_mom_min = int_mom_min_win\n",
    "### Weights array:\n",
    "list_weight = list(map(lambda iter_num: exp_weight_single(int_mom_hl, iter_num), range(int_mom_win)))[::-1]\n",
    "### Momentum factor calculation:\n",
    "ser_iter_factor = ser_iter_factor.groupby('Country').transform(rolling_cond_weighted_mean, -ser_iter_delta, int_mom_win, int_mom_min, list_weight, False)\n",
    "### Factor ISONing:\n",
    "ser_iter_factor = ser_iter_factor.to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "ser_iter_factor.name = 'Factor'\n",
    "### Regions clearing:\n",
    "ser_iter_factor = ser_iter_factor.loc[idx_date_range, All, list_ison]\n",
    "### Countries filtering:\n",
    "ser_iter_factor = ser_iter_factor.drop(list_countries_to_exclude, level = 'Country') \n",
    "### Factor and Multiplicator standartizing (Multiplicator shifting), multiplying and restandartizing:\n",
    "ser_iter_factor_std = single_factor_standartize(ser_iter_factor, list_truncate, within_market = bool_within_market)\n",
    "ser_iter_factor_std.name = 'Factor'                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        Country  Market\n",
       "1999-05-31  RU       EM        1.364668\n",
       "Name: Factor, dtype: float64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_iter_factor_std.loc[str_test_date, list_test_country, All]\n",
    "#ser_iter_factor.loc[str_test_date, list_test_country, All]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
