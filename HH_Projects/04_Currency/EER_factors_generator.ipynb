{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EER FACTORS CREATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZATION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats as ss\n",
    "import math     \n",
    "    \n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GENERAL DATA PREPARATION\n",
    "\n",
    "### Constants:\n",
    "All = slice(None)\n",
    "### Bloomberg structured data extraction parameters:\n",
    "str_path_bb_hdf = 'Data_Files/Source_Files/Bloomberg_prepared.h5'\n",
    "str_key_ret = 'bb_ret'\n",
    "str_key_mmr = 'bb_mmr'\n",
    "str_key_fx = 'bb_fx'\n",
    "str_key_mcap = 'bb_mcap'\n",
    "str_key_reer = 'bb_reer'\n",
    "str_key_neer = 'bb_neer'\n",
    "str_key_xcra = 'bb_xcra'\n",
    "### Factors parameters:\n",
    "str_date_start = '1996-08-01' # Start date to filter returns, market caps and factors\n",
    "str_date_end = '2020-04-30' # End date to filter returns, market caps and factors\n",
    "idx_date_range = pd.date_range(str_date_start, str_date_end, freq = 'BM')\n",
    "list_ison = ['DM', 'EM', 'FM']\n",
    "list_filter = ['DM', 'EM', 'FM']\n",
    "list_truncate = [2.5, 2.0] # Standartization boundaries\n",
    "bool_within_market = True # Standartization way\n",
    "list_countries_to_exclude = ['VE'] # Countries not to play the game\n",
    "flo_returns_similarity = 0.001\n",
    "flo_returns_completeness = 1 / 3\n",
    "int_concept_lag = 3 ### Lag in months for GDP  like concepts\n",
    "int_concept_divider = 1000 # Divider to equalize concepts and GDP scales\n",
    "int_regress_win = 60 # Regression window length for alternative sensitivity concept\n",
    "int_regress_hl = 3 # Half-life period for alternative sensitivity concept\n",
    "int_factor_addendum = 2.5 # list_truncate[0] # Factor scaler\n",
    "dict_numer_ma_win = {} # Moving average window length for factors numerators\n",
    "dict_numer_ma_win['LONG_TERM'] = round(260 / 12 * 3)\n",
    "dict_numer_ma_win['SHORT_TERM'] = round(260 / 52 * 2)\n",
    "dict_denom_ma_win = {} # Moving average window length for factors denomenators\n",
    "dict_denom_ma_win['LONG_TERM'] = 260 * 5\n",
    "dict_denom_ma_win['SHORT_TERM'] = round(260 / 12 * 6)\n",
    "### Transitional results parameters:\n",
    "str_path_trans_hdf = 'Data_Files/Test_Files/EER_factors_transitional.h5'\n",
    "str_key_trans_ret = 'trans_ret'\n",
    "str_key_trans_mcap = 'trans_mcap'\n",
    "str_key_trans_factor = 'trans_factor'\n",
    "### Measures parameters:\n",
    "list_measure = ['fmb_weighted', 'qtl4'] # Efficacy measures list\n",
    "list_back_period = [99, 10, 5] # Look back periods\n",
    "int_horizon = 12 # Measure stats horizon\n",
    "str_path_efficacy_hdf = 'Data_Files/Test_Files/EER_factors_stats.h5'\n",
    "str_key_efficacy = 'fmb_weight'\n",
    "str_path_efficacy_xlsx = 'Data_Files/Test_Files/EER_factors_stats.xlsx'\n",
    "### Not used yet\n",
    "int_lt_hl_months = 24\n",
    "int_st_hl_months = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING WEIGHTED AVERAGE\n",
    "\n",
    "def weighted_average(ser_data, ser_weight = False):\n",
    "    ### Default output:\n",
    "    num_result = np.NaN\n",
    "    ### Checking for data presence:\n",
    "    if ser_data.count():       \n",
    "        ### Checking for weights dataset:\n",
    "        if isinstance(ser_weight, bool):\n",
    "            ### Calculating of simple average:\n",
    "            num_result = np.nanmean(ser_data.values)\n",
    "        else:\n",
    "            ### Weights filtering:\n",
    "            list_weight = ser_weight[ser_data.dropna().index].values\n",
    "            ### Checking for weights presence:\n",
    "            if np.nansum(list_weight):\n",
    "                ### Data filtering:\n",
    "                list_data = ser_data.dropna().values\n",
    "                ### Weighted average calculating:\n",
    "                num_result = np.nansum(list_data * list_weight) / np.nansum(list_weight)\n",
    "    ### Results output:\n",
    "    return num_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION FOR SEPARATE SERIES\n",
    "\n",
    "def multistep_standartize(ser_data_source, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False):  \n",
    "    ### Arrays of iterations properties:\n",
    "    arr_mean = []\n",
    "    arr_std = []\n",
    "    ### Adding equal weights, when weights are absent:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_data_source.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Workhorse and resulting data vectors initialising:\n",
    "    ser_data_iter = ser_data_source.dropna()\n",
    "    ser_weight_iter = ser_weight.copy()\n",
    "    ser_data_full = pd.Series(np.NaN, index = ser_data_iter.index)\n",
    "    ### Looping by boundaries array:\n",
    "    for num_bound_iter in arr_truncate:\n",
    "        ### Properties calculating and saving:\n",
    "        num_mean_iter = weighted_average(ser_data_iter, ser_weight_iter)\n",
    "        num_std_iter = ser_data_iter.std()\n",
    "        arr_mean.append(num_mean_iter)\n",
    "        arr_std.append(num_std_iter)\n",
    "        ser_data_iter = (ser_data_iter - num_mean_iter) / num_std_iter       \n",
    "        ### Standartizing:\n",
    "        if reuse_outliers:\n",
    "            ser_data_iter[ser_data_iter.abs() >= num_bound_iter] = np.sign(ser_data_iter) * num_bound_iter \n",
    "        else:\n",
    "            ### Saving to result and excluding from further calculations truncated values:             \n",
    "            ser_data_full.where(ser_data_iter.abs() < num_bound_iter, np.sign(ser_data_iter) * num_bound_iter, inplace = True)\n",
    "            ser_data_iter = ser_data_iter[ser_data_iter.abs() < num_bound_iter]           \n",
    "    ### Aggregating result:\n",
    "    if (reuse_outliers):\n",
    "        ser_data_full = ser_data_iter\n",
    "    else:     \n",
    "        ser_data_full[ser_data_iter.index] = ser_data_iter\n",
    "    ### Centering result:\n",
    "    if (center_result):\n",
    "        ser_result = ser_data_full - weighted_average(ser_data_full, ser_weight) \n",
    "    else:\n",
    "        ser_result = ser_data_full    \n",
    "    ### Result output:\n",
    "    ser_result.name = str(ser_data_source.name) + '_standartized'\n",
    "    if (full_result):\n",
    "        return (ser_result, arr_mean, arr_std)\n",
    "    else:\n",
    "        return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR SEPARATE SERIES\n",
    "\n",
    "def ison_standartize(ser_to_manage, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, full_result = False, within_market = False):\n",
    "    ### Multi-step standartizing:\n",
    "    if (within_market):\n",
    "    ### Within market standartizing:\n",
    "        ser_result = ser_to_manage.groupby(by = 'Market', group_keys = False).apply(multistep_standartize, arr_truncate, ser_weight, \n",
    "                                                                                    reuse_outliers, center_result, full_result)\n",
    "    else:\n",
    "    ### Full universe standartizing:\n",
    "        ser_result = multistep_standartize(ser_to_manage, arr_truncate, ser_weight, reuse_outliers, center_result, full_result)\n",
    "    ### Results output:\n",
    "    return ser_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK\n",
    "\n",
    "def single_factor_standartize(ser_factor, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, within_market = False):\n",
    "    ### Weights preparing:\n",
    "    if isinstance(ser_weight, bool):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "        ser_weight.name = 'Weight'    \n",
    "    ### Multi-step standartizing:        \n",
    "    df_factor = ser_factor.to_frame().join(ser_weight, how = 'left')\n",
    "    df_factor.columns = ['Factor', 'Weight']\n",
    "    ser_result = df_factor.groupby('Date', group_keys = False).apply\\\n",
    "    (lambda iter_df: ison_standartize(iter_df['Factor'], arr_truncate, iter_df['Weight'], reuse_outliers, center_result, False, within_market))\n",
    "    ### Results output:\n",
    "    ser_result.name = ser_factor.name\n",
    "    return ser_result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING GROUP MULTI-STEP STANDARTIZATION BY MARKET FOR FULL FACTOR STACK FOR MULTIPLE FACTORS\n",
    "\n",
    "def multi_factor_standartize(df_factor, arr_truncate, ser_weight = False, reuse_outliers = False, center_result = True, within_market = False):\n",
    "    \n",
    "    dict_standartized = {}\n",
    "    ### Single factor standartizing:\n",
    "    for iter_factor in df_factor.columns:\n",
    "        dict_standartized[iter_factor] = single_factor_standartize(df_factor[iter_factor], arr_truncate, ser_weight, \n",
    "                                                                   reuse_outliers, center_result, within_market)\n",
    "    ### Concatenating to dataframe:\n",
    "    df_result = pd.concat(dict_standartized, axis = 1)\n",
    "    ### Results output:\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXPONENTIAL WEIGHT\n",
    "\n",
    "def exp_weight_single(halflife_len = 3, num_element = 0):\n",
    "    ### Weight calculating:\n",
    "    num_period_factor = math.exp(math.log(0.5) / round(halflife_len))\n",
    "    num_weight = np.exp(math.log(num_period_factor) * num_element)\n",
    "    ### Result output:\n",
    "    return num_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EFFICACY MEASURES FOR SINGLE FACTOR\n",
    "\n",
    "def single_factor_multiple_efficacy_measures(ser_factor, ser_return, ser_weight, arr_measure, return_shift = 0, arr_truncate = [2.5, 2.0]):\n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    dict_measure = {}\n",
    "    set_std_needed = {'fmb_std_eqw', 'fmb_std_weighted'}\n",
    "    num_precision = 5 # For quintile bins rounding and borders controlling\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining MatLab style prctile function:\n",
    "    def prctile(ser_to_perc, p):\n",
    "        ### Sorted list preparing:\n",
    "        list_to_perc = ser_to_perc.dropna().values\n",
    "        list_sorted = np.sort(list_to_perc)\n",
    "        ### Length calculating:\n",
    "        num_len = len(list_to_perc)    \n",
    "        ### Prctile calculating:\n",
    "        num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "        ### Results output:\n",
    "        return num_result\n",
    "    ### Defining quintile bins distribution:\n",
    "    def quintile_distribution(ser_iter_group, num_bins = 5, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True):\n",
    "        ### Bins preparing:\n",
    "        list_bin = list(np.arange(0, 100, 100 / num_bins))[1 : ]\n",
    "        list_bin = [round(iter_element / 100, 2) for iter_element in list_bin]\n",
    "        list_bin = [prctile(ser_iter_group, iter_element) for iter_element in list_bin]\n",
    "        list_bin = [ser_iter_group.min() - abs(ser_iter_group.min())] + list_bin + [ser_iter_group.max() + abs(ser_iter_group.max())]\n",
    "        list_bin = [round(iter_element, num_precision) for iter_element in list_bin]\n",
    "        if bool_populate_last:\n",
    "            list_bin[-2] -= 10 ** (-num_precision)\n",
    "        ### Bins distribution:\n",
    "        ser_iter_distribution = pd.cut(ser_iter_group, bins = list_bin, labels = range(num_bins), \n",
    "                                       right = bool_right_included, include_lowest = bool_include_lowest, precision = num_precision)\n",
    "        ### Results output:\n",
    "        return ser_iter_distribution    \n",
    "    ### Defining get_measure group level function:\n",
    "    def get_measure(df_to_measure, iter_measure):\n",
    "        ### Checking data sufficiency:\n",
    "        if (len(df_to_measure.dropna().index) > 1):           \n",
    "            ### Measure calculating:\n",
    "            if (iter_measure == 'ic_spearman'):\n",
    "                ### Spearmen information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values\n",
    "                num_result = ss.spearmanr(list_factor, list_return, nan_policy = 'omit').correlation\n",
    "            if (iter_measure == 'ic_pearson'):\n",
    "                ### Pearson information coefficient:\n",
    "                list_factor = df_to_measure[['Factor', 'Return']].dropna()['Factor'].values\n",
    "                list_return = df_to_measure[['Factor', 'Return']].dropna()['Return'].values                \n",
    "                num_result = ss.pearsonr(list_factor, list_return)[0]\n",
    "            if (iter_measure == 'fmb_eqw'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (equal weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return']].dropna()['Return'].values\n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_added = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()[['Factor', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values\n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]\n",
    "            if (iter_measure == 'fmb_std_eqw'):             \n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return']].dropna()['Return'].values                \n",
    "                wls_model = sm.OLS(endog = list_return, exog = list_factor_std_added, missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]                 \n",
    "            if (iter_measure == 'fmb_std_weighted'):\n",
    "                ### Fama-McBeth cross-sectional regression beta coefficient (market capitalization weighted residuals):\n",
    "                list_factor_std_added = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()[['Factor_std', 'Constant']].values\n",
    "                list_return = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Return'].values                \n",
    "                list_weight = df_to_measure[['Factor_std', 'Constant', 'Return', 'Weight']].dropna()['Weight'].values                \n",
    "                wls_model = sm.WLS(endog = list_return, exog = list_factor_std_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "                wls_results = wls_model.fit()\n",
    "                num_result = wls_results.params[0]  \n",
    "            if (iter_measure == 'clp'):\n",
    "                ### Constant leverage portfolio signed normalized multiplication sum:                \n",
    "                ser_clp_weighted = df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Factor']\n",
    "                ser_clp_weighted = ser_clp_weighted * df_to_measure[['Factor', 'Return', 'Weight']].dropna()['Weight'].transform(np.sqrt)\n",
    "                ser_clp_weighted.loc[ser_clp_weighted < 0] = -ser_clp_weighted / ser_clp_weighted[ser_clp_weighted < 0].sum()\n",
    "                ser_clp_weighted.loc[ser_clp_weighted > 0] = ser_clp_weighted / ser_clp_weighted[ser_clp_weighted > 0].sum()\n",
    "                num_result = (ser_clp_weighted * df_to_measure['Return']).sum()\n",
    "            if ('qtl' in iter_measure):\n",
    "                ### Interquntile range:\n",
    "                num_bins = int(iter_measure.split('qtl')[1])   \n",
    "                df_to_measure = df_to_measure[['Factor', 'Return', 'Constant']].dropna()\n",
    "                df_to_measure['Return'] = df_to_measure['Return'] - df_to_measure['Return'].mean()\n",
    "                df_to_measure['Factor'] = df_to_measure['Factor'].round(num_precision)\n",
    "                ### Distribution factor values between quintile bins:\n",
    "                ser_qtl_bins = quintile_distribution(df_to_measure['Factor'], num_bins, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True)\n",
    "                ser_qtl_bins.name = 'Bin'\n",
    "                ### Mean return for each bin calculating:\n",
    "                df_to_measure = df_to_measure.join(ser_qtl_bins)\n",
    "                df_qtl_rets = df_to_measure.loc[(All), ['Return', 'Bin']]\n",
    "                df_qtl_rets.set_index('Bin', append = True, inplace = True)\n",
    "                ser_qtl_rets = df_qtl_rets.unstack('Bin').mean(axis = 0).droplevel(0).squeeze()\n",
    "                num_result = ser_qtl_rets.iloc[-1] - ser_qtl_rets.iloc[0]                                 \n",
    "        else:                          \n",
    "            num_result = np.NaN\n",
    "        ### Preparing results: \n",
    "        return num_result\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    if (ser_weight.count() == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ser_weight = ser_weight.reset_index('Market', drop = True)\n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted, ser_weight], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return', 'Weight']\n",
    "    df_to_measure['Constant'] = 1\n",
    "    if set_std_needed.intersection(set(arr_measure)):\n",
    "        ser_factor_std = df_to_measure.dropna()['Factor'].groupby('Date').apply(ison_standartize, arr_truncate = arr_truncate, within_market = False)\n",
    "        df_to_measure['Factor_std'] = ser_factor_std.reindex(df_to_measure.index)\n",
    "    ### Looping efficacy measures for calculating measures timeseries:\n",
    "    for iter_measure in arr_measure:\n",
    "        dict_measure[iter_measure] = df_to_measure.groupby('Date').apply(get_measure, iter_measure = iter_measure)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_measure, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING MEASURE STATISTICS CALCULATOR\n",
    "\n",
    "def measure_stats(df_measures, arr_back_period = [99]):\n",
    "    ### Declaring local constants & variables:\n",
    "    dict_stats = {}\n",
    "    ### Stats calculating:\n",
    "    for iter_measure in df_measures.columns:\n",
    "        dict_period = {}\n",
    "        for iter_back_period in arr_back_period:\n",
    "            ser_iter_measure = df_measures[iter_measure].dropna()\n",
    "            idx_iter_range = pd.date_range(end = ser_iter_measure.index[-1], periods = iter_back_period * 12, freq = 'BM')\n",
    "            ser_iter_measure = ser_iter_measure[idx_iter_range]            \n",
    "            ser_iter_stats = pd.Series()\n",
    "            ser_iter_stats['count'] = ser_iter_measure.count()\n",
    "            ser_iter_stats['min'] = ser_iter_measure.min()\n",
    "            ser_iter_stats['max'] = ser_iter_measure.max()        \n",
    "            ser_iter_stats['mean'] = ser_iter_measure.mean()\n",
    "            ser_iter_stats['std'] = ser_iter_measure.std()\n",
    "            ser_iter_stats['median'] = ser_iter_measure.median()        \n",
    "            ser_iter_stats['perc_25'] = ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['perc_75'] = ser_iter_measure.quantile(0.75, 'midpoint')\n",
    "            ser_iter_stats['iq_range'] = ser_iter_measure.quantile(0.75, 'midpoint') - ser_iter_measure.quantile(0.25, 'midpoint')\n",
    "            ser_iter_stats['mean_abs'] = ser_iter_measure.abs().mean()\n",
    "            ser_iter_stats['t_stat'] = (ser_iter_measure.mean() / ser_iter_measure.std()) * np.sqrt(ser_iter_measure.count())  \n",
    "            dict_period[iter_back_period] = ser_iter_stats\n",
    "        dict_stats[iter_measure] = pd.concat(dict_period, axis = 1)\n",
    "    ### Preparing results:\n",
    "    return pd.concat(dict_stats, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SPECIAL CLP STATS\n",
    "\n",
    "def special_clp_stats(ser_factor, ser_return, ser_weight, return_shift = 0):\n",
    "    ### Declaring local constants & variables:    \n",
    "    dict_clp_stats = {}\n",
    "    list_bin_labels = ['Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining function for constant leverage portfolio normalizing:\n",
    "    def get_normalized_clp(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Constant leverage portfolio signed normalized:\n",
    "            ser_clp_weighted.loc[ser_clp_weighted < 0] = -ser_clp_weighted / ser_clp_weighted[ser_clp_weighted < 0].sum()\n",
    "            ser_clp_weighted.loc[ser_clp_weighted > 0] = ser_clp_weighted / ser_clp_weighted[ser_clp_weighted > 0].sum()\n",
    "            ser_result = ser_clp_weighted.copy()\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index)\n",
    "        ### Results output:\n",
    "        return ser_result\n",
    "    ### Defining function for constant leverage portfolio normalizing:\n",
    "    def get_normalized_factor(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Factor signed normalized:\n",
    "            ser_factor_normalized = df_to_measure['Factor']\n",
    "            ser_factor_normalized.loc[ser_factor_normalized < 0] = -ser_factor_normalized / ser_factor_normalized[ser_factor_normalized < 0].sum()\n",
    "            ser_factor_normalized.loc[ser_factor_normalized > 0] = ser_factor_normalized / ser_factor_normalized[ser_factor_normalized > 0].sum()\n",
    "            ser_result = ser_factor_normalized\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index) \n",
    "        ### Results output:\n",
    "        return ser_result            \n",
    "    ### Defining function for returns for constant leverage portfolio:\n",
    "    def get_normalized_return(df_to_measure):\n",
    "        ser_clp_weighted = df_to_measure['Factor'] * df_to_measure['Weight'].transform(np.sqrt)\n",
    "        ### Checking data sufficiency:\n",
    "        if (ser_clp_weighted.count() > 0):           \n",
    "            ### Normalized return:  \n",
    "            ser_result = df_to_measure['Return']\n",
    "        else:\n",
    "            ser_result = pd.Series(np.NaN, index = ser_clp_weighted.index)            \n",
    "        ### Results output:\n",
    "        return ser_result  \n",
    "    ### Defining MatLab style prctile function:\n",
    "    def prctile(ser_to_perc, p):\n",
    "        ### Sorted list preparing:\n",
    "        list_to_perc = ser_to_perc.dropna().values\n",
    "        list_sorted = np.sort(list_to_perc)\n",
    "        ### Length calculating:\n",
    "        num_len = len(list_to_perc)    \n",
    "        ### Prctile calculating:\n",
    "        num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "        ### Results output:\n",
    "        return num_result\n",
    "    ### Defining quintile bins distribution:\n",
    "    def quintile_distribution(ser_iter_group, num_bins = 5, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True):\n",
    "        ### Bins preparing:\n",
    "        list_bin = list(np.arange(0, 100, 100 / num_bins))[1 : ]\n",
    "        list_bin = [round(iter_element / 100, 2) for iter_element in list_bin]\n",
    "        list_bin = [prctile(ser_iter_group, iter_element) for iter_element in list_bin]\n",
    "        list_bin = [ser_iter_group.min() - abs(ser_iter_group.min())] + list_bin + [ser_iter_group.max() + abs(ser_iter_group.max())]\n",
    "        list_bin = [round(iter_element, num_precision) for iter_element in list_bin]\n",
    "        if bool_populate_last:\n",
    "            list_bin[-2] -= 10 ** (-num_precision)\n",
    "        ### Bins distribution:\n",
    "        ser_iter_distribution = pd.cut(ser_iter_group, bins = list_bin, labels = range(num_bins), \n",
    "                                       right = bool_right_included, include_lowest = bool_include_lowest, precision = num_precision)\n",
    "        ### Results output:\n",
    "        return ser_iter_distribution \n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    if (ser_weight.count() == 0):\n",
    "        ser_weight = pd.Series(1, index = ser_factor.index)\n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ser_weight = ser_weight.reset_index('Market', drop = True)      \n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted, ser_weight], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return', 'Weight']\n",
    "    ser_clp_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_clp)\n",
    "    ser_factor_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_factor)\n",
    "    ser_return_normalized = df_to_measure.dropna().groupby('Date', group_keys = False).apply(get_normalized_return)\n",
    "    ### CLP stats calculating:\n",
    "    dict_clp_stats['Average Bias'] = ser_factor_normalized.groupby('Country').mean()\n",
    "#    dict_clp_stats['Temp'] = pd.Series(np.NaN, index = dict_clp_stats['Average Bias'].index)\n",
    "    dict_clp_stats['Weights Sum'] = ser_clp_normalized.groupby('Country').sum()\n",
    "    dict_clp_stats['Average Return'] = ser_return_normalized.groupby('Country').mean()   \n",
    "    dict_clp_stats['Static Contribution'] = dict_clp_stats['Weights Sum'] * dict_clp_stats['Average Return']\n",
    "    dict_clp_stats['Total Contribution'] = (ser_clp_normalized * ser_return_normalized).groupby('Country').sum()\n",
    "    dict_clp_stats['Dynamic Contribution'] = dict_clp_stats['Total Contribution'] - dict_clp_stats['Static Contribution'] \n",
    "    ### CLP active weights calculating:\n",
    "    ser_clp_delta = ser_clp_normalized.unstack('Date').stack('Date', dropna = False).swaplevel().sort_index(level = ['Date', 'Country'])\n",
    "    ser_clp_delta = ser_clp_delta.fillna(0)\n",
    "    num_clp_mean = ser_clp_delta.groupby('Country').mean().abs().sum()\n",
    "    ser_clp_delta = ser_clp_delta.groupby('Country').apply(lambda iter_group: iter_group - iter_group.mean())\n",
    "    num_clp_delta = (ser_clp_delta.abs().groupby('Date').sum() / (ser_clp_delta.abs().groupby('Date').sum() + num_clp_mean)).mean()\n",
    "    df_clp_stats = pd.concat(dict_clp_stats, axis = 1).reindex(df_to_measure.index.get_level_values('Country').unique()).sort_index()\n",
    "    ### Preparing sum:\n",
    "    df_clp_sum = pd.DataFrame([[np.NaN, np.NaN, np.NaN, df_clp_stats['Static Contribution'].sum(), \n",
    "                               df_clp_stats['Total Contribution'].sum(), df_clp_stats['Dynamic Contribution'].sum()]], \n",
    "                              index = ['Sum'], columns = df_clp_stats.columns)\n",
    "    ### Preparing expected based on active weights:\n",
    "    df_clp_expected = pd.DataFrame([[np.NaN, np.NaN, np.NaN, \n",
    "                                     df_clp_stats['Total Contribution'].sum() * (1 - num_clp_delta), np.NaN, df_clp_stats['Total Contribution'].sum() * num_clp_delta]], \n",
    "                              index = ['Expected based on active weights =>'], columns = df_clp_stats.columns)    \n",
    "    ### Adding totals:\n",
    "    df_clp_stats = pd.concat([df_clp_stats, df_clp_sum, df_clp_expected], axis = 0, join = 'inner')\n",
    "    ### CLP Bias calculating:   \n",
    "    ser_clp_quintile = ser_clp_normalized.groupby('Date', group_keys = False).apply(quartile_distribution)\n",
    "    df_clp_bias = ser_clp_quintile.to_frame()  \n",
    "    df_clp_bias.columns = ['Bin']\n",
    "    df_clp_bias['Quintile'] = 1\n",
    "    df_clp_bias = df_clp_bias.set_index('Bin', append = True).unstack('Bin').fillna(0).droplevel(level = 0, axis = 1)\n",
    "    df_clp_bias.columns = list(df_clp_bias.columns)  \n",
    "    df_clp_bias = df_clp_bias[list_bin_labels]\n",
    "    df_clp_bias = df_clp_bias.groupby('Country').mean()\n",
    "    df_clp_bias.loc[:, 'Q5 - Q1'] = df_clp_bias['Q5'] - df_clp_bias['Q1']   \n",
    "    df_clp_bias = df_clp_bias.reindex(df_to_measure.index.get_level_values('Country').unique()).sort_index()\n",
    "    df_clp_bias = df_clp_bias\n",
    "    ### Output results:\n",
    "    return (df_clp_stats, df_clp_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SPECIAL QTL STATS\n",
    "\n",
    "def special_qtl_stats(ser_factor, ser_return, return_shift = 0, num_bins = 10):\n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    num_precision = 5 # For quintile bins rounding and borders controlling\n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result    \n",
    "    ### Defining MatLab style prctile function:\n",
    "    def prctile(ser_to_perc, p):\n",
    "        ### Sorted list preparing:\n",
    "        list_to_perc = ser_to_perc.dropna().values\n",
    "        list_sorted = np.sort(list_to_perc)\n",
    "        ### Length calculating:\n",
    "        num_len = len(list_to_perc)    \n",
    "        ### Prctile calculating:\n",
    "        num_result = np.interp(np.array(p), np.linspace(1 / (2 * num_len), (2 * num_len - 1) / (2 * num_len), num_len), list_sorted)\n",
    "        ### Results output:\n",
    "        return num_result\n",
    "    ### Defining quintile bins distribution:\n",
    "    def quintile_distribution(ser_iter_group, num_bins = 5, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True):\n",
    "        ### Bins preparing:\n",
    "        list_bin = list(np.arange(0, 100, 100 / num_bins))[1 : ]\n",
    "        list_bin = [round(iter_element / 100, 2) for iter_element in list_bin]\n",
    "        list_bin = [prctile(ser_iter_group, iter_element) for iter_element in list_bin]\n",
    "        list_bin = [ser_iter_group.min() - abs(ser_iter_group.min())] + list_bin + [ser_iter_group.max() + abs(ser_iter_group.max())]\n",
    "        list_bin = [round(iter_element, num_precision) for iter_element in list_bin]\n",
    "        if bool_populate_last:\n",
    "            list_bin[-2] -= 10 ** (-num_precision)\n",
    "        ### Bins distribution:\n",
    "        ser_iter_distribution = pd.cut(ser_iter_group, bins = list_bin, labels = range(num_bins), \n",
    "                                       right = bool_right_included, include_lowest = bool_include_lowest, precision = num_precision)\n",
    "        ### Results output:\n",
    "        return ser_iter_distribution    \n",
    "\n",
    "    ### Defining qts stats generator:\n",
    "    def get_qtl_stats(df_to_measure):\n",
    "        df_to_measure = df_to_measure[['Factor', 'Return', 'Constant']].dropna()\n",
    "        if len(df_to_measure.index):\n",
    "            df_to_measure['Return'] = df_to_measure['Return'] - df_to_measure['Return'].mean()\n",
    "            df_to_measure['Factor'] = df_to_measure['Factor'].round(num_precision)\n",
    "            ### Distribution factor values between quintile bins:\n",
    "            ser_qtl_bins = quintile_distribution(df_to_measure['Factor'], num_bins, bool_right_included = True, bool_include_lowest = True, bool_populate_last = True)\n",
    "            ser_qtl_bins.name = 'Bin'\n",
    "            df_to_measure = df_to_measure.join(ser_qtl_bins)        \n",
    "            ### Bins distribution:\n",
    "            df_qtl_bins = df_to_measure.loc[(All), ['Constant', 'Bin']]\n",
    "            df_qtl_bins.set_index('Bin', append = True, inplace = True)\n",
    "            ser_qtl_bins = df_qtl_bins.unstack('Bin').sum(axis = 0).droplevel(0).squeeze()\n",
    "            ### Return mean for each bin:\n",
    "            df_qtl_rets = df_to_measure.loc[(All), ['Return', 'Bin']]\n",
    "            df_qtl_rets.set_index('Bin', append = True, inplace = True)\n",
    "            ser_qtl_rets = df_qtl_rets.unstack('Bin').mean(axis = 0).droplevel(0).squeeze()\n",
    "        else:\n",
    "            ser_qtl_bins = pd.Series(np.NaN, index = range(num_bins))\n",
    "            ser_qtl_rets = pd.Series(np.NaN, index = range(num_bins))            \n",
    "        ### Results output:\n",
    "        ser_qtl_bins.name = 'Distribution'                \n",
    "        ser_qtl_rets.name = 'Mean return'\n",
    "        return pd.concat([ser_qtl_bins, ser_qtl_rets], axis = 0)      \n",
    "        \n",
    "    ### Region filter dropping:\n",
    "    ser_factor = ser_factor.reset_index('Market', drop = True)\n",
    "    ser_return = ser_return.reset_index('Market', drop = True)\n",
    "    ### Preparing shifted returns:\n",
    "    idx_date_range = ser_return.index.get_level_values(0).unique()\n",
    "    ser_return_shifted = ser_return.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, return_shift)\n",
    "    ### Preparing combined vectors for measures calculating:\n",
    "    df_to_measure = pd.concat([ser_factor, ser_return_shifted], axis = 1)\n",
    "    df_to_measure.columns = ['Factor', 'Return']\n",
    "    df_to_measure['Constant'] = 1\n",
    "    df_bins_stats = df_to_measure.groupby('Date', group_keys = False).apply(get_qtl_stats)\n",
    "    ### Output results:\n",
    "    df_bins_distribution = df_bins_stats.iloc[All, : num_bins]\n",
    "    df_bins_return_mean = df_bins_stats.iloc[All, num_bins :]    \n",
    "    return (df_bins_distribution, df_bins_return_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING SINGLE EFFICACY MEASURE FOR MULTIPLE FACTORS\n",
    "    \n",
    "def multiple_factor_single_efficacy_measure_stats(df_factors, ser_return, ser_weight, str_measure, num_back_period = 99, \n",
    "                                                  num_horizon = 12, list_region_xmo = ['DM', 'EM', 'FM']): \n",
    "    ### Declaring local constants & variables:\n",
    "    All = slice(None)\n",
    "    list_months = [1, 2, 3, 6, 9 ,12]\n",
    "    list_range = [iter_month - 1 for iter_month in list_months if iter_month <= num_horizon]\n",
    "    ### Defining full universe expanding for date:\n",
    "    def universe_reindex(iter_group, idx_universe):\n",
    "        df_iter_result = iter_group.unstack('Date').reindex(idx_universe).sort_index().stack('Date', dropna = False)\n",
    "        ### Results output:\n",
    "        return df_iter_result   \n",
    "    ### Defining date index shifting function:\n",
    "    def date_reindex(iter_group, idx_date_range, return_shift = 0):\n",
    "        ser_iter_result = iter_group.unstack('Country').reindex(idx_date_range).sort_index().shift(-return_shift).stack('Country', dropna = False).squeeze()\n",
    "        ### Results output:\n",
    "        return ser_iter_result   \n",
    "    ### Defining by date correrlation function:\n",
    "    def corr_by_date(iter_group):\n",
    "        num_iter_corr = iter_group['Corr_factor_minus'].corr(iter_group['Corr_factor_plus'])\n",
    "        ### Results output:\n",
    "        return num_iter_corr       \n",
    "    ### Preparing expanded universe for autocorrelation performing:\n",
    "    df_factors_region = df_factors.loc[(All, All, list_region_xmo), :]\n",
    "    idx_date_range = df_factors_region.index.get_level_values(0).unique()\n",
    "    idx_universe = df_factors_region.index.get_level_values(1).unique()\n",
    "    df_factors_full = df_factors_region.reset_index('Market', drop = True).groupby('Date', group_keys = False).apply(universe_reindex, idx_universe).swaplevel()\n",
    "    ### Factors looping:\n",
    "    dict_factors_measures = {} ### Container for all factor stats\n",
    "    dict_factors_autocorr = {} ### Container for autocorrelation results\n",
    "    for iter_factor in df_factors.columns:\n",
    "        ### Shifts looping for factors measures stats:\n",
    "        ### Stats calculation:\n",
    "        dict_factor_stats = {} ### Container for iterated factor stats\n",
    "#        for iter_shift in range(num_horizon):\n",
    "        for iter_shift in list_range:            \n",
    "#            df_factor_filtered = df_factors[iter_factor].loc[All, All, list_region_xmo]\n",
    "            df_factor_filtered = df_factors_region[iter_factor]\n",
    "            df_iter_shift_measure = single_factor_multiple_efficacy_measures(df_factor_filtered, ser_return, ser_weight, [str_measure], iter_shift, list_truncate)\n",
    "            df_iter_shift_stats = measure_stats(df_iter_shift_measure, [num_back_period])\n",
    "            dict_factor_stats[iter_shift] = df_iter_shift_stats.loc[['mean', 't_stat'], (str_measure, num_back_period)]\n",
    "        df_iter_factor_stats = pd.concat(dict_factor_stats, axis = 1)\n",
    "        df_iter_factor_stats.columns = df_iter_factor_stats.columns + 1\n",
    "        dict_factors_measures[iter_factor] = df_iter_factor_stats\n",
    "        ### Autocorrelation calculation:\n",
    "        ser_iter_factor = df_factors_full[iter_factor]\n",
    "        ser_iter_factor_plus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[1 : ]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ser_iter_factor_minus = ser_iter_factor.groupby('Country', group_keys = False).apply(lambda iter_group: iter_group.iloc[: -1]).\\\n",
    "                                sort_index(level = ['Date', 'Country'])\n",
    "        ### Artificial series combining for indexes synchronization:        \n",
    "        ser_iter_factor_plus_shifted = ser_iter_factor_plus.groupby('Country', group_keys = False).apply(date_reindex, idx_date_range, 1)\n",
    "        df_iter_factor_to_corr = pd.concat([ser_iter_factor_minus, ser_iter_factor_plus_shifted], axis = 1)\n",
    "        df_iter_factor_to_corr.columns = ['Corr_factor_minus', 'Corr_factor_plus']\n",
    "        dict_factors_autocorr[iter_factor] = pd.Series(df_iter_factor_to_corr.groupby('Date').apply(corr_by_date).mean(), index = ['autocorr'])\n",
    "    ### Results output:\n",
    "    df_factors_measures_stats = pd.concat(dict_factors_measures, axis = 0)\n",
    "    df_factors_autocorr =  pd.concat(dict_factors_autocorr, axis = 1).transpose()\n",
    "#    df_factors_coeff = df_factors_measures_stats.loc[(All, 'mean'), list_months].reset_index(1, drop = True)\n",
    "    df_factors_coeff = df_factors_measures_stats.loc[(All, 'mean'), All].reset_index(1, drop = True)    \n",
    "    df_factors_coeff.columns = [('coeff_' + str(iter_column)) for iter_column in df_factors_coeff.columns]\n",
    "#    df_factors_t_stat = df_factors_measures_stats.loc[(All, 't_stat'), list_months].reset_index(1, drop = True)\n",
    "    df_factors_t_stat = df_factors_measures_stats.loc[(All, 't_stat'), All].reset_index(1, drop = True)    \n",
    "    df_factors_t_stat.columns = [('t_' + str(iter_column)) for iter_column in df_factors_t_stat.columns]\n",
    "    df_factors_result = pd.concat([df_factors_autocorr, df_factors_coeff, df_factors_t_stat], axis = 1)    \n",
    "    return (df_factors_result, df_factors_measures_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINING EXTRACTION UNIVERSE DATA FROM GENERAL MS EXCEL SOURCE\n",
    "\n",
    "def ison_membership_loading(convert_to_daily = False):\n",
    "    ### Reindexing function declaring:\n",
    "    def reindex_month_ends(iter_group):\n",
    "        iter_range = pd.date_range(iter_group.first_valid_index(), iter_group.last_valid_index(), freq = 'BM')\n",
    "        iter_result = iter_group.reindex(iter_range)\n",
    "        return iter_result    \n",
    "    ### Declaring local constants & variables:\n",
    "    path_msci = 'Data_Files/Source_Files/sample_universe.xlsx' ### Path for membership source     \n",
    "    tab_monthly = 'universe_joined'    \n",
    "    arr_markets_needed = ['DM', 'FM', 'EM']   \n",
    "    dict_markets = {50 : 'DM', 57 : 'EM', 504 : 'FM'}\n",
    "    no_slice = slice(None)\n",
    "    ### Extracting universe data:\n",
    "    df_universe = pd.read_excel(io = path_msci, sheet_name = tab_monthly, skiprows = [0, 2], header = 0, parse_dates = True, \n",
    "                                na_values = ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', \n",
    "                                             '1.#QNAN', 'N/A', 'NULL', 'NaN', 'n/a', 'nan', 'null'], keep_default_na = False)\n",
    "    df_universe = df_universe.loc[no_slice, ['dates', 'region', 'ctry']]\n",
    "    df_universe.columns = ['Date', 'Market', 'Country']\n",
    "    df_universe.set_index(['Date', 'Country'], inplace = True)\n",
    "    ser_universe = df_universe.squeeze()\n",
    "    ser_universe.sort_index(level = [0, 1], inplace = True)\n",
    "    ser_universe.replace(dict_markets, inplace = True)\n",
    "    ser_market_membership = ser_universe[ser_universe.isin(arr_markets_needed)]\n",
    "    ### Reindexing to show absent monthes for future daily resampling: \n",
    "    if (convert_to_daily):\n",
    "        ser_market_membership = ser_market_membership.groupby('Country').apply(lambda iter_group: reindex_month_ends(iter_group.droplevel(1)))\n",
    "        ser_market_membership.index.names = ['Country', 'Date']\n",
    "        ser_market_membership = ser_market_membership.swaplevel()\n",
    "        ser_market_membership = ser_market_membership.reset_index('Country').groupby('Country').resample('B').ffill().drop('Country', axis = 1).squeeze()\n",
    "        ser_market_membership = ser_market_membership.swaplevel().sort_index(level = ['Country', 'Date'])\n",
    "        \n",
    "    return ser_market_membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: BLOOMBERG STRUCTURED DATA & ISON MEMBERSHIP EXTRACTION (NO PRELIMINARY DATA USING)\n",
    "\n",
    "ser_returns = pd.read_hdf(str_path_bb_hdf, key = str_key_ret)\n",
    "ser_mmr = pd.read_hdf(str_path_bb_hdf, key = str_key_mmr)\n",
    "ser_fx_country = pd.read_hdf(str_path_bb_hdf, key = str_key_fx)\n",
    "ser_mcap = pd.read_hdf(str_path_bb_hdf, key = str_key_mcap)\n",
    "ser_reer = pd.read_hdf(str_path_bb_hdf, key = str_key_reer)\n",
    "ser_neer = pd.read_hdf(str_path_bb_hdf, key = str_key_neer)\n",
    "df_xcra_filled = pd.read_hdf(str_path_bb_hdf, key = str_key_xcra)\n",
    "ser_ison = ison_membership_loading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "### List of countries with de-facto equal returns (to impact on hedged returns calculating)\n",
    "#ser_ret_similarity_test = ser_returns.unstack('Currency').groupby('Country').apply(lambda df_country: (df_country['LOC'] - df_country['USD']).abs().mean())\n",
    "#set_ret_usd_only = set(ser_ret_similarity_test.loc[ser_ret_similarity_test < flo_returns_similarity].index)\n",
    "#\n",
    "#ser_ret_similarity_test.loc[ser_ret_similarity_test < flo_returns_similarity].sort_values(ascending = False).sort_index()\n",
    "\n",
    "#print(dict_ser_ret['LOC'].groupby('Country').apply(lambda ser_country: ser_country.count() / len(ser_country.index)).sort_values().head())\n",
    "#print(dict_ser_ret['USD'].groupby('Country').apply(lambda ser_country: ser_country.count() / len(ser_country.index)).sort_values().head())\n",
    "#print(dict_ser_ret['HEDGED'].groupby('Country').apply(lambda ser_country: ser_country.count() / len(ser_country.index)).sort_values().head())\n",
    "\n",
    "#ser_ret_comleteness_test = ser_returns.groupby('Country').apply(lambda ser_country: ser_country.count() / len(ser_country.index))\n",
    "#set_not_complete = set(ser_ret_comleteness_test.loc[ser_ret_comleteness_test < flo_returns_completeness].index)\n",
    "#print(set_not_complete)\n",
    "#ser_returns.loc[All, All, set_not_complete] = np.NaN\n",
    "#sorted(ser_returns.index.get_level_values('Country').unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: DATA PREPARING (NO PRELIMINARY DATA USING)\n",
    "\n",
    "### List of countries with de-facto equal returns (to impact on hedged returns calculating)\n",
    "ser_ret_similarity_test = ser_returns.unstack('Currency').groupby('Country').apply(lambda df_country: (df_country['LOC'] - df_country['USD']).abs().mean())\n",
    "set_ret_usd_only = set(ser_ret_similarity_test.loc[ser_ret_similarity_test < flo_returns_similarity].index)\n",
    "### List of countries with unsufficient data quantity:\n",
    "ser_ret_comleteness_test = ser_returns.groupby('Country').apply(lambda ser_country: ser_country.count() / len(ser_country.index))\n",
    "set_not_complete = set(ser_ret_comleteness_test.loc[ser_ret_comleteness_test < flo_returns_completeness].index)\n",
    "### Filtering uncomplete countries:\n",
    "ser_returns.loc[All, All, set_not_complete] = np.NaN\n",
    "### Returns options preparing:\n",
    "dict_ser_ret = {}\n",
    "### Returns in local currency:\n",
    "dict_ser_ret['LOC'] = ser_returns.loc['LOC', All, All].droplevel(0)\n",
    "### Returns in USD:\n",
    "dict_ser_ret['USD'] = ser_returns.loc['USD', All, All].droplevel(0)\n",
    "### Hedged returns in local currency:\n",
    "dict_ser_hedged = {}\n",
    "### Filling data for countries with no MMR data:\n",
    "set_ison_countries = set(dict_ser_ret['LOC'].index.get_level_values(1).unique())\n",
    "set_mmr_countries = set(ser_mmr.index.get_level_values(1).unique())\n",
    "set_no_mmr_countries = (set_ison_countries - set_mmr_countries) | set_ret_usd_only\n",
    "set_to_hedge_countries = set_mmr_countries - set_no_mmr_countries\n",
    "dict_ser_hedged['No_MMR'] = dict_ser_ret['LOC'].loc[All, set_no_mmr_countries]\n",
    "### Money Market rates shifting forward:\n",
    "ser_mmr_shifted = ser_mmr.groupby('Country').shift(1)\n",
    "### Filling data for other countries:\n",
    "df_ser_hedged = pd.DataFrame()\n",
    "df_ser_hedged['Returns LOC'] = dict_ser_ret['LOC'].loc[All, set_to_hedge_countries]\n",
    "df_ser_hedged = df_ser_hedged.join(ser_mmr_shifted, how = 'left')\n",
    "df_ser_hedged.columns = ['Returns LOC', 'MMR LOC']\n",
    "dict_ser_hedged['MMR_Based'] = df_ser_hedged.groupby('Country', group_keys = False)\\\n",
    "                               .apply(lambda df_country: (1 + df_country['Returns LOC']) * (1 + ser_mmr_shifted.loc[All, 'US'] / 12) / (1 + df_country['MMR LOC'] / 12) - 1)\n",
    "#dict_ser_hedged['MMR_Based'] = df_ser_hedged.groupby('Country', group_keys = False)\\\n",
    "#                               .apply(lambda df_country: (1 + df_country['Returns LOC']) * (((1 + ser_mmr.loc[All, 'US']) / (1 + df_country['MMR LOC'])) ** (1 /12)) - 1)\n",
    "### Aggregating hedged returns:\n",
    "dict_ser_ret['HEDGED'] = pd.concat(dict_ser_hedged).droplevel(0).sort_index()\n",
    "\n",
    "### Effective exchange rates options preparing:\n",
    "dict_ser_eer = {}\n",
    "dict_ser_eer['REER'] = ser_reer\n",
    "dict_ser_eer['NEER'] = ser_neer\n",
    "\n",
    "### Concepts options preparing:\n",
    "dict_ser_concept = {}\n",
    "### XCRA concept data shifting:\n",
    "df_xcra_shifted = df_xcra_filled.groupby('Country').shift(int_concept_lag)\n",
    "### XCRA concepts calculating:\n",
    "dict_ser_concept['EXPIMP_GDP_rate'] = (df_xcra_shifted['Imports'] + df_xcra_shifted['Exports']) / df_xcra_shifted['GDP']\n",
    "dict_ser_concept['EXP_GDP_rate'] = df_xcra_shifted['Exports'] / df_xcra_shifted['GDP']\n",
    "dict_ser_concept['CA_GDP_rate'] = df_xcra_shifted['Current Account'] / df_xcra_shifted['GDP']\n",
    "### XCRA concepts adjusting:\n",
    "for iter_concept in dict_ser_concept:\n",
    "    dict_ser_concept[iter_concept] = dict_ser_concept[iter_concept] / int_concept_divider\n",
    "### XCRA concepts adjusting:\n",
    "for iter_concept in dict_ser_concept:\n",
    "    dict_ser_concept[iter_concept].loc[dict_ser_concept[iter_concept] <= -1] = -1 #-0.99\n",
    "    dict_ser_concept[iter_concept] = np.log(1 + dict_ser_concept[iter_concept])    \n",
    "### Neutral concept adding:\n",
    "dict_ser_concept['NO_CONCEPT'] = pd.Series(1, index = dict_ser_concept['EXP_GDP_rate'].index)\n",
    "\n",
    "### FX CONCEPT ADDING:\n",
    "\n",
    "### Apriori weights for WLS calculation:\n",
    "list_weights = []\n",
    "for iter_num in range(int_regress_win):\n",
    "    list_weights.append(exp_weight_single(int_regress_hl, iter_num))\n",
    "list_weights = list_weights[::-1]\n",
    "### Weighted least square performer defining:    \n",
    "def perform_wls(ser_country_in):\n",
    "    num_result = np.NaN\n",
    "    ### Data retrieval from indexes:\n",
    "    df_country_in = ser_country_in.reset_index(['Ret LOC', 'FX delta'])\n",
    "    ### Weights adding:\n",
    "    df_country_in['Weight'] = list_weights[-len(ser_country_in.index) : ]\n",
    "    ### Columns renaming and NaN dropping:\n",
    "    df_country_in.rename(columns = {0 : 'Constant'}, inplace = True)\n",
    "    df_country_in.dropna(inplace = True)\n",
    "    ### WLS performing:\n",
    "    if (len(df_country_in.index) > 1):\n",
    "        list_factor_added = df_country_in[['Ret LOC', 'Constant']].values\n",
    "        list_basis = df_country_in['FX delta'].values\n",
    "        list_weight = df_country_in['Weight'].values / df_country_in['Weight'].sum()\n",
    "        wls_model = sm.WLS(endog = list_basis, exog = list_factor_added, weights = pow(list_weight, 1/2), missing = 'drop', hasconst = False)\n",
    "        wls_results = wls_model.fit()\n",
    "        ### Beta extracting:\n",
    "        num_result = wls_results.params[0]\n",
    "    ### Results output:\n",
    "    return num_result\n",
    "### Ordinary least square performer defining:    \n",
    "def perform_ols(ser_country_in):\n",
    "    num_result = np.NaN\n",
    "    ### Data retrieval from indexes:\n",
    "    df_country_in = ser_country_in.reset_index(['Ret LOC', 'FX delta'])\n",
    "    ### Columns renaming and NaN dropping:\n",
    "    df_country_in.rename(columns = {0 : 'Constant'}, inplace = True)\n",
    "    df_country_in.dropna(inplace = True)\n",
    "    ### WLS performing:\n",
    "    if (len(df_country_in.index) > 1):\n",
    "        list_factor_added = df_country_in[['Ret LOC', 'Constant']].values\n",
    "        list_basis = df_country_in['FX delta'].values\n",
    "        wls_model = sm.OLS(endog = list_basis, exog = list_factor_added, missing = 'drop', hasconst = False)\n",
    "        wls_results = wls_model.fit()\n",
    "        ### Beta extracting:\n",
    "        num_result = wls_results.params[0]\n",
    "    ### Results output:\n",
    "    return num_result\n",
    "### Local returns on FX rate regression beta data preparing:\n",
    "ser_fx_delta = ser_fx_country.groupby('Country').transform(lambda ser_country: (ser_country / ser_country.shift(1) - 1))\n",
    "df_sensitivity = ser_returns.loc['LOC', All, All].droplevel(0).to_frame().join(ser_fx_delta)\n",
    "df_sensitivity.columns = ['Ret LOC', 'FX delta']\n",
    "df_sensitivity['Constant'] = 1\n",
    "### Fake indices creating for groupby -> rolling -> apply successfull performing:\n",
    "ser_sensitivity = df_sensitivity.set_index(['Ret LOC', 'FX delta'], append = True).squeeze()\n",
    "### Rolling WLS performing:\n",
    "#df_beta = ser_sensitivity.groupby('Country', group_keys = False).rolling(int_regress_win, int_regress_win // 2).apply(perform_wls, raw = False)\\\n",
    "#                                                                                                               .reset_index(['Ret LOC', 'FX delta'])\n",
    "df_beta = ser_sensitivity.groupby('Country', group_keys = False).rolling(int_regress_win, int_regress_win // 2).apply(perform_ols, raw = False)\\\n",
    "                                                                                                               .reset_index(['Ret LOC', 'FX delta'])\n",
    "df_beta.rename(columns = {'Constant' : 'Beta'}, inplace = True)\n",
    "### FX regression beta result preparing & inverting:\n",
    "dict_ser_concept['FX_BETA'] = df_beta.drop(['Ret LOC', 'FX delta'], axis = 1).squeeze()    \n",
    "dict_ser_concept['FX_BETA'] = -dict_ser_concept['FX_BETA']\n",
    "### Concept series renaming:\n",
    "for iter_concept in dict_ser_concept:\n",
    "    dict_ser_concept[iter_concept].name = 'Multiplicator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG_TERM / LOC / EXP_GDP_rate / REER\n",
      "LONG_TERM / LOC / EXP_GDP_rate / NEER\n",
      "LONG_TERM / LOC / EXPIMP_GDP_rate / REER\n",
      "LONG_TERM / LOC / EXPIMP_GDP_rate / NEER\n",
      "LONG_TERM / LOC / NO_CONCEPT / REER\n",
      "LONG_TERM / LOC / NO_CONCEPT / NEER\n",
      "LONG_TERM / LOC / FX_BETA / REER\n",
      "LONG_TERM / LOC / FX_BETA / NEER\n",
      "LONG_TERM / USD / EXP_GDP_rate / REER\n",
      "LONG_TERM / USD / EXP_GDP_rate / NEER\n",
      "LONG_TERM / USD / EXPIMP_GDP_rate / REER\n",
      "LONG_TERM / USD / EXPIMP_GDP_rate / NEER\n",
      "LONG_TERM / USD / NO_CONCEPT / REER\n",
      "LONG_TERM / USD / NO_CONCEPT / NEER\n",
      "LONG_TERM / USD / FX_BETA / REER\n",
      "LONG_TERM / USD / FX_BETA / NEER\n",
      "LONG_TERM / HEDGED / EXP_GDP_rate / REER\n",
      "LONG_TERM / HEDGED / EXP_GDP_rate / NEER\n",
      "LONG_TERM / HEDGED / EXPIMP_GDP_rate / REER\n",
      "LONG_TERM / HEDGED / EXPIMP_GDP_rate / NEER\n",
      "LONG_TERM / HEDGED / NO_CONCEPT / REER\n",
      "LONG_TERM / HEDGED / NO_CONCEPT / NEER\n",
      "LONG_TERM / HEDGED / FX_BETA / REER\n",
      "LONG_TERM / HEDGED / FX_BETA / NEER\n",
      "SHORT_TERM / LOC / EXP_GDP_rate / REER\n",
      "SHORT_TERM / LOC / EXP_GDP_rate / NEER\n",
      "SHORT_TERM / LOC / EXPIMP_GDP_rate / REER\n",
      "SHORT_TERM / LOC / EXPIMP_GDP_rate / NEER\n",
      "SHORT_TERM / LOC / NO_CONCEPT / REER\n",
      "SHORT_TERM / LOC / NO_CONCEPT / NEER\n",
      "SHORT_TERM / LOC / FX_BETA / REER\n",
      "SHORT_TERM / LOC / FX_BETA / NEER\n",
      "SHORT_TERM / USD / EXP_GDP_rate / REER\n",
      "SHORT_TERM / USD / EXP_GDP_rate / NEER\n",
      "SHORT_TERM / USD / EXPIMP_GDP_rate / REER\n",
      "SHORT_TERM / USD / EXPIMP_GDP_rate / NEER\n",
      "SHORT_TERM / USD / NO_CONCEPT / REER\n",
      "SHORT_TERM / USD / NO_CONCEPT / NEER\n",
      "SHORT_TERM / USD / FX_BETA / REER\n",
      "SHORT_TERM / USD / FX_BETA / NEER\n",
      "SHORT_TERM / HEDGED / EXP_GDP_rate / REER\n",
      "SHORT_TERM / HEDGED / EXP_GDP_rate / NEER\n",
      "SHORT_TERM / HEDGED / EXPIMP_GDP_rate / REER\n",
      "SHORT_TERM / HEDGED / EXPIMP_GDP_rate / NEER\n",
      "SHORT_TERM / HEDGED / NO_CONCEPT / REER\n",
      "SHORT_TERM / HEDGED / NO_CONCEPT / NEER\n",
      "SHORT_TERM / HEDGED / FX_BETA / REER\n",
      "SHORT_TERM / HEDGED / FX_BETA / NEER\n"
     ]
    }
   ],
   "source": [
    "### MAIN SCRIPT: FACTORS CALCULATING (NO PRELIMINARY DATA USING)\n",
    "\n",
    "### Containers for preliminary data:\n",
    "dict_trans_ret_hdf = {}\n",
    "dict_trans_mcap_hdf = {}\n",
    "dict_trans_factor_hdf = {}\n",
    "### Factors looping:\n",
    "for iter_factor in ['LONG_TERM', 'SHORT_TERM']: # dict_numer_ma_win: # ['LONG_TERM'] : #\n",
    "    ### Returns type looping:\n",
    "    for iter_ret in dict_ser_ret: # ['HEDGED']: #\n",
    "        ### Concept option looping:\n",
    "        for iter_concept in ['EXP_GDP_rate', 'EXPIMP_GDP_rate', 'NO_CONCEPT', 'FX_BETA']: # ['EXP_GDP_rate']: # dict_ser_concept: # ['EXP_GDP_rate', 'NO_CONCEPT']: #\n",
    "            ### Effective exchange rate type looping:\n",
    "            for iter_eer in ['REER', 'NEER']: # ['REER']: # dict_ser_eer: #\n",
    "                print(f'{iter_factor} / {iter_ret} / {iter_concept} / {iter_eer}')                \n",
    "                ### Iteration data loading:\n",
    "                ser_iter_ret = dict_ser_ret[iter_ret]\n",
    "                ser_iter_concept = dict_ser_concept[iter_concept]\n",
    "                ser_iter_eer = dict_ser_eer[iter_eer]\n",
    "                int_iter_numer = dict_numer_ma_win[iter_factor]\n",
    "                int_iter_denom = dict_denom_ma_win[iter_factor]                \n",
    "                ### Factor calculating and ISONing:\n",
    "                ser_iter_factor = -ser_iter_eer.groupby('Country').transform(lambda ser_country: np.log(ser_country.rolling(int_iter_numer, int_iter_numer // 2).mean() / \\\n",
    "                                                                                                        ser_country.rolling(int_iter_denom, int_iter_denom // 2).mean()))\n",
    "                ser_iter_factor = ser_iter_factor.to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "                ser_iter_factor.name = 'Factor'\n",
    "                ### Returns shifting and ISONing:\n",
    "                ser_iter_ret = ser_iter_ret.groupby('Country').shift(periods = -1).to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "                ### Concept multiplicator ISONing:\n",
    "                ser_iter_concept = ser_iter_concept.to_frame().join(ser_ison, how = 'left').set_index('Market', append = True).squeeze()\n",
    "                ### Regions clearing:\n",
    "                ser_iter_ret = ser_iter_ret.loc[idx_date_range, All, list_ison]\n",
    "                ser_iter_mcap = ser_mcap.loc[idx_date_range, All, list_ison]\n",
    "                ser_iter_factor = ser_iter_factor.loc[idx_date_range, All, list_ison]\n",
    "                ser_iter_concept = ser_iter_concept.loc[idx_date_range, All, list_ison]\n",
    "                ### Countries filtering:\n",
    "                ser_iter_ret = ser_iter_ret.drop(list_countries_to_exclude, level = 'Country')\n",
    "                ser_iter_mcap = ser_iter_mcap.drop(list_countries_to_exclude, level = 'Country')\n",
    "                ser_iter_factor = ser_iter_factor.drop(list_countries_to_exclude, level = 'Country') \n",
    "                ser_iter_concept = ser_iter_concept.drop(list_countries_to_exclude, level = 'Country') \n",
    "                ### Factor and Multiplicator standratizing (Multiplicator shifting), multiplying and restandartizing:\n",
    "                ser_iter_factor_std = single_factor_standartize(ser_iter_factor, list_truncate, within_market = bool_within_market)\n",
    "                if (iter_concept != 'NO_CONCEPT'):\n",
    "                    ser_iter_concept_std = single_factor_standartize(ser_iter_concept, list_truncate, within_market = bool_within_market)\n",
    "                    ser_iter_factor_std = ser_iter_factor_std * (ser_iter_concept_std + int_factor_addendum).abs()\n",
    "                    ser_iter_factor_std = single_factor_standartize(ser_iter_factor_std, list_truncate, within_market = bool_within_market)                    \n",
    "                ### Preliminary results saving:\n",
    "                str_iter_key = '__'.join([iter_factor, iter_ret, iter_concept, iter_eer])\n",
    "                dict_trans_ret_hdf[str_iter_key] = ser_iter_ret\n",
    "                dict_trans_mcap_hdf[str_iter_key] = ser_iter_mcap\n",
    "                dict_trans_factor_hdf[str_iter_key] = ser_iter_factor_std                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN SCRIPT: SAVING TRANSITIONAL RESULTS (NO PRELIMINARY DATA USING)\n",
    "\n",
    "for iter_key in dict_trans_ret_hdf:\n",
    "    dict_trans_ret_hdf[iter_key].to_hdf(str_path_trans_hdf, key = str_key_trans_ret + '__' + iter_key, mode = 'a')\n",
    "    dict_trans_mcap_hdf[iter_key].to_hdf(str_path_trans_hdf, key = str_key_trans_mcap + '__' + iter_key, mode = 'a')\n",
    "    dict_trans_factor_hdf[iter_key].to_hdf(str_path_trans_hdf, key = str_key_trans_factor + '__' + iter_key, mode = 'a')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG_TERM__LOC__EXP_GDP_rate__REER\n",
      "LONG_TERM__LOC__EXP_GDP_rate__NEER\n",
      "LONG_TERM__LOC__EXPIMP_GDP_rate__REER\n",
      "LONG_TERM__LOC__EXPIMP_GDP_rate__NEER\n",
      "LONG_TERM__LOC__NO_CONCEPT__REER\n",
      "LONG_TERM__LOC__NO_CONCEPT__NEER\n",
      "LONG_TERM__LOC__FX_BETA__REER\n",
      "LONG_TERM__LOC__FX_BETA__NEER\n",
      "LONG_TERM__USD__EXP_GDP_rate__REER\n",
      "LONG_TERM__USD__EXP_GDP_rate__NEER\n",
      "LONG_TERM__USD__EXPIMP_GDP_rate__REER\n",
      "LONG_TERM__USD__EXPIMP_GDP_rate__NEER\n",
      "LONG_TERM__USD__NO_CONCEPT__REER\n",
      "LONG_TERM__USD__NO_CONCEPT__NEER\n",
      "LONG_TERM__USD__FX_BETA__REER\n",
      "LONG_TERM__USD__FX_BETA__NEER\n",
      "LONG_TERM__HEDGED__EXP_GDP_rate__REER\n",
      "LONG_TERM__HEDGED__EXP_GDP_rate__NEER\n",
      "LONG_TERM__HEDGED__EXPIMP_GDP_rate__REER\n",
      "LONG_TERM__HEDGED__EXPIMP_GDP_rate__NEER\n",
      "LONG_TERM__HEDGED__NO_CONCEPT__REER\n",
      "LONG_TERM__HEDGED__NO_CONCEPT__NEER\n",
      "LONG_TERM__HEDGED__FX_BETA__REER\n",
      "LONG_TERM__HEDGED__FX_BETA__NEER\n",
      "SHORT_TERM__LOC__EXP_GDP_rate__REER\n",
      "SHORT_TERM__LOC__EXP_GDP_rate__NEER\n",
      "SHORT_TERM__LOC__EXPIMP_GDP_rate__REER\n",
      "SHORT_TERM__LOC__EXPIMP_GDP_rate__NEER\n",
      "SHORT_TERM__LOC__NO_CONCEPT__REER\n",
      "SHORT_TERM__LOC__NO_CONCEPT__NEER\n",
      "SHORT_TERM__LOC__FX_BETA__REER\n",
      "SHORT_TERM__LOC__FX_BETA__NEER\n",
      "SHORT_TERM__USD__EXP_GDP_rate__REER\n",
      "SHORT_TERM__USD__EXP_GDP_rate__NEER\n",
      "SHORT_TERM__USD__EXPIMP_GDP_rate__REER\n",
      "SHORT_TERM__USD__EXPIMP_GDP_rate__NEER\n",
      "SHORT_TERM__USD__NO_CONCEPT__REER\n",
      "SHORT_TERM__USD__NO_CONCEPT__NEER\n",
      "SHORT_TERM__USD__FX_BETA__REER\n",
      "SHORT_TERM__USD__FX_BETA__NEER\n",
      "SHORT_TERM__HEDGED__EXP_GDP_rate__REER\n",
      "SHORT_TERM__HEDGED__EXP_GDP_rate__NEER\n",
      "SHORT_TERM__HEDGED__EXPIMP_GDP_rate__REER\n",
      "SHORT_TERM__HEDGED__EXPIMP_GDP_rate__NEER\n",
      "SHORT_TERM__HEDGED__NO_CONCEPT__REER\n",
      "SHORT_TERM__HEDGED__NO_CONCEPT__NEER\n",
      "SHORT_TERM__HEDGED__FX_BETA__REER\n",
      "SHORT_TERM__HEDGED__FX_BETA__NEER\n"
     ]
    }
   ],
   "source": [
    "### FACTORS PERFORMING (PRELIMINARY DATA USED)\n",
    "\n",
    "### Results container:\n",
    "dict_measure_stats = {}\n",
    "### Factors looping:\n",
    "for iter_factor in ['LONG_TERM', 'SHORT_TERM']: # dict_numer_ma_win: # ['LONG_TERM'] : #\n",
    "    ### Returns type looping:\n",
    "    for iter_ret in dict_ser_ret: # ['HEDGED']: #\n",
    "        ### Concept option looping:\n",
    "        for iter_concept in ['EXP_GDP_rate', 'EXPIMP_GDP_rate', 'NO_CONCEPT', 'FX_BETA']: # ['EXP_GDP_rate']: # dict_ser_concept: # ['EXP_GDP_rate', 'NO_CONCEPT']: #\n",
    "            ### Effective exchange rate type looping:\n",
    "            for iter_eer in ['REER', 'NEER']: # ['REER']: # dict_ser_eer: #\n",
    "                ### Transitional data loading, filtering and concatenating:\n",
    "                str_iter_key = '__'.join([iter_factor, iter_ret, iter_concept, iter_eer])  \n",
    "                print(str_iter_key)\n",
    "                ser_iter_ret = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_ret + '__' + str_iter_key).loc[All, All, list_filter]\n",
    "                ser_iter_mcap = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_mcap + '__' + str_iter_key).loc[All, All, list_filter]\n",
    "                ser_iter_factor = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_factor + '__' + str_iter_key).loc[All, All, list_filter]\n",
    "                ser_iter_factor.name = str_iter_key\n",
    "                ### Measure calculating:\n",
    "                dict_measure_stats[str_iter_key] = multiple_factor_single_efficacy_measure_stats(ser_iter_factor.to_frame(), ser_iter_ret * 100, ser_iter_mcap,\n",
    "                                                                                                 list_measure[0], list_back_period[0], int_horizon)[0]\n",
    "### Results preparing:\n",
    "df_factor_measure_stats = pd.concat(dict_measure_stats).droplevel(0)\n",
    "df_factor_measure_stats.index.name = 'KEY'\n",
    "df_factor_measure_stats.reset_index(inplace = True)\n",
    "df_factor_measure_stats['Factor'] = df_factor_measure_stats['KEY'].str.split('__').str[0]\n",
    "df_factor_measure_stats['Returns'] = df_factor_measure_stats['KEY'].str.split('__').str[1]\n",
    "df_factor_measure_stats['Multiplicator'] = df_factor_measure_stats['KEY'].str.split('__').str[2]\n",
    "df_factor_measure_stats['EER'] = df_factor_measure_stats['KEY'].str.split('__').str[3]\n",
    "df_factor_measure_stats = df_factor_measure_stats.set_index(['Factor', 'Returns', 'Multiplicator', 'EER']).drop('KEY', axis = 1)\n",
    "df_factor_measure_stats.to_hdf(str_path_efficacy_hdf, key = str_key_efficacy, mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FACTORS EXTRACTING (PRELIMINARY DATA USED)\n",
    "\n",
    "df_factor_measure_stats = pd.read_hdf(str_path_efficacy_hdf, key = str_key_efficacy)\n",
    "df_factor_measure_stats.to_excel(str_path_efficacy_xlsx, merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LONG_TERM__HEDGED__EXP_GDP_rate__REER\n"
     ]
    }
   ],
   "source": [
    "### TESTING\n",
    "\n",
    "iter_factor = 'LONG_TERM' # 'SHORT_TERM' #\n",
    "iter_ret = 'HEDGED' # 'LOC' # 'USD' # 'HEDGED' #\n",
    "iter_concept = 'EXP_GDP_rate' # 'NO_CONCEPT' #\n",
    "iter_eer = 'REER' # 'NEER' #\n",
    "### Transitional data loading, filtering and concatenating:\n",
    "str_iter_key = '__'.join([iter_factor, iter_ret, iter_concept, iter_eer])  \n",
    "print(str_iter_key)\n",
    "ser_iter_ret = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_ret + '__' + str_iter_key).loc[All, All, list_filter]\n",
    "ser_iter_mcap = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_mcap + '__' + str_iter_key).loc[All, All, list_filter]\n",
    "ser_iter_factor = pd.read_hdf(str_path_trans_hdf, key = str_key_trans_factor + '__' + str_iter_key).loc[All, All, list_filter]\n",
    "ser_iter_ret.to_excel('Data_Files/Test_Files/Example_EER_Returns.xlsx', merge_cells = False)\n",
    "ser_iter_mcap.to_excel('Data_Files/Test_Files/Example_EER_Market_Caps.xlsx', merge_cells = False)\n",
    "ser_iter_factor.to_excel('Data_Files/Test_Files/Example_EER_Factor.xlsx', merge_cells = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEMP\n",
    "\n",
    "ser_iter_factor.name = str_iter_key\n",
    "df_test_res = single_factor_multiple_efficacy_measures(ser_iter_factor, ser_iter_ret * 100, ser_iter_mcap, list_measure)\n",
    "df_test_stats = measure_stats(df_test_res, list_back_period)\n",
    "df_bins_distribution, df_bins_return_mean = special_qtl_stats(ser_iter_factor, ser_iter_ret * 100, num_bins = 4)\n",
    "df_bins_return_mean.to_excel('Data_Files/Test_Files/QTL_debugging.xlsx', merge_cells = False)\n",
    "df_test_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
